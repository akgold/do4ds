[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DevOps for Data Science",
    "section": "",
    "text": "This is the website for the book DevOps for Data Science, currently in draft form.\nIn this book, you’ll learn about DevOps conventions, tools, and practices that can be useful to you as a data scientist. You’ll also learn how to work better with the IT/Admin team at your organization, and even how to do a little server administration of your own if you’re pressed into service.\nThis website is (and always will be) free to use, and is licensed under the Creative Commons Attribution-NonCommercial-NoDerivs 3.0 license. If you’d like a physical copy of the book, they will be available once it’s finished!\n\n\nI used the knitr package [@xie2015] and the quarto package [@quarto] to compile my book.\nPackage names are in bold text (e.g., rmarkdown), and inline code and filenames are formatted in a typewriter font (e.g., knitr::knit('foo.Rmd')). Function names are followed by parentheses (e.g., bookdown::render_book()).\n\n\n\nAlex Gold leads the Solutions Engineering team at RStudio.\nHe works with customers of RStudio’s professional software to help them deploy, configure, and use RStudio’s professional software and open source tooling in R and Python.\nIn his free time, he enjoys landscaping, handstands, and Tai Chi.\nHe occasionally blogs about data, management, and leadership at alexkgold.space.\n\n\n\nA lot of people are helping me write this book.\nThis book is published to the web using GitHub Actions from rOpenSci.\n\n\n\nTea Green: CAFFDO Steel Blue: 3E7CB1 Kombu Green: 273c2c Bright Maroon: B33951 Sandy Brown: FCAA67"
  },
  {
    "objectID": "chapters/intro.html",
    "href": "chapters/intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Data science itself is pretty useless.\nIt’s likely you became a data scientist because you love creating beautiful charts, minimizing model prediction error, or writing elegant code in R or Python.\nUltimately – and perhaps frustratingly – these things don’t matter. What matters is whether the output of your work is useful in affecting decisions at your organization or in the broader world.\nThat means, you’re going to have to share your work by putting it in production. Many data scientists think of in production as some mythical state of super computers running ultra-complex machine learning models running over dozens of shards of data, terrabytes each.\nIt definitely occurs on a misty mountaintop, and does not involve the google sheets, csv files, or half-baked database queries you probably wrangle every day.\nBut that’s wrong. If you’re a data scientist and you’re trying to put your work in front of someone else’s eyes, you’re in production. And if you’re in production, this book is for you.\nYou may sensibly be asking who I am to make such a proclimation.\nAs of this writing, I’ve been on the Solutions Engineering team at RStudio (soon to be Posit) for nearly four years. The Solutions Engineering team at RStudio helps users of our open source and professional tools understand how to deploy, install, configure, and use RStudio’s Professional Products.\nAs such, I’ve spoken with hundreds of organizations managing data science in production about what being in production means for them, and how to make their production systems for developing and sharing data science products more robust – both with RStudio’s Professional Products and using purely open source tooling.\nFor some orgaizations, in production means a report that will get emailed around at the end of each week. For others, it will mean hosting a live app or dashboard that people visit. For others, it means serving live predictions to another service from a machine learning model.\nRegardless of the actual character of the data science products, organizations are universally concerned with making these assets reliable, reproducible, and performant (enough).\nAt RStudio, the Solutions Engineering team is most directly responsible for engaging with the IT/Admin organizations at our customers. So that’s what this book is about – all of the stuff that is not data science that it takes to deploy a data science asset into production."
  },
  {
    "objectID": "chapters/intro.html#a-short-history-of-devops",
    "href": "chapters/intro.html#a-short-history-of-devops",
    "title": "Introduction",
    "section": "A short history of DevOps",
    "text": "A short history of DevOps\nHere’s the one sentence definition: DevOps is a set of cultural norms, practices, and supporting tooling to help make the process of developing and deploying software smoother and lower risk.\nIf you feel like that definition is pretty vague and unhelpful, you’re right. Like Agile software development, to which it is closely related, DevOps is a squishy concept. That’s partially because DevOps isn’t just one thing – it’s the application of some principles and process ideas to whatever context you’re actually working in. That malleability is one of the great strenths of DevOps, but also makes the concept quite squishy.\nThis squishiness is furthered by the ecosystem of companies enabling DevOps. There are dozens and dozens of companies prostelytizing their own particular flavor of DevOps – one that (curiously) reflects the capabilities of whatever product they’re selling.\nBut underneath the industry hype and the marketing jargon, there are some extremely valuable lessons to take from the field.\nTo understand better, let’s go back to the birth of the field.\nThe Manifesto for Agile Software Development was originally published in 2001. Throughout the 1990s, software developers had begun observing that delivering software in small units, quickly collecting feedback, and iterating was an effective model. After that point, many different frameworks of actual working patterns were developed and popularized.\nHowever, many of these frameworks were really focused on software development. What happened once the software was written?\nHistorically, IT Administrators managed the servers, networking, and workstations needed to deploy, release, and operate that software. So, when an application was complete (or perceived as such), it was hurled over the wall from Development to Operations. They’d figure out the hardware and networking requirements, check that it was performant enough, and get it going in the real world.\nNeedless to say, this pattern is very fragile and subject to many errors and it quickly became apparent that the Agile process of creating and getting feedback on small iterative changes to working software needed a complementary process to get that software deployed and into production.\nDevOps arose as this discipline – a way for software developers and the administrators of operational software to better collaborate on making sure the software being written was making it reliably and quickly into production. It took a little while for the field to be formalized, and the term DevOps came into common usage around 2010."
  },
  {
    "objectID": "chapters/intro.html#those-who-do-devops",
    "href": "chapters/intro.html#those-who-do-devops",
    "title": "Introduction",
    "section": "Those who do DevOps",
    "text": "Those who do DevOps\nThroughout this book, I’ll use two different terms – and though they may sound similar, I mean very different things by them.\nDevOps refers to the knowledge, practices, and tools that make it easier, safer, and faster to put work into production. So if you’re a software developer (and as a data scientist, you are) you need to be thinking about DevOps.\nMost organizations also have a set of people and roles who have the permission and responsibility for managing the servers and computers at your organization. I’m going to refer to this group as IT/Admins. Their names vary widely by organization – they might be named Information Technology (IT), SysAdmin, Site Reliability Engineering (SRE), or DevOps.1\nDepending on what you’re trying to accomplish, the relevant IT/Admins may change. For example, if you’re trying to get access to a particular database, the relevant IT/Admins may be a completely different group of people than if you’re trying to procure a new server.\nFundamentally, DevOps is about creating good patterns for people to collaborate on developing and deploying software. As a data scientist, you’re on the Dev side of the house, and so a huge part of making DevOps work at your organization is about finding some Ops counterparts with whom you can develop a successful collaboration. There are many different organizational structures that support collaboration between data scientists and IT/Admins.\nHowever, I will point out three patterns that are almost always red flags – mostly because they make it hard to develop relationships that can sustain the kind of collaboration DevOps neccesitates. If you find yourself in these situations, you’re not doomed – you can still get things done. But progress is likely to be slow.\n\nAt some very large organizations, IT/Admin functions that are split into small atomic units like security, databases, networking, storage, procurement, cloud, and more. This is useful for keeping the scope-of-work manageable for the people in that group (and often results in super deep expertise within the group), but also means that you’ll need to bring people together from disparate teams to actually get anything done. And even when you find the person who can help you with one task, they’re probably not the right person to help you with anything else and they may not even know who is.\nSome organizations have chosen to outsource their IT/Admin functions. This isn’t a problem per-se – the people who work for outsourced IT/Admin companies are often very competent, but it does indicate a lack of commitment to the Ops half of DevOps internally. The main issues in this case tend to be logistical. Many outsourced IT/Admin teams are often in India, so it can be hard to find meeting times with American and European teams. Additionally, and I’m not quite sure why, but turnover on projects and systems tends to be very high among outsourced IT/Admin organizations. That means it can be really hard to find anyone who’s an expert on a particular system – or to be able to go back to them once you’ve found them.\nAt some very small organizations, there isn’t yet an IT/Admin function. And at others, the IT/Admins are preoccupied with other tasks and don’t have the capacity to help the data science team.This isn’t a tragedy, but it probably means you’re about to become your own IT/Admin. Luckily, you’ve picked up this book, so you’re in the right place.\n\nWhether your organization has a IT/Admin setup that facilitates DevOps best practices or not, hopefully this book can help you take the first steps towards making your path to production smoother and simpler."
  },
  {
    "objectID": "chapters/intro.html#whats-in-this-book",
    "href": "chapters/intro.html#whats-in-this-book",
    "title": "Introduction",
    "section": "What’s in this book?",
    "text": "What’s in this book?\nMy hope for this book is twofold.\nFirst, I’d like to share some patterns.\nDevOps is a well-developed field of its own right. However, a simple 1-1 transposition of DevOps practices would be a mistake. Over the course of engaging with so many organizations at RStudio, I’ve observed some particular patterns, borrowed from DevOps, that work particularly well to grease the path to production for data scientists.\nHopefully, by the time you’re done with this book, you’ll have a pretty good mental model of some patterns and principles you can apply in your own work to make deployment more reliable. That’s what’s in the first section of this book.\nSecond, I want to equip you with some technical knowledge.\nIT administration is an older field that DevOps or data science, full of arcane language and technologies. My hope in this book is to equip you with the vocabulary to talk to the IT/Admins at your organization and the (beginning of) skills you’ll need if it turns out that you need to DIY a lot of what you’re doing.\nThe second section is designed for data scientists who have to interact with IT/Admins. We’ll get into things that you probably shouldn’t manage yourself, but where it’s helpful to have a working knowledge of what particular technologies are and how they work. My hope is that reading this section will help you plan ahead for how you want to work with the IT/Admins at your organization, including the questions you’ll need to ask along the way.\nThe final section, is the hands-on section for anyone who’s administering data science for themselves. This section is designed to equip you with some language and tools to get started administering data science in production."
  },
  {
    "objectID": "chapters/sec1/sec-intro.html",
    "href": "chapters/sec1/sec-intro.html",
    "title": "DevOps Lessons for Data Science",
    "section": "",
    "text": "DevOps Is a set of processes, tools, and cultural norms designed to simplify, speed, and lower the risk of putting software into production.\nThe term DevOps is a portmanteau reflecting the two halves of software delivery it is meant to bring closer together – development and operations. DevOps is a somewhat slippery concept as it’s not a specific dogma or set of tools. Instead, it’s the application of principles and norms – combined with tooling – to the particular situation you face in delivering software.\nAnd for you, the particular situation you face is delivering data science assets. Delivering data science fundamentally is a form of software development. Whether you consciously acknowledge it or not, delivering a data science asset is the same as delivering software in many important ways."
  },
  {
    "objectID": "chapters/sec1/sec-intro.html#the-problems-devops-solves",
    "href": "chapters/sec1/sec-intro.html#the-problems-devops-solves",
    "title": "DevOps Lessons for Data Science",
    "section": "The Problems DevOps Solves",
    "text": "The Problems DevOps Solves\nDevOps started as an offshoot of the Agile software movement. In particular, Agile’s focus on quick iteration via frequent delivery of small chunks and immediate feedback proved completely incompatible with a pattern where developers completed software and hurled it over an organizational the wall to somehow be put into production by an IT/Admin team.\nThere are a few particular problems that DevOps attempted to solve – problems that will probably feel familiar if you’ve ever tried to put a data science asset into production.\nThe first issue DevOps addresses is the “works on my machine” phenomenon. If you’ve ever collaborated on a piece of data science code, you’ve almost certainly gotten an email, instant message, or quick shout that some code that was working great for you is now failing that your colleague is trying to work on it to collaborate.\nThe processes and tooling of DevOps is designed to link application much more closely to environment in order to prevent the “works on my machine” phenomenon from rearing its head.\nThe second problem DevOps addresses is the “breaks on deployment” issue. Perhaps you wrote some code and tested it lovingly on your machine, but didn’t have the chance to test it against a production configuration. Or perhaps you don’t really have patterns around testing code in your organization. Even if you tested thoroughly, you might not know if something breaks when its deployed. DevOps is designed to reduce the risk of deploying code that won’t function as intended the first time it’s deployed.\nDevOps is designed to incorporate ideas about scaling into the genesis of software, helping avoid software that works fine locally, but can’t be deployed for real."
  },
  {
    "objectID": "chapters/sec1/sec-intro.html#core-principles-and-best-practices-of-devops",
    "href": "chapters/sec1/sec-intro.html#core-principles-and-best-practices-of-devops",
    "title": "DevOps Lessons for Data Science",
    "section": "Core principles and best practices of DevOps",
    "text": "Core principles and best practices of DevOps\nAs I’ve mentioned, the term DevOps is squishy. So squishy that there isn’t even agreeement on what the basic tenets of DevOps are that help solve the problems its attempting to solve. Basically every resource on DevOps lists a different set of core principles and frameworks.\nAnd the profusion of xOps like DataOps, MLOps, and more just add confusion about what DevOps itself actually is.\nI’m going to name five core tenets of DevOps. Some lists of DevOps have more components, and some fewer, but this is a good-faith attempt to summarize what I believe the core components are.\n\nCode should be well-tested and tests should be automated.\nUpdates should be frequent and low-risk.\nSecurity concerns should be considered up front as part of architecture.\nProduction systems should have monitoring and logging.\nFrequent opportunities for review, change, and updating should be built into the system – both culturally and technically.\n\nThese five tenets are a great philosophical stance, they’re about things that should happen, and they seem pretty inarguably good."
  },
  {
    "objectID": "chapters/sec1/sec-intro.html#applying-devops-to-data-science",
    "href": "chapters/sec1/sec-intro.html#applying-devops-to-data-science",
    "title": "DevOps Lessons for Data Science",
    "section": "Applying DevOps to data science",
    "text": "Applying DevOps to data science\nHopefully, you’re convinced that the principles of DevOps are relevant to you as a data scientist and you’re excited to learn more!\nHowever, it would be inappropriate to just take the DevOps principles and practices and apply them to data science.\nAs a data scientist, the huge majority of what you’re doing is taking data generated by a business process, deriving some sort of signal from that data flow, and making it available to other people or other software. Fundamentally, data science apps are consumers of data, almost by definition.\nIn contrast, most traditional software either don’t involve meaningful data flows, or are producers of business data. An online store, software for managing inventory, and electronic health record – these tools all produce data.\nThere’s a major architectural and process implication from this difference – how much freedom you have. Software engineers get to dream up data structures and data flows from scratch, designing them to work optimally for their systems. In contrast, you are stuck with the way the data flows into your system – most likely designed by someone who wasn’t thinking about the needs of data science at all.\n\n\n\n\n\n\nLanguage-specific tooling\n\n\n\nThere’s one other important difference between data science and general purpose software development. As of the writing of this book, a huge majority of data science work is done in just two programming languages, R and Python (and SQL). For that reason, this book on DevOps for Data Science can get much deeper into the particularities of applying DevOps principles to those specific languages than a general purpose book on DevOps ever would.\n\n\nSo while the problems DevOps attempts to solve will probably resonate with most data scientists, and the core principles seem equally applicable, the technical best practices need some translation.\nSo here are four technical best practices from DevOps and their equivalents in the data science world.1\n\nUse CI/CD\nContinuous Integration/Continuous Delivery/Continuous Deployment (CI/CD) is the notion that there should be a central repository of code where changes are merged. Once these changes are merged, the code should be tested, built, and delivered/deployed in an automated way.\nThe data science analog of CI/CD is code promotion and integration processes. This chapter will help you think about how to structure your app or report so that you can feel secure moving an app into production and updating it later. This chapter will also include an introduction to real CI/CD tools, so that you can get started using them in your own work.\n\n\nInfrastructure as Code\nThe underlying infrastructure for development and deployment should be reproducible using code so it can be updated and replaced with minimal fuss or disruption.\nThe data science analog is thinking about managing environments as code. This chapter will help you think about how to create a reproducible and secure project-level data science environment so you can be confident it can be used, secured, and resurrected later (or somewhere else) as need be.\n\n\nMicroservices\nAny large application should be decomposed into smaller services that are as atomic and lightweight as possible. This makes large projects easier to reason about and makes interfaces between components clearer so changes and updates are safer.\nI believe this technical best practice has the furthest translation to apply to data science, so this chapter is about how to think of your data science project in terms of its components.This chapter will help you think about what the various components of your projects are and how to split them up for painless and simple updating and atom-izing.\n\n\nMonitoring and Logging\nApplication metrics and logs are essential for understanding the usage and performance of production services, and should be leveraged as much as possible to have a holistic picture at all times.\nThe fourth chapter in this section is on monitoring and logging, which is – honestly – in its infancy in the data science world, but deserves more love and attention.\n\n\nOther Things\nMost DevOps frameworks also include communication, collaboration, and review practices as part of their framework, as the technical best practices of DevOps exist to support the work of the people who use them. This is obviously equally important in the data science world – it’s what the entire second section is about.\nAnd in the fifth chapter, we’ll learn about Docker – a tool that has become so common in DevOps practices that it deserves some discussion all on its own. In this section, you’ll get a general intro to what Docker is and how it works – as well as a hands-on intro to using Docker yourself."
  },
  {
    "objectID": "chapters/sec1/code-promotion.html",
    "href": "chapters/sec1/code-promotion.html",
    "title": "1  Code promotion and integration",
    "section": "",
    "text": "If you’ve ever taken an app or report into production, you know the moment. You get a call, a text, a slack – “the app is down”. A bead of cold sweat runs down your back. Your world narrows to you and your agonizingly nonfunctional app that the CEO, of course, needs right now.\nBasically all of DevOps is designed around preventing this moment.\nPromotion workflows are the core of DevOps practices.\nIn this chapter in particular, we’ll be thinking about deployments – the moment something goes into production, whether that’s a new app entirely or an update to an existing app. You want this moment to be as smooth as possible. Thus, you want to ensure that deployments don’t happen unless they’re supposed to and that when a deployment happens, it is as seamless as possible, with minimal downtime in the live production system.\nIn this chapter, we’ll explore how to set up multiple environments so you can safely develop and test your app before going to production and how to design and execute a promotion workflow that makes your deployment completely seamless."
  },
  {
    "objectID": "chapters/sec1/code-promotion.html#the-three-environments",
    "href": "chapters/sec1/code-promotion.html#the-three-environments",
    "title": "1  Code promotion and integration",
    "section": "1.1 The Three Environments",
    "text": "1.1 The Three Environments\nThe best way to ensure that things only get deployed when you mean them to – and sleep securely knowing that it won’t be disturbed – is to have standalone environments for development, testing, and production. Having separate environments and a plan for promoting content from one to the other is the core of workflows that de-risk pushing to production.\nUsually, these environments are referred to as Dev, Test, and Prod.\nTODO: Picture of asset promotion Dev/Test/Prod\nThe best way to ensure that deployments go smoothly is to make them as minimal and predictable as possible. This requires that the dev, test, and prod environments be very close mirrors of each other. We’ll get into how to accomplish that at the end of this chapter.\n\n1.1.1 Dev for Data Science\nDev is a sandbox where people can install packages and try new things with no risk of breaking anything in production.\nIf you’re a data scientist, you probably have a really good idea what your Dev environment looks like. It’s probably a Jupyter Notebook or an IDE with data science capabilities like RStudio, Spyder, or PyCharm. You use this environment to do exploratory data analysis, try out new types of charts and graphs, and test model performance against new features that you might design. This is really different than what most IT/Admins imagine is happening in Dev.\nFor a pure software engineering project, the Dev environment isn’t about exploration and experimentation – it’s about building. The relationship between data science and software engineering is akin to the difference between archaeology and architecture. Data science is about exploring existing relationships and sharing them with others once they’ve been discovered. The path is often meandering – and it’s usually not clear whether it’s even a possible one when you start. In contrast, pure software engineering is like designing a building. You know from the beginning that you’re designing a building for a particular purpose – you might need a little exploration to ensure you’ve thought through all of the nooks and crannies – or that you’ve chosen the right materials or that you’re going to stay on budget, but you’ll have a pretty good idea up front whether it’s possible to design the building you want.\nThis means that Dev environments look really different for a data scientist vs a software engineer.\nThe biggest difference is that most IT/Admins are going to think of Dev, Test, and Prod being three identical copies of the same environment. That’s close to what you need as a data scientist, but really it’s more like you need a sandbox, a test environment, and prod. That means that if you’re using a deployment platform like RStudio Connect or Dash Enterprise, you probably don’t need it in your Dev environment, and that you don’t need your development tool in Test or Prod (any changes should go back through the deployment pipeline).\n\n\n1.1.2 Test and Prod\nTest is (shockingly) an environment for testing. Depending on the type of asset you’re developing, the test environment might incorporate testing that you do, testing by outside entities like security, and/or performance testing. Generally, the test environment facilitates User Acceptance Testing (UAT) where you can investigate whether labels and buttons are clear or whether plots and graphs meet the need. Depending on your organization, test might be collapsed with dev, it might be a single environment, or it could be multiple for the different types of testing.\nProd is the gold standard environment where things run without any manual human intervention. Usually the only way to get things into prod is through some type of formalized process – sometimes backed by a computer process like a git merge or push from a Continuous Integration/Continuous Deployment (CI/CD) platform (more on that below). One of the most important ways to keep prod stable is that nothing changes in prod other than via a simple promotion from the test environment.\n\n\n1.1.3 Protecting Prod Data\nOne of the biggest risks during the dev and test parts of an assets lifecycle is that you might mess up real data during your work. In a software engineering context, it’s common to use completely fake data or for the app to be the data generation tool.\nIn contrast, data science is all about using and learning from your organization’s actual data. So a dev environment that doesn’t include access to your organization’s real data is going to be completely useless if it doesn’t have real data in it. This is often a difficult thing to convince an IT/Admin of.\nIn many cases, data science assets and reports are read-only so if you’re mostly building visualizations or dashboards that just consume the business data, perhaps clean it for analytics purposes, you can happily accept a read-only connection to your organization’s data.1 In this case, it works just fine to connect to your real data from your Dev and Test environments and create graphs, models, and dashboards based on the real data, testing out new visualizations and model features in a safe sandbox while the existing version of your app or report runs smoothly in prod.\nOn the other hand, if your app or report actually writes data, you’ll have to be a little more clever. In general, you’ll have to figure out how to redirect your apps output into a test data store, or to mock responses from the real services you’re interacting with. The easiest way to do this is by including your output locations as variables inside your code and then setting them at runtime based on an environment variable. See below for an example of how to do this."
  },
  {
    "objectID": "chapters/sec1/code-promotion.html#cicd-and-the-mechanics-of-code-promotion",
    "href": "chapters/sec1/code-promotion.html#cicd-and-the-mechanics-of-code-promotion",
    "title": "1  Code promotion and integration",
    "section": "1.2 CI/CD and the mechanics of code promotion",
    "text": "1.2 CI/CD and the mechanics of code promotion\nThe common term for the mechanics of code promotion is Continuous Integration/Continuous Deployment (CI/CD).2\nIn this book I’m mainly going to focus on a few workflows and a few tools that I’ve found particularly useful in a data science context.\nThe most concrete way that CI/CD practices get expressed is through integrations into source control software – usually git. If you’re not already familiar, I’d suggest spending some time learning git. I will admit that learning git is nontrivial. People who say git is easy are either lying to look smarter or learned so long ago that they have forgotten the early-to-git sense that you’re likely to mess up your entire workflow at any moment.\n\nIf you don’t already know git and want to learn, I’d recommend HappyGitWithR by Jenny Bryan. It’s a great on-ramp to learn git.\nEven if you’re a Python user, the sections on getting started with git, on basic git concepts, and on workflows will be useful since they approach git from a data science perspective.\n\nFor the purposes of this section, I’m going to assume you at least conceptually understand what git branches are and what a merge is – as much of your CI/CD pipeline will be based on what happens when you merge.\nFor production data science assets, I generally recommend long-running dev (or test) and prod branches, with feature branches for developing new things. The way this works is that new features are developed in a feature branch, merged into dev for testing, and then promoted to prod when you’re confident it’s ready.\nFor example, if you had two new plots you were adding to an existing dashboard, your git commit graph might look like this:\n\nCI/CD adds a layer on top of this. CI/CD allows you to integrate functional testing by automatically running those tests whenever you do something in git. These jobs can run when a merge request is made, and are useful for tasks like spellchecking, linting, and running tests.\nFor the purposes of CI/CD, the most interesting jobs are those that do something after there’s a commit or a completed merge, often deploying the relevant asset to its designated location.\nSo a CI/CD integration using the same git graph as above would have released 3 new test versions of the app and 2 new prod versions. Note that in this case, the second test release revealed a bug, which was fixed and tested in the test version of the app before a prod release was completed.\nIn years past, the two most popular CI/CD tools were called Travis and Jenkins. By all accounts, these tools were somewhat unwieldy and difficult to get set up. More recently, GitHub – the foremost git server – released GitHub Actions (GHA), which is CI/CD tooling directly integrated into GitHub that’s free for public repositories and free up to some limits for private ones.\nIt’s safe to say GHA is eating the world of CI/CD.3\nFor example, if you’re reading this book online, it was deployed to the website you’re currently viewing using GHA. I’m not going to get deep into the guts of GHA, but instead talk generally about the pattern for deploying data science assets, and then go through how I set up getting this book to run on GHA.\n\n1.2.1 Using CI/CD to deploy data science assets\nIn general, using a CI/CD tool to deploy a data science asset is pretty straightforward. The mental model to have is that the CI/CD tool stands up a completely empty server for you, and runs some code on it.\nThat means that you’re just doing something simple like spellchecking, you can probably just specify to run spellcheck. If you’re doing something more complicated, like rendering an R Markdown document or Jupyter Notebook and then pushing it to a server, you’ll have to take a few extra steps to be sure the right version of R or Python is on the CI/CD server, that your package environment is properly reproduced, and that you have the right code to render your document.\nFeel free to take a look through the code for the GitHub Action for this book. It’s all YAML, so it’s pretty human-readable.\nHere’s what happens every time I make a push to the main branch of the repository for this book:4\n\nCheckout the current main branch of the book.\nUse the r-lib action to install R.\nUse the r-lib action to setup pandoc (a required system library for R Markdown to work).\nGet the cached renv library for the book.\nRender the book.\nPush the book to GitHub Pages, where this website serves from.\n\nYou’ll see that it uses a mixture of pre-defined actions created for general use, pre-defined actions created by people in the R community, and custom R code I insert to restore an renv library and render the book itself."
  },
  {
    "objectID": "chapters/sec1/code-promotion.html#per-environment-configuration",
    "href": "chapters/sec1/code-promotion.html#per-environment-configuration",
    "title": "1  Code promotion and integration",
    "section": "1.3 Per-Environment Configuration",
    "text": "1.3 Per-Environment Configuration\nSometimes you want a little more flexibility – for example the option to switch many the environment variables depending on the environment.\nIn R, the standard way to do this is using the config package. There are many options for managing runtime configuration in Python, including a package called config.\nFor example, let’s consider this shiny app. In this app, every time I press the button, the app sends a POST request to an external service indicating that the button has been pressed.\n\nlibrary(shiny)\n\n# UI that's just a button\nui <- fluidPage(\n  actionButton(\"button\", \"Press Me!\")\n)\n\n# Do something on button press\nserver <- function(input, output) {\n  observeEvent(\n    input$button, \n    httr::POST(\n      \"www.my-external-system.com\", \n      body = list(button_pressed = TRUE)\n    )\n  )\n}\n\n# Run the application \nshinyApp(ui = ui, server = server)\n\nWith the URL hardcoded like this, it’s really hard to imagine doing this in a Dev or Test environment.\nHowever, with R’s config package, you can create a config.yml file that looks like this:\n\ndev:\n  url: \"www.test-system.com\"\n  \nprod:\n  url: \"www.my-external-system.com\"\n\nThen you can use an environment variable to the correct config and apply that configuration inside the app.5\n\nlibrary(shiny)\nconfig <- config::get()\n\n# UI that's just a button\nui <- fluidPage(\n  actionButton(\"button\", \"Press Me!\")\n)\n\n# Do something on button press\nserver <- function(input, output) {\n  observeEvent(\n    input$button, \n    httr::POST(\n      config$url, \n      body = list(button_pressed = TRUE)\n    )\n  )\n}\n\n# Run the application \nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "chapters/sec1/code-promotion.html#creating-and-maintaining-identical-environments",
    "href": "chapters/sec1/code-promotion.html#creating-and-maintaining-identical-environments",
    "title": "1  Code promotion and integration",
    "section": "1.4 Creating and Maintaining Identical Environments",
    "text": "1.4 Creating and Maintaining Identical Environments\nIn the IT world, there’s a phrase that servers should be cattle, not pets. The idea here is that servers should be unremarkable and that each one should be more-or-less interchangeable. This matters, for example, in making sure your test and prod environments look exactly the same.\nTODO: Notes on using virtual environments + Docker.\nFor example, doing test on a Windows laptop and then going to prod on a Linux server introduces a potential that things that worked in test suddenly don’t when going to prod. For that reason, making all three (or at least test and prod) match as precisely as possible is essential. The need to match these three environments so precisely is one reason for data science workloads moving onto servers.\nA bad pattern then would look like this – I develop an update to an important Shiny or Dash app in my local environment and then move it onto a server. At that point, the app doesn’t quite work and I make a bunch of manual changes to the environment – say adjusting file paths or adding R or Python packages. Those manual changes end up not really being documented anywhere. A week later, when I go to update the app in prod, it breaks on first deploy, because the server state of the test and prod servers drifted out of alignment.\nThe main way to combat this kind of state drift is to religiously use state-maintaining infrastructure as code (IaC) tooling. That means that all changes to the state of your servers ends up in your IaC tooling and no “just login and make it work” shenanigans are allowed in prod.\nTODO: Graphic - fixing problems using IaC tooling\nIf something breaks, you reproduce the error in staging, muck around until it works, update your IaC tooling to fix the broken thing, test that the thing is fixed, and then (and only then) push the updated infrastructure into prod directly from your IaC tooling."
  },
  {
    "objectID": "chapters/sec1/code-promotion.html#exercises",
    "href": "chapters/sec1/code-promotion.html#exercises",
    "title": "1  Code promotion and integration",
    "section": "1.5 Exercises",
    "text": "1.5 Exercises\n\nCreate something and add GHA integration \\[TODO: What to use as example?\\]\nStand up some virtual environments using ___ \\[TODO: Which ones to try?\\]"
  },
  {
    "objectID": "chapters/sec1/env-as-code.html",
    "href": "chapters/sec1/env-as-code.html",
    "title": "2  Environments as Code",
    "section": "",
    "text": "I like to think of doing data science much like producing a delicious meal. In my data, I have the raw ingredients I’ll slice, dice, and recombine to make something great, and my code is the recipe I’ll follow to get there.1\nNovice cooks generally think that their prep is done once they’ve gathered their ingredients and recipes. They’ll grab a tomato and chop/smush it with whatever dull butter knife happens to be at hand. But that’s not how the pros think. A pro probably has a knife just for tomatoes and they frequently – perhaps every time they use it – hone it to razor sharpness so they can slice paper-thin slices off even the mushiest tomato.\nRegardless of your proficiency in the kitchen, you’re a pro (or aiming to be one) at data science. In this chapter, we’re going to talk about the data science equivalent of prepping your knives in your kitchen – actively managing your data science environments using code.\nKitchen metaphors aside, your data science environment is the stack of software and hardware below the level of your code, from the R and Python packages you’re using right down to the physical hardware your code runs on.\nMost data scientists are like novice cooks, and think little – or not at all – about the readiness of their environment and the sharpness of their tools. One of the primary results of this is the oft-spoken, and dreaded phrase, “well, it works on my machine” after attempting to share code with a colleague or deploy an app into a different environment.\nSome people who read this will just throw up their hands and say, “well, it’s impossible to make things completely reproducible, so I’m not going to bother”. And they’re right about the first part. Trying to craft an environment that’s completely reproducible is somewhat of a fool’s errand.\nThere’s a tradeoff. Making things more reproducible is generally takes more work – in a way that’s frustratingly asymptotic.\nSome industries are highly-regulated and need to be able to guarantee they can reproduce an analysis exactly – down to the layer of machine instructions – a decade later. In this world, the general reproducibility practice for an analysis is to take a physical piece of hardware that they know runs the analysis, make a backup or two, and just keep that physical piece of hardware running for many years.\nBut you don’t have to go all the way there – making things 100% reproducible is really, really hard. But making things a little more reproducible is really quite easy.\nThe first step towards making environments more reproducible is by starting to create Environments as Code. In the DevOps world, the aim is to create environments that are largely “stateless” – functionally identical copies of any environment can be created and destroyed at whim using code.\nThe glib, but useful shorthand for the idea of Infrastucture or Environments as code is that they should be “cattle, not pets” – interchangeable one for the other.\nIn this chapter, we’ll get into the why and how of capturing a data science environments in code, saving them for later, and easily moving them around from place to place."
  },
  {
    "objectID": "chapters/sec1/env-as-code.html#environments-have-layers",
    "href": "chapters/sec1/env-as-code.html#environments-have-layers",
    "title": "2  Environments as Code",
    "section": "2.1 Environments have layers",
    "text": "2.1 Environments have layers\nWhen you first start thinking about environments, it can be hard to wrap your head around. The environment seems like a monolith and it can be hard to figure out what the different components are.\nI generally think of three layers in the data science environments – and these are in order, each layer of the environment is actually built on the ones below. Once you understand the layers of an environment, you can think more clearly about what your actual reproducibility needs are, and which environmental layers you need to target putting in code.\n\n\n\n\n\n\n\nLayer\nContents\n\n\n\n\nPackages\nR + Python Packages\n\n\nSystem\nR + Python Language Versions\nOther System Libraries\nOperating System\n\n\nHardware\nVirtual Hardware\nPhysical Hardware\n\n\n\nNote that your code and your data are not the environment – they are what the environment is for. As you’re thinking about reproducibility, I’d encourage you to think about how they fit inside the environment and how they might be reproduced.2 But we’re not going to address them in this book.\nFor most data scientists, the biggest bang for buck is getting the package layer right. In a lot of organization another team entirely will be responsible for the system and hardware layers, but the package layer is always your responsibility as the data scientist. Moreover, managing that layer isn’t terribly hard, and if you get it right, you’ll solve a huge fraction of the “runs on my machine” issues you’re likely to encounter.\n\n2.1.1 Package environments as code\nA successful package Environment as Code setup has two key attributes:\n\nYour package environment is isolated and cannot be disrupted by other activities on the same machine.\nYour package environment can easily be captured and transported elsewhere.\n\nBefore we get to what a good Environment as Code setup looks like, let’s dive into what bad setups look like.\nIn a lot of cases, data scientists have the habit of starting a project, and when they need to install packages, they just run an install.packages command in their console or pip install in their terminal. This works fine for a while. But the problem with this is that the default has you installing things into a cache that’s shared among every project on your system.\nWhat happens if you come back to a project after a year and you’ve been installing things into your machine-wide package cache the whole time. It’s very possible you won’t have the right versions and your code will break.\nThe other problem happens when it comes time to share a project with others. It’s not uncommon to see an intro to an R script that looks something like:\n\n# Check if dplyr installed\nif (!\"dplyr\" %in% row.names(installed.packages())) {\n  # install if not\n  install.packages(\"dplyr\")\n}\n\nACK! Please don’t do this!\nNumber one, this is very rude. If someone runs your code, you’ll be installing packages willy-nilly into their system. Additionally, because this doesn’t specify a version of the {dplyr} package, it doesn’t even really fix the problem!\n\n\n2.1.2 Step 1: Standalone Package Libraries\nAs a data scientist, you’ve very familiar with installing packages from a repository using the install.packages command in R or pip install or conda install in Python. But do you really understand what’s happening when you type that command?\nLet’s first level set on what are the various states for R or Python packages. There are three states packages can be in – and we’re going to go back to our data science as cooking analogy.\nPackages can be stored in a repository, like CRAN or BioConductor in R or PyPI or Conda in Python. You can think of a package in a repository like food at the grocery store – it’s packaged up and ready to go, but inert. Setting aside groovy bodegas with eat-in areas, you don’t get to eat at the grocery store. You’ve got to buy the food and take it home before you can use it – and you’ve got to install the food before you can use it.\nAnd then your library is your pantry, where you keep a private set of packages, bespoke to you and the food you like to cook – the projects you’re likely to do. Loading a package with a library or import command is like taking the food out of the pantry and putting it on the counter so you can actually cook with it.\n[Diagram of package states]\nBy default, when you install an R or Python package, that package installs into user space. That means that it installs into a package library that is specific to your user, but is shared among every time that package is used by you on the machine.\nThis isn’t a disastrous situation, but it is a recipe for package incompatibilities down the road.\n[TODO: diagram of user-level vs project level installs]\nThe most important thing to understand about package libraries is that libraries can only have one version of any given package at a time. So that means that if I have code that relies on version 1.0 of a given package and I install a new version of that package, version 1.0 is gone and I am likely to run into package incompatibility issues.\nIt’s for this reason that you want to have standalone package libraries for each project on your system. Hopefully, you already have good practices around having each project in a standalone directory on your system and making a git repo in that system. Now just make the base directory of that directory a standalone library as well.\n\n\n\n\n\n\nWhat if I have multiple content items?\n\n\n\nIn many data science projects, you’ve got multiple content items within a single project. Maybe you have an ETL script and an API and an app. After a lot of experimenting, my recommendation is to create one git repo for the whole project and have content-level package libraries.\nThis is not a rule or anything…just a suggestion about how I’ve found it works best over time.\n[TODO: Add image]\n\n\n\n2.1.2.1 What’s really happening?\nI happen to think the grocery store metaphor for package management is a useful one, but you might be wondering what the heck is actually happening when you’re using {renv} or {venv}. How does this package magic happen?\nFirst, let’s quickly go over what happens when you install or load a package.\n\n\nWhenever you install a package, there are two key settings that R or Python consult – the URL of the repository to install from and the library to install to. And similarly, when you load an R or Python library, the install checks the library location. In R, the command used is .libPaths() and in python it’s sys.path.\nSo you can see that it’s (conceptually) pretty simple to create a standalone package library for any project – when the virtual environment is activated, just make sure that the project level library is what comes back when checking the library path.\nYou can see it pretty easily in R – if I run .libPaths() before and after activating an {renv} environment, the first entry from the .libPaths() call changes from a user level library /Users/alexkgold to a project level library /Users/alexkgold/Documents/do4ds/.\n\n.libPaths()\n[1] \"/Users/alexkgold/Library/R/x86_64/4.2/library\"                 \n[2] \"/Library/Frameworks/R.framework/Versions/4.2/Resources/library\"\nrenv::activate()\n* Project '~/Documents/do4ds/docker/docker/plumber' loaded. [renv 0.15.5]\n.libPaths()\n[1] \"/Users/alexkgold/Documents/do4ds/docker/docker/plumber/renv/library/R-4.2/x86_64-apple-darwin17.0\"\n[2] \"/Library/Frameworks/R.framework/Versions/4.2/Resources/library\"  \n\nSimilarly in Python, it looks like this – note that the after version replaces the last line of the sys.path with a project-level library:\n\n❯ python3 -m site                                       \nsys.path = [\n    '/Users/alexkgold/Documents/python-examples/dash-app',\n    '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python38.zip',\n    '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8',\n    '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/lib-dynload',\n    '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/site-packages',\n]\nUSER_BASE: '/Users/alexkgold/Library/Python/3.8' (doesn't exist)\nUSER_SITE: '/Users/alexkgold/Library/Python/3.8/lib/python/site-packages' (doesn't exist)\nENABLE_USER_SITE: True\n\n❯ source .venv/bin/activate                       \n(.venv)\n\n❯ python3 -m site\nsys.path = [\n    '/Users/alexkgold/Documents/python-examples/dash-app',\n    '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python38.zip',\n    '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8',\n    '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/lib-dynload',\n    '/Users/alexkgold/Documents/python-examples/dash-app/.venv/lib/python3.8/site-packages',\n]\nUSER_BASE: '/Users/alexkgold/Library/Python/3.8' (doesn't exist)\nUSER_SITE: '/Users/alexkgold/Library/Python/3.8/lib/python/site-packages' (doesn't exist)\nENABLE_USER_SITE: False\n(.venv)\n\n\n\n\n2.1.3 Step 2: Document environment state.\nUsing standalone package libraries for each project ensures that your projects remain undisturbed when you come back to them months or years later and keeps your work reproducible.\nBut it doesn’t solve the sharing problem.\nThat is, you still need some help when it comes time to share an environment with someone else. So how does that work?\n[TODO: image – anatomy of a lockfile]\nBoth R and Python have great utilities that make it easy to capture the current state of a library into a lockfile or requirements.txt and to restore those libraries later or somewhere else.\nIn R, {renv} is the standard on this front. In Python, there are many different options. In the context of production data science, I recommend {virtualenv}/{venv} and related tools.\nNow, when you share your project with someone else, your lockfile or requirements.txt goes along for the ride. Sometimes people are dismayed that their library doesn’t go along as well and that people have to install the packages themselves – but this is by design!\nOne – package libraries can be very small, so putting just a short lockfile or requirements file into git is definitely preferred. The other reason is that the actual package install can differ system to system. For example, if you’re working on a Windows laptop and your colleague is on a Mac, an install of {dplyr} 1.0 means that different files are installed – but with exactly the same functionality. You want to respect this, so instead of sending the whole library along for the ride, you just send the specification that dplyr 1.0 is needed.\n\n\n\n\n\n\nA sidebar on Conda\n\n\n\nMany data scientists love Conda for managing their Python environments.\nConda is a great tool for its main purpose – allowing you to create a data science environment on your local laptop, especially when you don’t have root access to your laptop because it’s a work machine that’s locked down by the admins.\nIn the context of a production environment, Conda smashes together the language version, the package management, and (sometimes) the system library management. This has the benefit of being conceptually simple and easy-to-use. But I’ve often seen it go awry in production environments, and I generally recommend people use a tool that’s just for package management, like {venv}, as opposed to an all-in-one tool like Conda.\n\n\nOn a high level, the workflows for these tasks are similar between R and Python. However, there are some meaningful differences in tooling – especially because virtually every computer arrives with a system version of Python installed, while R is only ever installed by a user trying to do data science tasks. At the end of the day, this actually makes it harder to use Python because you do not want to use your system Python for your data science work…but sometimes it accidentally gets into the mix.\nA general suggestion of workflows for data science package management, whether in R or Python – this should be independently done for every project:\n\n\n2.1.4 Reproducing the rest of the stack\nSometimes, just recording the package environment and moving that around is sufficient. In many cases, old versions of R and Python are retained in the environment, and that’s sufficient.\nThere are times where you need to reproduce elements further down the stack. In some highly-regulated industries, you’ll need to go further down the stack because of requirements for numeric reproducibility. Numeric routines in both R and Python call on system-level libraries, often written in C++ for speed. While it’s unlikely that upgrades to these libraries would cause changes to the numeric results you get, it can happen, and it may be worth maintaining parts of the stack.\nIn other cases, your R or Python library might basically just be a wrapper for system libraries. For example, many popular packages for geospatial analysis are just thin language wrappers that call out to the system libraries. In this case, it might be important to be able to maintain a particular version of the underlying system library to ensure that your code runs at all in the future.\nThere are many tools you can use to record and reproduce the R and Python versions you’re using, the system libraries, and the operating system itself. Many of these fall into the category of Infrastructure-as-Code configuration tools.\nThese days, the clear leader of the pack on this front is Docker. It has become an increasingly popular way to create, maintain, and use standalone environments – and for good reason! In fact, the next chapter is going to be all about the use of Docker in data science. However, it’s worth keeping in mind that if you’re working in the context of a formally-supported IT organization, they may have other tooling they prefer to create and maintain environments, and they can be equally valid."
  },
  {
    "objectID": "chapters/sec1/env-as-code.html#environments-as-code-cheatsheet",
    "href": "chapters/sec1/env-as-code.html#environments-as-code-cheatsheet",
    "title": "2  Environments as Code",
    "section": "2.2 Environments as Code Cheatsheet",
    "text": "2.2 Environments as Code Cheatsheet\n\n2.2.1 Checking your library + repository status\n\n\n2.2.2 Creating and Using a Standalone Project Library\n\n\n\n\n\n\n\n\nStep\nR Command\nPython Command\n\n\n\n\nMake a standalone project directory.\n-\n-\n\n\nMake sure you’ve got {renv}/{venv}.\ninstall.packages(\"renv\")\nIncluded w/ Python 3.5+\n\n\nCreate a standalone library.\nrenv::init()\npython -m venv <dir>\nRecommend: use .venv for <dir>\n\n\nActivate project library.\nrenv::activate()\nHappens automatically if using projects.\nsource <dir>/bin/activate\n\n\nInstall packages as normal.\ninstall.packages(\"<pkg>\")\npython -m pip install <pkg>\n\n\nSnapshot package state.\nrenv::snapshot()\npip freeze > requirements.txt\n\n\nExit project environment.\nLeave R project.\ndeactivate\n\n\n\n\n\n2.2.3 Collaborating on someone else’s project\n\n\n\n\n\n\n\n\nStep\nR Command\nPython Command\n\n\n\n\nDownload project.\n\n\n\n\nMove into project directory.\nsetwd(\"<project-dir>\")\nOr just open R project in RStudio.\ncd <project-dir>\n\n\nCreate project environment.\nrenv::init()\npython -m venv <dir>\nRecommend: use .venv for <dir>\n\n\nEnter project environment.\nHappens automatically.\nsource <dir> /bin/activate\n\n\nRestore packages.\nMay happen automatically or renv::restore()\npip install -r requirements.txt"
  },
  {
    "objectID": "chapters/sec1/proj-components.html",
    "href": "chapters/sec1/proj-components.html",
    "title": "3  Data Science Project Components",
    "section": "",
    "text": "One of the fundamental building blocks of good DevOps practices is microservices. The idea of microservices is that you want to build each application as a collection of smaller, separable components. These components interact via well-defined interfaces so that it’s easy to change the internal workings of each component without messing up the system as whole. It also allows you to make these changes with a much higher degree of confidence that changes to one component can be integrated and deployed without unanticipated downstream consequences.\nYou can think of microservices as the idea of writing functions in your code (you are writing functions, right?), but one level up. Instead of thinking of a single function, you’re thinking at the level of an entire component of your analysis, report, app, or pipeline.\nThe idea of microservices for a general software engineering project is pretty straightforward, you have services that interact with each other over APIs – RESTful ones where possible. Honestly, this isn’t a terrible idea for data scientists, and we’ll get a little bit into what RESTful APIs are in this chapter and how to use them.\nBut a data science app stack is actually much more complicated than a generic three-tier web app architecture.\nIn software engineering, there’s the notion of a three-tier app. It’s helpful to be acquainted with this model as a data scientist, but I find it’s woefully insufficient for data science purposes.\nThe three-tier app architecture is\nIn general, the presentation tier is the front-end, while the other two are the back-end. Usually, the application tier will be one or more APIs and the data tier is some sort of database storage.\nTo think of a simple webapp, say that allows someone to buy a copy of a book online, a user visits the website, picks out a book, puts in their credit card, and clicks a button to purchase.\nIn the application layer, an API receives their order, pushes that order into the “orders to be shipped” queue in the warehouse, and adds the user’s contact info and order history to the database, which is the data tier.\nThe big difference between many software engineering projects and data science is the direction of the data flow. This is what makes it hard to take software engineering workflows and simply adapt them to data science.\nTODO - image of software engineering data flowing front to back, and data science flowing back to front\nIn most software engineering workflows, the user interactions are what create the data. A user interacts with a webpage and creates an order or history that needs to be captured or used somehow. The software engineer, by circumscribing the ways the user interacts with the app actually defines the data flowing through the app.\nIn contrast, most data science apps are designed to give the user a view or a way to interact with an existing bunch of data – and that initial data flow is usually real-world data. It takes work to reshape that data into something useful to the end user – that’s in fact the entire practice of data science.\nSo in data science, the set of microservices you’ll want to consider is pretty broad, and they each look pretty different. I’ve come up with seven reasonably distinct components that make up most data science projects. Some may not have all these components, but most data science processes fit somewhere into these seven services:\nBeyond this taxonomy, I don’t have much to say about each of these layers, except that you should probably have a separate piece of code for each one. Keeping them separate allows you to change and tweak each layer without major implications for the other ones.\nI don’t have anything particularly special to say about data ingestion + refining, except that you should almost certainly keep these processes separate. It’s tempting not to save original data (if you’re respoonsible for ingesting) and to just keep refined data. This is almost always a mistake. I can’t tell you how many times I’ve gone back later and realized that I wanted to update my refining process in a way that relied on still having the source data. Save your source data.\nSimilarly, I have nothing of interest to say about model training, except that this step is by far the most over-hyped part of the data science process. If you’re a young data scientist, you’ll almost surely get more success out of getting really good at any other step of this process rather than focusing on model training. It’s the most crowded and the most automate-able.\nSimilarly, for app operations, there are lots of books on how to write good apps. I think the Shiny framework is great, and Hadley Wickham’s Mastering Shiny book is the go-to reference if you want to…well, you know.\nBUT, I’ve got a lot to say about steps 4 and 5. All too often, I see monolithic Shiny apps of thousands or tens of thousands of lines of code, mixing up business logic and app logic. Or that serve model predictions right inside the app. These apps would almost always be better served by moving the business logic into a standalone API.\nAnd then there’s the question of how to serve your data into the running app. That’s what I’ll spend much of the rest of this chapter on."
  },
  {
    "objectID": "chapters/sec1/proj-components.html#you-should-use-apis-more",
    "href": "chapters/sec1/proj-components.html#you-should-use-apis-more",
    "title": "3  Data Science Project Components",
    "section": "3.1 You should use APIs more",
    "text": "3.1 You should use APIs more\nYou are a data scientist. I don’t know you. I’ve almost certainly never looked at your code. But I can tell you, that in very high likelihood, you should be writing more APIs.\nAPIs can seem intimidating if you’ve never written one before. Writing an API seems like crossing the rubicon between data scientist and software engineer.\nI’m here to tell you that if you know how to write a function in R or Python, you can write an API. For all intents and purposes, an API is just a function that is ready for you to run outside your console.\nWhile the concept of APIs can be extremely intimidating, the actual process of writing an API is no more complicated that writing and documenting a function in R or Python. There are a variety of API frameworks in both R and Python. As of this writing, the ones I’d recommend the most are FastAPI in Python and Plumber in R.\nTo demystify an API, let’s start with a use case.\nLet’s say I’ve got a shiny app that visualizes certain data points from the palmer penguins data set.\nTODO – see if I can deploy to shinyapps and iframe in\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(palmerpenguins)\nlibrary(shiny)\n\n# Define UI\nui <- fluidPage(\n  # Sidebar with a slider input for number of bins \n  sidebarLayout(\n    sidebarPanel(\n      selectInput(\n        inputId = \"species\", \n        label = \"Species\", \n        choices = c(\"Adelie\", \"Chinstrap\", \"Gentoo\"), \n        selected = c(\"Adelie\", \"Chinstrap\", \"Gentoo\"),\n        multiple = TRUE\n      )\n    ),\n    # Show a plot of the generated distribution\n    mainPanel(\n      plotOutput(\"penguinPlot\")\n    )\n  )\n)\n\nserver <- function(input, output) {\n  \n  output$penguinPlot <- renderPlot({\n    # Filter data\n    dat <- palmerpenguins::penguins %>%\n      dplyr::filter(\n        species %in% input$species\n      )\n    \n    # Render Plot\n    dat %>%\n      ggplot(\n        aes(\n          x = flipper_length_mm,\n          y = body_mass_g,\n          color = sex\n        )\n      ) +\n      geom_point()\n  })\n  \n}\n\n# Run the application \nshinyApp(ui = ui, server = server)\n\nThe structure of this Shiny app is bad. Now, it’s not a huge deal, because this is a simple shiny app that’s pretty easy to parse, but I’ve seen many much larger apps with this same structure. Why is this bad? Because all of the app’s logic is contained inside a plotRender-er.\nIn this case, I’ve combined business logic and app logic. As your apps get more complicated, you’ll want to keep the app itself strictly for the logic of the app – what selections has the user made and what needs to be displayed to them as a result.\nThe business logic – what those decisions mean, and the resulting calculations should – at minimum – be moved into standalone functions.\nRewriting this app to make better use of functions, it looks something like this:\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(palmerpenguins)\nlibrary(shiny)\n\n# Define UI\nui <- fluidPage(\n  # Sidebar with a slider input for number of bins \n  sidebarLayout(\n    sidebarPanel(\n      selectInput(\n        inputId = \"species\", \n        label = \"Species\", \n        choices = c(\"Adelie\", \"Chinstrap\", \"Gentoo\"), \n        selected = c(\"Adelie\", \"Chinstrap\", \"Gentoo\"),\n        multiple = TRUE\n      )\n    ),\n    # Show a plot of the generated distribution\n    mainPanel(\n      plotOutput(\"penguinPlot\")\n    )\n  )\n)\n\nfilter_data <- function(species) {\n  palmerpenguins::penguins %>%\n    dplyr::filter(\n      species %in% !!species\n    )\n}\n\nserver <- function(input, output) {\n  \n  # Filter data\n  dat <- reactive(filter_data(input$species))\n  \n  # Render Plot\n  output$penguinPlot <- renderPlot({\n    dat() %>%\n    ggplot(\n      aes(\n        x = flipper_length_mm,\n        y = body_mass_g,\n        color = sex\n      )\n    ) +\n    geom_point()\n  })\n  \n}\n\n# Run the application \nshinyApp(ui = ui, server = server)\n\nNow, I’ve separated the user interaction and visualization layer from the business logic, which is pretty trivial in this case.\nHere’s what a plumber API version of my business logic looks like:\n\nlibrary(plumber)\n\n#* @apiTitle Penguin Explorer\n#* @apiDescription An API for exploring palmer penguins.\n\n#* Get data set based on parameters\n#* @param species which penguin species to include\n#* @get /data\nfunction(species = c(\"Adelie\", \"Chinstrap\", \"Gentoo\")) {\n  palmerpenguins::penguins %>%\n    dplyr::filter(\n      species %in% !!species\n    )\n}\n\nI’ll need to change my function in the app somewhat to actually call the API, but it’s pretty easy.\n\nfilter_data <- function(species) {\n  httr::GET(\n    url = \"http://127.0.0.1:9046\", \n    path = \"data\", \n    query = list(species = species)\n  ) \n}\n\nI can now host this plumber API somewhere, and everyone I’ve allowed to have access can access the data just as easily as the app can. This is a really powerful ability for you."
  },
  {
    "objectID": "chapters/sec1/proj-components.html#separate-updating-data-from-code",
    "href": "chapters/sec1/proj-components.html#separate-updating-data-from-code",
    "title": "3  Data Science Project Components",
    "section": "3.2 Separate updating data from code",
    "text": "3.2 Separate updating data from code\nIn this chapter, we’re going to explore how you decide how to store the data for your app. But first, let’s just talk through what the options are so it makes sense as we talk about why you might choose one over the other in the rest of the chapter.\n\n3.2.1 Storage Format\nThe first question of how to store the data is the storage format. There are really three distinct options for storage format.\nFlat file storage describes writing the data out into a simple file. The canonical example of a flat file is a csv file. However, there are also other formats that may make data storage smaller because of compression, make reads faster, and/or allow you to save arbitrary objects rather than just rectangular data. In R, the rds format is the generic binary format, while pickle is the generic binary format in python.\nFlat files can be moved around just like any other file on your computer. You can put them on your computer, share them through tools like dropbox, google drive, scp, or more.\nThe biggest disadvantage of flat file data storage is twofold – and is related to their indivisibility. In order to use a flat file in R or Python, you’ll need to load it into your R or Python session. For small data files, this isn’t a big deal. But if you’ve got a large file, it can take a long time to read, which you may not want to wait for. Also, if your file has to go over a network, that can be a very slow operation. Or having to load it into an app at startup. Also, there’s generally no way to version data, or just update part, so if you’re saving archival versions, they can take up a lot of space very quickly.\nAt the other extreme end from a flat file format is a database. A database is a standalone server with its own storage, memory, and compute. In general, you’ll recall things from a database using some sort of query language. Most databases you’ll interact with in a data science context are designed around storing rectangular data structures and use Structured Query Language (SQL) to get at the data inside.\nThere are other sorts of databases that store other kinds of objects – you may need these depending on the kind of objects you’re working with. Often the IT/Admin group will have standard databases they work with or use, and you can just piggyback on their decisions. Sometimes you’ll also have choices to make about what database to use, which are beyond the scope of this book.\nThe big advantage of a database is that the data is stored and managed by an independent process. This means that accessing data from your app is often a matter of just connecting to the database, as opposed to having to move files around.\nWorking with databases can also be frought – you usually end up in one of two situations – either the database isn’t really for the data science team, in which case you can probably get read access, but not write. So you’ll be able to use the database as your source of truth, but you won’t be able to write there for intermediate tables and other things you might need. Or you’ll have freedom to set up your own database, in which case you’ll have to own it – and that comes with its own set of headaches.\nThere’s a third option for data storage that is quickly rising in popularity for medium size data. These options are ones that allow you to store data in a flat file, but access it in a smarter way than “just load all the data into memory”. SQLite is a classic on this front that gives you SQL access to what is basically just a flat file. There are also newer entrants into this place that are better from an analytics perspective, like combining Apache Arrow with feather and parquet files and the dask project in Python.\nThese tools can give you the best of both worlds – you get away from the R and Python limitation of having to load all your data into memory, without having to run a separate database server. But you’ll still have to keep track of where the actual files are and make them accessible to your app.\n\n\n3.2.2 Storage Location\nThe second question after what you’re storing is where. If you are using a database, then the answer is easy. The database just lives where it lives, and you’ll need to make sure you have a way to access it – both in terms of network access – as well as making sure you can authenticate into it (more on that below).\nIf you’re not using a database, then you’ll have to decide where to store the data for your app. Most apps that aren’t using a database start off rather naively with the data in the app bundle.\n<TODO: Image of data in app bundle>\nThis works really well during development and is an easy pattern to get started with. The problem is that this pattern generally falls apart when it goes to production. Usually this pattern works fine for a while. Problems start to arise when the data needs updating, and most data needs updating. Usually, you’ll be ready to update the data in the app long before you’re ready to update the app itself.\nAt this point, you’ll be kicking yourself that you now have to update the data inside the app every time you want to want to make a data update. It’s generally a better idea to have the data live outside the app bundle. Then you can update the data without mucking around with the app itself.\nA few options for this include just putting a flat file (or flat with differential read) into a directory near the app bundle. The pins package is also a great option here"
  },
  {
    "objectID": "chapters/sec1/proj-components.html#choosing-your-storage-solution",
    "href": "chapters/sec1/proj-components.html#choosing-your-storage-solution",
    "title": "3  Data Science Project Components",
    "section": "3.3 Choosing your storage solution",
    "text": "3.3 Choosing your storage solution\n\n3.3.1 How frequently are the data updated relative to the code?\nMany data apps have different update requirements for different data in the app.\nFor example, imagine you were the data scientist for a wildlife group that needed a dashboard to track the types of animals that had been spotted by a wilderness wildlife camera. You probably have a table that gives parameters for the animals themselves, perhaps things like endangered status, expected frequency, and more. That table probably needs to be updated very infrequently.\nOn the other hand, the day to day counts of the number of animals spotted probably needs to be updated much more frequently. <TODO: change to hospital example?>\nIf your data is updated only very infrequently, it might make sense to just bundle it up with the app code and update it on a similar cadence to the app itself.\n<TODO: Picture data in app bundle>\nOn the other hand, the more frequently updated data probably doesn’t make sense to update at the same cadence as the app code. You probably want to access that data in some sort of external location, perhaps on a mounted drive outside the app bundle, in a pin or bucket, or in a database.\nIn my experience, you almost never want to actually bundle data into the app. You almost always want to allow for the app data (“state”) to live outside the app and for the app to read it at runtime. Even data that you think will be updated infrequently, is unlikely to be updated as infrequently as your app code. Animals move on and off the endangered list, ingredient substitutions are made, and hospitals open and close and change their names in memoriam of someone.\nIt’s also worth considering whether your app needs a live data connection to do processing, or whether looking up values in a pre-processed table will suffice. The more complex the logic inside your app, the less likely you’ll be able to anticipate what users need, and the more likely you’ll have to do a live lookup.\n\n\n3.3.2 Is your app read-only, or does it have to write?\nMany data apps are read-only. This is nice. If you’re going to allow your app to write, you’ll need to be careful about permissions, protecting from data loss via SQL injection or other things, and have to be careful to check data quality.\nIf you want to save the data, you’ll also need a solution for that. There’s no one-size-fits-all answer here as it really depends on the sort of data you’re using. The main thing to keep in mind is that if you’re using a database, you’ll have to make sure you have write permissions."
  },
  {
    "objectID": "chapters/sec1/proj-components.html#when-does-the-app-fetch-its-data",
    "href": "chapters/sec1/proj-components.html#when-does-the-app-fetch-its-data",
    "title": "3  Data Science Project Components",
    "section": "3.4 When does the app fetch its data?",
    "text": "3.4 When does the app fetch its data?\nAt app open or throughout runtime?\nThe first important question you’ll have to figure out is what the requirements are for the code you’re trying to put into production.\n\n3.4.1 How big are the data in the app?\nWhen I ask this question, people often jump to the size of the raw data they’re using – but that’s often a completely irrelevant metric. You’re starting backwards if you start from the size of the raw data. Instead, you should figure out what’s the size of data you actually need inside the app.\nTo make this a little more concrete, let’s imagine you work for a large retailer and are responsible for creating a dashboard that will allow people to visualize the last week’s worth of sales for a variety of products. With this vague prompt, you could end up needing to load a huge amount of data into your app – or very little at all.\nOne of the most important questions is how much you can cache before someone even opens the app. For example, if you need to\n-> data granularity -> does it even need to be an app, or will a report do?\n\n\n3.4.2 What are the performance requirements for the app?\nOne crucial question for your app is how much wait time is acceptable for people wanting to see the app – and when is that waiting ok? For example, if people need to be able to make selections and see the results in realtime, then you probably need a snappy database, or all the data preloaded into memory when they show up.\nFor some apps, you want the data to be snappy throughout runtime, but it’s ok to have a lengthy startup process (perhaps because it can happen before the user actually arrives) and you want to load a lot of data as the app is starting and do much less throughout the app runtime.=\n\n\n3.4.3 Creating Performant Database Queries\nIf you are using a database, you’ll want to be careful about how you construct your queries to make sure they perform well. The main way to think about this is whether your queries will be eager or lazy.\nIn an eager app, you’ll pull basically all of the data for the app as it starts up, while a lazy app will pull data only as it is need.\n<TODO: Diagram of eager vs lazy data pulling>\nMaking your app eager is usually much simpler – you just read in all the data at the beginning. This is often a good first cut at writing an app, as you’re not sure exactly what requirements your app has. For relatively small datasets, this is often good enough.\nIf it seems like your app is starting up slowly – or your data’s too big to all pull in, you may want to pull data more lazily.\n\n\n\n\n\n\nTip\n\n\n\nBefore you start converting queries to speed up your app, it’s always worthwhile to profile your app and actually check that the data pulling is the slow step. I’ve often been wrong in my intuitions about what the slow step of the app is.\nThere’s nothing more annoying than spending hours refactoring your app to pull data more lazily only to realize that pulling the data was never the slow step to begin with.\n\n\nIt’s also worth considering how to make your queries perform better, regardless of when they occur in your code. Obviously you want to pull the minimum amount of data possible, so making data less granular, pulling in a smaller window of data, or pre-computing summaries is great when possible (though again, it’s worth profiling before you take on a lot of work that might result in minimal performance improvements).\nOnce you’ve decided whether to make your app eager or lazy, you can think about whether to make the query eager or lazy. In most cases, when you’re working with a database, the slowest part of the process is the actual process of pulling the data. That means that it’s generally worth it to be lazy with your query. And if you’re using dplyr from R, being eager vs lazy is simply a matter of where in the chain you put the collect statement.\nSo you’re better off sending a query off to the database, letting the database do a bunch of computations, and pulling a small results set back rather than pulling in a whole data set and doing computations in R.\n\n\n3.4.4 How to connect to databases?\nIn R, there are two answers to how to connect to a database. You can either use a direct connector to connect to the database, this generally will provide a driver to the DBI package. There are other database alternatives, but they’re pretty rare.\n<TODO: image of direct connection vs through driver>\nYou can also use an ODBC/JDBC driver to connect to the database. In this case, you’ll use something inside your R or Python session to use an database driver that has nothing to do with R or Python. Many organizations like these because IT/Admins can configure them on behalf of users and can be agnostic about whether users are using them from R, Python, or something else entirely.\nIf you’re in R, the odbc package gives you a way to interface with ODBC drivers. I’m unaware of a general solution for conencting to odbc drivers in Python.\nA DSN is a particular way to configure an ODBC driver. They are nice because it means that the Admin can fill in the connection details ahead of time, and you don’t need to know any details of the connection, other than your username and password.\n<TODO: image of how DSN works>\nIn R, writing a package that creates database connections for users is also a very popular way to provide database connections to the group."
  },
  {
    "objectID": "chapters/sec1/proj-components.html#how-do-i-do-data-authorization",
    "href": "chapters/sec1/proj-components.html#how-do-i-do-data-authorization",
    "title": "3  Data Science Project Components",
    "section": "3.5 How do I do data authorization?",
    "text": "3.5 How do I do data authorization?\nThis is a question you probably don’t think about much as you’re puttering around inside RStudio or in a Jupyter Notebook. But when you take an app to production, this becomes a crucial question.\nThe best and easiest case here is that everyone who views the app has the same permissions to see the data. In that case, you can just allow the app access to the data, and you can check whether someone is authorized to view the app as a whole, rather than at the data access layer.\nIn some cases, you might need to provide differential data access to different users. Sometimes this can be accomplished in the app itself. For example, if you can identify the user, you can gate access to certain tabs or features of your app. Many popular app hosting options for R and Python data science apps pass the username into the app as an environment variable.\nSometimes you might also have a column in a table that allows you to filter by who’s allowed to view, so you might just be able to filter to allowable rows in your database query.\nSometimes though, you’ll actually have to pass database credentials along to the database, which will do the authorization for you. This is nice, because then all you have to do is pass along the correct credntial, but it’s also a pain because you have to somehow get the credential and send it along with the query.\n<TODO: Image of how a kinit/JWT flow work>\nMost commonly, kerberos tickets or JWTs are used for this task. Usually your options for this depend on the database itself, and the ticket/JWT granting process will likely have to be handled by the database admin.\n\n3.5.1 Securely Managing Credentials\nThe single most important thing you can do to secure your credentials for your outside services is to avoid ever putting credentials in plaintext. The simplest alternative is to do a lookup from environment variables in either R or Python. There are many more secure things you can do, but it’s pretty trivial to put Sys.getenv(\"my_db_password\") into an app rather than actually typing the value. In that case, you’d set the variable in a .Rprofile or .Renviron .\nSimilarly, in Python, you can get and set environment variables using the os module. os.environ['DB_PASSWORD'] = 'my-pass' and os.getenv('DB_PASSWORD'), os.environ.get('DB_PASSWORD') or os.environ('DB_PASSWORD'). If you want to set environment variables from a file, generally people in Python use thedotenv package along with a .env file.\nYou should not commit these files to git, but should manually move them across environments, so they never appear anywhere centrally accessible.\nIn some organizations, this will still not be perceived secure enough, because the credentials are not encrypted at rest. Any of the aforementioned files are just plain text files – so if someone unauthorized were to get access to your machine, they’d be able to grab all of the goodies in your .Rprofile and use them themselves.\nSome hosting software, like RStudio Connect, can take care of this problem, as they store your environment variables inside the software in an encrypted fashion and inject them into the R runtime.\nThere are a number of more secure alternatives – but they generally require a little more work.\nThere are packages in both R and Python called keyring that allow you to use the system keyring to securely store environment variables and recall them at runtime. These can be good in a development environment, but run into trouble in a production environment because they generally rely on a user actually inputting a password for the system keyring.\nOne popular alternative is to use credentials pre-loaded on the system to enable using a ticket or token – often a Kerberos token or a JWT. This is generally quite do-able, but often requires some system-level configuration.\n<TODO: image of kerberos>\nYou may need to enable running as particular Linux users if you don’t want to do all of the authentication interactively in the browser. You usually cannot just recycle login tokens, because they are service authorization tokens, not token-granting tokens.1"
  },
  {
    "objectID": "chapters/sec1/proj-components.html#some-notes-if-youre-administering-the-service",
    "href": "chapters/sec1/proj-components.html#some-notes-if-youre-administering-the-service",
    "title": "3  Data Science Project Components",
    "section": "3.6 Some notes if you’re administering the service",
    "text": "3.6 Some notes if you’re administering the service\nThis chapter has mostly been intended for consumption by app authors who will be creating data science assets. But in some cases, you might also have to administer the service yourself. Here are a few tips and thoughts.\nChoosing which database to use in a given case is very complicated. There are dozens and dozens of different kinds of databases, and many different vendors, all trying to convince you that theirs is superior.\nIf you’re doing certain kinds of bespoke analytics, then it might really matter. In my experience, using postgres is good enough for most things involving rectangular data of moderate size, and a storage bucket is often good enough for things you might be tempted to put in a NoSQL database until the complexity of the data gets very large.\nEither way, a database as a service is a pretty basic cloud service. For example, AWS’s RDS is their simplest database service, and you can get a PostgreSQL, MySQL, MariaDB, or SQL Server database for very reasonable pricing.2 It works well for reasonably sized data loads (up to 64Tb).\nRDS is optimized for individual transactions. In some cases, you might want to consider a full data warehouse. AWS’s is called Redshift and it runs a flavor of PostgreSQL. In general it’s better when you have a lot of data (goes up to several petabytes) or are doing demanding queries a lot more often that you’re querying for individual rows. Redshift is a good bit more expensive, so it’s worth keeping that in mind.\nIf you’re storing data on a share drive, you’ll have to make sure that it’s available in your prod environment. This process is called mounting a drive or volume onto a server. It’s quite a straightforward process, but needing to mount a drive into two different servers places some constraints on where those servers have to be.\n<TODO: picture of mounting a drive>\nWhen you’re working in the cloud, you’ll get compute separate from volumes. This works nicely, because you can get the compute you need and separately choose a volume size that works for you. There are many nice tools around volumes that often include automated backups, and the ability to easily move snapshots from place to place – including just moving the same data onto a larger drive in just a few minutes.\nEBS is AWS’s name for their standard storage volumes. You get one whenever you’ve got an EC2 instance.\nIn some cases, you’ll need to have a drive that’s mounted to multiple machines at once, then you’ll need some sort of network drive.\nThe mounting process works exactly the same, but the underlying technology needs to be a little more complex to accommodate how it works. Depending on whether you’re talking about connecting multiple Linux hosts, you might use NFS (Network File Share), or SMB/CIFS (Windows only), or some combination of the two might use Samba. If you’re getting to this level, it’s probably a good idea to involve a professional IT/Admin.\nhttps://www.varonis.com/blog/cifs-vs-smb\nhttps://www.reddit.com/r/sysadmin/comments/1ei3ht/the_difference_between_nfs_cifs_smb_samba/\nhttps://cloudinfrastructureservices.co.uk/nfs-vs-smb/\nhttps://cloudinfrastructureservices.co.uk/nfs-vs-cifs/"
  },
  {
    "objectID": "chapters/sec1/proj-components.html#exercises",
    "href": "chapters/sec1/proj-components.html#exercises",
    "title": "3  Data Science Project Components",
    "section": "3.7 Exercises",
    "text": "3.7 Exercises\n\nConnect to and use an S3 bucket from the command line and from R/Python.\nStand up a container with Postgres in it, connect using isql, then from R/Python using ODBC/JDBC.\nStand up an EC2 instance w/ EBS volume, change EBS to different instances.\nStand up an NFS instance and mount onto a server."
  },
  {
    "objectID": "chapters/sec1/monitor-log.html",
    "href": "chapters/sec1/monitor-log.html",
    "title": "4  Logging and Monitoring",
    "section": "",
    "text": "Logging and monitoring are the tools used to make the operations of a computer system visible to the outside world. Logging is the process of keeping track of what’s happening inside an app for the purpose of understanding usage or debugging if something goes wrong. Monitoring is the process of checking on the health and performance of a system at a given point in time.\nBoth logging and monitoring have two halves – emitting and aggregating.\nThink, for example, about building a Shiny app. As you’re building your app, you have to think about what are the kinds of events you want to log – perhaps every time someone switches tabs in the app – and put code to do so in your app. Similarly, you’ve got to think about monitoring the app – are you going to save performance metrics for certain operations so you can monitor them over time.\nOn the flip side, there’s the aggregation part of the coin – let’s say your app does a great job emitting logs of all the right things. How will you be able to access those logs from the outside? And if your app is emitting monitoring data, how will you aggregate the monitoring metrics, detect if anything is awry, and alert the right people.\nTODO: diagram of monitoring + logging emission vs aggregation"
  },
  {
    "objectID": "chapters/sec1/monitor-log.html#you-should-do-more-logging",
    "href": "chapters/sec1/monitor-log.html#you-should-do-more-logging",
    "title": "4  Logging and Monitoring",
    "section": "4.1 You should do more logging",
    "text": "4.1 You should do more logging\nLogging is a really essential tool that’s underused in data science apps and assets.\nActually implementing logging is not hard. In fact, as a data scientist, you already do a lot of logging!\nThe language around logging and monitoring seems most relevant in the context of a running app or API. But there are all kinds of other things data scientists do, like create reports and run batch jobs.\nA (mildly) controversial take of mine is that data scientists should never run actual jobs in .R or .py files. Instead, all running analysis code should be in a self-documenting literate programming format like a Jupyter Notebook, R Markdown file, or Quarto document. If you exclusively use literate programming formats and are diligent about saving the rendered output after they run, you’ve already got logs of every time things run!\nTODO: Diagram of functions in .R and .py sourced into notebook.\nSo, what should go into .R and .py files? Function and class definitions. Use these formats to create the objects you’ll manipulate in your actual analysis that you can render and make visible to end users or yourself later.\nAnd if you’re thoughtful about what you note in the notebook, the document itself is the log.\nIf you’re creating an interactive app or API and are adding logging, you should use a formal logging library. For example, python has the standard logging package, and there are a number of great logging packages in R. I’m a particularly big fan of log4r.\n\n4.1.1 What to log\nI’ve generally seen three main purposes for logging in data science apps. The first is to log access operations for when people visit your app. If you aggregate these later, they can be really useful for making a business case about how important your app is. In addition to overall access to the app, logging when people access particular tabs or parts of the app can be really useful.\nIt’s worth noting that some commercial products where you might host your app, like RStudio Connect, create app access logs for you. You’re still responsible for logging anything that happens within your app, but the deployment platform takes care of logging who’s accessing your app and when.\nThe second is for audit trail reasons. In many cases, data science apps are read-only with respect to production data sources. I’d say this is generally a best practice, and if you can avoid writing production data sources from your app, that’s a great thing – create copies of data as needed. But if you must write to production data sources, being able to keep an audit trail of who made changes in the app and what they did is really essential.\nThe last purpose is for debugging reasons. If you’re running a production app, it’s really useful to log what people are doing inside your app. That way if you have an error that occurs, it’ll be much easier to understand what happened immediately before the error. The first step to debugging an error in an app is to be able to reproduce the conditions that caused it, which will be way easier if you implement a good logging system.\nOne important consideration is to make sure that you’re not accidentally logging sensitive information, like API keys to outside data sources in your logs.\n\n4.1.1.1 How to structure logs\nLogging is structured around the criticality of the logged message. For example, if you’re emitting logging from a Shiny app, you probably want to log every time someone changes tabs in the app, and also if something happens that makes the entire app crash – but you want to log them in different ways and be able to only pay attention to the logs you care about.\nFormal logging libraries offer you actual levels of logging, which you can then selectively expose in the logs themselves. While you will have to use whatever levels your logging library exposes, it’s great to have a conceptual model of different logging levels, so even if you’re not using a formal logging framework (for example in a notebook), you have a sense of what you’d want to log and how to do so.\nMost logging libraries have 5-7 levels of logging. The six below are reasonably common. If you understand these, you can condense or expand to match whatever framework you’re actually using. Six levels of logging from least to most granular:\n\nCritical/Fatal - an error so big that the app itself shuts down. For example, if your app cannot run without a connection to an outside service, you might log an inability to connect as a Critical error.\nError – an issue that will make an operation not work, but that won’t bring down your app. In the language of software engineering, you might think of this as a caught exception. An example might be a user submitting invalid input.\nWarning – an unexpected application issue that isn’t fatal. For example, you might include having to retry doing something or noticing that resource usage is high. If something were to go wrong later, these might be helpful breadcrumbs to look at.\nInfo – something normal happened in the app. These record things like starting and stopping, successfully making database and other connections, and configuration options that are used.\nTrace – a record of user interaction. If a user were to run into an issue, a trace should allow you to reconstruct what the user was doing when they ran into the issue and (hopefully) reproduce it.\nDebug – a deep record of what the app was doing. The difference between trace and debug is that the trace would be useful to reconstruct a sequence of events even if you weren’t familiar with the inner workings of the app. On the other hand, the debug log is meant for people who actually understand the app itself. Where a trace log would record “user pressed the button”, the debug might record the actual functions invoked and their arguments.\n\nThese aren’t hard-and-fast rules, but can be useful rules of thumb for how to use logging. Remember, far more important than sticking to any particular logging framework is creating logging that’s useful for you. Almost all frameworks include Info, Warn, Error, and Critical/Fatal. Some have several levels more acute than Error to alert on what should happen as a result. Others (log4r, for example) only include one level more granular than info, so you’ll have to figure out how you want to adjust for that.\n\n\n4.1.1.2 An Example App with Logging\nTODO"
  },
  {
    "objectID": "chapters/sec1/monitor-log.html#do-you-really-need-to-monitor",
    "href": "chapters/sec1/monitor-log.html#do-you-really-need-to-monitor",
    "title": "4  Logging and Monitoring",
    "section": "4.2 Do you really need to monitor?",
    "text": "4.2 Do you really need to monitor?\nMany (probably most) data science assets are used to display the output of an analysis to stakeholders. So the running data science processes are either async scripts that run to output a static report or analysis that stakeholders can access, or they’re apps that are largely used to display data. In many cases, these apps are important, but minute-to-minute access is not and so the effort required to implement good monitoring and logging simply isn’t worth it.\nFor most data science use cases, monitoring is not terribly important. You don’t really care if your ETL job takes a few extra minutes, and the hassle of setting up a monitoring platform generally isn’t worth it if the audience for your app is small and they won’t care too much if it’s down for a few hours.\nThe one exception to this is monitoring machine learning models. In that case, you want to model performance over time to make sure that the model performance isn’t changing too much over time. This is a distinct question from whether speed performance of serving predictions is changing or degrading over time.\nRight now, the field of ML Ops is the next hot thing, and it’s not clear what the industry standards will be for monitoring machine learning model performance over time. My (admittedly biased) take is that I’m excited by the vetiver project, which is an open source framework for deploying machine learning models and monitoring their performance."
  },
  {
    "objectID": "chapters/sec1/monitor-log.html#aggregating-monitoring-and-logging",
    "href": "chapters/sec1/monitor-log.html#aggregating-monitoring-and-logging",
    "title": "4  Logging and Monitoring",
    "section": "4.3 Aggregating Monitoring and Logging",
    "text": "4.3 Aggregating Monitoring and Logging\nTODO\nPrometheus"
  },
  {
    "objectID": "chapters/sec1/docker.html",
    "href": "chapters/sec1/docker.html",
    "title": "5  Docker for Data Science",
    "section": "",
    "text": "If you’re in the data science world, especially the world of “production data science”, you’ve probably heard of Docker – but you might not really be sure what it is or whether you need it.\nThis chapter is designed to clarify what Docker is and how it might help you. We’ll start with a general intro to Docker, proceed with some discussion of data science-relevant Docker workflows, and then dive in to a hands-on lab that will be just the thing to get you started if you’re not sure where to go.\nIt’s worth noting that there are many entire books on how to use Docker in a software engineering context. This chapter is really just meant to give you a jumping off point. If you find that you want to get deep, I’d recommend you pick up another resource after this."
  },
  {
    "objectID": "chapters/sec1/docker.html#containers-are-a-packaging-tool",
    "href": "chapters/sec1/docker.html#containers-are-a-packaging-tool",
    "title": "5  Docker for Data Science",
    "section": "5.1 Containers are a Packaging Tool",
    "text": "5.1 Containers are a Packaging Tool\nLet’s tell a story that might feel familiar. A collaborator sends you a piece of code. You go to run it on your machine and an error pops up Python 3.7 not found. So you spend an hour on Stack Overflow, figuring out how to install version 3.7 of Python.\nThen you try to run the code again, which creates some maps, and this time get an error System library gdal not found. “Augh!” you cry, “Why is there not a way to include all of these dependencies with the code?!?”\nYou have just discovered one of the primary use cases for a container.\nContainers are a way to package up some code with all of its dependencies, making it easy to run the code later, share it with someone else for collaboration, or put it onto a production server – all while being reasonably confident that you won’t ever have to say, “well, it runs on my machine”.\nDocker is by far the most popular open-source containerization platform. So much so that for most purposes container is a synonym for Docker container.1 In this chapter, containers will exclusively refer to Docker containers.\nIn addition to making it easy to get all of the dependencies with an app, Docker also makes it easy to run a bunch of different isolated apps without having them interfere with each other.\nVirtual machines of various sorts have been around since the 1960s, and are still used for many applications. In contrast to a virtual machine, Docker is much more lightweight. Once a container has been downloaded to your machine, it can start up in less than a second.\nThis is why Docker – not the only, or even the first – open source containerization system was the first to hit the mainstream, as much as any esoteric code-development and deployment tool can be said to “hit the mainstream”.\nThis means that – for the most part – anything that can run in a Docker container in one place can be run on another machine with very minimal configuration.\n\n\n\n\n\n\nNote\n\n\n\nThere are exceptions. Until recently, a huge fraction of laptop CPUs were of a particular architecture called x86.\nApple’s recent M1 and M2 chips run on an ARM64 architecture, which had previously been used almost exclusively for phones and tablets. The details aren’t super important, but the upshot is that getting containers working on Apple silicon may not be trivial.\n\n\nDocker doesn’t completely negate the need for other sorts of IT tooling, because you still have to provision the physical hardware somehow, but it does make everything much more self-contained. And if you’ve already got a laptop, you can easily run Docker containers with just a few commands (we’ll get to that below)."
  },
  {
    "objectID": "chapters/sec1/docker.html#containers-for-data-science",
    "href": "chapters/sec1/docker.html#containers-for-data-science",
    "title": "5  Docker for Data Science",
    "section": "5.2 Containers for Data Science",
    "text": "5.2 Containers for Data Science\nIn a data science context, there are two main ways you might use containers – as a way to package a development environment for someone else to use, and as a way to package a finished app for archiving, reproducibility, and production.\n\n\n\n\n\n\nThe Data Science Reproducibility Stack\n\n\n\nA reminder from the reproducibility chapter:\nThe data science reproducibility stack generally includes 6 elements:\n\nCode\nData\nR + Python Packages\nR + Python Versions\nOther System Libraries\nOperating System\n\n\n\nIf you’re running RStudio Server or JupyterHub on a centralized server, Docker can be a great way to maintain that server. In my opinion, maintaining a Docker container is one of the easiest ways to start on an infrastructure-as-code journey.\nWe’re not going to get terribly deep into this use case, as creating the overwhelming majority of the work involved is standard IT/Admin tasks for hosting a server - things like managing networking, authentication and authorization, security, and more.\n\n\n\n\n\n\nNote\n\n\n\nIf you’re thinking about hosting a data science workbench in Docker, you should think carefully about whether you want to deal with standalone containers, or whether you’re really looking for container orchestration using Kubernetes.\n\n\nIn this chapter, I’ll suggest trying to stand up RStudio Server in a container on your desktop, but don’t let the ease fool you. The majority of difficulties with administering a server are the same, even if you put your application stack into a Docker container. Section II of this book will have a lot more on those challenges, and I suggest you check it out if you’re interested.\nInstead, we’re going to stick with talking about how actual data scientists would want to use Docker – to archive and share completed data science assets.\n\nIn this pattern, you’ll put your whole reproducibility stack inside the container itself – perhaps minus your data."
  },
  {
    "objectID": "chapters/sec1/docker.html#container-gotchas",
    "href": "chapters/sec1/docker.html#container-gotchas",
    "title": "5  Docker for Data Science",
    "section": "5.3 Container Gotchas",
    "text": "5.3 Container Gotchas\nDocker containers are great for certain purposes, but there are also some tradeoffs from working inside a Docker container that it’s worth being aware of.\nThe first is the tradeoff of Docker’s strength – a container only gets access to the resources it has specifically been allowed to access.\nThis is a great feature for security and process isolation, but it means you may run into some issues with networking and access to system resources, like your files. You’ll have to develop a reasonably good mental model of the relationship of the Docker container to the rest of your machine in order to be able to develop effectively.\nIt’s worth noting that in some environments – especially highly-regulated ones – a Docker container may not be a sufficient level of reproducibility. Differences between machines at the physical hardware level could potentially mean that numeric solutions could differ across machines, even with the same container. You probably know if you’re in this kind of environment and you have to maintain physical machines.\nThere are also several antipatterns that using a container could facilitate.\nThe biggest reproducibility headache for most data scientists is managing R and Python package environments. While you can just install a bunch of packages into a container, save the container state, and move on, this really isn’t a good solution.\nIf you do this, you’ve got the last state of your environment saved, but it’s not really reproducible. If you come back next year and need to add a new package, you’ll have no way to do it without potentially breaking the whole environment.\nThe obvious solution is to write down the steps for creating your Docker container – in a file called a Dockerfile. Here, it’s tempting to create a Dockerfile that looks like:\n...\nRUN /opt/R/4.1.0/bin/R install.packages(c(\"shiny\", \"dplyr\"))\n...\nBut this is also completely non-reproducible. Whenever you rebuild your container, you’ll install the newest versions of Shiny and Dplyr afresh, potentially ruining the reproducibility of your code. For that reason, the best move is to combine the ability of R and Python-specific libraries for capturing package state – like renv and rig in R and virtualenv , conda , and pyenv in Python are a much better choice than going all in on Docker. More on those topics in the chapter on environments."
  },
  {
    "objectID": "chapters/sec1/docker.html#trying-out-docker",
    "href": "chapters/sec1/docker.html#trying-out-docker",
    "title": "5  Docker for Data Science",
    "section": "5.4 Trying out Docker",
    "text": "5.4 Trying out Docker\nIf you’ve read this far, you have a reasonably good mental model of when you might want to use Docker to encapsulate your data science environment or when you might not. The rest of this chapter will be a hands-on intro to using Docker to run a finished app.\nThere are many great resources out there for really learning Docker. I’d suggest picking up one of them rather than relying on this book to learn everything you need to know about Docker – but hopefully this can get you started.\n\n5.4.1 Prerequisites\nIn order to get started with Docker, you’ll need to know a few things. The first is that you’ll have to actually have Docker installed on an environment you can use. The easiest way to do this is to install Docker Desktop on your laptop, but you can also put Docker on a server environment.\nIn order to follow along, you’ll also need to be able to open and use a terminal, see the section on using the command line if you’re not already comfortable.\n\n\n5.4.2 Getting Started\nLet’s get started with an example that demonstrates the power of Docker right off the bat.\nOnce you’ve got Docker Desktop installed and running, type the following in your terminal:\ndocker run --rm -d \\\n  -p 8000:8000 \\\n  --name palmer-plumber \\\n  alexkgold/plumber\nOnce you type in this command, it’ll take a minute to pull, extract, and start the container.\nOnce the container starts up, you’ll see a long sequence of letters and numbers. Now, navigate to http://localhost:8000/docs/ in your browser (this URL has to be exact!), and you should see the documentation for an R language API that lets you explore the Palmer Penguins data set\nThat was probably pretty uninspiring. It took a long time to download and get started. In order to show the real power of Docker, let’s now kill the container with\ndocker kill palmer-plumber\nYou can check that the container isn’t running by trying to visit that URL again. You’ll get an error.\nLet’s bring the container back up by running the docker run command above again.\nThis time is should be quick – probably less than a second – now that you’ve got the container downloaded. THIS is the power of Docker.\n\nAs you click around, seeing penguin stats and seeing plots, you might notice that nothing is showing up on the command line…but what if I want logs of what people are doing? Or I need to look at the app code?\nYou can get into the container to poke around using the command\ndocker exec -it palmer-plumber /bin/bash\nOnce you’re in, try cat api/plumber.R to look at the code of the running API.\ndocker exec is a general purpose command for executing a command inside a running container. The overwhelming majority of the time I use it, it’s to get a terminal inside a running container so I can poke around.\nYou can spend a lot of time getting deep into why the command works, but just memorizing (or, more likely, repeatedly googling) docker exec -it <container> /bin/bash will get you pretty far.\n\n\n\n\n\n\nNote\n\n\n\nIf you’re used to running things on servers, you might be in the habit of SSH-ing in, poking around, and fixing things that are broken. This isn’t great for a lot of reasons, but it’s a huge anti-pattern in Docker land.\nContainers are stateless and immutable. This means that anything that happens in the container stays in the container – even when the container goes away. If something goes wrong in your running container, you may need to exec in to poke around, but you should fix it by rebuilding and redeploying your image, not by changing the running container.\n\n\nOne nicety of Docker is that it gives you quick access to the most common reason you’d probably exec into the container – looking at logs.\nAfter you’ve clicked around a little in the API, try running:\ndocker logs palmer-plumber\n\nGreat! We’ve played around with this container pretty thoroughly.\nBefore we get into how this all works, let’s try one more example.\nGo back into your terminal and navigate to a directory you can play around in (the cd command is your friend here). Run the following in your terminal:\ndocker run \\\n-v $(pwd):/project-out \\\nalexkgold/batch:0.1\nIt’ll take a minute to download – this container is about 600Mb. You may need to grant the container access to a directory on your machine when it runs. This container will take a few moments to run. If you go to the directory in file browser, you should be able to open hello.html in your web browser – it should be a rendered version of a Jupyter Notebook.\nThis notebook is just a very basic visualization, but you can see how it’s nice to be able to render a Jupyter Notebook locally without having to worry about making sure you had any of the dependencies installed. This is good both for running on demand, and also for archival purposes.\n\nNow that we’ve got Docker working for you, let’s take a step back, explain what we just did, and dive deeper into how this can be helpful.\nHopefully these two examples are exciting – in the first, we got an interactive web API running like a server on our laptop in just a few seconds – and without installing any of the packages or even a version of R locally. In the second, we rendered a Jupyter Notebook using the quarto library – again, without worrying about downloading it locally."
  },
  {
    "objectID": "chapters/sec1/docker.html#container-lifecycle",
    "href": "chapters/sec1/docker.html#container-lifecycle",
    "title": "5  Docker for Data Science",
    "section": "5.5 Container Lifecycle",
    "text": "5.5 Container Lifecycle\nBefore we dig into the nitty-gritty of how that all worked – and how you might change it for your own purposes, let’s spend just a minute clarifying the lifecycle of a Docker container.\nThis image explains the different states a Docker container can be in, and the commands you’ll need to move them around.\n\nA container starts its life as a Dockerfile. A Dockerfile is a set of instructions for how to build a container. Dockerfiles are usually stored in a git repository, just like any other code, and it’s common to build them on push via a CI/CD pipeline.\nA working Dockerfile gets built into a Docker image with the build command. Images are immutable snapshots of the state of the container at a given time.\nIt is possible to interactively build a container as you go and snapshot to create an image, but for the purposes of reproducibility, it’s generally preferable to build the image from a Dockerfile, and adjust the Dockerfile if you need to adjust the image.\nUsually, the image is going to be the thing that you share with other people, as it’s the version of the container that’s compiled and ready to go.\nDocker images can be shared directly like any other file, or via sharing on an image registry via the push and pull commands.\nIf you’re familiar with git, the mental model for Docker is quite similar. There is a public Docker Hub you can use, and it’s also possible to run private image registries. Many organizations make use of the image registries as a service offerings from cloud providers. The big 3’s are Amazon’s Elastic Container Registry (ECR), Azure Container Registry, and Google Container Registry.\nOnce you’ve got an image downloaded locally, you can run it with the run command. Note that you generally don’t have to pull before running a container, as it will auto-pull if it’s not available.\nNow that you’re all excited, let’s dig in on how the docker run command works, and the command line flags we used here, which are the ones you’ll use most often."
  },
  {
    "objectID": "chapters/sec1/docker.html#understanding-docker-run",
    "href": "chapters/sec1/docker.html#understanding-docker-run",
    "title": "5  Docker for Data Science",
    "section": "5.6 Understanding docker run",
    "text": "5.6 Understanding docker run\nAt it’s most basic, all you need to know is that you can run a Docker image locally using the docker run <name> command. However, Docker commands usually use a lot of command line flags – so many that it’s easy to miss what the command actually is.\n\n\n\n\n\n\nNote\n\n\n\nA command line flag is an argument passed to a command line program. If you’re not familiar, it might be helpful to start by checking out that section of the book.\n\n\nLet’s pull apart the two commands we just used – which use the command line flags you’re most likely to need.\n\n5.6.1 Parsing container names\nTo start with, let’s parse the name of the container. In this example, you used two different container names – alexkgold/plumber and alexkgold/batch:0.1. All containers have an id, and they may also have a tag. If you’re using the public DockerHub registry, like I am, container ids are of the form <user>/<name>. This should look very familiar if you already use a git repository.\nIn addition to an id, containers can also have a tag. For example, for the alexkgold/batch image, we specified a version: 0.1. If you don’t specify a tag when pulling or pushing an image, you’ll automatically create or get latest – the newest version of a container that was pushed to the registry.\nUsers often create tags that are relevant to the container – often versions of the software contained within. For example, the rocker/r-ver container, which is a container pre-built with a version of R in it uses tags for the version of R.\nAll these examples use the public DockerHub. Many organizations use a private image registry, in which case you can prefix the container name with the URL of the registry.\n\n\n5.6.2 docker run flags\nIn this section we’re going to go through the docker run flags we used in quite a bit of detail.\n\n\n\n\n\n\nNote\n\n\n\nIf you just want a quick reference later, there’s a cheatsheet in the appendix.\n\n\nLet’s first look at how we ran the container with the plumber API in it.\nFor this container, we used the --rm flag, the -d flag, the -p flag with the argument 8000:8000, and the --name flag with the argument plumber-palmer.\nThe --rm flag removes the container after it finishes running. This is nice when you’re just playing around with a container locally because then you can use the same container name repeatedly, but it’s a flag you’ll almost never use in production because it removes everything from the container, including logs.\nYou can check this by running docker kill palmer-plumber to make sure the container is down and then try to get to the logs with docker logs palmer-plumber. But they don’t exist because they got cleaned up!\nFeel free to try running to container without the --rm flag, playing around, killing the container, and then looking at the logs. Before you’re able to bring back another container with the same name, you’ll have to remove the container with docker rm palmer-plumber.\nThe -d flag instructs the container to run in detached mode so the container won’t block the terminal session. You can feel free to run the container attached – but you’ll have to quit the container by aborting the command from inside the terminal (Ctrl + c), or opening another terminal to docker kill the container.\nThe -p flag publishes a port from inside the container to the host machine. So by specifying -p 8000:8000, we’re taking whatever’s available on the port 8000 inside the container and making it available at the same port on the localhost of the machine that’s hosting the container.\nTODO: picture of ports\nPort forwarding is always specified as <host port>:<container port>. Try playing around with changing the values to make the API available on a different port, perhaps 9876. For a more in-depth treatment of ports, see the section later in the book.\nTODO: link to ports section\nThe --name flag gives our container a name. This is really just a convenience so that you could do commands like docker kill in terms of the container name, rather than the container ID, which will be different for each person who runs the command.\nIn a lot of cases, you won’t bother with a name for the container.\nYou can find container ID using the docker ps command to get the process status. In the case below, I could control the container with the name palmer-plumber, or with the container ID. You can abbreviate container IDs as long as they’re unique – I tend to use the first three characters.\n❯ docker ps                                                         [12:23:13]\n\nCONTAINER ID   IMAGE               COMMAND                  CREATED          STATUS          PORTS                    NAMES\n\n35bd54e44015   alexkgold/plumber   \"R -e 'pr <- plumber…\"   29 seconds ago   Up 28 seconds   0.0.0.0:8000->8000/tcp   palmer-plumber\n\nNow let’s head over to the batch document rendering, where we only used one command line flag -v $(pwd):/project-out, short for volume. To demonstrate what this argument does, navigate to a new directory on your command line and re-run the container without the argument.\nWait…where’d my document go?\nRemember – containers are completely ephemeral. What happens in the container stays in the container. This means that when my document is rendered inside the container, it gets deleted when the container ends its job.\nBut that’s not what I wanted – I wanted to get the output back out of the container.\nThe solution – making data outside the container available to the container and vice-versa – is accomplished by mounting a volume into the container using the -v flag. Like with mounting a port, the syntax is -v <directory outside container>:<directory inside container>.\n\nThis is an essential concept to understand when working with containers. Because containers are so ephemeral, volumes are the way to get anything from your host machine in, and to persist anything that you want to outlast the lifecycle of the container.\nIn this case, we actually used a variable $(pwd), which will be evaluated to the current working directory to be the directory project-out inside the container, so the rendered document can be persisted after the container goes away."
  },
  {
    "objectID": "chapters/sec1/docker.html#build-your-own-with-dockerfiles",
    "href": "chapters/sec1/docker.html#build-your-own-with-dockerfiles",
    "title": "5  Docker for Data Science",
    "section": "5.7 Build your own with Dockerfiles",
    "text": "5.7 Build your own with Dockerfiles\nSo far, we’ve just been working on running containers based on images I’ve already prepared for you. Let’s look at how those images got created so you can try building your own.\nA Dockerfile is just a set of instructions that you use to build a Docker image. If you have a pretty good idea how to accomplish something on a running machine, you shouldn’t have too much trouble building a Dockerfile to do the same as long as you remember two things:\nTODO: Image of build vs run time\n\nThe difference between build time and run time. There are things that should happen at build time – like setting up the versions of R and Python, copying in the code you’ll run, and installing the system requirements. That’s very different from the thing I want to have happen at run time – rendering the notebook or running the API.\nDocker containers only have access to exactly the resources you provide to them at both build and runtime. That means that they won’t have access to libraries or programs unless you give them access, and you also won’t have access to files from your computer unless you make them available.\n\nThere are many different commands you can use inside a Dockerfile, but with just a handful, you’ll be able to build most images you might need.\nHere are the important commands you’ll need for getting everything you need into your images.\n\nFROM – every container starts from a base image. In some cases, like in my Jupyter example, you might start with a bare bones container that’s just the operating system (ubuntu:20.04). In other cases, like in my shiny example, you might start with a container that’s almost there, and all you need to do is to copy in a file or two.\nRUN – run any command as if you were sitting at the command line inside the container. Just remember, if you’re starting from a very basic container, you may need to make a command available before you can run it (like wget in my container below).\nCOPY – copy a file from the host filesystem into the container. Note that the working directory for your Dockerfile will be whatever your working directory is when you run your build command.\n\nOne really nice thing about Docker containers is that they’re build in layers. Each command in the Dockerfile defines a new layer. If you make changes below a given layer in your Dockerfile, rebuilding will be easy, because Docker will only start rebuilding at the layer with changes.\nIf you’re mainly building containers for finished data science assets to be re-run on demand, there’s only one command you need:\n\nCMD - Specifies what command to run inside the container’s shell at runtime. This would be the same command you’d use to run your project from the command line.\n\nIf you do much digging, you’ll probably run into the ENTRYPOINT command, which can take a while to tell apart from CMD. If you’re building containers to run finished data science assets, you shouldn’t need ENTRYPOINT. If you’re building containers to – for example – accept a different asset to run or allow for particular arguments, you’ll need to use ENTRYPOINT to specify the command that will always run and CMD to specify the default arguments to ENTRYPOINT, which can be overridden on the command line.2\nSo here’s the Dockerfile I used to build the container for the Jupyter Notebook rendering. Look through it, can you understand what it’s doing?\n# syntax=docker/dockerfile:1\nFROM ubuntu:20.04\n\n# Copy external files\nRUN mkdir -p /project/out/\n\nCOPY ./requirements.txt /project/\nCOPY ./hello.ipynb /project/\n\n# Install system packages\nRUN apt-get update && apt-get install -y \\\n  wget python3 python3-pip\n\n# Install quarto CLI + clean up\nRUN wget https://github.com/quarto-dev/quarto-cli/releases/download/v0.9.83/quarto-0.9.83-linux-amd64.deb\nRUN dpkg -i ./quarto-0.9.83-linux-amd64.deb\nRUN rm -f ./quarto-0.9.83-linux-amd64.deb\n\n# Install Python requirements\nRUN pip3 install -r /project/requirements.txt\n\n# Render notebook\nCMD cd /project && \\\n  quarto render ./hello.ipynb && \\\n  # Move output to correct directory\n  # Needed because quarto requires relative paths in --output-dir: \n  # https://github.com/quarto-dev/quarto-cli/issues/362\n  rm -rf /project-out/hello_files/ && \\\n  mkdir -p /project-out/hello_files && \\\n  mv ./hello_files/* /project-out/hello_files/ && \\\n  mv ./hello.html /project-out/\nOnce you’ve created your Dockerfile, you build it into an image using docker build -t <image name> and then you can push that to DockerHub or another registry using docker push."
  },
  {
    "objectID": "chapters/sec1/docker.html#exercises",
    "href": "chapters/sec1/docker.html#exercises",
    "title": "5  Docker for Data Science",
    "section": "5.8 Exercises",
    "text": "5.8 Exercises\n\nSee if you can get a copy of RStudio Server running in a container on your desktop using the rocker/rstudio container. Can you access your home directory from RStudio Server?\n\nHint 1: You’ll probably need all of the docker run flags you’ve learned and one more – the -e KEY=value flag provides an environment variable to the running container.\nHint 2: The default port for RStudio Server is 8787.\nHint 3: If you’re running a laptop with Apple Silicon (M1 or M2), you may need to try a different container. amoselb/rstudio-m1 worked for me.3\n\nCan you build your own container to house an interactive app built in Shiny, Dash, Streamlit, or another framework of your choice?"
  },
  {
    "objectID": "chapters/sec2/sec-intro.html",
    "href": "chapters/sec2/sec-intro.html",
    "title": "Working with IT",
    "section": "",
    "text": "In recent years, as data science has become more central to organizations, many have been moving their operations off of individual contributors’ laptops and onto centralized servers. Depending on your organization, the centralization of data science operations can make your life way easier – or it can be kinda a bummer.\nServer migrations can work well regardless of whether they’re instigated by the data science or the IT organization. The biggest determinant is how well the data science and IT/DevOps teams can collaborate.\nData scientists are good at manipulating and using data, but most have little expertise in SysAdmin work, and aren’t really that interested. On the flip side, IT/DevOps organizations usually don’t really understand data science workflows, the data science development process, or how data scientists use R and Python.\nOften, migrations to a server are instigated by the data scientists themselves – usually because they’ve run out of horsepower on their laptops. If you, or one of your teammates, enjoys and is good as SysAdmin work, this can be a great situation! You get the hardware you need for your project quickly and with minimal interference.\nOn the other hand, most data scientists don’t really want to be SysAdmins, and these systems are often fragile, isolated from other corporate systems, and potentially susceptible to security vulnerabilities.\nOther organizations are moving to servers as well, but led by the IT group. For many IT groups, it’s way easier to maintain a centralized server environment, as opposed to helping each data scientist maintain their own environment on their laptop.\nHaving just one platform makes it much easier to give shared access to more powerful computing platforms, to data sources that require some configuration, and to R and Python packages that wrap around system libraries and can be a pain to configure (looking at you, rJava).\nThis can be a great situation for data scientists! If the platform is well-configured and scoped, you can get instant access through their web browser to more compute resources, and don’t have to worry about maintaining local installations of data science tools like R, Python, RStudio, and Jupyter, and you don’t need to worry about how to connect to important data sources – those things are just available for use.\nBut this can also be a bad experience. Long wait times for hardware or software updates, overly restrictive policies – especially around package management – and misunderstandings of what data scientists are trying to do on the platforms can lead to servers going largely unused.\nSo much of whether the server-based experience is good or not depends on the relationship between the data science and IT/Admin group. In organizations where these groups work together smoothly, this can be a huge win for everyone involved. However, there are some organizations where IT/Admins are so concerned with stability and security that they make it impossible to do data science, and the data scientists spend all their time playing cat-and-mouse games to try to get work done behind IT/Admin’s backs.\nIf you work at such a place, it’s frankly hard to get much done on the server. It’s probably worth investing some time into improving your relationship with your favorite person on the IT/Admin team. Hopefully, this book will help you understand a little of what’s on the minds of people in the IT group, and a sense of how to talk to them better.\nAs data science increasingly moves to the cloud, it’s helpful to have specific cloud recommendations. That’s why this book will go pretty deep into how to configure the things we’re discussing inside AWS. You always have the option of using other clouds, and any conceptual information you learn in this book will 100% apply, but trying to provide detailed directions across more than 1 cloud provider is just too difficult."
  },
  {
    "objectID": "chapters/sec2/servers.html",
    "href": "chapters/sec2/servers.html",
    "title": "6  Computers, Computing, and Servers",
    "section": "",
    "text": "Data Science is a delightful mashup of statistics and computer science. While you can be a great data scientists without a deep understanding of computational theory, a mental model of how your computer works is helpful, especially when you head to production.\nIn this chapter, we’ll develop a mental model for how computers work, and explore how well that mental model applies to both the familiar computers in your life, but also more remote servers.\nIf you’re into pedantic nitpicking, you’re going to love this chapter apart, as I’ve grossly oversimplified how computers work. On the other hand, this basic mental model has served me well across hundreds of interactions with data scientists and IT/DevOps professionals.\nAnd by the end of the chapter, we’ll get super practical – giving you a how-to on getting a server of your very own to play with."
  },
  {
    "objectID": "chapters/sec2/servers.html#computers-are-addition-factories",
    "href": "chapters/sec2/servers.html#computers-are-addition-factories",
    "title": "6  Computers, Computing, and Servers",
    "section": "6.1 Computers are addition factories",
    "text": "6.1 Computers are addition factories\nAs a data scientist, the amount of computational theory it’s really helpful to understand in your day-to-day can be summarized in three sentences:\n\nComputers can only add.\nModern ones do so very well and very fast.\nEverything a computer “does” is just adding two (usually very large) numbers, reinterpreted.1\n\nI like to think of computers as factories for doing addition problems.\n\nWe see meaning in typing the word umbrella or jumping Mario over a Chomp Chain and we interpret something from the output of some R code or listening to Carly Rae Jepsen’s newest bop, but to your computer it’s all just addition.\nEvery bit of input you provide your computer is homogenized into addition problems. Once those problems are done, the results are reverted back into something we interpret as meaningful. Obviously the details of that conversion are complicated and important – but for the purposes of understanding what your computer’s doing when you clean some data or run a machine learning model, you don’t have to understand much more than that.\n\n6.1.1 Compute\nThe addition assembly line itself – where the work actually gets done – is referred to as compute. It’s where 2+2 gets turned into 4, and where 345619912 + 182347910 gets turned into 527967822. The heart of the factory in your computer is the central processing unit (CPU).\nThere are two elements to the total speed of your compute – the total number of cores, which you can think of as an individual conveyor belt doing a single problem at a time, and the speed at which each belt is running.\nThese days, most consumer-grade laptops have between 4 and 16 cores, and may have additional capabilities that effectively doubles that number. So most laptop CPUs can do between 4 and 32 simultaneous addition problems.\nIn your computer, the basic measure of conveyor belt speed is single-core “clock speed” in hertz (hz) – operations per second. The cores in your laptop probably run between 2-5 gigahertz (GHz): 2-5 billion operations per second.\n\nA few decades ago, there was a steady march in increases in clock speed – increasing by approximately 10x during the 90s, by 2-3x in the 2000s, and somewhere between not at all and 1.5x in the 2010s. Computers have gotten much faster through this entire time, but mostly from sources other than boosts in single-core clock speeds.\nFor example, in 2020, Apple released a series of Macs with its new M1 chip. Real-world performance of the M1 chip was dramatically better than earlier models, despite modest improvements in clock speeds.\n\n\n6.1.1.1 GPU Computing\nWhile compute usually just refers to the CPU, it’s not completely synonymous. Computers can offload some problems to a graphical processing unit (GPU). GPUs are specialized chips used for tasks like editing photo or videos, rendering video game graphics, some kinds of machine learning, and (yes) Bitcoin mining.\nWhere the CPU has a few fast cores, the GPU takes the opposite approach, with many slower cores. Where a consumer-grade CPU has 4-16 cores, mid-range GPUs have 700-4,000, but each one runs between 1% and 10% the speed of a CPU core.\nFor GPU-centric tasks, the overwhelming parallelism of a GPU is more important than the speed of any individual core, and GPU computation can be dramatically faster. For the purposes of data science, many popular machine learning techniques – including neural networks, XGBoost, and other tree-based models – potentially run much much faster on GPUs relative to CPUs.\n\n\n\n6.1.2 Memory (RAM)\nYour computer’s random access memory (RAM) is its short term storage. Your computer uses RAM to store addition problems it’s going to tackle soon, and results it thinks it might need again in the near future.\nThe benefit of RAM is that it’s very fast to access. The downside is that it’s temporary. When your computer turns off, the RAM gets wiped.2\n\nYou probably know this, but memory and storage is measured in bytes. These days, most consumer hard drives are between 1/2 to a few terrabytes (trillion bytes). Some enterprise data stores run on the scales of thousands of terrabytes (pettabytes) or even thousands of pettabytes (yottabytes).\n\nModern consumer-grade laptops come with somewhere between 4 and 16 Gb of memory.\n\n\n6.1.3 Storage (Hard Drive/Disk)\nYour computer’s storage is like a freezer. It keeps things permanently, whether the factory is operating or not. The tradeoff is that things there are cold, and take some time to warm up before they can be used.\nA few years ago, all hard drives were (HDDs), and were basically the same. HDDs have a bunch of magnetic disks that spin very fast (5,400 and 7,200 RPM are common speeds). Magnetized read/write heads move among the disks and save and read your data.\nIn the last few years, solid-state drives (SSDs) have been taking over. SSDs, which are collections of flash memory chips, are up to 15x faster than HDDs. They also can take a wider variety of shapes and sizes, and are more reliable and durable because they have no moving parts. The main drawback is that they’re usually more expensive per byte, but prices are still quite reasonable.\nMany consumer laptops have only an SSD at this point. Some desktops and high-end laptops combine a smaller SSD with a larger HDD."
  },
  {
    "objectID": "chapters/sec2/servers.html#choosing-the-right-data-science-machine",
    "href": "chapters/sec2/servers.html#choosing-the-right-data-science-machine",
    "title": "6  Computers, Computing, and Servers",
    "section": "6.2 Choosing the right data science machine",
    "text": "6.2 Choosing the right data science machine\nIn my experience as a data scientist and talking to IT/DevOps organizations trying to equip data scientists, the same questions about choosing a computer come up over and over again. Here are the guidelines I often share.\n\n6.2.1 Get as much RAM as feasible\nIn most cases, R and Python have to load all of your data into memory. Thus, the size of the data you can use is limited to the size of your machine’s RAM. Most other limits of your machine will just result in things being slower than you’d really want, but trying to load too much data into memory will result in a session crash, and you won’t be able to do your analysis.\n\nYou can get around the in-memory limitation by using a database or libraries that facilitate on-disk operations like Apache Arrow or dask.\n\nIt’s easy to say that you’ll always want more RAM, but a rough rule of thumb for whether you’ve got enough is the following:\n\nAmount of RAM = max amount of data * 3\n\nBecause you’ll often be doing some sort of transformation that results in invisible data copies and your computer can’t devote all of its memory, you’ll want to leave plenty of room over your actual data size. I’ve never run into problems with a scaling factor of 3, but this is definitely only a rough rule-of-thumb.\n\n\n6.2.2 Go for fewer, faster cores in the CPU\nR and Python are single-threaded. Unless you’re using special libraries for parallel processing, you’ll end up red-lining a single CPU core while the other just look on in silence. Therefore, single core clock speed matters more than the number of cores, and fewer, faster cores are usually preferable to many slower\nIf you’re buying a laptop or desktop, there usually aren’t explicit choices between a few fast cores and more slow cores. Most modern CPUs are pretty darn good, and you should just get one that fits your budget. If you’re standing up a server, you often have an explicit choice between more slower cores and fewer faster ones.\n\n\n6.2.3 Get a GPU…maybe…\nIf you’re doing machine learning that can be improved by GPU-backed operations, you might want a GPU. In general, only highly parallel machine learning problems like training a neural network or tree-based models will benefit from GPU computation.\nOn the other hand, GPUs are expensive, non-machine learning tasks like data processing don’t benefit from GPU computation, and many machine learning tasks are amenable to linear models that run well CPU-only.\n\n\n6.2.4 Get a lot of storage, it’s cheap\nAs for storage – get a lot – but don’t think about it too hard, because it’s cheap. Both a 1TB SSD and a 4TB HDD are around $100. Storage is cheap enough these days that it is almost always more cost efficient to buy more storage rather than making a highly-paid professional spend their time trying to figure out how to move things around.\nOne litmus test of an IT organization that is well-equipped to support data science is whether they understand this. Smart organizations know that just getting more storage is easily worth the cost in terms of the time of admins and data scientists."
  },
  {
    "objectID": "chapters/sec2/servers.html#is-a-server-different",
    "href": "chapters/sec2/servers.html#is-a-server-different",
    "title": "6  Computers, Computing, and Servers",
    "section": "6.3 Is a server different?",
    "text": "6.3 Is a server different?\nNo.\nBut also yes.\nAt its core, a server is exactly the same sort of addition factory as your laptop, and the same mental model of what is happening under the hood will serve you well.\n\nThe big difference is in how the input and output is done. While you interact directly with a computer through keyboard and mouse/touchpad, servers generally don’t have built in graphical interfaces – by default all interaction occurs via command line tools.\nOne of the reasons is that the overwhelming majority of the world’s servers run the Linux operating system, as opposed to the Windows or Mac OS your laptop probably runs.3 There are many different distributions (usually called “distros”) of Linux. For day-to-day enterprise server use, the most common of these are Ubuntu, CentOS, Red Hat Enterprise Linux (RHEL), SUSE Enterprise Linux.\nAlong with most of the world’s servers, almost all of the world’s embedded computers – in ATMs, cars and planes, TVs, and most other gadgets and gizmos – run on Linux. If you have an Android phone or a Chromebook – that’s Linux. Basically all of the world’s supercomputers use Linux."
  },
  {
    "objectID": "chapters/sec2/servers.html#exercises",
    "href": "chapters/sec2/servers.html#exercises",
    "title": "6  Computers, Computing, and Servers",
    "section": "6.4 Exercises",
    "text": "6.4 Exercises\n\nThink about the scenarios below – which part of your computer would you want to upgrade to solve the problem?\n\nYou try to load a big csv file into pandas in Python. It churns for a while and then crashes.\nYou go to build a new ML model on your data. You’d like to re-train the model once a day, but it turns out training this model takes 26 hours on your laptop.\nYou design an visualization Matplotlib , and create a whole bunch in a loop/"
  },
  {
    "objectID": "chapters/sec2/auth.html",
    "href": "chapters/sec2/auth.html",
    "title": "7  Logging in with auth",
    "section": "",
    "text": "Unless you’re a special kind of nerd, you’ve probably never thought hard about auth. But you do it all the time – every time you log into your bank to check your account or open Instagram on your phone or access RStudio via your corporate Okta account, there’s auth happening under the hood.\nAuth is shorthand for two different things – authentication and authorization.\nAuthentication is the process of verifying identites. When I show up at a website that isn’t just open to the internet, how do I prove that I am who I say I am.\nAuthorization is the process of managing and checking permissions. Once you know that it’s me at the front door, am I allowed to come in?\nFor the most part, this chapter is designed to help you talk to the folks who manage auth at your organization. Unless you’ve got a small organization with only a few data scientists, you probably won’t have to manage this yourself. But it can be extremely helpful to understand how auth works when you’re trying to ask for something from the organization’s admins.\nIn order to talk concretely about logging into systems, it’s helpful to clarify some terms. For the most part, these terms are industry standard, but I’m also going to generalize some terms because they’re used for certain types of auth, and they’re really useful.\nFor the purposes of this document, we’re going to be talking about trying to login to a service. A service is something you want to login to – it could be your phone or your email, or a server, a database, or software like RStudio Server or JupyterHub.\nWhen you go to login to a service, there are two things that have to happen. First, you have to assert and verify who you are. This process is called authentication, and the assertion and proof of identity are your credentials or creds. The most common credentials are a username and password, but there are other options including SSH keys, multi-factor authentication, or biometrics.\nOnce you’ve proven who you are, then the system needs to determine what you’re supposed to have access to. This process is called authorization.\nOften, the authentication + authorization process is referred to collectively as auth.\n[#TODO: Image of Auth]\nSo, to summarize when you go to login to a service, the service reaches out to some sort of system to verify your identity. Depending on the method, it may also check your authorization. The name of that system varies by the auth method, but for the purposes of this chapter, we’ll refer to it generally as the identity store. Depending on the auth method, the identity store may store just authentication records, or both authentication and authorization."
  },
  {
    "objectID": "chapters/sec2/auth.html#the-many-flavors-of-auth-or-what-does-sso-mean",
    "href": "chapters/sec2/auth.html#the-many-flavors-of-auth-or-what-does-sso-mean",
    "title": "7  Logging in with auth",
    "section": "7.1 The many flavors of auth (or what does SSO mean?)",
    "text": "7.1 The many flavors of auth (or what does SSO mean?)\nSingle Sign On (SSO) is a slippery term, so it is almost always necessary to clarify what is meant by the term when you hear it. At some organizations, identity management isn’t centralized at all. This means that usernames and passwords are unique to each service, onboarding and offboarding of users has to be handled independently for each service, and users have to login frequently. In short, it’s often not a great system. This is never referred to as SSO.\n\nMost organizations of a meaningful size have centralized identity management. This means that identities, credentials, authorization, onboarding, and offboarding are handled centrally. However, you may still need to independently login to each system. For example, in this system, every service might take the same username and password as your credentials, but if you go to RStudio Server followed by JupyterHub, you’ll need to provide that username and password independently to each service. This system is often facilitated by PAM, and LDAP/AD. Some organizations call this SSO, because there’s only one set of credentials.\nIn true SSO, users login once and are given a token or ticket.1 Then, when they go to the next service, they don’t have to login again because that service can just look at the token or ticket to do auth for that user. For example, in this system, I could go to RStudio Server and login, and then go to JupyterHub and get in without being prompted again for my password. This type of auth is facilitated by Kerberos, SAML, or OAuth."
  },
  {
    "objectID": "chapters/sec2/auth.html#auth-techniques",
    "href": "chapters/sec2/auth.html#auth-techniques",
    "title": "7  Logging in with auth",
    "section": "7.2 Auth Techniques",
    "text": "7.2 Auth Techniques\nIf you have five data scientists in your group, and the only shared resource you have is an RStudio Server instance, you probably don’t need to think terribly hard about auth. It’s pretty straightforward to just make users on a server and give them access to everything.\nBut as organizations get larger with hundreds or thousands of users, there’s constant churn of people joining and leaving. The number of services can creep into the dozens or hundreds and people may have very different authorization levels to different services. Trying to manage auth on the individual services is a nightmare – as is trying to keep that many usernames and passwords straight for users. That is why almost all organizations with more than a few users have centrally managed auth.\n\n7.2.1 You get a permission and you get a permission!\nFor the most part, we think of people being authenticated and authorized into services. However, it’s sometimes useful to consider the broader class of entities that could do auth. There are two common non-human entities that are included in auth systems that are worth considering.\nService Accounts are accounts given to non-human entities when you want it to be able to do something on its own behalf. For example, maybe you’ve got a Shiny app that users use to visualize data that’s in a database. Very often, you don’t want the app to have the same permisions as the app’s author, or to inherit the permissions of the people viewing the app. Instead, you want the app to be able to have permissions to do certain database operations. In that case, you would create a service account to give to the Shiny app that has exactly those permissions.\nThere are also times where it’s useful to go one level up and give permissions to an entire instance or service. In that case, you might assign permissions to an instance. For example, you could make it the case that anyone who is logged into the JupyterHub server is allowed to read from the database.\nInstance permissions are rather broad, and so they are usually only applied when you’ve got multiple resources inside a private network. In that case, authentication and authorization are only done at a single point and authorization is pretty broad.\n\n\n7.2.2 Authorization is kinda hard\nFrom a management perspective, authentication is pretty simple. A person is given a set of credentials, and they have to supply those credentials when prompted to prove they are who they say they are.\nAuthorization is a whole other can of worms. There is a meaningful literature on varieties of authorization and how they work. We’re not going to get too deep into the weeds, other than to define some common terms and how they’re used.\nThe atomic basis for authorization is a permission. Permissions are a binary switch that answers the question is this person allowed to do the thing they are trying to do?2\nThe simplest way of assigning permissions is called an access control list (ACL). In systems that use ACLs, each piece of content has a list of users who are allowed access. Sometimes, ACLs are also assigned to groups, which are simply sets of users – think data-scientists.\n\nOne ACL implementation with which you may be familiar is file permissions on a Linux server. For example, if you have a Mac or are on a Linux server, you can open your terminal, navigate to a directory and do the following:\n$ ls -l\n-rwxr-xr-x   1 alexkgold  staff   2274 May 10 12:09 README.md\nThat first set of characters describes the ACL for the README.md file. The first character - indicates this is a file, as opposed to a directory of files (which would be d). Then there are three sets of 3 characters, rwx, which are short for read, write, and execute, with the first group for the owner, alexkgold, the second group for anyone else in the group staff, and the third set for anyone else.\nSo you can read -rwxr-xr-x as, this is a file that alexkgold can read, write or execute, and anyone else can read or execute, but not edit.\nACLs are pretty intuitive, but it turns out that when you are managing a lot of users across a lot of files, directories, and services, they can get pretty difficult to manage, so many organizations use Role Based Access Control (RBAC).\nRBAC adds a layer of abstraction between users and permissions, which makes it a little harder to understand, but ultimately results in a much more flexible system. In RBAC, permissions are not assigned to individual pieces of content or to users or groups. Instead, permissions are assigned to roles, and roles are given to users or groups.3\n\nThere are also further iterations on the RBAC model, like Attribute Based Access Control (ABAC) or Policy Based Access Control (PBAC) in which there’s a long list of attributes that could be considered for a user to compute their permissions for a given service."
  },
  {
    "objectID": "chapters/sec2/auth.html#auth-technologies",
    "href": "chapters/sec2/auth.html#auth-technologies",
    "title": "7  Logging in with auth",
    "section": "7.3 Auth Technologies",
    "text": "7.3 Auth Technologies\n\n7.3.1 Username + Password\nMany pieces of software come with integrated authentication. When you use those system, the product stores encrypted username and password pairs in a database.\nThese setups are often really easy from an admin perspective – you just set up individual users on the server. However, the flip side is that users have one more username and password to remember, which is annoying for them. Moreover, if you have more than a few users, or the system is one of more than a few, it’s hard to manage users on a lot of different systems. It can be a real pain to create accounts on a ton of different systems when a new person joins the organization, or to remove their permissions one-by-one when they leave.\nFor this reason, most IT/Admin organizations strongly prefer using some sort of centralized identity store.\n\n\n7.3.2 PAM\nPluggable Authentication Modules (PAM) is a Linux system for doing authentication. As of this writing, PAM is the default authentication method for both RStudio Server and JupyterHub.\n\nConceptually PAM is pretty straightforward. You install a service on a Linux machine and configure it to use PAM authentication from the underlying host. By default, PAM just authenticates against the users configured on the Linux server, but it can also be configured to use other sorts of “modules” to authenticate against other systems – most commonly LDAP/AD or Kerberos. PAM can also be used to do things when users login – the most common being initializing tokens or tickets to other systems, like a database.\nPAM is often paired with System Security Services Daemon (SSSD), which is most commonly used to automatically create Linux users on a server based on the identities stored in an LDAP/AD instance.\nThough conceptually simple, reading, writing, and managing PAM modules is kinda painful.\n#TODO: Add PAM example\n\n\n7.3.3 LDAP/AD\nLightweight Directory Access Protocol (LDAP) is a relatively old, open, protocol used for maintaining a set of entities and their attributes. To be precise, LDAP is actually a protocol for maintaining and accessing entities and their attributes in a tree. It happens that this is a really good structure for maintaining permissions and roles of users at an organization, and it’s the main thing LDAP is used for.\nActive Directory (AD) is Microsoft’s implementation of LDAP, and is by-far the most common LDAP “flavor” out there. AD so thoroughly owns the LDAP enterprise market, that LDAP is often referred to as LDAP/AD. There are other implementations you may run across, the most common being OpenLDAP.\n\nAzure, Microsoft’s cloud provider, offers an authentication service called Azure Active Directory (AAD). Confusingly, AAD is usually used in combination with SAML, not LDAP.\n\nIt’s worth distinguishing the use of LDAP as an identity store from its use as an authentication technology. As a tree-based database, LDAP is uniquely well-suited to storing the identities, and other attributes of people at the organization. However, as discussed below, using LDAP to authenticate into actual services has security and convenience drawbacks, and many organizations consider it outdated and insecure.\nA lot of organizations are moving away from LDAP for authentication in favor of token-based technologies like SAML or OAuth, but many are keeping LDAP as their identity “source of truth” that is referenced by the SAML or OAuth Identity Provider.\nLDAP has three main disadvantages relative to other technologies. First, LDAP requires that your credentials (username and password, usually) actually be provided to the service you’re trying to use. This is fundamentally insecure relative to a system where your credentials are provided only to the identity provider, and the service just gets a token verifying who you are. In token-based systems, adding additional requirements like MFA or biometrics are easy, as they’re simply added at the IdP layer. In contrast, doing those things in LDAP would require the service to implement them, which usually is not the case, so you’re usually limited to username and password.\nThe second disadvantage of LDAP is that it does not allow for central administration of permissions. LDAP directly records only objects and their attributes. Say, for example, you want only users of a particular group to have access to a certain resource. In LDAP, you would have to specify in that resource that it should only allow in users of that group. This is in contrast to SAML/OAuth, where the authorization is centrally managed.\nLastly, LDAP authentication is based on each service authenticating. Once you authenticate, the service might give you a cookie so that your login persists, but there is no general-purpose token that will allow you to login to multiple services.\n\n7.3.3.1 How LDAP Works\nWhile the technical downsides of LDAP are real, the technical operations of LDAP are pretty straightforward. In short, you try to login to a service, the service collects your username and password, sends it off to the LDAP server, and checks that your username and password are valid.\n\nNote that LDAP is purely for authentication. When you’re using LDAP, authorization has to be handled separately, which is one of the disadvantages.\n\n\n7.3.3.2 Deeper Than You Need on LDAP\nLDAP is a tree-based entity and value store. This means that LDAP stores things and their attributes, which include a name and one or more values. For example, my entry in a corporate LDAP directory might look like this:\ncn: Alex Gold\nmail: alex.gold@example.com\nmail: alex.gold@example.org\ndepartment: solutions\nmobile: 555-555-5555\nobjectClass = Person\nMost of these attributes should be pretty straightforward. cn is short for common name, and is part of the way you look up an entity in LDAP (more on that below). Each entity in LDAP must have an objectClass, which determines the type of entity it is. In this case, I am a Person , as opposed to a device, domain, organizationalRole, or room – all of which are standard objectClasses.\nLet’s say that your corporate LDAP looks like the tree below:\n\n#TODO: make solutions an OU in final\nThe most common way to look up LDAP entities is with their distinguished name (DN), which is the path of names from the point you’re starting all the way back to the root of the tree. In the tree above, my DN would be cn=alex,ou=solutions,dc=example,dc=com.\nNote that you read the DN from right to left to work your way down the tree. Aside from cn for common name, other common fields include ou for organizational unit, and dc for domain component.\n\n\n7.3.3.3 Trying out LDAP\nNow that we understand in theory how LDAP works, let’s try out an actual example.\nTo start, let’s stand up LDAP in a docker container:\n#TODO: update ldif\ndocker network create ldap-net\ndocker run -p 6389:389 \\\n  --name ldap-service \\\n  --network ldap-net \\\n  --detach alexkgold/auth\nldapsearch is a utility that lets us run queries against the LDAP tree. Let’s try it out against the LDAP container we just stood up.\nLet’s say I want to return everything in the subtree under example.org. In that case, I would run ldapsearch -b dc=example,dc=org, where b indicates my search base, which is a dn. But in order to make this actually work, we’ll need to include a few more arguments, including\n\nthe host where the LDAP server is, indicated by -H\nthe bind DN we’ll be using, flagged with -D\nthe bind password we’ll be using, indicated by -w\n\nSince we’re testing, we’re also going to provide the flag -x to use whatever certificate is present on the server. Putting it altogether, along with the commands to reach the docker container, let’s try:\nldapsearch -x -H ldap://localhost:6389 -b dc=example,dc=org -D \"cn=admin,dc=example,dc=org\" -w admin\n\n# extended LDIF\n#\n# LDAPv3\n# base <dc=example,dc=org> with scope subtree\n# filter: (objectclass=*)\n# requesting: ALL\n#\n\n# example.org\ndn: dc=example,dc=org\nobjectClass: top\nobjectClass: dcObject\nobjectClass: organization\no: Example Inc.\ndc: example\n\n# admin, example.org\ndn: cn=admin,dc=example,dc=org\nobjectClass: simpleSecurityObject\nobjectClass: organizationalRole\ncn: admin\ndescription: LDAP administrator\nuserPassword:: e1NTSEF9d3IyVFp6SlAyKy9xT2RsQ0owTDYzR0RzNFo0NUFrQ00=\n\n# search result\nsearch: 2\nresult: 0 Success\n\n# numResponses: 3\n# numEntries: 2\nYou should be able to read what got returned pretty seamlessly. One thing to notice is that the user password is returned, so it can be compared to a password provided. It is encrypted, so it doesn’t appear in plain text.\nNote that ldap is a protocol – so it takes the place of the http you’re used to in normal web operations. Like there’s https, there is also a protocol called LDAPS, which is ldap + tls for the same reason you’ve got https. LDAP is (almost) always running in the same private network as the service, so many organizations don’t require using LDAPS, but others do require it.\nRunning the ldapadmin\ndocker run -p 6443:443 \\\n        --name ldap-admin \\\n        --env PHPLDAPADMIN_LDAP_HOSTS=ldap-service \\\n        --network ldap-net \\\n        --detach osixia/phpldapadmin\ndn for admin cn=admin,dc=example,dc=org pw: admin\nhttps://localhost:6443\n# Replace with valid license\nexport RSC_LICENSE=XXXX-XXXX-XXXX-XXXX-XXXX-XXXX-XXXX\n\n# Run without persistent data and using default configuration\ndocker run -it --privileged \\\n    --name rsc \\\n    --volume $PWD/rstudio-connect.gcfg:/etc/rstudio-connect/rstudio-connect.gcfg \\\n    -p 3939:3939 \\\n    -e RSC_LICENSE=$RSC_LICENSE \\\n    --network ldap-net \\\n    rstudio/rstudio-connect:latest\n\n\n7.3.3.4 Single vs Double Bind\nThere are two different ways to establish a connection between your server and the LDAP server. The first method is called Single Bind. In a single bind authentication, the user credentials are used both to authenticate to the LDAP server, and to query the server.\nIn a Double Bind configuration, there is a separate administrative service account, used to authenticate to the LDAP server. Once authentication is complete, then the user is queried in the system.\nSingle bind configurations are often more limited than double bind ones. For example, in most cases you’ll only be able to see the single user as well as the groups they’re a part of. This can limit application functionality in some cases. On the other hand, there need be no master key maintained on your server, and some admins may prefer it for security reasons.\nWe can see this really concretely. In the example above, you used a double bind by supplying admin credentials to LDAP. Let’s say instead, you just provide a single user’s credentials. In that case, I don’t get anything back if I just do a general search.\nldapsearch -x -H ldap://localhost:6389 -b dc=example,dc=org -D \"cn=joe,dc=engineering,dc=example,dc=org\" -w joe                                       \n# extended LDIF\n#\n# LDAPv3\n# base <dc=example,dc=org> with scope subtree\n# filter: (objectclass=*)\n# requesting: ALL\n#\n\n# search result\nsearch: 2\nresult: 32 No such object\n\n# numResponses: 1\nBut just searching for information about Joe does return his own information.\nldapsearch -x -H ldap://localhost:6389 -b cn=joe,dc=engineering,dc=example,dc=org -D \"cn=joe,dc=engineering,dc=example,dc=org\" -w joe                    32 ✘\n# extended LDIF\n#\n# LDAPv3\n# base <cn=joe,dc=engineering,dc=example,dc=org> with scope subtree\n# filter: (objectclass=*)\n# requesting: ALL\n#\n\n# joe, engineering.example.org\ndn: cn=joe,dc=engineering,dc=example,dc=org\ncn: joe\ngidNumber: 500\ngivenName: Joe\nhomeDirectory: /home/joe\nloginShell: /bin/sh\nmail: joe@example.org\nobjectClass: inetOrgPerson\nobjectClass: posixAccount\nobjectClass: top\nsn: Golly\nuid: test\\joe\nuidNumber: 1000\nuserPassword:: e01ENX1qL01raWZrdk0wRm1sTDZQM0MxTUlnPT0=\n\n# search result\nsearch: 2\nresult: 0 Success\n\n# numResponses: 2\n# numEntries: 1\n\n\n\n7.3.4 Kerberos Tickets\nKerberos is a relatively old ticket-based auth technology. In Kerberos, encrypted tickets are passed around between servers. Because these tickets live entirely on servers under the control of the organization, they are generally quite secure.\nThough Kerberos is freely available, it was widely adopted along with Active Directory, and it’s used almost exclusively in places that are running a lot of Microsoft products. A frequent use of Kerberos tickets is to establish database connections.\nBecause the tickets are passed around from server to server, Kerberos can be used to create a true SSO experience for users.\n\n7.3.4.1 How Kerberos Works\nAll of Kerberos works by sending information to and from the central Kerberos Domain Controller (KDC). In Kerberos, authentication and authorization are handled independently.\nWhen a Kerberos session is initialized, the service sends the users credentials off to the KDC and requests something called the Ticket Granting Ticket (TGT) from the KDC. TGTs have a set expiration period. When they expire, the client has to request an updated TGT. This is one reason why Kerberos is considered quite secure - even if someone managed to steal a TGT, they’d only be able to use it for a little while before it went stale and could be revoked.\nWhen the user wants to actually do something, they send the TGT back to the KDC again and get a session key (sometimes referred to as a service ticket) that allows access to the service, usually with a specified expiration period.\n\n\n\n7.3.4.2 Try out Kerberos\n#TODO\n\n\n\n7.3.5 SAML\nThese days Security Assertion Markup Language (SAML) is probably the most common system that provides true SSO – including single login and centrally-managed permissions. SAML does this by passing around XML tokens.4\nThe way this generally works is that a user attempts to login to a Service Provider (SP). The SP redirects the user to an Identity Provider (IdP), which checks either for a preexisting token in the users browser, or verifies the users credentials. The IdP checks for the user’s authorization to access the SP in question, and sends an authorization token back to the SP.\n\nRelative to LDAP/AD, which is from the early 1990s, SAML is a new kid on the block. SAML 1.0 was introduced in 2002, and SAML 2.0, which is the current standard, came out in 2005. Many large enterprises are switching their systems over to use SAML or have already done so.\nOne superpower of SAML IdPs is that many of them can federate identity management to other systems. So, it’s pretty common for large enterprises to maintain their user base in one or more LDAP/AD system, but actually use a SAML IdP to do authentication and authorization. In fact, this is what Azure Active Directory (AAD), which is Microsoft Azure’s hosted authentication offering does. It is possible to use LDAP/AD with AAD, but most organizations use it with SAML.\nOne of the nice things about SAML is that credentials are never shared directly with the SP. This is one of the ways in which SAML is fundamentally more secure than LDAP/AD – the users credentials are only ever shared with the IdP.\nThere are two different ways logins can occur – starting from the SP, or starting from the IdP.\nIn SAML, the XML tokens that are passed back and forth are called assertions.\n\n7.3.5.1 Try SAML\nWe’re going to use a simple SAML IdP to try out SAML a bit. This container only supports a single SP. Any IdP that might be used in an enterprise environment is going to support many SPs simultaneously.\nLet’s go through the environment variables we’re providing to this docker run command. We’re providing three different arguments:\n\nThe SP_ENTITY_ID is the URL of the\nSP_ASSERTION_CONSUMER_SERVICE is the URL of the SP that is prepared to receive the authorized responses coming back from the SAML IdP.\nSP_SINGLE_LOGOUT_SERVICE is the URL where the SP will receive a logout command once someone has been logged out at the IdP level. Many SPs do not implement single logout.\n\ndocker run --name=saml_idp \\\n-p 8080:8080 \\\n-p 8443:8443 \\\n-e SIMPLESAMLPHP_SP_ENTITY_ID=http://app.example.com \\\n-e SIMPLESAMLPHP_SP_ASSERTION_CONSUMER_SERVICE=http://localhost/simplesaml/module.php/saml/sp/saml2-acs.php/test-sp \\\n-e SIMPLESAMLPHP_SP_SINGLE_LOGOUT_SERVICE=http://localhost/simplesaml/module.php/saml/sp/saml2-logout.php/test-sp \\\n-d kristophjunge/test-saml-idp:1.15\nhttp://localhost:8080/simplesaml\nadmin/secret\n\n\n\n7.3.6 OIDC/OAuth2.0\nOIDC/OAuth is slightly newer than SAML, created in 2007 by engineers at Google and Twitter. OAuth 2.0 – the current standard was released in 2012. If you’re being pedantic, OAuth is a authorization protocol, and OpenID Connect (OIDC) is an authorization protocol that uses OAuth. In most cases, people will just call it OAuth.\n\n#TODO: this picture is bad\nIn an enterprise context, OAuth/OIDC is conceptually very similar to SAML – but instead of passing around XML tokens, it’s based on JSON Web Tokens (JWT, usually pronounced “jot”).\n\nThe fact that JSON is much more human-readable than XML is one of the big advantages of OIDC/OAuth compared to SAML.\n\n#TODO: try it out\n\n7.3.6.1 OAuth/OIDC vs SAML\nFrom a practical perspective, the biggest difference between OAuth/OIDC and SAML is that SAML is quite strict about what SPs are allowed. Each SP needs to be registered at a specific web address that the IdP knows it’s allowed to receive requests from.\nIn contrast, OAuth/OIDC was designed to be used to delegate authentication and authorization to different kinds of services that might be widely available on the internet. If you’ve ever allowed a website to Login with Apple/Google/Facebook/Github, that has been an application of OAuth/OIDC.\nBecause the set of allowable SPs is fixed under SAML, it’s more common in enterprise settings. Some admins consider SAML more secure for that reason as well.\nIn some situations, SAML is used for authentication and OAuth is used for access to other services. Most commonly in the data science world, this can come up when a user logs into a service like RStudio Server and is then authorized to a database using an OAuth JWT.\nResources: https://www.okta.com/identity-101/saml-vs-oauth/ https://www.okta.com/identity-101/whats-the-difference-between-oauth-openid-connect-and-saml/ https://phoenixnap.com/blog/kerberos-authentication https://www.dnsstuff.com/rbac-vs-abac-access-control"
  },
  {
    "objectID": "chapters/sec2/scaling.html",
    "href": "chapters/sec2/scaling.html",
    "title": "8  Scaling",
    "section": "",
    "text": "At some point, your data science environment may reach the stage where you need to start thinking about how you’re going to scale your environment out to accommodate more users – or sometimes this will come up in the context of disaster recovery or maintaining service uptime.\nHopefully, if you’re at this level of complexity, you’ve got someone in the IT/Admin organization to help you. This chapter is mostly going to be focused around the conceptual components of how scaling works, and how to talk about them. We’re not going to get deep into the weeds on how to configure them.\nThere are two main types of scaling that get discussed. This is an example where the language definitely impedes understanding rather than furthering it.\nVertical scaling is just a fancy way of saying making a server bigger. So maybe you’re running RStudio Server or JupyterHub on a server of a particular size. Vertically scaling that server just means making the server itself bigger. If you’re running your own server, this is a huge pain. You’ve got to buy and configure a new server, and switch over. The ability to quickly vertically scale hardware is one of the best things about the cloud. Taking a server down, transferring the attached volume to a new server of a different size, and putting it back up takes just a few minutes.\nVertical scaling in the cloud is great – but there are limits. ]For example, the AWS C line of instances instances are their “compute optimized” instances that have fast CPUs and are good for general-purpose data science workloads. Generally, AWS scales their EC2 instances linearly in terms of the number of CPU cores offered on the instance, but across most reasonably-priced instance types, the instance sizes max out at 96-128 cores these days. That’s probably sufficient for many workloads, but if you’ve got an RStudio Server with 50 concurrent users doing reasonably heavy compute loads, that can quickly get eaten up.\nHorizontal scaling means distributing the workload across multiple servers or machines. It is almost always more complicated than it seems like it should be, and more complicated than you want it to be. Horizontal scaling is often referred to as load balancing.\nTODO: Image of vertical + horizontal scaling\nSometimes horizontal scaling is undertaken for pure scaling purposes, but sometimes it’s undertaken for cluster resilience purposes. For example, you might want the cluster to be resilient to a node randomly failing, or being taken down for maintenance. In this context, horizontal scaling is often called high availability. Like many other things, high availability is a squishy term, and different organizations have very different definitions of what it means.\nFor example, in one organization, high availability might just mean that there’s a robust disaster recovery plan so servers can be brought back online with little data loss. In another organization, high availability might mean having duplicate servers that aren’t physically colocated to avoid potential outages due to server issues or natural disasters. In other contexts, it might be a commitment to a particular amount of uptime.1\nSpannning Multiple AZs\nAs the requirements for high availability get steeper, the engineering cost to make sure the service really is that resilient rise exponentially…so be careful how much uptime you’re trying to achieve."
  },
  {
    "objectID": "chapters/sec2/scaling.html#k8s",
    "href": "chapters/sec2/scaling.html#k8s",
    "title": "8  Scaling",
    "section": "8.1 Container Deployment + Orchestration",
    "text": "8.1 Container Deployment + Orchestration\nOne tool that comes up increasingly frequently when talking about scaling is Kubernetes (sometimes abbreviated as K8S).4 Kubernetes is the way people orchestrate Docker containers in production settings.5 So basically that it’s the way to put containers into production when you want more than one to interact – say you’ve got an app that separately has a database and a front end in different containers, or, like in this chapter, multiple load-balanced instances of the same containers.\nWhile the operational details of Kubernetes are very different from the horizontal scaling patterns we’ve discussed so far in this chapter, the conceptual requirements are the same.\nTODO: Diagram of K8S\nMany people like Kubernetes because of its declarative nature. If you recall from the section on Infrastructure as Code, declarative code allows you to make a statement about what the thing is you want and just get it, instead of specifying the details of how to get there.\nOf course, in operation this all can get much more complicated, but once you’ve got the right containers, Kubernetes makes it easy to say, “Ok, I want one instance of my load balancer container connected to three instances of my compute container with the same volume connected to all three.”\n\n\n\n\n\n\nKubernetes Tripwire!\n\n\n\nIf you’re reading this and are extremely excited about Kubernetes – that’s great! Kubernetes does make a lot of things easy that used to be hard. Just know, networking configuration is the place you’re likely to get tripped up. You’ve got to deal with networking into the cluster, networking among the containers inside the cluster, and then networking within each container.\nComplicated kubernetes networking configurations are not for the faint of heart.\n\n\nFor individual data scientists, Kubernetes is usually overkill for the type of work you’re doing. If you find yourself in this territory, it’s likely you should try to work with you organization’s IT/Admin group.\nOne of the nice abstraction layers Kubernetes provides is that in Kubernetes, you provide declarative statements of the containers you want to run, and any requirements you have. You separately register actual hardware with the cluster, and Kubernetes takes care of placing the conatiners onto the hardware depending on what you’ve got available.\nIn practice, unless you’re part of a very sophisticated IT organization, you’ll almost certainly use Kubernetes via one of the cloud providers’ Kubernetes clusters as a service. AWS’s is called Elastic Kubernetes Service (EKS).6\nOne really nice thing about using these Kubernetes clusters as a service is that adding more compute power to your cluster is generally as easy as a few button clicks. On the other hand, that also makes it dangerous from a cost perspective.\nIt is possible to define a Kubernetes cluster “on the fly” and deploy things to a cluster in an ad hoc way. I wouldn’t recommend this for any production system. Helm is the standard tool for defining kubernetes deployments in code, and Helmfile is a templating system for Helm.\nSo, for example, if you had a standard “Shiny Server” that was one load balancer containers, two containers each running a Shiny app, and a volume mounted to both, you would define that cluster in Helm. If you wanted to be able to template that Helm code for different clusters, you’d use Helmfile."
  },
  {
    "objectID": "chapters/sec2/scaling.html#exercises",
    "href": "chapters/sec2/scaling.html#exercises",
    "title": "8  Scaling",
    "section": "8.2 Exercises",
    "text": "8.2 Exercises\nTODO"
  },
  {
    "objectID": "chapters/sec3/cloud.html",
    "href": "chapters/sec3/cloud.html",
    "title": "9  The Cloud and Cloud Providers",
    "section": "",
    "text": "Like so many tech buzzwords, it’s pretty hard to get a sense for what the cloud actually is beneath all the hype.\nBack in the day – and still in some heavily-regulated industries – getting a new server involved buying a physical piece of hardware, installing it in a server room, getting it configured and up and running, installing the software you want on the server, and configuring access to the server.\nThe Cloud provides layers of “as a service” on top of this former world where someone at your organization would be responsible for buying and maintaining actual hardware.\nIn this chapter, you’ll learn a little bit about how The Cloud works, and how to demystify cloud offerings."
  },
  {
    "objectID": "chapters/sec3/cloud.html#the-rise-of-services",
    "href": "chapters/sec3/cloud.html#the-rise-of-services",
    "title": "9  The Cloud and Cloud Providers",
    "section": "9.1 The Rise of Services",
    "text": "9.1 The Rise of Services\nLike much of the rest of the economy, server provisioning and use has gone the way of services. Instead of buying, owning, and maintaining a physical object, a huge proportion of the world’s server hardware is rented.\n[TODO: quote in paragraph below: https://www.srgresearch.com/articles/cloud-market-growth-rate-nudges-amazon-and-microsoft-solidify-leadership]\nIn the US, a huge fraction runs on servers rented from one of three organizations (in order of how significant they are) – Amazon Web Services (AWS), Microsoft Azure, and Google Cloud Platform (GCP). These three companies account for a huge proportion of what we think of as “The Cloud”. There are other smaller players, and also companies that are popular for particular tasks, like Netlify for hosting static websites.\nHowever, in many cases, the cloud doesn’t just refer to renting a server itself. There are also layers and layers of services on top of the “rent me a server” business.\nIn general, the “rent me a server” model is called Infrastructure as a Service (IaaS - pronounced eye-az). So when you stood up an EC2 instance from AWS in the first chapter, you were using AWS’s IaaS offering.\nIn general, people split the layers on top of IaaS into two categories – Platform as a Service (PaaS – pronounced pass), and Software as a Service (SaaS pronounced sass).\nPaaS is where you rent an environment in which to do development. On the other hand, SaaS is renting software as an end user.\nOne common way to explain the difference is using a baking metaphor. Consider making a cake. An on-prem install would be where you make the cake completely from scratch. A IaaS experience would be buying cake mix and baking at home. PaaS would be buying a premade blank cake that you decorate yourself, and SaaS would be just buying a store-bought cake.\nI find these categories and this metaphor sorta helpful in the abstract, but when getting down to concrete real-world examples, the distinctions get fuzzier, and you have to be careful which perspective you’re talking about.\nFor example, RStudio Cloud is a service where you can get an environment with RStudio preconfigured and ready to use. From the perspective of a data scientist, this is clearly a PaaS offering. RStudio is providing a platform where you can learn or do work as a data scientist.\nBut from the perspective of an IT/Admin considering how to set up a server-based data science environment inside their company, RStudio Cloud is clearly a SaaS offering – you just getting the software configured and ready to use.\nMaking the issue even more difficult, many companies go out of their way to make their services sound grand and important and don’t just say, “this is ___ as a service”. Moreover, it’s very common (especially in AWS) to have many different services that fulfill similar needs, and it can be really hard to concretely tell the difference.\nFor example, if you go to AWS’s database offerings for a “database as a service”, your options include Redshift, RDS, Aurora, DynamoDB, ElastiCache, MemoryDB for Redis, DocumentDB, Keyspaces, Neptune, Timestream, and more.\nThere’s a reason why there’s a meaningful industry of people whose full time job is to consult on which AWS service your company needs and how to take advantage of the pricing rules to make sure you get a good deal.\nThere are a few reasons why organizations are moving to the cloud. Primary among them is that maintaining physical servers is often not the core competency of IT/Admin organizations. They’d rather manage higher-level abstractions than physical servers – or increasingly even virtual servers.\nOne reason that people cite, but very rarely comes to pass, is cost. In theory, the flexibility of the cloud should allow organizations to stand up servers as needed and spin them down when they’re not needed. This flexibility is real, there are times when it’s super useful to be able to bring up a server for a particular project – it’s often far quicker and easier than buying and installing a server of your own.\nIn reality, the engineering needed to stand up and spin down these servers at the right time is really difficult and costly – enough so that most organizations could probably substantailly save money if they repatriated their cloud workloads.\nFor more established organizations, running workloads in the cloud may, in fact, be substantially more costly than just bringing those workloads on prem."
  },
  {
    "objectID": "chapters/sec3/cloud.html#serverless-computing",
    "href": "chapters/sec3/cloud.html#serverless-computing",
    "title": "9  The Cloud and Cloud Providers",
    "section": "9.2 Serverless Computing",
    "text": "9.2 Serverless Computing\nIn the past few years, there has been a rise in interest in “serverless” computing. This is a buzzword and there’s no one shared definition of what serverless means. It’s worth making clear that there is no such thing as truly serverless computing. Every bit of cloud computation runs on a server - the question is whether you have to deal with the server or if someone else is doing it for you.\nHowever, there are two distinct things happening that can meaningfully be described as serverless…but they’re completely different.\nOne is the rise of containerization. In chapter XXXX, we’ll get deep into the weeds on using docker yourself. Docker is a very cool tool that makes software much more portable, because you can bring the environment – all the way down to the operating system – around with you very easily. This is kinda a superpower, and many organizations are moving towards using docker containers as the atomic units of their IT infrastructure, so the IT organization doesn’t manage any servers directly, and instead just manages docker containers.\nIn some sense, this is meaningfully serverless. You’ve moved the level of abstraction up a layer from servers and virtual machines to docker containers. And managing docker containers is often meaningfully easier than managing virtual machines directly. However, you still face a lot of the same problems like versioning operating systems, dealing with storage and networking yourself, and more.\nThere is another, stronger, use of serverless that is rising and is also pretty cool, but is super different. In these services, you just hand off a function, written in soem programming langauge to a cloud provider, and they run that function as a service. In a trivial example, imagine a service that adds two numbers together. You could write a Python or R function that does this addition and returns it.\nIt is possible to just deploy this function to a Cloud provider’s environment and then only pay for the actual compute time needed to complete your function calls. This is obviously very appealing because you really don’t have to manage anything at the server level. The disadvantage is that this works only for certain types of operations."
  },
  {
    "objectID": "chapters/sec3/cloud.html#common-services-that-will-be-helpful-to-know-offhand",
    "href": "chapters/sec3/cloud.html#common-services-that-will-be-helpful-to-know-offhand",
    "title": "9  The Cloud and Cloud Providers",
    "section": "9.3 Common Services That will be helpful to know offhand",
    "text": "9.3 Common Services That will be helpful to know offhand\nLuckily, if you’re thinking about setting up and running a standard data science platform in one of the major cloud providers, you’re likely to use one of a few reasonably standard tools.\nIt’s helpful to keep in mind that at the very bottom, there are four basic cloud services: renting servers, configuring networking, identity management, and renting storage. All the other services are recombinations and software installed on top of that.1\nAzure and GCP tend to name their offerings pretty literally. AWS, on the other hand, uses names that have little relationship to the actual task at hand, and you’ll just need to learn them.\n\n9.3.1 IaaS\n\nCompute - AWS EC2, Azure VMs, Google Compute Engine\nStorage\n\nfile - EBS, Azure managed disk, Google Persistent Disk\nNetwork drives - EFS, Azure Files, Google Filestore\nblock/blob - S3, Azure Blob Storage, Google Cloud Storage\n\nNetworking:\n\nPrivate Clouds: VPC, Virtual Network, Google Virtual Private Cloud\nDNS - Route53, Azure DNS + Traffic Manager, Google Cloud DNS\n\n\n\n\n9.3.2 Not IaaS\n\nContainer Hosting - ECS, Azure Container Instances + Container Registry\nK8S cluster as a service - EKS, AKS, GKE\nRun a function as a service - Lambda, Azure Functions, Google Cloud Functions\nDatabase - RDS/Redshift, Azure Database, Google Cloud SQL\nSageMaker - ML platform as a service, Azure ML, Google Notebooks\n\nhttps://docs.microsoft.com/en-us/azure/architecture/aws-professional/services#networking https://cloud.google.com/free/docs/aws-azure-gcp-service-comparison"
  },
  {
    "objectID": "chapters/sec3/cloud.html#cloud-tooling",
    "href": "chapters/sec3/cloud.html#cloud-tooling",
    "title": "9  The Cloud and Cloud Providers",
    "section": "9.4 Cloud Tooling",
    "text": "9.4 Cloud Tooling\n\nIdentity MGMT - IAM, Azure AD\nBilling Mgmt\n\nIaaC tooling\n\n9.4.1 AWS Instance Classes for Data Science\nt3 – good b/c of instance credits, limited size\nCs – good b/c fast CPUs\nR - good b/c high amount of RAM\nP - have GPUs, but also v expensive\nInstance scale linearly w/ number of cores (plot?)"
  },
  {
    "objectID": "chapters/sec3/cloud.html#exercises",
    "href": "chapters/sec3/cloud.html#exercises",
    "title": "9  The Cloud and Cloud Providers",
    "section": "9.5 Exercises",
    "text": "9.5 Exercises\n\nExample of something you’d want to build – for each of the 3 providers, which services would you use if you wanted an IaaS solution, a PaaS solution?"
  },
  {
    "objectID": "chapters/sec3/cmd-line.html",
    "href": "chapters/sec3/cmd-line.html",
    "title": "10  The Command Line + SSH",
    "section": "",
    "text": "Using can also be super useful in day-to-day data science work. It also is required for most any work administering servers. Indeed, in most server environments, the command line is the only access you’ll have.\nAdditionally, as you go through this book, you’ll be doing most of your work from the command line. This chapter will give you the background necessary to effectively use the command line and get you set up on your own machine."
  },
  {
    "objectID": "chapters/sec3/cmd-line.html#step-1-accessing-the-command-line",
    "href": "chapters/sec3/cmd-line.html#step-1-accessing-the-command-line",
    "title": "10  The Command Line + SSH",
    "section": "10.1 Step 1: Accessing the Command Line",
    "text": "10.1 Step 1: Accessing the Command Line\nThe way you access the command line varies depending on your operating system.\n\n10.1.1 If you use a Mac\nThere is a built in terminal app that comes packaged with your computer. It’s fine.\nI’d recommend downloading the free iTerm2, which is a terminal replacement app that adds a bunch of niceties like better theming and multiple tabs.\nMany people also like replacing the default bash shell with more advanced shells like zsh or fish that allow for things like better auto completion, spellchecking, and a huge variety of plugins. Most people manage their zsh install with a tool like prezto or oh-my-zsh.\n\n\n\n\n\n\nTip\n\n\n\nI personally use iTerm2 with zsh and prezto.\n\n\nI’m not going to go through the steps of installing and configuring these tools – there are numerous online walkthroughs and guides.\n\n\n10.1.2 If you use a PC\n#TODO\n\n\n10.1.3 If you use Linux\nGet out of here. You already know way too much about using the command line. You’ve probably already got a way more complicated setup than I do."
  },
  {
    "objectID": "chapters/sec3/cmd-line.html#lets-try-it",
    "href": "chapters/sec3/cmd-line.html#lets-try-it",
    "title": "10  The Command Line + SSH",
    "section": "11.1 Let’s try it",
    "text": "11.1 Let’s try it\n\nThe terminal\n3 step process\n\ngenerate public/private keys ssh-keygen\nplace keys in appropriate place\nuse ssh to do work\n\nPermissions on key"
  },
  {
    "objectID": "chapters/sec3/cmd-line.html#getting-comfortable-in-your-own-setup",
    "href": "chapters/sec3/cmd-line.html#getting-comfortable-in-your-own-setup",
    "title": "10  The Command Line + SSH",
    "section": "11.2 Getting Comfortable in your own setup",
    "text": "11.2 Getting Comfortable in your own setup\n\nUsing ssh-config"
  },
  {
    "objectID": "chapters/sec3/cmd-line.html#advanced-ssh-tips-tricks",
    "href": "chapters/sec3/cmd-line.html#advanced-ssh-tips-tricks",
    "title": "10  The Command Line + SSH",
    "section": "11.3 Advanced SSH Tips + Tricks",
    "text": "11.3 Advanced SSH Tips + Tricks\n\nSSH Tunneling/Port Forwarding\n-vvvv, -i, -p [Diagram: Port Forwarding]\n\nTODO: talk about command line programs + flags"
  },
  {
    "objectID": "chapters/sec3/cmd-line.html#exercises",
    "href": "chapters/sec3/cmd-line.html#exercises",
    "title": "10  The Command Line + SSH",
    "section": "11.4 Exercises",
    "text": "11.4 Exercises\n\nDraw your own diagram of the sequence of things that occur when you execute ssh user@server.company.com in the terminal.\nStand up a new server on one of the major cloud providers. Try logging in using the provided key file. Create a new user on the server.\nGenerate a new SSH key locally and copy the correct key onto the server (think for a moment about which key is the correct one – consult your diagram from step 1 if necessary).\nSet up an SSH alias so you can SSH into your server using just ssh testserver (hint: look at your SSH config).\nCreate a local text file and SFTP it to the server. SSH in, edit the file, and SFTP it back.\n[Advanced] Stand up a nginx server on your remote instance. Don’t open the port to the world, but SSH port forward the server page to your local browser."
  },
  {
    "objectID": "chapters/sec3/aws-walkthrough.html",
    "href": "chapters/sec3/aws-walkthrough.html",
    "title": "11  Lab 1: Get your own server",
    "section": "",
    "text": "Throughout the rest of the book, we’re going to talk about how to set up a real data science workbench in the cloud. In this first lab, we’re just going to go through the simple steps of getting a free server from AWS to use.\nIf you’ve never set up a server before, I’d strongly recommend you play along.\nThroughout the rest of the book, we’re going to be taking this toy server and upgrading it in various ways. If you’re trying to work along, you’ll probably do this a dozen times or more over the course of the book.\nThere are a number of topics that we’re going to gloss over in this chapter including networking and storage configuration, security, and how SSH works. By the time you’re done with this book, you’ll be able to come back to this chapter and fully understand everything you did.\nYou’ll also understand why this first server is ready for production data science work in the same way throwing a tarp over a tree branch in the forest is a house. It might be a nice place to stay for a little, but you’re going to need something more robust longer-term.\nFor now, you can get a server of your own in 10-15 minutes if you just follow along, which is what I recommend."
  },
  {
    "objectID": "chapters/sec3/aws-walkthrough.html#exercises",
    "href": "chapters/sec3/aws-walkthrough.html#exercises",
    "title": "11  Lab 1: Get your own server",
    "section": "11.1 Exercises",
    "text": "11.1 Exercises\nTry standing up a new server and installing R and RStudio Server Open Source, or Python and JupyterHub.\n\nHint 1: Remember that your instance only allows traffic to SSH in on port 22 by default. You access RStudio on port 8787 by default and JupyterHub on port 8000. You control what ports are open via the Security Group.\nHint 2: You’ll need to create a user on the server. The adduser command is your friend.\n\n#TODO: test out JupyterHub"
  },
  {
    "objectID": "chapters/sec3/understanding-traffic.html",
    "href": "chapters/sec3/understanding-traffic.html",
    "title": "12  Computer Networks and the Internet",
    "section": "",
    "text": "In chapter 1, we got into how the computer on your desk, on your lap, or in your hand works. These days, many or even most of the things we want to do involve sending data across computer networks. When you visit a website, wirelessly print a document, or login to your email, you are making use of a computer network.\nThe computer network we’re all most familiar with is the biggest of them all – The Internet. But there are myriad other networks, like the very small private network of the devices (phones, computers, TVs, etc) connected to your home wifi router, to the somewhat larger VPN (it stands for virtual private network, after all) you might connect to for school or work.\nA computer network is a set of computers that can communicate with each other to send data in the form of network traffic back and forth to each other. These networks are basically self-similar – once you understand the wifi network in your house, you’ve also got a reasonably good understanding of how the entire internet works, which is great if you’re an author trying to explain how this all works.\nIn this chapter, you’ll learn the basics of how computer networks work. In particular, we’ll get into some of the layers of protocols that define how computers communicate with each other. This chapter is mostly going to be background for the next few chapters, where we’ll get into the nitty gritty of how to configure both the public and private elements of networking for your data science workshop."
  },
  {
    "objectID": "chapters/sec3/understanding-traffic.html#computer-communication-packets-traversing-networks",
    "href": "chapters/sec3/understanding-traffic.html#computer-communication-packets-traversing-networks",
    "title": "12  Computer Networks and the Internet",
    "section": "12.1 Computer communication = packets traversing networks",
    "text": "12.1 Computer communication = packets traversing networks\nThe virtual version of the processes that get your letter from your house to your penpal’s is called packet switching, and it’s really not a bad analogy. Like the physical mail, your computer dresses up a message with an address and some other details, sends it on its way, and waits for a response.1 The set of rules – called a protocol – that defines a valid address, envelope type, and more is called TCP/IP.\nUnderneath these protocols is a bunch of hardware, which we’re basically going to ignore.\nEach computer network is governed by a router. For the purposes of your mental model, you can basically think of your router as doing two things – maintaining a table of the IP addresses it knows, and following this algorithm over and over again.\n#TODO: Turn into visual tree – also visual of networks and sub-networks\n\nDo I know where this address is?\n\nYes: Send the packet there.\nNo: Send the packet to the default address and cross fingers.\n\n\nIn general, routers only know about the IP addresses of sub-networks and devices, so if you’re printing something from your laptop to the computer in the next room, the packet just goes to your router and then straight to the printer.\n\nIn your home’s local area network (LAN), your router does one additional thing – as devices like your phone, laptop, or printer attach to the network, it assigns them IP addresses based on the addresses available in a process called Dynamic Host Configuration Protocol (DHCP).\n\nOn the other hand, if you’re sending something to a website or server that’s far away, your computer has no idea where that IP address is. Clever people have solved this problem by setting the default address in each router to be an “upstream” router that is a level more general.\nSo immediately upstream of your router is probably a router specific to your ISP for a relatively small geographic area. Upstream of that is probably a router for a broader geographic area. So your packet will get passed upstream to a sufficiently general network and then back downstream to the actual IP address you’re trying to reach.\n#TODO: Image of computer network w/ upstream and downstream networks\nWhen the packets are received and read – something happens. Maybe you get to watch your show on Netflix, or your document gets printed – or maybe you get an error message back. In any event, the return message will be transmitted exactly the same way as your initial message, though it might follow a different path."
  },
  {
    "objectID": "chapters/sec3/understanding-traffic.html#more-details-about-ip-addresses",
    "href": "chapters/sec3/understanding-traffic.html#more-details-about-ip-addresses",
    "title": "12  Computer Networks and the Internet",
    "section": "12.2 More details about IP Addresses",
    "text": "12.2 More details about IP Addresses\nIP addresses are, indeed, addresses. They are how one computer or server finds another on a computer network, and they are unique within that network.\nMost IP addresses you’ve probably seen before are IPv4 addresses. They’re four blocks of 8-bit fields, so they look something like 65.77.154.233, where each of the four numbers is something between 0 and 255.\nSince these addresses are unique, each server and website on the internet needs a unique IP address. If you do the math, you’ll realize there are “only” about 4 billion of these. It turns out that’s not enough for the public internet and we’re running out.\nIn the last few years, adoption of the new standard, IPv6, has started. IPv6 addresses are eight blocks of hexadecimal (0-9 + a-f) digits separated by colons, with certain rules that allow them to be shortened, so 4b01:0db8:85a3:0000:0000:8a2e:0370:7334 or 3da4:66a::1 are both examples of valid IPv6 addresses.\nIPv6 will coexist with IPv4 for a few decades, and we’ll eventually switch entirely to IPv6. There’s no worry about running out of IPv6 addresses any time soon, because the total number of IPv6 addresses is a number 39 digits long.\n\n12.2.0.1 Special IP Addresses\nAs you work more with IP addresses, there are a few you’ll see over and over. Here’s a quick cheatsheet:\n\n\n\n\n\n\n\nCode\nMeaning\n\n\n\n\n127.0.0.1\nlocalhost or loopback – the machine that originated the request\n\n\n192.168.x.x, 172.16.x.x.x,\n10.x.x.x\nProtected address blocks used for private IP addresses. More on public vs private addresses in chapter XX."
  },
  {
    "objectID": "chapters/sec3/understanding-traffic.html#ports",
    "href": "chapters/sec3/understanding-traffic.html#ports",
    "title": "12  Computer Networks and the Internet",
    "section": "12.3 Ports",
    "text": "12.3 Ports\nTODO\nPort forwarding outside:inside\nWhy you’d want to"
  },
  {
    "objectID": "chapters/sec3/understanding-traffic.html#application-layer-protocols-define-valid-messages",
    "href": "chapters/sec3/understanding-traffic.html#application-layer-protocols-define-valid-messages",
    "title": "12  Computer Networks and the Internet",
    "section": "12.4 Application Layer Protocols define valid messages",
    "text": "12.4 Application Layer Protocols define valid messages\nIf we think of the TCP/IP protocol defining valid addresses, package sizes and shapes, and how the mail gets routed, then application layer protocols are one layer down – they define what are valid messages to put inside the envelope.\nThere are numerous application layer protocols. Some you will see in this book include SSH for direct server access, (S)FTP for file transfers, SMTP for email, LDAP(S) for authentication and authorization, and websockets for persistent bi-directional communication – used for interactive webapps created by the Shiny package in R and the Streamlit package in Python.\nWe’ll talk more about some of those other protocols later in the book. For now, let’s focus on the one you’ll spend most of your time thinking about – http.\n\n12.4.1 http is the most common application layer protocol\nHyptertext transfer protocol (http) is the protocol that underlies a huge fraction of internet traffic. http defines how a computer can initiate a session with a server, request the server do something, and receive a response.\nSo whenever you go to a website, http is the protocol that defines how the underlying interactions that happen as your computer requests the website and the server sends back the various assets that make up the web page, which might include the HTML skeleton for the site, the CSS styling, interactive javascript elements, and more.\n\nIt’s worth noting that these days, virtually all http traffic over the internet is in the form of secured https traffic. We’ll get into what the s means and how it’s secured in the next chapter.\n\nThere are a few important elements to http requests and responses:\n\nRequest Method – getting deep into HTTP request methods is beyond the scope of this book, but there are a variety of different methods you might use to interact with things on the internet. The most common are GET to get a webpage, POST or PUT to change something, and DELETE to delete something.\nStatus Code - each HTTP response includes a status code indicating the response category. Some special codes you’ll quickly learn to recognize are below. The one you’ll (hopefully) see the most is 200, which is a successful response.\nResponse and Request Headers – headers are metadata included with the request and response. These include things like the type of the request, the type of machine you’re coming from, cookie-setting requests and more. In some cases, these headers include authentication credentials and tokens, and other things you might want to inspect.\nBody - this is the content of the request or response.\n\nIt’s worth noting that GET requests for fetching something generally don’t include a body. Instead, any specifics on what is to be fetched are specified through query parameters, the part of the URL that shows up after the ?. They’re often something like, ?first_name=alex&last_name=gold\n\n\n\n\n12.4.2 Understand http traffic by inspecting it\nThe best way to understand http traffic is to take a close look at some. Luckily, you’ve got an easy tool – your web browser!\nOpen a new tab in your browser and open your developer tools. How this works will depend on your browser. In Chrome, you’ll go to View > Developer > Developer Tools and then make sure the Network tab is open.\nNow, navigate to a URL in your browser (say google.com).\nAs you do this, you’ll see the traffic pane fill up. These are the requests and responses going back and forth between your computer and the server.\nIf you click on any of them, there are a few useful things you can learn.\n\nAt the top, you can see the timing. This can be helpful in debugging things that take a long time to load. Sometimes it’s helpful to see what stage in the process bogs down.\nIn the pane below, you can inspect the actual content that is going back and forth between your computer and the server you’re accessing including the request methods, status codes, headers, and bodies.\n\n12.4.2.1 Special HTTP Codes\nAs you work more with http traffic, you’ll learn some of the common codes. Here’s a cheatshet for some of the most frequent you’ll see.\n\n\n\n\n\n\n\nCode\nMeaning\n\n\n\n\n200\nEveryone’s favorite, a successful response.\n\n\n3xx\nYour query was redirected somewhere else, usually ok.\n\n\n4xx and 5xx\nErrors with, respectively, the request itself and the server.\n\n\n\nParticular Error Codes\n\n\n400\nBad request. This isn’t a request the server can understand.\n\n\n401 and 403\nUnauthorized or forbidden. Often means required authentication hasn’t been provided.\n\n\n404\nNot found. There isn’t any content at the address you’re trying to access.\n\n\n504\ngateway timeout. This means that a proxy or gateway between you and the server you’re trying to access timed out before it got a response from the server."
  },
  {
    "objectID": "chapters/sec3/understanding-traffic.html#exercises",
    "href": "chapters/sec3/understanding-traffic.html#exercises",
    "title": "12  Computer Networks and the Internet",
    "section": "12.5 Exercises",
    "text": "12.5 Exercises\n#TODO"
  },
  {
    "objectID": "chapters/sec3/using-urls.html",
    "href": "chapters/sec3/using-urls.html",
    "title": "13  Getting a real URL",
    "section": "",
    "text": "```\n1.An IP address just specifies the location while a URL specifies location, protocol, and specific resource\n2.URL requires a DNS server while an IP address doesn’t\n3.URLs are unlimited while IP addresses are limited\n4.IP addresses and URLs have a one to many relationship\n\nRead more: Difference Between URL and IP Address | Difference Between http://www.differencebetween.net/technology/internet/difference-between-url-and-ip-address/#ixzz7GHcaYyk6\n```\nIn the last chapter, we talked about how network traffic knows where to go and what to do when it gets there. That’s all fine and dandy, but you noticed that we spoke almost entirely in terms of IP addresses. You probably almost never work with IP addresses. Instead, we’re used to visiting websites at universal resource locators (URLs), like google.com. What gives?\nA URL is a more complete description of how to get to a website and what do to with the traffic than just an IP address. In this chapter, we’ll discuss what a URL is and how each of the components is determined.\nA URL looks like this:\n\\[\n\\overbrace{\\text{http://}}^\\text{protocol}\\overbrace{\\text{example.com}}^\\text{server location}\\overbrace{\\text{:80}}^\\text{port}\\overbrace{\\text{/}}^\\text{resource}\n\\]\nNow, this probably isn’t what you type into your address bar in your browser. That’s because modern browsers do most of this for you by default. So if you type \\(google.com\\) into your browser’s address bar, your browser will automatically assume the correct defaults for the rest. Try going to https://google.com:443/. What do you get?"
  },
  {
    "objectID": "chapters/sec3/using-urls.html#using-ports-to-get-to-the-right-service",
    "href": "chapters/sec3/using-urls.html#using-ports-to-get-to-the-right-service",
    "title": "13  Getting a real URL",
    "section": "13.1 Using ports to get to the right service",
    "text": "13.1 Using ports to get to the right service\nLet’s say you want to run software on a server. One of the big differences between server software, and software on your laptop is that server software needs to be able to interact with the outside world to be useful.\nFor example, when you want to use Microsoft Word on your computer, you just click on the button and then it’s ready to go. But say I want to use RStudio Server. I don’t have a desktop where I click to open RStudio Server. Instead, I go to a particular URL and I expect that RStudio will be there, ready to listen."
  },
  {
    "objectID": "chapters/sec3/using-urls.html#special-ip-addresses-and-ports",
    "href": "chapters/sec3/using-urls.html#special-ip-addresses-and-ports",
    "title": "13  Getting a real URL",
    "section": "13.2 Special IP Addresses and Ports",
    "text": "13.2 Special IP Addresses and Ports\nAll ports below 1024 reserved.\n80 - HTTP default\n443 - HTTPS default\n22 - SSH default\nNormally, you’ll see a URL written something like this:\n\\[\nexample.com\n\\]\nIt doesn’t seem like this little sni\n\\[\n\\overbrace{https://}^{\\text{Protocol}}\\overbrace{\\underbrace{www}_{\\text{Subdomain}}.\\underbrace{example}_{\\text{Primary Domain}}.\\underbrace{com}_{\\text{Top-Level Domain}}}^{\\text{Domain}}/\\overbrace{engineering}^{\\text{Path}}\n\\]\nEven worse, IP addresses generally aren’t permanent – they can change when individual servers are replaced, or if you were to change the server architecture (say by adding and load-balancing a second instance – see chapter XX).\nAnatomy of a URL\nIn order to have something human-friendly and permanent, we access internet resources at uniform resource locators (URLs), like google.com, rather than an IP address."
  },
  {
    "objectID": "chapters/sec3/using-urls.html#getting-your-own-domain",
    "href": "chapters/sec3/using-urls.html#getting-your-own-domain",
    "title": "13  Getting a real URL",
    "section": "13.3 Getting your own domain",
    "text": "13.3 Getting your own domain\nIn the last chapter, we spent most of the time talking about server locations in terms of IP addresses. And it’s true – the “real” address of any server is its IP address. But we generally don’t access websites or other resources at IP addresses – they’re hard to remember, and they can also change over time.\nInstead, we generally use domains for websites, and hostnames for individual servers. We’ll get into hostnames later on – for now we’re going to focus on domains.\nA domain is simply a convenient alias for an IP address. The domain name system (DNS) is the decentralized internet phonebook that translates back and forth between domains and IP addresses. The details of how DNS resolution works are quite intricate – but the important thing to know is that there are layers of DNS servers that eventually return an IP address to your computer for where to find your website.\nFrom the perspective of someone trying to set up their own website, there’s only one DNS server that matters to you personally – the DNS server for your domain name registrar.\nDomain name registrars are the companies that actually own domains. You can buy or rent one from them in order to have a domain on the internet. So let’s say you take the data science server you set up in lab 1 and decide that you want to host it at a real domain.\nYour first stop would be a domain name registrar where you’d find an available domain you like and pull out your credit card.\nCosts for domain names vary widely. Buying a meaningless domain in a less popular top-level domain, say ladskfmlsdf.me can cost as little as $3 per year. On the other hand, buying a .com domain that’s a real word or phrase can be a few thousand dollars – and there are articles every few years about some major company accidentally allowing their domain name to lapse and ransoming it back for stupid amounts of money.\nSo, conceptually, it’s easy to understand how a domain comes to stand in for an IP address, with DNS being the directory that ties the two together.\n\n13.3.1 Configuring DNS to connect IP addresses and Domains\nThe harder part is the nitty gritty of how you accomplish that mapping yourself, which we’ll get into now.\nConfiguration of DNS is done by way of records, of which there are a menagerie of types you can configure. Luckily, most simple configurations only need CNAME and A records.\nHere’s an imaginary DNS record table for the domain example.com:\n\n\n\nPath/Host\nType\nTarget\n\n\n\n\n@\nA\n143.122.8.32\n\n\nwww\nCNAME\nexample.com\n\n\n*\nA\n143.122.8.33\n\n\n\nLet’s go through how to read this table.\nSince we’re configuring example.com, the paths/hosts in this table are relative to example.com.\nIn the first row we’re configuring an A record to go to the target IP address. A records (or their IPv6 cousin AAAA records) map a domain to an actual IP address. The path @ is a special symbol meaning exact match. So by this configuration, any traffic to example.com will be passed straight through to the specified IP address.\nThe second row deals with traffic to the www subdomain. CNAME records alias sub-domains. They’re most frequently used to map sub-domains to the main domain. Since this is a CNAME record for example.com, this record indicates that traffic to www.example.com should be treated exactly like traffic to example.com. Some domain providers do automatic redirection of www traffic, and so this row may not be necessary in some configurations.\nThe last record uses the wildcard symbol * to send all subdomain traffic that’s not already spoken for – say blog.example.com or info.example.com directly to the IP address specified. In this case, I’m sending all of those subdomains to a different IP address, maybe a 404 (not found) page – or maybe I’m serving all the subdomains off a different server.\nSo what happens is that your query goes through several layers of public DNS servers to get to the DNS entry for your domain name registrar. In many cases, you’ll directly configure your domain name registrar to point to your website or server – but you also can configure the domain name registrar to point at another set of DNS servers you actually control with an NS record.\nIf you’re setting up your own server, this probably isn’t the case, but some large enterprises do run their own private DNS servers.\n\nYou should always configure your domain provider as narrowly as possible – and you should configure your website or server first.\n#TODO: why?\n\n\n\n13.3.2 Learning to Hate DNS\nAs you get deeper into using servers, you will learn to hate DNS with a fiery passion. While it’s necessary so we’re not running around trying to remember incomprehensible IP addresses, it’s also very hard to debug as a server admin.\nLet’s say I’ve got the public domain example.com, and I’m taking down the server and putting up a new one. I’ve got to alter the public DNS record so that everyone going to example.com gets routed to the new IP address, and not the old one.\nThe thing that makes it particularly challenging is that the DNS system is decentralized. There are thousands of public DNS servers that a request could get routed to, and many of them may need updating.\nObviously, this is a difficult problem to solve, and it can take up to 24 hours for DNS changes to propagate across the network. So making changes to DNS records and checking if they’ve worked is kinda a guessing game of whether enough time has passed that you can conclude that your change didn’t work right, or if you should just wait longer.\nTo add an additional layer of complexity, DNS lookups are slow, so your browser caches the results of DNS lookups it has done before. That means that it’s possible you’ll still get an old website even once the public DNS record has been updated. If a website has ever not worked for you and then worked when you tried a private browser, DNS caching is likely the culprit. Using a private browsing window sidesteps your main DNS cache and forces lookups to happen afresh.\n\n\n13.3.3 Trying it out\nGo through hosting this book somewhere."
  },
  {
    "objectID": "chapters/sec3/using-urls.html#exercises",
    "href": "chapters/sec3/using-urls.html#exercises",
    "title": "13  Getting a real URL",
    "section": "13.4 Exercises",
    "text": "13.4 Exercises\n\nFind a cheap domain you like and buy it.\nPut an EC2 server back up with the Nginx hello-world example.\nConfigure your server to be available at your new domain.\n\nHint: In AWS, Route 53 is the service that handles incoming networking. They can serve as a domain name registrar, or you can buy a domain elsewhere and just configure the DNS using Route 53."
  },
  {
    "objectID": "chapters/sec3/using-urls.html#securing-traffic-with-https",
    "href": "chapters/sec3/using-urls.html#securing-traffic-with-https",
    "title": "13  Getting a real URL",
    "section": "13.5 Securing Traffic with https",
    "text": "13.5 Securing Traffic with https\nWhen you go to a website on the internet, you’ll see the URL prefixed by the https (though it’s sometimes hidden by your browser because it’s assumed). https is actually a mashup that is short for http with secure sockets layer (SSL).\nThese days, almost everyone actually uses the successor to SSL, transport layer security (TLS). However, because the experience of configuring TLS is identical to SSL, admins usually just talk about configuring SSL even when they mean TLS.\nThese days, almost every bit of internet traffic is actually https traffic. You will occasionally see http traffic inside private networks where encryption might not be as important – but more and more organizations are requiring end-to-end use of SSL.\nSecuring your website or server using SSL/TLS is one of the most basic things you can do to make sure your website traffic is safe. You should always configure https – full stop.\nSSL/TLS security is accomplished by configuring your site or server to use a SSL certificate (often abbreviated to cert). We’ll go through the details of how to get and configure an SSL certificate in this chapter – but first a little background on how SSL/TLS works.\n\n13.5.1 How SSL/TLS Enhances Security\nSSL accomplishes two things for you – identity validation and traffic encryption.\nWhen you go to a website, SSL/TLS is the technology that verifies that you’re actually reaching the website you think you’re reaching. This prevents something called a man-in-the-middle attack where a malicious actor manages to get in between the server and the client of network traffic. So, for example, you might think you’re putting your bank login information into your normal bank website, but there’s a hacker sitting in the middle, reading all of the traffic back and forth.\n[TODO: Image of man-in-the-middle]\nYou can see this in action in your web browser. When you go to a website protected by https, you’ll see a little lock icon to the left of the URL. That means that this website’s SSL certificate matches the website and therefore your computer can verify you’re actually at the website you mean to be at.\nBut how does your computer know what a valid SSL certificate is? Your computer has a list of trusted Certificate Authorities (CAs) who create, sell, and validate SSL/TLS certificates. So when you navigate to a website, the website sends back a digital signature. Your computer checks the signature against the indicated CA to verify that it was issued to the site in question.\n[TODO: image of SSL validation]\nThe second type of scary scenario SSL prevents is a snooping/sniffing attack. Even if you’re getting to the right place, your traffic travels through many different channels along the way – routers, network switches, and more. This means that someone could theoretically look at all your traffic along the way to its meaningful destination.\nWhen your computer gets back the digital signature to verify the site’s identity, it also prompts an exchange of encryption keys. These keys are used to encrypt traffic back and forth between you and the server so anyone snooping on your message will just see garbled nonsense and not your actual content. You can think of the SSL/TLS encryption as the equivalent of writing a message on a note inside an envelope, rather than on a postcard anyone could read along the way.\n\n\n13.5.2 Getting a cert of your own\nIn order to configure your site or server with SSL, there are three steps you’ll want to take: getting an SSL certificate, putting the certificate on the server, and making sure the server only accepts https traffic.\nYou can either buy an SSL certificate or make one yourself, using what’s called a self-signed cert.\nThere are a variety of places you can buy an SSL/TLS certificate, in many cases, your domain name registrar can issue you one when you buy your domain.\nWhen you create or buy your cert, you’ll have to choose the scope. A basic SSL certificate covers just the domain itself, formally known as a fully qualified domain name (FQDN). So if you get a basic SSL certificate for www.example.com, www.blog.example.com will not be covered. You can get a wildcard certificate that would cover every subdomain of *.example.com.\n\nNote that basic SSL/TLS certification only validates that when you type example.com in your browser, that you’ve gotten the real example.com. It doesn’t in any way validate who owns example.com, whether they’re reputable, or whether you should trust them.\nThere are higher levels of SSL certification that do validate that, for example, the company that owns google.com is actually the company Google.\n\nBut sometimes it’s not feasible to buy certificates. While a basic SSL certificate for a single domain can cost $10 per year or less, wildcard certificates will all the bells and whistles can cost thousands per year. This can get particularly expensive if you’ve got a lot of domains for some reason.\nMoreover, there are times when you can’t buy a certificate. If you’re encrypting traffic inside a private network, you will need certificates for hosts or IP addresses that are only valid inside the private network, so there’s no public CA to validate them.\nThere are two potential avenues to follow. In some cases, like inside a private network, you want SSL/TLS for the encryption, but don’t really care about the identity validation part. In this case, it’s usually possible to skip that identity validation part and automatically trust the certificate for encryption purposes.\nIt’s also possible to create your own private CA, which would verify all your SSL certificates. This is pretty common in large organizations. At some point, every server and laptop needs to have the private CA added to its set of trusted certificate validators.\nA warning: it is deceptively easy to generate and configure a self-signed SSL certificate. It’s usually just a few lines of shell commands to create a certificate, and adding the certificate to your server or website is usually just a copy/paste affair.\nHowever, it’s pretty common to run into problems with self-signed certs or private CAs. Making sure the certificate chain is correct, or running into a piece of software that doesn’t ignore the identity validation piece right is pretty common. This shouldn’t dissuade you from using SSL/TLS. It’s an essential, and basic, component of any security plan – but using a self-signed cert probably isn’t as easy as it seems.\nWhen you configure your site or server, there will likely be an option to redirect all http traffic to https traffic. If your server or site is open to the internet, you should set this option."
  },
  {
    "objectID": "chapters/sec3/tooling.html",
    "href": "chapters/sec3/tooling.html",
    "title": "14  DevOps for DevOps",
    "section": "",
    "text": "If you’re reading this section and trying it out, you’ve moved past DevOps for Data Science. This is just plain ’ol DevOps. In this chapter, you’ll learn about some standard DevOps tooling you might want to use, and some ways to think about how to divide your work as a data scientist from your (forced) moonlighting as a DevOps engineer."
  },
  {
    "objectID": "chapters/sec3/tooling.html#infrastructure-as-code-tooling",
    "href": "chapters/sec3/tooling.html#infrastructure-as-code-tooling",
    "title": "14  DevOps for DevOps",
    "section": "14.1 Infrastructure As Code Tooling",
    "text": "14.1 Infrastructure As Code Tooling\nThere are many, many varieties of infrastructure as code tooling. There are many books on infrastructure as code tooling and I won’t be covering them in any depth here. Instead, I’ll share a few of the different “categories” (parts of the stack) of infrastructure as code tooling and suggest a few of my favorites.\nTo get from “nothing” to a usable server state, there are (at minimum) two things you need to do – provision the infrastructure you need, and configure that infrastructure to do what you want.\nFor example, let’s say I’m standing up a server to deploy a simple shiny app. In order to get that server up, I’ll need to stand up an actual server, including configuring the security settings and networking that will allow the proper people to access the server. Then I’ll need to install a version of R on the server, the Shiny package, and a piece of hosting software like Shiny Server.\nSo, for example, you might use AWS’s CloudFormation to stand up a virtual private cloud (VPC), put an EC2 server instance inside that VPC, attach an appropriately-sized storage unit, and attach the correct networking rules. Then you might use Chef to install the correct software on the server and get your Shiny app up-and-running.\nIn infrastructure as code tooling, there generally isn’t a clear dividing line between tools that do provisioning and tools that do configuration…but most tools lean one way or the other.\nBasically any tool does provisioning will directly integrate into the APIs of the major cloud providers to make it easy to provision cloud servers. Each of the cloud providers also has their own IaC tool, but many people prefer to use other tools when given the option (to be delicate).\nThe other important division in IaC tools is declarative vs imperative. In declarative tooling, you simply enumerate the things you want, and the tool makes sure they get done in the right order. In contrast, an imperative tool requires that you provide actual instructions to get to where you want to go.\nIn many cases, it’s easy to be declarative with provisioning servers, but it’s often useful to have a way to fall back to an imperative mode when configuring them because there may be dependencies that aren’t obvious to the provisioning tool, but are easy to put down in code. If the tool does have an imperative mode, it’s also nice if it’s compatible with a language you’d be comfortable with.\nOne somewhat complicated addition to the IaC lineup is Docker and related orchestration tools. There’s a whole chapter on containerization and docker, so check that out if you want more details. The short answer is that docker can’t really do provisioning, but that you can definitely use docker as a configuration management IaC tool, as long as you’re disciplined about updating your Dockerfiles and redeployment when you want to make changes to the contents.\nBasically none of these tools will save you from your own bad habits, but they can give you alternatives.\nIn short, exactly which tool you’ll need will depend a lot on what you’re trying to do. Probably the most important question in choosing a tool is whether you’ll be able to get help from other people at your organization on it. So if you’re thinking about heading into IaC tooling, I’d suggest doing a quick survey of some folks in DevOps and choosing something they already know and like."
  },
  {
    "objectID": "chapters/sec3/tooling.html#devtestprod-for-itadmin",
    "href": "chapters/sec3/tooling.html#devtestprod-for-itadmin",
    "title": "14  DevOps for DevOps",
    "section": "14.2 Dev/Test/Prod for IT/Admin",
    "text": "14.2 Dev/Test/Prod for IT/Admin\nIn many organizations, the entire data science stack is supported by the IT/Admin group. In this case, you probably want a two-dimensional Dev/Test/Prod setup. The IT/Admin group maintains their own Dev/Test/Prod configuration.\nDev and Test are where they try out and test new hardware configurations – the data scientists doing their work only have access to the Prod environment. They have their own Dev/Test/Prod setup within the IT/Admin prod environment.\nFor simplicity of terminology, I often refer to the IT/Admin Dev + Test environments as staging to differentiate from the Data Science Dev/Test/Prod environments.\nIn this setup, you would select the IT configuration that works for your organization and maintain one or two copies of the entire environment. I often call this a staging environment to differentiate it from the dev/test/prod environments for the data science assets.\nSo when you wanted to make a chance to the underlying servers or their architecture, that would be tested in the staging environment and then deployed to production. Data scientists would never work in the staging environment (except as testers), that’s purely for IT/Admin testing. The staging environment would include all of the environments data scientists would use – dev, test, and prod.\nThen, data science code promotion through dev/test/prod would be distinct from how server changes get made."
  },
  {
    "objectID": "chapters/append/linux-cmd.html",
    "href": "chapters/append/linux-cmd.html",
    "title": "Appendix A — Useful Shell Commands",
    "section": "",
    "text": "Unlike on your Windows or Mac desktop, most work on a server is done via the shell – an interactive text-based prompt. If you master the set of commands below, you’ll always feel like a real hacker.\nIn most cases, you’ll be using the bash shell. If you’re using Linux or Mac, that’ll be the default. Below, I’m intentionally mixing up bash commands and Linux system commands because they’re useful. If you know the difference and are pedantic enough to care, this list isn’t for you anyway."
  },
  {
    "objectID": "chapters/append/linux-cmd.html#miscellaneous-symbols",
    "href": "chapters/append/linux-cmd.html#miscellaneous-symbols",
    "title": "Appendix A — Useful Shell Commands",
    "section": "A.1 Miscellaneous Symbols",
    "text": "A.1 Miscellaneous Symbols\n\n\n\n\n\n\nSymbol\n\n\n\n\nWhat it is\n\n\n\n\nHelpful options\n\n\n\n\nExample\n\n\n\n\n\n\n\n\n/\n\n\n\n\nsystem root\n\n\n\n\n\n\n\n\n\n\n~\n\n\n\n\nyour home directory\n\n\n\n\n\n\necho ~\n\n\n/ home/alex.gold\n\n\n\n\n\n\n.\n\n\n\n\ncurrent working directory\n\n\n\n\n\n\n\n\n\n\nman\n\n\n\n\nmanual\n\n\n\n\n\n\n\n\n\n\n|\n\n\n\n\nthe pipe\n\n\n\n\n\n\n\n\n\n\necho\n\n\n\n\n\n\n\n\n\n\n\n\n$\n\n\n\n\n\n\n\n\n\n\n\n\nsudo\n\n\n\n\n\n\n\n\n\n\n\n\nsu"
  },
  {
    "objectID": "chapters/append/linux-cmd.html#moving-yourself-and-your-files",
    "href": "chapters/append/linux-cmd.html#moving-yourself-and-your-files",
    "title": "Appendix A — Useful Shell Commands",
    "section": "A.2 Moving yourself and your files",
    "text": "A.2 Moving yourself and your files\n\n\n\n\n\n\nC ommand\n\n\n\n\nWhat it does\n\n\n\n\nHelpful options\n\n\n\n\nExample\n\n\n\n\n\n\n\n\npwd\n\n\n\n\nprint working directory\n\n\n\n\n\n\n$ pwd\n\n\n/U sers/alex.gold/\n\n\n\n\n\n\ncd\n\n\n\n\nchange directory\n\n\n\n\n\n\n$ cd ~/Documents\n\n\n\n\n\n\nls\n\n\n\n\nlist\n\n\n\n\n-l - format as list\n\n\n-a - all include hidden files\n\n\n\n\n$ ls .\n\n\n$ ls -la\n\n\n\n\n\n\nrm\n\n\n\n\nremove delete permanently!\n\n\n\n\n-r - recursively a directory and included files\n\n\n-f - force - don’t ask for each file\n\n\n\n\n$ rm old_doc\n\n\nr m -rf old_docs/\n\n\nBE VERY CAREFUL WITH -rf\n\n\n\n\n\n\ncp\n\n\n\n\ncopy\n\n\n\n\n\n\n\n\n\n\nmv\n\n\n\n\nmove\n\n\n\n\n\n\n\n\n\n\nchmod"
  },
  {
    "objectID": "chapters/append/linux-cmd.html#checking-out-files",
    "href": "chapters/append/linux-cmd.html#checking-out-files",
    "title": "Appendix A — Useful Shell Commands",
    "section": "A.3 Checking out Files",
    "text": "A.3 Checking out Files\nOften useful in server contexts for reading log files.\n\n\n\nC ommand\nWhat it does\nHelpful options\nExample\n\n\n\n\ncat\n\n\n\n\n\nless\n\n\n\n\n\ntail\n\n-f\n\n\n\ngrep\n\n\n\n\n\ntar"
  },
  {
    "objectID": "chapters/append/linux-cmd.html#checking-out-server-activity",
    "href": "chapters/append/linux-cmd.html#checking-out-server-activity",
    "title": "Appendix A — Useful Shell Commands",
    "section": "A.4 Checking out Server Activity",
    "text": "A.4 Checking out Server Activity\n\n\n\nC ommand\nWhat it does\nHelpful options\nExample\n\n\n\n\ndf\n\n-h\n\n\n\ntop\n\n\n\n\n\nps\n\n\n\n\n\nlsof"
  },
  {
    "objectID": "chapters/append/linux-cmd.html#checking-out-networking",
    "href": "chapters/append/linux-cmd.html#checking-out-networking",
    "title": "Appendix A — Useful Shell Commands",
    "section": "A.5 Checking out Networking",
    "text": "A.5 Checking out Networking\n\n\n\nC ommand\nWhat it does\nHelpful options\nExample\n\n\n\n\nping\n\n\n\n\n\nne tstat\n\n\n\n\n\ncurl"
  },
  {
    "objectID": "chapters/append/linux-cmd.html#user-management",
    "href": "chapters/append/linux-cmd.html#user-management",
    "title": "Appendix A — Useful Shell Commands",
    "section": "A.6 User Management",
    "text": "A.6 User Management\n\n\n\nC ommand\nWhat it does\nHelpful options\nExample\n\n\n\n\nw hoami\n\n\n\n\n\np asswd\n\n\n\n\n\nus eradd"
  },
  {
    "objectID": "chapters/append/docker-cheatsheet.html",
    "href": "chapters/append/docker-cheatsheet.html",
    "title": "Appendix B — Docker Cheatsheet",
    "section": "",
    "text": "Command\nPurpose\nExample\n\n\n\n\ndocker run\nRun an image as a container\ndocker run me/my-image\n\n\ndocker ps\nList all containers\ndocker ps\n\n\ndocker kill\nKill a container\ndocker kill my-container\n\n\ndocker exec\nRun a command inside a running container\ndocker exec -it /bin/bash\n\n\ndocker build\nBuild a Dockerfile\ndocker built -t me/my-image .\n\n\ndocker logs\nGet logs from a container\ndocker logs my-container\n\n\ndocker pull\nPull a container from a registry\ndocker pull me/my-image\n\n\ndocker push\nPush a container to a registry\ndocker push me/my-image"
  },
  {
    "objectID": "chapters/append/docker-cheatsheet.html#docker-run-command-flags",
    "href": "chapters/append/docker-cheatsheet.html#docker-run-command-flags",
    "title": "Appendix B — Docker Cheatsheet",
    "section": "B.2 docker run command flags",
    "text": "B.2 docker run command flags\n\n\n\n\n\n\n\n\nFlag\nPurpose\nExample\n\n\n\n\n-d\nRun in “detached” mode that doesn’t block your terminal\ndocker run -d ...\n\n\n--rm\nRemove the container on stop\nReminder: don’t use in prod\ndocker run --rm …\n\n\n-p\nPublish ports from container to host\ndocker run -p 8000:8000 …\n\n\n-v\nMount a volume into the container\ndocker run -v $(pwd):/data\n\n\n--name\nGive container a human-friendly name\ndocker run --name my-container\n\n\n\nReminder - -p and -v order is <host>:<container>"
  },
  {
    "objectID": "chapters/append/docker-cheatsheet.html#dockerfile-commands",
    "href": "chapters/append/docker-cheatsheet.html#dockerfile-commands",
    "title": "Appendix B — Docker Cheatsheet",
    "section": "B.3 Dockerfile Commands",
    "text": "B.3 Dockerfile Commands\nThese are the commands that go in a Dockerfile when you’re building it.\n\n\n\n\n\n\n\n\nCommand\nPurpose\nExample\n\n\n\n\nFROM\nIndicate base container\nFROM rocker/r-ver:4.1.0\n\n\nRUN\nRun a command when building\nRUN apt-get update\n\n\nCOPY\nCopy from the working directory into the container\nCOPY . /app/\n\n\nCMD\nSpecify the command to run when the container starts\nCMD quarto render ."
  }
]