[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DevOps for Data Science",
    "section": "",
    "text": "Welcome!\nThis is the website for the book DevOps for Data Science, currently in draft form.\nIn this book, you’ll learn about DevOps conventions, tools, and practices that can be useful to you as a data scientist. You’ll also learn how to work better with the IT/Admin team at your organization, and even how to do a little server administration of your own if you’re pressed into service.\nThis website is (and always will be) free to use, and is licensed under the Creative Commons Attribution-NonCommercial-NoDerivs 3.0 license. If you’d like a physical copy of the book, they will be available once it’s finished!"
  },
  {
    "objectID": "index.html#software-information-and-conventions",
    "href": "index.html#software-information-and-conventions",
    "title": "DevOps for Data Science",
    "section": "Software information and conventions",
    "text": "Software information and conventions\nI used the knitr package [@xie2015] and the quarto package [@quarto] to compile my book.\nPackage names are in bold text (e.g., rmarkdown), and inline code and filenames are formatted in a typewriter font (e.g., knitr::knit('foo.Rmd')). Function names are followed by parentheses (e.g., bookdown::render_book())."
  },
  {
    "objectID": "index.html#about-the-author",
    "href": "index.html#about-the-author",
    "title": "DevOps for Data Science",
    "section": "About the Author",
    "text": "About the Author\nAlex Gold leads the Solutions Engineering team at RStudio.\nHe works with customers of RStudio’s professional software to help them deploy, configure, and use RStudio’s professional software and open source tooling in R and Python.\nIn his free time, he enjoys landscaping, handstands, and Tai Chi.\nHe occasionally blogs about data, management, and leadership at alexkgold.space."
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "DevOps for Data Science",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nA lot of people are helping me write this book.\nThis book is published to the web using GitHub Actions from rOpenSci."
  },
  {
    "objectID": "index.html#color-palette",
    "href": "index.html#color-palette",
    "title": "DevOps for Data Science",
    "section": "Color palette",
    "text": "Color palette\nTea Green: CAFFDO Steel Blue: 3E7CB1 Kombu Green: 273c2c Bright Maroon: B33951 Sandy Brown: FCAA67"
  },
  {
    "objectID": "chapters/intro.html#a-short-history-of-devops",
    "href": "chapters/intro.html#a-short-history-of-devops",
    "title": "Introduction",
    "section": "A short history of DevOps",
    "text": "A short history of DevOps\nHere’s the one sentence definition: DevOps is a set of cultural norms, practices, and supporting tooling to help make the process of developing and deploying software smoother and lower risk.\nIf you feel like that definition is pretty vague and unhelpful, you’re right. Like Agile software development, to which it is closely related, DevOps is a squishy concept. That’s partially because DevOps isn’t just one thing – it’s the application of some principles and process ideas to whatever context you’re actually working in. That malleability is one of the great strengths of DevOps, but it also makes the concept quite squishy.\nThis squishiness is furthered by the ecosystem of companies enabling DevOps. There are dozens and dozens of companies proselytizing their own particular flavor of DevOps – one that (curiously) reflects the capabilities of whatever product they’re selling.\nBut underneath the industry hype and the marketing jargon, there are some extremely valuable lessons to take from the field.\nTo understand better, let’s go back to the birth of the field.\nThe Manifesto for Agile Software Development was originally published in 2001. Throughout the 1990s, software developers had begun observing that delivering software in small units, quickly collecting feedback, and iterating was an effective model. After that point, many different frameworks of actual working patterns were developed and popularized.\nHowever, many of these frameworks were really focused on software development. What happened once the software was written?\nHistorically, IT Administrators managed the servers, networking, and workstations needed to deploy, release, and operate that software. So, when an application was complete (or perceived as such), it was hurled over the wall from Development to Operations. They’d figure out the hardware and networking requirements, check that it was performant enough, and get it going in the real world.\nThis pattern is very fragile and subject to many errors, and it quickly became apparent that the Agile process – creating and getting feedback on small, iterative changes to working software – needed a complementary process to get that software deployed and into production.\nDevOps arose as this discipline – a way for software developers and the administrators of operational software to better collaborate on making sure the software being written was making it reliably and quickly into production. It took a little while for the field to be formalized, and the term DevOps came into common usage around 2010."
  },
  {
    "objectID": "chapters/intro.html#those-who-do-devops",
    "href": "chapters/intro.html#those-who-do-devops",
    "title": "Introduction",
    "section": "Those who do DevOps",
    "text": "Those who do DevOps\nThroughout this book, I’ll use two different terms – DevOps and IT/Admin – and though they may sound similar, I mean very different things by them.\nDevOps refers to the knowledge, practices, and tools that make it easier, safer, and faster to put work into production. So, if you’re a software developer (and as a data scientist, you are) you need to be thinking about DevOps.\nMost organizations also have a set of people and roles who have the permission and responsibility for managing the servers and computers at your organization. I’m going to refer to this group as IT/Admins. Their names vary widely by organization – they might be named Information Technology (IT), SysAdmin, Site Reliability Engineering (SRE), or DevOps.1\nDepending on what you’re trying to accomplish, the relevant IT/Admins may change. For example, if you’re trying to get access to a particular database, the relevant IT/Admins may be a completely different group of people than if you’re trying to procure a new server.\nFundamentally, DevOps is about creating good patterns for people to collaborate on developing and deploying software. As a data scientist, you’re on the Dev side of the house, and so a huge part of making DevOps work at your organization is about finding some Ops counterparts with whom you can develop a successful collaboration. There are many different organizational structures that support collaboration between data scientists and IT/Admins.\nHowever, I will point out three patterns that are almost always red flags – mostly because they make it hard to develop relationships that can sustain the kind of collaboration DevOps neccesitates. If you find yourself in these situations, you’re not doomed – you can still get things done. But progress is likely to be slow.\n\nAt some very large organizations, IT/Admin functions are split into small atomic units like security, databases, networking, storage, procurement, cloud, and more. This is useful for keeping the scope-of-work manageable for the people in that group – and often results in super deep expertise within the group – but also means that you’ll need to bring people together from disparate teams to actually get anything done. And even when you find the person who can help you with one task, they’re probably not the right person to help you with anything else. They may not even know who is.\nSome organizations have chosen to outsource their IT/Admin functions. This isn’t a problem per-se – the people who work for outsourced IT/Admin companies are often very competent, but it does indicate a lack of commitment at your organization. The main issues in this case tend to be logistical. Outsourced IT/Admin teams are often in India, so it can be hard to find meeting times with American and European teams. Additionally, turnover on projects and systems tends to be very high at outsourced IT/Admin organizations. That means it can be really hard to find anyone who’s an expert on a particular system – or to be able to go back to them once you’ve found them.\nAt some very small organizations, there isn’t yet an IT/Admin function. And at others, the IT/Admins are preoccupied with other tasks and don’t have the capacity to help the data science team. This isn’t a tragedy, but it probably means you’re about to become your own IT/Admin. If your organization is pretty happy to let you do what you want, you’ve picked up this book, so you’re in the right place. If you’re going to have to fight for money to get servers, that’s an uphill battle.\n\nWhether your organization has an IT/Admin setup that facilitates DevOps best practices or not, hopefully this book can help you take the first steps towards making your path to production smoother and simpler.\nTODO: Ways to ameliorate red flags"
  },
  {
    "objectID": "chapters/intro.html#whats-in-this-book",
    "href": "chapters/intro.html#whats-in-this-book",
    "title": "Introduction",
    "section": "What’s in this book?",
    "text": "What’s in this book?\nMy hope for this book is twofold.\nFirst, I’d like to share some patterns.\nDevOps is a well-developed field in its own right. However, a simple 1-1 transposition of DevOps practices from traditional software to data science would be a mistake. Over the course of engaging with so many organizations at RStudio, I’ve observed some particular patterns, borrowed from traditional DevOps, that work particularly well to grease the path to production for data scientists.\nHopefully, by the time you’re done with this book, you’ll have a pretty good mental model of some patterns and principles you can apply in your own work to make deployment more reliable. That’s what’s in the first section of this book.\nSecond, I want to equip you with some technical knowledge.\nIT administration is an older field than DevOps or data science, full of arcane language and technologies. My hope in this book is to equip you with the vocabulary to talk to the IT/Admins at your organization and the (beginning of) skills you’ll need if it turns out that you need to DIY a lot of what you’re doing.\nThe second section is the hands-on section for anyone who’s administering a data science server for themselves. In this section, we’re going to DIY a data science environment. If you have to, this could be an environment you actually work in. If not, this section will help you learn the language and tools to better collaborate with the IT/Admins at your organization.\nThe final section is about taking the DIY environment we’ll set up in section 2 and making it enterprise-grade. Hopefully, if you have enterprise requirements, you also have enterprise IT support, so this section will be more focused on the conceptual knowledge and terminology you’ll need to productively interact with the IT/Admins who are (hopefully) responsible for helping you. And if you don’t have IT/Admins to help you, it will at least give you some terms to google.\n\nQuestions, Portfolio Exercises, and Labs\nFor each chapter, I’ll provide a chance for you to check if you’ve understood the content. Sections 1 and 3 are mostly informational, so in those sections, I’ll provide comprehension questions at the end of each chapter.\n\n\n\n\n\n\nMental Models + Mental Maps\n\n\n\nThroughout the book, I’ll talk a lot about building a mental model. What a mental model is is reasonably intuitive, but just to make clear – a mental model is a model in your head of various entities and how they relate. Many of the chapters in this book will be about helping you build mental models of various DevOps concepts and how they relate to Data Science.\nA mental map is a particularly helpful way to represent mental models. Because I believe generating mental maps is a great test of mental models, I’ll suggest them as exercises in a number of places.\nIn a mental map, you’ll draw out each of the entities I suggest and connect them with arrows. Each node will be a noun, and you’ll write the relationship between entities in the arrows.\nHere’s an example about this book:\n\n\n\n\ngraph LR\n    A[I] -->|Wrote| B[DO4DS]\n    C[You] --> |Are Reading| B\n    B --> |Includes| D[Exercises]\n    D --> |Some Are| E[Mind Maps]\n\n\n\n\n\n\n\n\nNote how every node is a noun, and the edges (labels on the arrows) are verbs. It’s pretty simple! But writing down the relationships between entities like this is a great check on understanding.\n\n\nIn many of the chapters, I’ll include a Portfolio Exercise. These are exercises that you can complete to demonstrate that you’ve really understood the content of the chapter. If you’re looking for a job and you complete these portfolio exercises, you’ll have a great bit of work that shows that you know how to get data science into production.\nSection 2 is all about creating your own data science workbench, so each of the chapters will include a lab – a walkthrough of setting up your data science workbench. By the end of the chapter, you’ll have a place where you can do actual data science work in AWS.\n\nChapter List (FOR DEV PURPOSES ONLY, WILL BE REMOVED):\nSection 1: DevOps for DS\n\nCode Promotion\nEnvironments as Code\nProject Components\nLogging and Monitoring\nDocker for Data Science\n\nSection 2: DIY Data Science Workbench\n\n\n\n\n\n\n\n\nNumber\nExplain\nLab\n\n\n\n\n1\nCloud\nAWS Console + Get Instance\n\n\n2\nCommand Line + SSH\nSSH into server\n\n\n3\nLinux SysAdmin\nInstall R, Py, RS, JH\n\n\n4\nNetworking, DNS, SSL\nURL, SSL\n\n\n5\nHow servers work + Choosing the right one\nTake down instance + attach to a bigger one\n\n\n\nSection 3: Steps not Taken\n\nCode Promotion for DevOps, Dev/Test/Prod, Docker\nBetter Networking (Proxies, Bastion)/Offline\nAuth Integrations\nScaling your servers"
  },
  {
    "objectID": "chapters/sec1/1-0-sec-intro.html#the-problems-devops-solves",
    "href": "chapters/sec1/1-0-sec-intro.html#the-problems-devops-solves",
    "title": "DevOps Lessons for Data Science",
    "section": "The Problems DevOps Solves",
    "text": "The Problems DevOps Solves\nDevOps started as an offshoot of the Agile software movement. In particular, Agile’s focus on quick iteration via frequent delivery of small chunks and immediate feedback proved completely incompatible with a pattern where developers completed software and hurled it over an organizational wall to somehow be put into production by an IT/Admin team.\nThere are a few particular problems that DevOps attempted to solve – problems that will probably feel familiar if you’ve ever tried to put a data science asset into production.\nThe first issue DevOps addresses is the “works on my machine” phenomenon. If you’ve ever collaborated on a piece of data science code, you’ve almost certainly gotten an email, instant message, or quick shout that some code that was working great for you is failing now that your colleague is trying to work on it to collaborate.\nThe processes and tooling of DevOps is designed to link application much more closely to environment in order to prevent the “works on my machine” phenomenon from rearing its head.\nThe second problem DevOps addresses is the “breaks on deployment” issue. Perhaps you wrote some code and tested it lovingly on your machine, but didn’t have the chance to test it against a production configuration. Or perhaps you don’t really have patterns around testing code in your organization. Even if you tested thoroughly, you might not know if something breaks when its deployed. DevOps is designed to reduce the risk of deploying code that won’t function as intended the first time it’s deployed.\nDevOps is designed to incorporate ideas about scaling into the genesis of software, helping avoid software that works fine locally, but can’t be deployed for real."
  },
  {
    "objectID": "chapters/sec1/1-0-sec-intro.html#core-principles-and-best-practices-of-devops",
    "href": "chapters/sec1/1-0-sec-intro.html#core-principles-and-best-practices-of-devops",
    "title": "DevOps Lessons for Data Science",
    "section": "Core principles and best practices of DevOps",
    "text": "Core principles and best practices of DevOps\nAs I’ve mentioned, the term DevOps is squishy. So squishy that there isn’t even agreeement on what the basic tenets of DevOps are that help solve the problems its attempting to solve. Basically every resource on DevOps lists a different set of core principles and frameworks.\nAnd the profusion of xOps like DataOps, MLOps, and more just add confusion about what DevOps itself actually is.\nI’m going to name five core tenets of DevOps. Some lists of DevOps have more components, and some fewer, but this is a good-faith attempt to summarize what I believe the core components are.\n\nCode should be well-tested and tests should be automated.\nCode updates should be frequent and low-risk.\nSecurity concerns should be considered up front as part of architecture.\nProduction systems should have monitoring and logging.\nFrequent opportunities for reviewing, changing, and updating process should be built into the system – both culturally and technically.\n\nThese five tenets are a great philosophical stance, they’re about things that should happen, and they seem pretty inarguably good.1 But they’re also kinda vague and it’s not clear how to go from them to a your actual day-to-day work."
  },
  {
    "objectID": "chapters/sec1/1-0-sec-intro.html#applying-devops-to-data-science",
    "href": "chapters/sec1/1-0-sec-intro.html#applying-devops-to-data-science",
    "title": "DevOps Lessons for Data Science",
    "section": "Applying DevOps to data science",
    "text": "Applying DevOps to data science\nHopefully, you’re convinced that the principles of DevOps are relevant to you as a data scientist and you’re excited to learn more!\nHowever, it would be inappropriate to just take the DevOps principles and practices and apply them to data science.\nAs a data scientist, the huge majority of what you’re doing is taking data generated by a business process, deriving some sort of signal from that data flow, and making it available to other people or other software. Fundamentally, data science apps are consumers of data, almost by definition.\nIn contrast, most traditional pieces of software either don’t involve meaningful data flows, or are producers of business data. An online store, software for managing inventory, and electronic health records – these tools all produce data.\nThere’s a major architectural and process implication from this difference – how much freedom you have. Software engineers get to dream up data structures and data flows from scratch, designing them to work optimally for their systems. In contrast, you are stuck with the way the data flows into your system – most likely designed by someone who wasn’t thinking about the needs of data science at all.\n\n\n\n\n\n\nLanguage-specific tooling\n\n\n\nThere’s one other important difference between data science and general purpose software development. As of the writing of this book, a huge majority of data science work is done in just two programming languages, R and Python (and SQL). For that reason, this book on DevOps for Data Science can get much deeper into the particularities of applying DevOps principles to those specific languages than a general purpose book on DevOps ever would.\n\n\nSo while the problems DevOps attempts to solve will probably resonate with most data scientists, and the core principles seem equally applicable, the technical best practices need some translation.\nSo here are four technical best practices from DevOps and their equivalents in the data science world.2\n\nUse CI/CD\nContinuous Integration/Continuous Delivery/Continuous Deployment (CI/CD) is the notion that there should be a central repository of code where changes are merged. Once these changes are merged, the code should be tested, built, and delivered/deployed in an automated way.\nThe data science analog of CI/CD is code promotion and integration processes. This chapter will help you think about how to structure your app or report so that you can feel secure moving an app into production and updating it later. This chapter will also include an introduction to real CI/CD tools, so that you can get started using them in your own work.\n\n\nInfrastructure as Code\nThe underlying infrastructure for development and deployment should be reproducible using code so it can be updated and replaced with minimal fuss or disruption.\nThe data science analog is thinking about managing environments as code. This chapter will help you think about how to create a reproducible and secure project-level data science environment so you can be confident it can be used, secured, and resurrected later (or somewhere else) as need be.\n\n\nMicroservices\nAny large application should be decomposed into smaller services that are as atomic and lightweight as possible. This makes large projects easier to reason about and makes interfaces between components clearer, so changes and updates are safer.\nI believe this technical best practice has the furthest to go to translate to data science, so this chapter is about how to think of your data science project in terms of its components. This chapter will help you think about what the various components of your projects are and how to split them up for painless and simple updating and atomizing.\n\n\nMonitoring and Logging\nApplication metrics and logs are essential for understanding the usage and performance of production services, and should be leveraged as much as possible to have a holistic picture at all times.\nThe fourth chapter in this section is on monitoring and logging, which is – honestly – in its infancy in the data science world, but deserves more love and attention.\n\n\nOther Things\nMost DevOps frameworks also include communication, collaboration, and review practices as part of their framework, as the technical best practices of DevOps exist to support the work of the people who use them. This is obviously equally important in the data science world – it’s what the entire third section is about.\nAnd in the fifth chapter, we’ll learn about Docker – a tool that has become so common in DevOps practices that it deserves some discussion all on its own. In this section, you’ll get a general intro to what Docker is and how it works – as well as a hands-on intro to using Docker yourself."
  },
  {
    "objectID": "chapters/sec1/1-1-code-promotion.html#the-three-environments",
    "href": "chapters/sec1/1-1-code-promotion.html#the-three-environments",
    "title": "1  Code promotion and integration",
    "section": "1.1 The Three Environments",
    "text": "1.1 The Three Environments\nThe best way to ensure that things only get deployed when you mean them to – and sleep securely knowing that it won’t be disturbed – is to have standalone environments for development, testing, and production. Having separate environments and a plan for promoting content from one to the other is the core of workflows that de-risk pushing to production.\nUsually, these environments are referred to as Dev, Test, and Prod.\n\nThe best way to ensure that deployments go smoothly is to make them as minimal and predictable as possible. This requires that the dev, test, and prod environments be very close mirrors of each other. We’ll get into how to accomplish that at the end of this chapter.\n\n1.1.1 Dev for Data Science\nDev is a sandbox where people can install packages and try new things with no risk of breaking anything in production.\nIf you’re a data scientist, you probably have a really good idea what your Dev environment looks like. It’s probably a Jupyter Notebook or an IDE with data science capabilities like RStudio, Spyder, or PyCharm. You use this environment to do exploratory data analysis, try out new types of charts and graphs, and test model performance against new features that you might design. This is really different than what most IT/Admins imagine is happening in Dev.\nFor a pure software engineering project, the Dev environment isn’t about exploration and experimentation – it’s about building. The relationship between data science and software engineering is akin to the difference between archaeology and architecture. Data science is about exploring existing relationships and sharing them with others once they’ve been discovered. The path is often meandering – and it’s usually not clear whether it’s even a possible one when you start. In contrast, pure software engineering is like designing a building. You know from the beginning that you’re designing a building for a particular purpose. You might need a little exploration to ensure you’ve thought through all of the nooks and crannies, or that you’ve chosen the right materials, or that you’re going to stay on budget, but you’ll have a pretty good idea up front whether it’s possible to design the building you want.\nThis means that Dev environments look really different for a data scientist versus a software engineer.\nThe biggest difference is that most IT/Admins are going to think of Dev, Test, and Prod being three identical copies of the same environment. That’s close to what you need as a data scientist, but really it’s more like you need a sandbox, a test environment, and prod. That means that if you’re using a deployment platform like RStudio Connect or Dash Enterprise, you probably don’t need it in your Dev environment, and that you don’t need your development tool in Test or Prod (any changes should go back through the deployment pipeline).\n\n\n1.1.2 Test and Prod\nTest is (unsurprisingly) an environment for testing. Depending on the type of asset you’re developing, the test environment might incorporate testing that you do, testing by outside entities like security, and/or performance testing. Generally, the test environment facilitates User Acceptance Testing (UAT), where you can investigate whether labels and buttons are clear or whether plots and graphs meet the need. Depending on your organization, test might be collapsed with dev, it might be a single environment, or it could be multiple environments for the different types of testing.\nProd is the gold standard environment where things run without any manual human intervention. Usually the only way to get things into prod is through some type of formalized process – sometimes backed by a computer process like a git merge or push from a Continuous Integration/Continuous Deployment (CI/CD) platform (more on that below). One of the most important ways to keep prod stable is that nothing changes in prod other than via a simple promotion from the test environment.\n\n\n1.1.3 Protecting Prod Data\nOne of the biggest risks during the dev and test parts of an assets lifecycle is that you might mess up real data during your work. In a software engineering context, it’s common to use completely fake data or for the app to be the data generation tool.\nIn contrast, data science is all about using and learning from your organization’s actual data. So a dev environment that doesn’t include access to your organization’s real data is going to be completely useless if it doesn’t have real data in it. This is often a difficult thing to convince an IT/Admin of.\nIn many cases, data science assets and reports are read-only. If you’re mostly building visualizations or dashboards that just consume the business data, perhaps clean it for analytics purposes, you can happily accept a read-only connection to your organization’s data.1 In this case, it works just fine to connect to your real data from your Dev and Test environments and create graphs, models, and dashboards based on the real data, testing out new visualizations and model features in a safe sandbox while the existing version of your app or report runs smoothly in prod.\nOn the other hand, if your app or report actually writes data, you’ll have to be a little more clever. In general, you’ll have to figure out how to redirect your apps output into a test data store, or to mock responses from the real services you’re interacting with. The easiest way to do this is by including your output locations as variables inside your code and then setting them at runtime based on an environment variable. See below for an example of how to do this."
  },
  {
    "objectID": "chapters/sec1/1-1-code-promotion.html#taking-data-science-to-production-with-cicd",
    "href": "chapters/sec1/1-1-code-promotion.html#taking-data-science-to-production-with-cicd",
    "title": "1  Code promotion and integration",
    "section": "1.2 Taking data science to production with CI/CD",
    "text": "1.2 Taking data science to production with CI/CD\nThe common term for the mechanics of code promotion is Continuous Integration/Continuous Deployment (CI/CD).\nCI/CD is about moving your data science projects, including code, data, and environment, into successively more formal states of production. While it’s not a requirement to use source control to make this happen, the mechanics of CI/CD is usually tightly linked to source control procedures.\n\n1.2.1 A Rough Intro to Git\nIf you’re not already familiar, I’d suggest spending some time learning git. If you’re just starting, you’re in for a bit of a ride.\nPeople who say git is easy are either lying to look smarter or learned so long ago that they have forgotten how easy it is to mess up your entire workflow at any moment.\nI’m not going to get into the mechanics of git in this book– what it means to add, commit, push, pull, merge, and more. There are lots of great resources out there that I’m not aiming to reproduce.\n\nIf you don’t already know git and want to learn, I’d recommend HappyGitWithR by Jenny Bryan. It’s a great on-ramp to learn git.\nEven if you’re a Python user, the sections on getting started with git, on basic git concepts, and on workflows will be useful since they approach git from a data science perspective.\n\nI will provide a quick git cheatsheet for memory purposes, and I will talk about some git strategies that match well with using git to execute a data science code promotion strategy.\n\n1.2.1.1 Git Cheatsheet\n\n\n\n\n\n\n\nCommand\nWhat it Does\n\n\n\n\ngit clone <remote>\nClone a remote repo – make sure you’re using SSH URL.\n\n\ngit add <files/dir>\nAdd files/dir to staging area.\n\n\ngit commit -m <message>\nCommit your staging area.\n\n\ngit push origin <branch>\nPush to a remote.\n\n\ngit pull origin <branch>\nPull from a remote.\n\n\ngit checkout <branch name>\nCheckout a branch.\n\n\ngit checkout -b <branch name>\nCreate and checkout a branch.\n\n\ngit branch -d <branch name>\nDelete a branch.\n\n\n\nFor production data science assets, I generally recommend long-running dev (or test) and prod branches, with feature branches for developing new things. The way this works is that new features are developed in a feature branch, merged into dev for testing, and then promoted to prod when you’re confident it’s ready.\nFor example, if you had two new plots you were adding to an existing dashboard, your git commit graph might look like this:\n\nCI/CD adds a layer on top of this. CI/CD allows you to integrate functional testing by automatically running those tests whenever you do something in git. These jobs can run when a merge request is made, and are useful for tasks like spellchecking, linting, and running tests.\nFor the purposes of CI/CD, the most interesting jobs are those that do something after there’s a commit or a completed merge, often deploying the relevant asset to its designated location.\nA CI/CD integration using the same git graph as above would have released 3 new test versions of the app and 2 new prod versions. Note that in this case, the second test release revealed a bug, which was fixed and tested in the test version of the app before a prod release was completed.\nIn years past, the two most popular CI/CD tools were called Travis and Jenkins. By all accounts, these tools were somewhat unwieldy and difficult to get set up. More recently, GitHub – the foremost git server – released GitHub Actions (GHA), which is CI/CD tooling directly integrated into GitHub that’s free for public repositories and free up to some limits for private ones.\nIt’s safe to say GHA is eating the world of CI/CD.2\nFor example, if you’re reading this book online, it was deployed to the website you’re currently viewing using GHA. I’m not going to get deep into the guts of GHA, but instead talk generally about the pattern for deploying data science assets, and then go through how I set up this book on GHA.\n\n\n\n1.2.2 Using CI/CD to deploy data science assets\nIn general, using a CI/CD tool to deploy a data science asset is pretty straightforward. The mental model to have is that the CI/CD tool stands up a completely empty server for you, and runs some code on it.\nThat means that if you’re just doing something simple like spellchecking, you can probably just specify to run spellcheck. If you’re doing something more complicated, like rendering an R Markdown document or Jupyter Notebook and then pushing it to a server, you’ll have to take a few extra steps to be sure the right version of R or Python is on the CI/CD server, that your package environment is properly reproduced, and that you have the right code to render your document.\nFeel free to take a look through the code for the GitHub Action for this book. It’s all YAML, so it’s pretty human-readable.\nHere’s what happens every time I make a push to the main branch of the repository for this book:3\n\nCheckout the current main branch of the book.\nUse the r-lib action to install R.\nUse the r-lib action to setup pandoc (a required system library for R Markdown to work).\nGet the cached renv library for the book.\nRender the book.\nPush the book to GitHub Pages, where this website serves from.\n\nYou’ll see that it uses a mixture of pre-defined actions created for general use, pre-defined actions created by people in the R community, and custom R code I insert to restore an renv library and render the book itself."
  },
  {
    "objectID": "chapters/sec1/1-1-code-promotion.html#per-environment-configuration",
    "href": "chapters/sec1/1-1-code-promotion.html#per-environment-configuration",
    "title": "1  Code promotion and integration",
    "section": "1.3 Per-Environment Configuration",
    "text": "1.3 Per-Environment Configuration\nSometimes you want a little more flexibility – for example the option to switch many the environment variables depending on the environment.\nIn R, the standard way to do this is using the config package. There are many options for managing runtime configuration in Python, including a package called config.\nFor example, let’s consider this shiny app. In this app, every time I press the button, the app sends a POST request to an external service indicating that the button has been pressed.\n\nlibrary(shiny)\n\n# UI that's just a button\nui <- fluidPage(\n  actionButton(\"button\", \"Press Me!\")\n)\n\n# Do something on button press\nserver <- function(input, output) {\n  observeEvent(\n    input$button, \n    httr::POST(\n      \"www.my-external-system.com\", \n      body = list(button_pressed = TRUE)\n    )\n  )\n}\n\n# Run the application \nshinyApp(ui = ui, server = server)\n\nWith the URL hardcoded like this, it’s really hard to imagine doing this in a Dev or Test environment.\nHowever, with R’s config package, you can create a config.yml file that looks like this:\n\ndev:\n  url: \"www.test-system.com\"\n  \nprod:\n  url: \"www.my-external-system.com\"\n\nThen you can switch to the correct config and apply that configuration inside the app.\n\nlibrary(shiny)\nconfig <- config::get(\"prod\")\n\n# UI that's just a button\nui <- fluidPage(\n  actionButton(\"button\", \"Press Me!\")\n)\n\n# Do something on button press\nserver <- function(input, output) {\n  observeEvent(\n    input$button, \n    httr::POST(\n      config$url, \n      body = list(button_pressed = TRUE)\n    )\n  )\n}\n\n# Run the application \nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "chapters/sec1/1-1-code-promotion.html#sec-env-vars",
    "href": "chapters/sec1/1-1-code-promotion.html#sec-env-vars",
    "title": "1  Code promotion and integration",
    "section": "1.4 The power of environment variables",
    "text": "1.4 The power of environment variables\nEnvironment variables are an extremely powerful way to manage the context in which your code is running.\nWhenever your code is running, your system maintains a number of different values to be used by processes running on the system. These variables are not specific to the R or Python process on the system – but they are usable by R or Python to powerful ends.\nIt is convention to make environment variable names in all caps, words separated by underscores. The values are always simple character values, though these can be cast to some other type inside R or Python.\nIn R, you can set environment variables in an .Renviron file or using Sys.setenv(VAR_NAME = VALUE) in a .Rprofile. These files are sourced on process startup, so the values are always available inside the running process with Sys.getenv(\"VAR_NAME\"). You can have user and/or project level files and even server-level ones with .Renviron.site and .Rprofile.site.\nIn Python, environment variables are accessible one-by-one using the os.getenv function or as a dictionary in os.environ. If you’re using a lot of environment variables, it’s common to import them using a statement like from os import environ as env.\nThere is no equivalent of the .Rprofile that’s sourced before startup in Python, but it’s common to use the python-dotenv package to manually load the contents of a .env file into the environment as one of the first few lines in your code.\nYou should not commit these files to git, but should manually move them across environments, so they never appear anywhere centrally accessible.\nOnce you’re using environment variables, they’re extremely powerful. It’s very common to set the default value of a function argument to be the value of an environment variable. For example, in the example using the R {config} package, config::get() uses the value of the R_CONFIG_ACTIVE environment variable to choose which configuration to use.\nThat means that switching from the dev to the prod version of the app is as easy as making sure you’ve got the correct environment variable set on your system.4\nIt can also be a great way to safely store secrets on your system. If you’ve got a function that, for example, creates a database connection, it’s a good idea to have the username and password arguments of your function default to reading from an environment variable so you don’t ever have to expose them in your code.\nThat might look something like this in R:\n\nmy_db_con <- function(user = Sys.getenv(\"DB_USER\"), pw = Sys.getenv(\"DB_PW\")) {\n  make_db_con(user, pw)\n}\n\nand in Python\nTODO: Check python version works\n\ndef my_db_con(user = os.getenv(\"db_user\"), pw = os.getenv(\"db_pw\")):\n  make_db_con(user, pw)"
  },
  {
    "objectID": "chapters/sec1/1-1-code-promotion.html#creating-and-maintaining-identical-environments",
    "href": "chapters/sec1/1-1-code-promotion.html#creating-and-maintaining-identical-environments",
    "title": "1  Code promotion and integration",
    "section": "1.5 Creating and Maintaining Identical Environments",
    "text": "1.5 Creating and Maintaining Identical Environments\nIn the IT world, there’s a phrase that servers should be cattle, not pets. The idea here is that servers should be unremarkable and that each one should be more-or-less interchangeable. This matters, for example, in making sure your test and prod environments look exactly the same.\nTODO: Notes on using virtual environments + Docker.\nFor example, doing test on a Windows laptop and then going to prod on a Linux server introduces a potential that things that worked in test suddenly don’t when going to prod. For that reason, making all three (or at least test and prod) match as precisely as possible is essential. The need to match these three environments so precisely is one reason for data science workloads moving onto servers.\nA bad pattern then would look like this:\n\nI develop an update to an important Shiny or Dash app in my local environment and then move it onto a server.\nAt that point, the app doesn’t quite work and I make a bunch of manual changes to the environment – say adjusting file paths or adding R or Python packages. Those manual changes end up not really being documented anywhere.\nA week later, when I go to update the app in prod, it breaks on first deploy, because the server state of the test and prod servers drifted out of alignment.\n\nThe main way to combat this kind of state drift is to religiously use state-maintaining infrastructure as code (IaC) tooling. That means that all changes to the state of your servers ends up in your IaC tooling and no “just login and make it work” shenanigans are allowed in prod.\nTODO: Graphic - fixing problems using IaC tooling\nIf something breaks, you reproduce the error in staging, muck around until it works, update your IaC tooling to fix the broken thing, test that the thing is fixed, and then (and only then) push the updated infrastructure into prod directly from your IaC tooling."
  },
  {
    "objectID": "chapters/sec1/1-1-code-promotion.html#comprehension-questions",
    "href": "chapters/sec1/1-1-code-promotion.html#comprehension-questions",
    "title": "1  Code promotion and integration",
    "section": "1.6 Comprehension Questions",
    "text": "1.6 Comprehension Questions\n\nWrite down a mental map of the relationship between the three environments for data science.\nWhat are the options for protecting production data in a dev or test environment?\nWhy is git so important to a good code promotion strategy? Can you have a code promotion strategy without git?\nWhat is the relationship between git and CI/CD? What’s the benefit of using git and CI/CD together?"
  },
  {
    "objectID": "chapters/sec1/1-1-code-promotion.html#portfolio-exercise-blog-automation",
    "href": "chapters/sec1/1-1-code-promotion.html#portfolio-exercise-blog-automation",
    "title": "1  Code promotion and integration",
    "section": "1.7 Portfolio Exercise: Blog Automation",
    "text": "1.7 Portfolio Exercise: Blog Automation\nMany people in the data science community have personal websites or blogs. They’re a great way to show off your portfolio of work! You may even have one. But how is it built?\nMany people build their blog locally and then just push it up to a website.\nFor this challenge, create a personal website if you don’t have one and configure it so that you can push changes to git and have them render to your website.\nHere are a few suggestions on components you might use:\n\nBuild the website itself using quarto. Quarto is a library for scientific and technical publishing that makes it easy to include R or Python code if you want. It’s the tool this book was written with.\nDeploy the website onto GitHub Pages. At least as of this writing, you can deploy a free website for your GitHub account or for any individual project in your account.\n\nBy default, your website will have a github.io URL for now. If you want to change it, I’d suggest checking out Chapter 12 on networking for more information.\n\nUse GitHub Actions to automate what happens when you push to your website. The GitHub repo for this book (akgold/do4ds) may prove useful as a reference."
  },
  {
    "objectID": "chapters/sec1/1-2-env-as-code.html#environments-have-layers",
    "href": "chapters/sec1/1-2-env-as-code.html#environments-have-layers",
    "title": "2  Environments as Code",
    "section": "2.1 Environments have layers",
    "text": "2.1 Environments have layers\nWhen you first start thinking about environments, it can be hard to wrap your head around them. The environment seems like a monolith, and it can be hard to figure out what the different components are.\nI generally think of three layers in data science environments, and these are in order – each layer of the environment is actually built on the ones below. Once you understand the layers of an environment, you can think more clearly about what your actual reproducibility needs are, and which environmental layers you need to target putting into code.\n\n\n\n\n\n\n\nLayer\nContents\n\n\n\n\nPackages\nR + Python Packages\n\n\nSystem\nR + Python Language Versions\nOther System Libraries\nOperating System\n\n\nHardware\nVirtual Hardware\nPhysical Hardware\n\n\n\nNote that your code and your data are not the environment – they are what the environment is for. As you’re thinking about reproducibility, I’d encourage you to think about how they fit inside the environment and how they might be reproduced.2 But we’re not going to address them in this book.\nFor most data scientists, the biggest bang for your buck is getting the package layer right. In a lot of organizations, another team entirely will be responsible for the system and hardware layers, but the package layer is always your responsibility as the data scientist. Moreover, managing that layer isn’t terribly hard, and if you get it right, you’ll solve a huge fraction of the “runs on my machine” issues you’re likely to encounter.\n\n2.1.1 Package environments as code\nA successful package Environment as Code setup has two key attributes:\n\nYour package environment is isolated and cannot be disrupted by other activities on the same machine.\nYour package environment can easily be captured and transported elsewhere.\n\nBefore we get to what a good Environment as Code setup looks like, let’s dive into what bad setups look like.\nIn a lot of cases, data scientists have the habit of starting a project, and, when they need to install packages, they just run an install.packages command in their console or pip install in their terminal. This works fine for a while. But the problem with this is that the default has you installing things into a cache that’s shared among every project on your system.\nWhat happens if you come back to a project after a year and you’ve been installing things into your machine-wide package cache the whole time. It’s very possible you won’t have the right versions and your code will break.\nThe other problem happens when it comes time to share a project with others. It’s not uncommon to see an intro to an R script that looks something like this:\n\n# Check if dplyr installed\nif (!\"dplyr\" %in% row.names(installed.packages())) {\n  # install if not\n  install.packages(\"dplyr\")\n}\nACK! Please don’t do this!\nNumber one, this is very rude. If someone runs your code, you’ll be installing packages willy-nilly into their system. Additionally, because this doesn’t specify a version of the {dplyr} package, it doesn’t even really fix the problem!\n\n\n2.1.2 Step 1: Standalone Package Libraries\nAs a data scientist, you’re probably familiar with installing packages from a repository using the install.packages command in R or pip install or conda install in Python. But do you really understand what’s happening when you type that command?\nLet’s first level-set on what the various states for R or Python packages are. There are three states packages can be in – and we’re going to go back to our data science as cooking analogy.\n\nPackages can be stored in a repository, like CRAN or BioConductor in R or PyPI or Conda in Python. You can think of a package in a repository like food at the grocery store – it’s packaged up and ready to go, but inert. Setting aside groovy bodegas with eat-in areas, you don’t get to eat at the grocery store. You’ve got to buy the food and take it home before you can use it – and you’ve got to install the food before you can use it.\nAnd then your library is your pantry, where you keep a private set of packages, bespoke to you and the food you like to cook – the projects you’re likely to do.\nLoading a package with a library or import command is like taking the food out of the pantry and putting it on the counter so you can actually cook with it.\n\n[Diagram of package states]\nBy default, when you install an R or Python package, that package installs into user space. That means that it installs into a package library that is specific to your user, but is shared among every time that package is used by you on the machine.\nThis isn’t a disastrous situation, but it is a recipe for package incompatibilities down the road.\n[TODO: diagram of user-level vs project-level installs]\nThe most important thing to understand about package libraries is that libraries can only have one version of any given package at a time. So that means that if I have code that relies on version 1.0 of a given package and I install a new version of that package, version 1.0 is gone and I am likely to run into package incompatibility issues.\nIt’s for this reason that you want to have standalone package libraries for each project on your system. Hopefully, you already have good practices around having each project in a standalone directory on your system and making a git repo in that system. Now just make the base directory of that directory a standalone library as well.\n\n\n\n\n\n\nWhat if I have multiple content items?\n\n\n\nIn many data science projects, you’ve got multiple content items within a single project. Maybe you have an ETL script and an API and an app. After a lot of experimenting, my recommendation is to create one git repo for the whole project and have content-level package libraries.\nThis is not a rule. It’s just a suggestion about how I’ve found it works best over time.\n[TODO: Add image]\n\n\n\n2.1.2.1 What’s really happening?\nI happen to think the grocery store metaphor for package management is a useful one, but you might be wondering what the heck is actually happening when you’re using {renv} or {venv}. How does this package magic happen?\nFirst, let’s quickly go over what happens when you install or load a package.\nWhenever you install a package, there are two key settings that R or Python consult: the URL of the repository to install from, and the library to install to. Similarly, when you load an R or Python library, the install checks the library location. In R, the command used is .libPaths(), and in Python it’s sys.path.\nSo you can see that it’s (conceptually) pretty simple to create a standalone package library for any project – when the virtual environment is activated, just make sure that the project-level library is what comes back when checking the library path.\nYou can see it pretty easily in R. If I run .libPaths() before and after activating an {renv} environment, the first entry from the .libPaths() call changes from a user-level library /Users/alexkgold to a project-level library /Users/alexkgold/Documents/do4ds/.\n.libPaths()\n[1] \"/Users/alexkgold/Library/R/x86_64/4.2/library\"                 \n[2] \"/Library/Frameworks/R.framework/Versions/4.2/Resources/library\"\nrenv::activate()\n* Project '~/Documents/do4ds/docker/docker/plumber' loaded. [renv 0.15.5]\n.libPaths()\n[1] \"/Users/alexkgold/Documents/do4ds/docker/docker/plumber/renv/library/R-4.2/x86_64-apple-darwin17.0\"\n[2] \"/Library/Frameworks/R.framework/Versions/4.2/Resources/library\"  \nSimilarly, in Python it looks like this. Note that the “after” version replaces the last line of the sys.path with a project-level library:\n❯ python3 -m site                                       \nsys.path = [\n    '/Users/alexkgold/Documents/python-examples/dash-app',\n    '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python38.zip',\n    '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8',\n    '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/lib-dynload',\n    '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/site-packages',\n]\nUSER_BASE: '/Users/alexkgold/Library/Python/3.8' (doesn't exist)\nUSER_SITE: '/Users/alexkgold/Library/Python/3.8/lib/python/site-packages' (doesn't exist)\nENABLE_USER_SITE: True\n\n❯ source .venv/bin/activate                       \n(.venv)\n\n❯ python3 -m site\nsys.path = [\n    '/Users/alexkgold/Documents/python-examples/dash-app',\n    '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python38.zip',\n    '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8',\n    '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/lib-dynload',\n    '/Users/alexkgold/Documents/python-examples/dash-app/.venv/lib/python3.8/site-packages',\n]\nUSER_BASE: '/Users/alexkgold/Library/Python/3.8' (doesn't exist)\nUSER_SITE: '/Users/alexkgold/Library/Python/3.8/lib/python/site-packages' (doesn't exist)\nENABLE_USER_SITE: False\n(.venv)\n\n\n\n2.1.3 Step 2: Document environment state.\nUsing standalone package libraries for each project ensures that your projects remain undisturbed when you come back to them months or years later and keeps your work reproducible.\nBut it doesn’t solve the sharing problem.\nThat is, you still need some help when it comes time to share an environment with someone else. So how does that work?\n[TODO: image – anatomy of a lockfile]\nBoth R and Python have great utilities that make it easy to capture the current state of a library into a lockfile or requirements.txt and to restore those libraries at a later date or somewhere else.\nIn R, {renv} is the standard on this front. In Python, there are many different options. In the context of production data science, I recommend {virtualenv}/{venv} and related tools.\nNow, when you share your project with someone else, your lockfile or requirements.txt goes along for the ride. Sometimes people are dismayed that their library doesn’t go along as well and that people have to install the packages themselves – but this is by design!\nFirst, the actual package libraries can be very large, so putting just a short lockfile or requirements file into git is definitely preferred. The other reason is that the actual package install can differ from system to system. For example, if you’re working on a Windows laptop and your colleague is on a Mac, an install of {dplyr} 1.0 means that different files are installed – but with exactly the same functionality. You want to respect this, so instead of sending the whole library along for the ride, you just send the specification that dplyr 1.0 is needed.\n\n\n\n\n\n\nA sidebar on Conda\n\n\n\nMany data scientists love Conda for managing their Python environments.\nConda is a great tool for its main purpose – allowing you to create a data science environment on your local laptop, especially when you don’t have root access to your laptop because it’s a work machine that’s locked down by the admins.\nIn the context of a production environment, Conda smashes together the language version, the package management, and (sometimes) the system library management. This has the benefit of being conceptually simple and easy-to-use. But I’ve often seen it go awry in production environments, and I generally recommend people use a tool that’s just for package management, like {venv}, as opposed to an all-in-one tool like Conda.\n\n\nOn a high level, the workflows for these tasks are similar between R and Python. However, there are some meaningful differences in tooling – especially because many computers arrive with a system version of Python installed, while R is only ever installed by a user trying to do data science tasks.\nAt the end of the day, this actually makes it harder to use Python because you do not want to use your system Python for your data science work…but sometimes it accidentally gets into the mix.\n\n\n2.1.4 Step 3: Collaborate or Deploy\nThe nice thing about these tools is that they make collaboration easy. One big problem people run into is that they plan to share their package library with collaborators. Don’t do this!\nThere are two reasons – first, sharing a library is a bad idea because it can change under people with no warning, leaving them confused about why things are breaking. If you’re potentially working on a different operating system from your collaborators, this won’t even work in the first place. And if you’re trying to deploy - probably on a Linux server – and you’re developing on your Windows or Mac laptop, it’s not going to work at all.\nThe other reason is that this collaboration works really nicely over git. Putting large files – like an entire package library – into git is generally a bad idea. In most cases, you’ll want to gitignore the library itself and instead just share the lockfile.\n\n\n\n\n\n\nSharing package caches\n\n\n\nIf you’re on a shared server, you may want to share a package cache across users. This generally isn’t necessary, but can save some space on the server. Both {renv} and venv include settings to allow you to relocate the package cache to a shared location on the server. You’ll need to make sure that all the relevant users have read and write privileges to this location.\n\n\nIf you are collaborating, both Python and R have one-line commands for restoring a library from a lockfile. In R, you should make sure to set your working directory to the project directory – or open the project in RStudio – and run renv::restore().\nIn Python, you’ll want to make sure you activate your project-level virtual environment using venv source ./venv/bin/activate and then install from a requirements file using pip install -r requirements.txt.\n\n\n2.1.5 Reproducing the rest of the stack\nSometimes, just recording the package environment and moving that around is sufficient. In many cases, old versions of R and Python are retained in the environment, and that’s sufficient.\nThere are times where you need to reproduce elements further down the stack. In some highly-regulated industries, you’ll need to go further down the stack because of requirements for numeric reproducibility. Numeric routines in both R and Python call on system-level libraries, often written in C++ for speed. While it’s unlikely that upgrades to these libraries would cause changes to the numeric results you get, it can happen, and it may be worth maintaining parts of the stack.\nIn other cases, your R or Python library might basically just be a wrapper for system libraries. For example, many popular packages for geospatial analysis are just thin language wrappers that call out to the system libraries. In this case, it might be important to be able to maintain a particular version of the underlying system library to ensure that your code runs at all in the future.\nThere are many tools you can use to record and reproduce the R and Python versions you’re using, the system libraries, and the operating system itself.\nThese days, the clear leader of the pack on this front is Docker. It has become an increasingly popular way to create, maintain, and use standalone environments – and for good reason! In Chapter 9 we’ll get deep into the use of Docker in data science. However, it’s worth keeping in mind that if you’re working in the context of a formally-supported IT organization, they may have other tooling they prefer to use to create and maintain environments, and they can be equally valid."
  },
  {
    "objectID": "chapters/sec1/1-2-env-as-code.html#environments-as-code-cheatsheet",
    "href": "chapters/sec1/1-2-env-as-code.html#environments-as-code-cheatsheet",
    "title": "2  Environments as Code",
    "section": "2.2 Environments as Code Cheatsheet",
    "text": "2.2 Environments as Code Cheatsheet\n\n2.2.1 Checking your library + repository status\nTODO\n\n\n2.2.2 Creating and Using a Standalone Project Library\n\n\n\n\n\n\n\n\nStep\nR Command\nPython Command\n\n\n\n\nMake a standalone project directory.\n-\n-\n\n\nMake sure you’ve got {renv}/{venv}.\ninstall.packages(\"renv\")\nIncluded w/ Python 3.5+\n\n\nCreate a standalone library.\nrenv::init()\npython -m venv <dir>\nRecommend: use .venv for <dir>\n\n\nActivate project library.\nrenv::activate()\nHappens automatically if using projects.\nsource <dir>/bin/activate\n\n\nInstall packages as normal.\ninstall.packages(\"<pkg>\")\npython -m pip install <pkg>\n\n\nSnapshot package state.\nrenv::snapshot()\npip freeze > requirements.txt\n\n\nExit project environment.\nLeave R project.\ndeactivate\n\n\n\n\n\n2.2.3 Collaborating on someone else’s project\n\n\n\n\n\n\n\n\nStep\nR Command\nPython Command\n\n\n\n\nDownload project.\n\n\n\n\nMove into project directory.\nsetwd(\"<project-dir>\")\nOr just open R project in RStudio.\ncd <project-dir>\n\n\nCreate project environment.\nrenv::init()\npython -m venv <dir>\nRecommend: use .venv for <dir>\n\n\nEnter project environment.\nHappens automatically.\nsource <dir>/bin/activate\n\n\nRestore packages.\nMay happen automatically or renv::restore()\npip install -r requirements.txt"
  },
  {
    "objectID": "chapters/sec1/1-2-env-as-code.html#comprehension-questions",
    "href": "chapters/sec1/1-2-env-as-code.html#comprehension-questions",
    "title": "2  Environments as Code",
    "section": "2.3 Comprehension Questions",
    "text": "2.3 Comprehension Questions\n\nWhy does difficulty increase as the level of required reproducibility increase for a data science project. In your day-to-day work, what’s the hardest reproducibility challenge?\nDraw a mental map of the relationships between the 7 levels of the reproducibility stack. Pay particular attention to why the higher layers depend on the lower ones.\nWhat are the two key attributes of environments as code? Why do you need both of them? Are there cases where you might only care about one?\nDraw a mental map of the relationships between the following: package repository, package library, package, project-level-library, .libPaths() (R) or sys.path(python), lockfile\nWhy is it a bad idea to share package libraries? What’s the best way to collaborate with a colleague using an environment as code? What are the commands you’ll run in R or Python to save a package environment and restore it later?"
  },
  {
    "objectID": "chapters/sec1/1-2-env-as-code.html#portfolio-exercise-use-renv-or-venv",
    "href": "chapters/sec1/1-2-env-as-code.html#portfolio-exercise-use-renv-or-venv",
    "title": "2  Environments as Code",
    "section": "2.4 Portfolio Exercise: Use {renv} or venv",
    "text": "2.4 Portfolio Exercise: Use {renv} or venv\nCreate a post for your (new?) personal website that includes a plot or other bit of R or Python code.\nCreate a project-specific library for your blog and integrate it into your existing GitHub Actions pipeline so you can build the website as well as the R or Python Content."
  },
  {
    "objectID": "chapters/sec1/1-3-data-arch.html#the-composition-of-a-data-pipeline",
    "href": "chapters/sec1/1-3-data-arch.html#the-composition-of-a-data-pipeline",
    "title": "3  Data Pipeline Architecture",
    "section": "3.1 The composition of a data pipeline",
    "text": "3.1 The composition of a data pipeline\nThe right way to design a data pipeline is to begin at the end. You want to start by thinking about the output of your project. It might be charts, reports, visualizations, apps, and more. Most often, this layer uses either an interactive app library like Shiny, Dash, or Streamlit or a report-generation library like R Markdown, Quarto, or Voila. In some cases, the presentation phase is just an API for machine to machine communication – but that’s still relatively uncommon.\nThere are often several kinds of data that have to get prepared to go into the presentation layer.\nThe first kind is rectangular business data. That’s the kind of data that obviously belongs in a data.frame in R or a pandas.DataFrame in Python. It’s the data you’re actually doing data science on. Depending on your organization, you might get it as completely raw data you actually have to enter yourself. In other organizations, data engineering teams have created complete data pipelines that deliver highly-refined data right to your model training process.\nMost organizations fall somewhere in the middle. There’s a central home for data – a data warehouse, data lake, or share drive for files - but it’s not fully ready to be presented, and you’ll have to do some processing to get there.\nThen there’s the other kinds of data that are the output of your analytics pipeline. Maybe you trained one or more machine learning model, derived descriptive statistics, or did causal inference. You also might’ve pulled out data specifically for the presentation layer during your pipeline.\nThese would be things like lists of valid categories for slicers and selectors, dates to include in the analysis, and metadata you’ll want to include like plot headers and prettier names for columns in your data.\nDepending on the exact type of data, some of it might be pretty rectangular – something like a list of store locations and metadata about them or model coefficients and standard errors. Or it might be something really non-rectangular like a machine learning model.\nIf you take one thing away from this chapter, let it be this – you should separate your data pipeline from your presentation layer.\nThis may be a big change. Many data scientists just write a bunch of code, assuming that the data is just the data. It’s very common to see a Shiny app that mushes a bunch of data cleaning into the app itself. Or a Quarto report that does processing in the body of the report.\nFor some projects, this is a valid assumption and your data will never change. If so, you probably don’t need this chapter.\nBut for many data projects, data will change – perhaps even on a regular basis. In these cases, you’ll want to think carefully about how to link your data to your app. This chapter will review ways to write your project so that you can update and change out the data later without having to completely rewrite or rework your code."
  },
  {
    "objectID": "chapters/sec1/1-3-data-arch.html#can-you-load-all-your-data",
    "href": "chapters/sec1/1-3-data-arch.html#can-you-load-all-your-data",
    "title": "3  Data Pipeline Architecture",
    "section": "3.2 Can you load all your data?",
    "text": "3.2 Can you load all your data?\nJust loading your data into memory is a good option if you haven’t established you can’t do this. Basically, you should try this first and see if it works. If it does, go no further. If it doesn’t – or you anticipate it won’t, you’ll want to choose a different answer.\nYou don’t have to figure out any clever way to access the data or how to make sure you’re getting all the data you need. You’ve got it all! “Real engineers” may scoff at this pattern, but don’t let their criticism dissuade you. If your data size is small and your app performance is good enough, just read in all of your data and operate on it live. Don’t complicate things more than they need to. After all, “Premature optimization is the root of all evil.” Donald Knuth\nSo the main question is whether your data is too big to load. Preprocessing will generally happen on a schedule, so as long as your data fits into memory and processing it doesn’t kill your server resources, you’re good!\nIn the presentation layer, the question is a little more complicated. It’s quick to jump to the size of the raw data you’re using and assume you can’t load all the data in, but that’s often a completely irrelevant metric. You’re starting backwards if you start from the size of the raw data. Instead, you should figure out how big the data is that you actually need in the presentation layer. In a lot of cases, you can preprocess the input data into a small dataset you can just read wholly into the presentation layer.\nTo make this a little more concrete, let’s imagine you work for a large retailer and are responsible for creating a dashboard that will allow people to visualize the last week’s worth of sales for a variety of products. With this vague prompt, you could end up needing to load a huge amount of data into your project – or very little at all.\nOne of the most important questions is how much you can cache before someone even opens the app. For example, if you need to provide total weekly sales at the department level, that’s probably just a few data points. And even if you need to go back a long ways, it’s just a few hundred data points – load all that in!\nBut if you start needing to slice and dice the data in a lot of directions, then the data size starts multiplying, and you may have to include the entire raw data set in the report. For example, if you need to include weekly sales at the department level, then the size of your data is the \\(\\text{number of weeks} * \\text{number of departments}\\). If you need to include more dimensions – say you need to add geographies, then your data size multiplies by the number of geographies.\nNow that you’ve figured out how big the data is, you need to determine whether it’s too big. In this case, too big is really a matter of performance.\nFor some apps, you want the data to be snappy throughout runtime, but it’s ok to have a lengthy startup process (perhaps because it can happen before the user actually arrives). In that case, it’s probably still ok to use a flat file and read it all in on startup.\nOne crucial question for your project is how much wait time is acceptable for people wanting to see the project – and when is that waiting ok? For example, if people need to be able to make selections and see the results in realtime, then you probably need a snappy database, or all the data preloaded into memory when they show up.\nBut maybe you have high performance requirements or a lot of data. Or maybe you know your data will grow over time. In that case, you’re going to want to choose some other system for accessing your data.\nIf you can just load in all your data, the question then becomes how. One common way to load all your data is just to read in a whole database table. If that’s an option for you, it can be really nice.\nIf you don’t have a database handy, the most common way to save and read data is with a flat file. A flat file is what happens when you just save an active data frame somewhere. If you have relatively small data in a flat file or a database table, just moving flat files around is by far the simplest way to manage your data pipeline.\n\n3.2.1 Choose your flat file format\nFlat file storage describes writing the data out into a simple file. The canonical example of a flat file is a csv file. However, there are also other formats that may make data storage smaller because of compression, make reads faster, and/or allow you to save arbitrary objects rather than just rectangular data. In R, the rds format is the generic binary format, while pickle is the generic binary format in python.\nFlat files can be moved around just like any other file on your computer. You can put them on your computer, and share them through tools like dropbox, google drive, scp, or more.\nThe biggest disadvantage of flat file data storage is twofold – and is related to their indivisibility. In order to use a flat file in R or Python, you’ll need to load it into your R or Python session. For small data files, this isn’t a big deal. But if you’ve got a large file, it can take a long time to read, which you may not want to wait for. Also, if your file has to go over a network, that can be a very slow operation. Or you might have to load it into an app at startup. Also, there’s generally no way to version data, or just update part, so, if you’re saving archival versions, they can take up a lot of space very quickly.\n---\nThere are a few different options for types of flat files. The most common is a comma separated value (csv) file, which is just a literal text file of the values in your data with commas as separators.1 You could open it in a text editor and read it if you wanted to.\nThe advantage of csvs is that they’re completely ubiquitous. Basically every programming language has some way to read in a csv file and work with it.\nOn the downside, csvs are completely uncompressed and they can only hold rectangular data. That makes them quite large relative to other sorts of files and slow to read and write. So if you’re trying to save a machine learning model, a csv doesn’t make any sense.\nBoth R and Python have language-specific flat-file types – rds in R and pickle in Python. Relative to csvs, these are nice because they usually include some amount of compression. They also can hold non-rectangular data, which can be great if you want to save a machine learning model or some other sort of object. The other advantage of rds and pickle is that they can be used with built-in data types that might not come through when saving to csv. For example, if you’re saving dates, an rds or pickle file will be able to save the data frame so that it knows to come back as a date. If you’re reading from a csv, you’d probably need to read it in as text and cast it to a character that can be annoying and – depending on the size of your data – time-consuming."
  },
  {
    "objectID": "chapters/sec1/1-3-data-arch.html#how-and-where-to-save-a-flat-file-or-blob",
    "href": "chapters/sec1/1-3-data-arch.html#how-and-where-to-save-a-flat-file-or-blob",
    "title": "3  Data Pipeline Architecture",
    "section": "3.3 How and where to save a flat file or blob",
    "text": "3.3 How and where to save a flat file or blob\nSo let’s say you’ve decided that you’re ok to just load all the data, and you don’t have a database to store it in.\nThere’s one place you usually should not save your data, and that’s inside your presentation bundle. The bundle is the set of code and assets that make up your presentation layer. For example, let’s say you’re building a simple Dash app that’s just a single file. You could create a project structure like this.\nmy-project/\n├─ my_app.py\n├─ data/\n│  ├─ my_data.csv\n│  ├─ my_model.pkl\nYou could do this, but you shouldn’t.\n\n\n\n\n\n\nNote\n\n\n\nThere’s one situation in which it’s ok to just put the data inside the project, and that’s when your data will be updated at the same cadence as the app or report itself. This is pretty rare for an in-production app, but sometimes there are cases where you’re just sharing an existing dataset in the form of a report/app.\n\n\nThis is convenient as a first cut at a project, but it can make it really difficult to update the data once your project is in production. You have to keep track of where the project is on your deployment file system, reach in, and update it.\nInstead, you want to decouple the data location from the project location.\nThere are a few ways you can do this. The most basic way is just to put the data on a location in your file system that isn’t inside the app bundle. This is, again, easy. Just make a different location in your directory and you’re good.\nBut when it comes to deployment this can also be complicated. If you’re writing your app and deploying it on the same server, then you can access the same directory. If not, you’ll need to worry about how to make sure that directory is also accessible on the server where you’re deploying your project.\nAdditionally, if you just put the file somewhere on the file system, you’ll need to control access using Linux file permissions. That’s not the end of the world, but controlling Linux file permissions is generally harder than controlling other sorts of access.\nIf you’re not going to store the flat file on the filesystem and you’re in the cloud, the most common option for where it can go is in blob storage. Blob storage is the term for storage where you store and recall things by name.2 Each cloud provider has blob storage – AWS’s is s3 (short for simple storage service), Azure has Azure Blob store, and GCP has Google Storage.\nThe nice thing about blob storage is that it can be accessed from anywhere that has access to the cloud. You can also control access using standard cloud identity management tooling, so you could control who has access using individual credentials or could just say that any request for a blob coming from a particular server would be valid.\nThere are packages in both R and Python for interacting with AWS that are very commonly used for getting access to s3 – Boto3 in Python, and paws in R.\nThere’s also the popular pins package in both R and Python that basically wraps using blob storage into neater code. It can use a variety of storage backends, including cloud blob storage, networked or cloud drives like Dropbox, Microsoft365 sites, and Posit Connect.\nLastly, a google sheet can be a great, temporary, way to save and recall a flat file. While this isn’t the most robust from an engineering perspective, it can often be a good first step while you’re still figuring out what the right answer is for your pipeline. It’s primary engineering weakness – that it’s editable by someone who logs in – can also be an asset if you need someone to edit the data. If you have to get something up and running right away, storing the data in a google sheet can be a great temporary solution while you figure out the long-term home for your data."
  },
  {
    "objectID": "chapters/sec1/1-3-data-arch.html#what-to-do-when-you-cant-load-all-the-data",
    "href": "chapters/sec1/1-3-data-arch.html#what-to-do-when-you-cant-load-all-the-data",
    "title": "3  Data Pipeline Architecture",
    "section": "3.4 What to do when you can’t load all the data",
    "text": "3.4 What to do when you can’t load all the data\nThere are some scales at which you literally cannot hold all your data in memory. It also may be the case that your data isn’t literally too big to fit into memory but its impractical – usually because it would make app startup times unacceptably long.\nIn this case, you’ll need to store your data somewhere else and run live queries against the data as you’re using it.\nIf you are using a database, you’ll want to be careful about how you construct your queries to make sure they perform well. The main way to think about this is whether your queries will be eager or lazy.\nIn an eager app, you’ll pull basically all of the data for the project as it starts up, while a lazy project will pull data only as it is need.\n<TODO: Diagram of eager vs lazy data pulling>\nMaking your project eager is usually much simpler – you just read in all the data at the beginning. This is often a good first cut at writing an app, as you’re not sure exactly what requirements your project has. For relatively small datasets, this is often good enough.\nIf it seems like your project is starting up slowly – or your data’s too big to all pull in, you may want to pull data more lazily.\n\n\n\n\n\n\nTip\n\n\n\nBefore you start converting queries to speed up your app, it’s always worthwhile to profile your project and actually check that the data pulling is the slow step. I’ve often been wrong in my intuitions about what the slow step of the project is.\nThere’s nothing more annoying than spending hours refactoring your project to pull data more lazily only to realize that pulling the data was never the slow step to begin with.\n\n\nIt’s also worth considering how to make your queries perform better, regardless of when they occur in your code. You want to pull the minimum amount of data possible, so making data less granular, pulling in a smaller window of data, or pre-computing summaries is great when possible (though again, it’s worth profiling before you take on a lot of work that might result in minimal performance improvements).\nOnce you’ve decided whether to make your project eager or lazy, you can think about whether to make the query eager or lazy. In most cases, when you’re working with a database, the slowest part of the process is actually pulling the data. That means that it’s generally worth it to be lazy with your query. And if you’re using dplyr from R, being eager vs lazy is simply a matter of where in the chain you put the collect statement.\nSo you’re better off sending a query to the database, letting the database do a bunch of computations, and pulling a small results set back, rather than pulling in a whole data set and doing computations in R or Python."
  },
  {
    "objectID": "chapters/sec1/1-3-data-arch.html#query-able-storage",
    "href": "chapters/sec1/1-3-data-arch.html#query-able-storage",
    "title": "3  Data Pipeline Architecture",
    "section": "3.5 Query-able storage",
    "text": "3.5 Query-able storage\nThere are basically two choices when you need to do live queries – use an external system to store and access your data or use an on-disk storage system.\nAn external system – a database or Spark cluster are the most common – are very common. The advantages of an external system is that you’ve got an external server that does all the computation. You ask for some computation, it goes off to this external system and then you get back results you can use.\nAnother advantage is that making this system accessible is as simple as getting credentials to it from the group that controls it at your organization.\nBut the downside is that (hopefully) another organization controls it. Being a database administrator (DBA) is yet another real job. Depending on your organization, having someone else control the cluster can be a blessing – it’s just there! It works! Or it can be a curse – it takes weeks or months to get access! You can’t get anywhere to write to! It really depends on your organization.\nThe other option, which have been rising in popularity for medium-sized data is on-disk storage of data that is offline query-able. DuckDB is the most popular of these tools. The basic idea is that you just store your data on disk and then you run a process only when you need to that is able to query the data and load just what you need into memory.\nDuckDB is most powerful when backed with Apache Parquet or Arrow – these relatively new data formats save your data in a clever way that make it really quick to query it without loading it into memory.\nI can’t stress enough how cool these tools are – all of a sudden you don’t really need a database for data well into the 10s of Gb or more.\nIf you have even more data than that, you need a big data tool.\nThese days Spark is more-or-less the standard big data tool for analytics teams. There are two ways you can use Spark. The first is just to treat it as a glorified database and access the data inside using SQL. That can work great if you just have big data and you need to access it.\nBut Spark is far more powerful than that - it can also do a number of other tasks, including certain kinds of machine learning inside the Spark cluster. In that case, you should use pyspark or sparklyr to do all kinds of cool Spark stuff. If you’re just getting started, I strongly recommend the book Mastering Spark with R (free online version, like this book!) for getting started. Even if you’re using pyspark, the conceptual introduction is fantastic.\n\n3.5.1 How to connect to databases?\nIn both R and Python, there are two general ways to connect to a database.\nThe first option is to use a direct connector to connect to the database. These connectors are most often thin R or Python wrappers around a bunch of C++ code. If this exists for the database you’re using, it’s generally the most optimized for your database and your should probably use it.\nThe second option is to connect to the database using a system-level driver and then connect to that database from R or Python. Many organizations like these because IT/Admins can configure them on behalf of users and can be agnostic about whether users are using them from R, Python, or something else entirely.\n<TODO: image of direct connection vs through driver>\nIn this case, you’ll have to get the driver from your database provider, configure it on the system, and connect to it from R or Python. There are two main standards for system drivers – Java Database Connectivity (JDBC) and Open Database Connectivity (ODBC).\nIn general, I recommend using ODBC over JDBC if available. If you’re using JDBC, you have to deal with configuring Java in addition to your driver. Anyone who’s ever had to configure the rJava package will share horror stories. Enough said.\nIn R I strongly recommend using an ODBC driver and using the odbc and DBI packages to make this connection. There are other ODBC packages and there are JDBC drivers available. But odbc and DBI are very actively maintained by Posit.\nIn Python, pyodbc is the main package for using ODBC connections.\n\n\n3.5.2 Data authorization is key in production\nThis is a question you probably don’t think about much as you’re puttering around inside RStudio or in a Jupyter Notebook. But when you take an app to production, this becomes a crucial question.\nThe best and easiest case here is that everyone who views the app has the same permissions to see the data. In that case, you can just allow the app access to the data, and you can check whether someone is authorized to view the app as a whole, rather than at the data access layer.\nIn some cases, you might need to provide differential data access to different users. Sometimes this can be accomplished in the app itself. For example, if you can identify the user, you can gate access to certain tabs or features of your app. Many popular app hosting options for R and Python data science apps pass the username into the app as an environment variable.\nSometimes you might also have a column in a table that allows you to filter by who’s allowed to view, so you might just be able to filter to allowable rows in your database query.\nSometimes though, you’ll actually have to pass database credentials along to the database, which will do the authorization for you. This is nice, because then all you have to do is pass along the correct credential, but it’s also a pain because you have to somehow get the credential and send it along with the query.\n\n\n3.5.3 Securely Managing Credentials\nThe single most important thing you can do to secure your credentials for your outside services is to avoid ever putting credentials in your code. That means that the actual value of your username or password should never actually appear in your code.\nThere are a few alternatives for how to make your credentials safe.\nThe first, and simplest option, is just to set them into an environment variable. If you need a refresher on how to set or use environment variables, see Section 1.4.\nIn R, it’s also very common for organizations to create internal packages that provide connections to databases. The right way to do this is to create a function that returns a database connection object that you can use in conjunction with the DBI or dplyr packages.\nHere’s an example of what that might look like if you were using a Postgres database:\n\n#' Return a database connection\n#'\n#' @param user username, character, defaults to value of DB_USER\n#' @param pw password, character, defaults to value of DB_PW\n#' @param ... other arguments passed to \n#' @param driver driver, defaults to RPostgres::Postgres\n#'\n#' @return DBI connection\n#' @export\n#'\n#' @examples\n#' my_db_con()\nmy_db_con <- function(\n    user = Sys.getenv(\"DB_USER\"), \n    pw = Sys.getenv(\"DB_PW\"), \n    ..., \n    driver = RPostgres::Postgres()\n) {\n  DBI::dbConnect(\n    driver,\n    dbname = 'my-db-name', \n    host = 'my-db.example.com', \n    port = 5432, # default for Postgres\n    user = user,\n    password = pw)\n}\n\nIn some organizations, using environment variables may be fine for configuration, but will not be perceived as secure enough, because the credentials are not encrypted at rest.\nInformation that is encrypted at rest is cryptographically secured against unauthorized access. There are at least two different things people can mean. Whole-disk encryption means that your drive is encrypted so that it can only be accessed by your machine. That means that if someone were to take your hard drive to another computer, your data wouldn’t be usable. This has become more-or-less standard. Both Mac and Windows have built-in utilities to do full-disk encryption.\nThere’s another level to encrypted at rest that some organizations want, which is to avoid storing credentials in plaintext at all. For example, an .Rprofile file is just a simple text file. If an unauthorized user were to get access to my laptop, they could steal the credentials from my .Rprofile and use them themselves. Some organizations have prohibitions against ever storing credentials in plaintext. In these cases, the credentials must be stored in a cryptographically secure way and are only decrypted when they’re actually used. If you use a password manager or store passwords in your browser, this would be an example.\nThere are a number of more secure alternatives – but they generally require a little more work.\nThere are packages in both R and Python called keyring that allow you to use the system keyring to securely store environment variables and recall them at runtime. These can be good in a development environment, but run into trouble in a production environment because they generally rely on a user actually inputting a password for the system keyring.\nConfiguring a production system that never exposes credentials in plaintext is a nontrivial undertaking. Generally, you’ll need to work with an IT/Admin to pass through the identity of the person trying to access the system to a service that will grant them a token or ticket. This is something you’ll need help from an IT/Admin to accomplish and there’s more on how to discuss the issue in Chapter 18.\n\n\n\n\n\n\nNote\n\n\n\nSome hosting software, like Posit Connect, can take care of this problem, as they store your environment variables inside the software in an encrypted fashion and inject them into the runtime.\nYou still have the issue of how you’re going to store the environment variables in your development environment, as a .env or .Rprofile file are still the most common.\n\n\nIf you are using ODBC, you or your admin may want to consider configuring a Data Source Name (DSN). A DSN provides connection details for an ODBC connection. They can be configured at a system or a user level. This means that you could put the connection details like the hostname, port, database name, etc into a simple text file right on your system and users don’t need to know anything beyond their username and password to login."
  },
  {
    "objectID": "chapters/sec1/1-3-data-arch.html#comprehension-questions",
    "href": "chapters/sec1/1-3-data-arch.html#comprehension-questions",
    "title": "3  Data Pipeline Architecture",
    "section": "3.6 Comprehension Questions",
    "text": "3.6 Comprehension Questions\n\nWhat are the different options for data storage formats for apps? What are the advantages and disadvantages of each?\nWhen should an app fetch all of the data up front? When is it better for the app to do live queries?\nWhat are the different flat file types and how can they be stored?\nWhat is a good way to create a database query that performs badly? (Hint: what’s the slowest part of the query)"
  },
  {
    "objectID": "chapters/sec1/1-3-data-arch.html#portfolio-exercise-standing-up-a-loosely-coupled-app",
    "href": "chapters/sec1/1-3-data-arch.html#portfolio-exercise-standing-up-a-loosely-coupled-app",
    "title": "3  Data Pipeline Architecture",
    "section": "3.7 Portfolio Exercise: Standing Up a Loosely Coupled App",
    "text": "3.7 Portfolio Exercise: Standing Up a Loosely Coupled App\nFind a public data source you like that’s updated on a regular candence. Some popular ones include weather, public transit, and air travel.\nCreate a job that uses R or Python to access the data and create a job to put it into a database (make sure this is compliant with the data use agreement).\nCreate an API in Plumber (R) or Flask or FastAPI (Python) that allows you to query the data – or maybe create a machine learning model that you can run against new data coming in.\nCreate an app that visualizes what’s coming out of the API using Dash or Streamlit (Python) or Shiny (either R or Python)."
  },
  {
    "objectID": "chapters/sec1/1-4-apis.html#separate-business-and-interaction-logic",
    "href": "chapters/sec1/1-4-apis.html#separate-business-and-interaction-logic",
    "title": "4  Designing Your Application Layers",
    "section": "4.1 Separate business and interaction logic",
    "text": "4.1 Separate business and interaction logic\nAll too often, I see monolithic Shiny apps of thousands or tens of thousands of lines of code, with button definitions, UI bits, and user interaction definitions mixed in among the actual work of the app.\nThere are two reasons this doesn’t work particularly well.\nIt’s much easier to read through your app or report code and understand what it’s doing when the app itself is only concerned with displaying UI elements to the user, passing choices and actions to the backend, and then displaying the result back to the user.\nThe application tier is where your business logic should live. The business logic is that actual work that the application does. For a data science app, this is often slicing and dicing data, computing statistics on that data, generating model predictions, and constructing plots.\nSeparating presentation from business layers means that you want to encapsulate the business logic somehow so that you can work on how the business logic works independently from changing the way users might interact with the app. For example, this might mean creating standalone functions to write plots or create statistics.\nIf you’re using Shiny, R Markdown, or Quarto, this will mean passing values to those functions that are no longer reactive and that have been generated from input parameters.\nLet’s start with a counterexample.\nHere’s a simple app in Shiny for R that visualizes certain data points from the Palmer Penguins data set. This is an example of bad app architecture.\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(palmerpenguins)\nlibrary(shiny)\n\nall_penguins <- c(\"Adelie\", \"Chinstrap\", \"Gentoo\")\n\n# Define UI\nui <- fluidPage(\n  sidebarLayout(\n    sidebarPanel(\n      # Select which species to include\n      selectInput(\n        inputId = \"species\", \n        label = \"Species\", \n        choices = all_penguins, \n        selected = all_penguins,\n        multiple = TRUE\n      )\n    ),\n    # Show a plot of penguin data\n    mainPanel(\n      plotOutput(\"penguinPlot\")\n    )\n  )\n)\n\nserver <- function(input, output) {\n  \n  output$penguinPlot <- renderPlot({\n    # Filter data\n    dat <- palmerpenguins::penguins %>%\n      dplyr::filter(\n        species %in% input$species\n      )\n    \n    # Render Plot\n    dat %>%\n      ggplot(\n        aes(\n          x = flipper_length_mm,\n          y = body_mass_g,\n          color = sex\n        )\n      ) +\n      geom_point()\n  })\n  \n}\n\n# Run the application \nshinyApp(ui = ui, server = server)\n\nThe structure of this Shiny app is bad. Now, it’s not a huge deal, because this is a simple Shiny app that’s pretty easy to parse if you are reasonably comfortable with R and Shiny.\nWhy is this bad? Look at the app’s server block. Because all of the app’s logic is contained inside a single plotRender statement.\nplotRender is a presentation function – it renders plots. But I’ve got logic in there that generates the data set I need and generates the plot.\nAgain, because this app is simple, it’s not a huge deal here. But imagine if this app had several tabs, multiple input dropdowns, a dozen or more plots, and complicated logic dictating how to process the dropdown choices into the plots. It would be a mess!\nInstead, we should separate the presentation logic from the business logic. That is, let’s separate the code for generating the UI, taking the user’s choice of penguin\nThe business logic – what those decisions mean, and the resulting calculations – should, at minimum, be moved into standalone functions.\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(palmerpenguins)\nlibrary(shiny)\n\nall_penguins <- c(\"Adelie\", \"Chinstrap\", \"Gentoo\")\n\n# Define UI\nui <- fluidPage(\n  sidebarLayout(\n    sidebarPanel(\n      # Select which species to include\n      selectInput(\n        inputId = \"species\", \n        label = \"Species\", \n        choices = all_penguins, \n        selected = all_penguins,\n        multiple = TRUE\n      )\n    ),\n    # Show a plot of penguin data\n    mainPanel(\n      plotOutput(\"penguinPlot\")\n    )\n  )\n)\n\nserver <- function(input, output) {\n  \n  # Filter data\n  dat <- reactive(\n    filter_data(input$species)\n    )\n  \n  # Render Plot\n  output$penguinPlot <- renderPlot(\n    make_penguin_plot(dat())\n    )\n}\n\n# Run the application \nshinyApp(ui = ui, server = server)\n\nNow you can see that the app itself had gotten much simpler. The UI hasn’t changed at all, but the server block is effectively now just two lines! And since I used descriptive function names, it’s really easy to understand what happens in each of the places where my app has reactive behavior.\nEither in the same file, or in another file I can source in, I can now include the two functions that include my business logic:\n\n#' Get the penguin data\n#'\n#' @param species character, which penguin species\n#' @return data frame\n#'\n#' @examples\n#' filter_data(\"Adelie\")\nfilter_data <- function(species = c(\"Adelie\", \"Chinstrap\", \"Gentoo\")) {\n  palmerpenguins::penguins %>%\n    dplyr::filter(\n      species %in% !!species\n    )\n}\n\n#' Create a plot of the penguin data\n#'\n#' @param data data frame\n#'\n#' @return ggplot object\n#'\n#' @examples\n#' filter_data(\"Adelie\") |> plot_gen()\nplot_gen <- function(data) {\n  data %>%\n    ggplot(\n      aes(\n        x = flipper_length_mm,\n        y = body_mass_g,\n        color = sex\n      )\n    ) +\n    geom_point()\n}\n\nNote that somewhere along the way, I also added function definitions and comments using ROxygen. This isn’t an accident! Writing standalone functions is a great way to force yourself to be clear about what should happen, and writing examples is the first step towards writing tests for your code.\n\n4.1.1 Consider using an API for long-running processes\nIn the case of a true three-layer app, it is almost always the case that the middle tier will be an application programming interface (API). In a data science app, separating business logic into functions is often sufficient. But if you’ve got a long-running bit of business logic, it’s often helpful to separate it into an API.\nYou can basically think of an API as a “function as a service”. That is, an API is just one or more functions, but instead of being called within the same process that your app is running or your report is processing, it will run in a completely separate process.\nFor example, let’s say you’ve got an app that allows users to feed in input data and then generate a model based on that data. If you generate the model inside the app, the user will have the experience of pressing the button to generate the model and having the app seize up on them while they’re waiting. Moreover, other users of the app will find themselves affected by this behavior.\nIf, instead, the button in the app ships the long-running process to a separate API, it gives you the ability to think about scaling out the presentation layer separate from the business layer.\nLuckily, if you’ve written functions for your app, turning them into an API is trivial.\nLet’s take that first function for getting the appropriate data set and turn it into an API using the plumber library in R. The FastAPI library is a popular Python library for writing APIs.\n\nlibrary(plumber)\n\n#* @apiTitle Penguin Explorer\n#* @apiDescription An API for exploring palmer penguins.\n\n#* Get data set based on parameters\n#* @param species character, which penguin species\n#* @get /data\nfunction(species = c(\"Adelie\", \"Chinstrap\", \"Gentoo\")) {\n  palmerpenguins::penguins %>%\n    dplyr::filter(\n      species %in% !!species\n    )\n}\n\nYou’ll notice that there are no changes to the actual code of the function. The commented lines that provide the function name and arguments are now prefixed by #* rather than #', and there are a few more arguments, including the type of query this function accepts and the path.\nI’ll also need to change my function in the app somewhat to actually call the API, but it’s pretty easy using a package like httr2 in R or requests in Python."
  },
  {
    "objectID": "chapters/sec1/1-4-apis.html#querying-apis",
    "href": "chapters/sec1/1-4-apis.html#querying-apis",
    "title": "4  Designing Your Application Layers",
    "section": "4.2 Querying APIs",
    "text": "4.2 Querying APIs\nBefore we get any deeper into building your own APIs, let’s understand how an API works. If you’re using R, you’ll use the httr2 package to query an API. In Python, you’ll use requests. Both of these packages have great documentation and you can look at the specifics of how to use them in those packages.\nBut both of those packages are just wrappers to help you write idiomatic R or Python that gets passed along to the system curl command to actually get information from an API via an http call.\nIn this section, we’re going to focus on what happens at that level. What is an http call curl might make and how to understand what it requests and what comes back.\n\n4.2.1 An API query is the same as an http call\nAn API call asks for an endpoint to do something.\nYour computer communicates with an API via a request-response model. Your computer requests a resource and the API sends it back. Your computer actually does this constantly as your navigate across the web.\nWhat you experience as visiting a website, your computer views as a handful of http requests to a server to fetch whatever is at that URL. The site owner’s server responds with the various assets that make up the web page, which might include the HTML skeleton for the site, the CSS styling, interactive javascript elements, and more.\nOnce your computer receives them, your browser reassembles them into a webpage for you to interact with. And when you click on a button in the site, one or more http requests go off and the responses dictate what happens next.\nSo when you manually query an API via httr2 or requests, you’re just manually making the kind of requests your computer is already making constantly.\n\n\n\n\n\n\nWhat about REST?\n\n\n\nYou may have heard the term REST API or REST-ful.\nREST is a set of architectural standards for how to build an API. An API that conforms to those standards is called REST-ful or a REST API.\nIf you’re using standard methods for constructing an API like R’s {plumber} package or FastAPI in Python, they’re going to be REST-ful – or at least close enough for standard usage.\nIn this section, I’m using the term API and REST API interchangeably.\n\n\nThe best way to understand http traffic is to take a close look at some. Luckily, you’ve got an easy tool – your web browser!\nOpen a new tab in your browser and open your developer tools. How this works will depend on your browser. In Chrome, you’ll go to View > Developer > Developer Tools and then make sure the Network tab is open.\nNow, navigate to a URL in your browser (say google.com).\nAs you do this, you’ll see the traffic pane fill up. These are the individual requests and responses going back and forth between your computer and the server.\nThere are a few parts of the requests and responses that are worth understanding in some depth. The first is the status code. Status codes appear only on responses and indicate what happened with your request to the server. In your browser, you’re probably seeing mostly 200 codes, which indicates a successful response. There’s a cheatsheet below of some other codes and what they mean.\nIf you click on an individual line in the traffic pane, you can see some additional details. One key component is the request method. The http protocol specifies a number of verbs or request methods you’re allowed to use.\nWhen you’re just loading a page on the web, it’ll be almost entirely GET requests, which fetch something. The other three basic http methods are POST or PUT to change or update something or a DELETE to (you guessed it) delete something. There are also a number of more esoteric http methods, but I’ve never seen a need for one.\nYou can see the response codes right next to each request, in chrome, codes other than 200s show up in other colors to make them easy to find.\nThe other most useful part of the display is the waterfall chart on the far right. This shows how long each request took to come back. If you’re finding that something is slow to load, inspecting the waterfall chart can help diagnose which requests are taking a long time.\nIf you click into an individual request, you can see a variety of information, including the headers and the response. Very often, inspecting the headers is a great way to debug malfunctioning http requests.\nAnother important component of both requests and responses are the headers.\nHeaders specify metadata information about the request or response. These often include the type of machine that is sending the request, authentication credentials or tokens, cookies, and more. Making sure you’ve got the correct headers on requests and responses is an important thing to check if you’re running into trouble with API calls – especially with authentication.\nLastly, requests and responses often include a body.\n\n\n\n\n\n\nNote\n\n\n\nBodies are allowed for GET and DELETE requests, but they generally are not used. Instead, the request endpoint should fully specify the resource for a DELETE, and details for GET requests are specified via query parameters, the part of the URL that shows up after the ?, like ?first_name=alex&last_name=gold.\n\n\nIn a request, the body is going to provide details on what you’re trying to do with your request. In a response, the body is the information that is being sent back. JSON is the most common body response types for an API you might build, but a website is likely to have other types of bodies, like images.\n\n4.2.1.1 Special HTTP Codes\nAs you work more with http traffic, you’ll learn some of the common codes. Here’s a cheatshet for some of the most frequent you’ll see.\n\n\n\n\n\n\n\nCode\nMeaning\n\n\n\n\n200\nEveryone’s favorite, a successful response.\n\n\n3xx\nYour query was redirected somewhere else, usually ok.\n\n\n4xx\nErrors with the request\n\n\n400\nBad request. This isn’t a request the server can understand.\n\n\n401 and 403\nUnauthorized or forbidden. Required authentication hasn’t been provided.\n\n\n404\nNot found. There isn’t any content to access here.\n\n\n5xx\nErrors with the server once your request got there.\n\n\n500\nGeneric server-side error. Your request was received, but there was an error processing it.\n\n\n504\nGateway timeout. This means that a proxy or gateway between you and the server you’re trying to access timed out before it got a response from the server.\n\n\n\n\n\n\n4.2.2 What does an API in R or Python look like?\nA great mental model for an API is as a “function as a service”. So if we think about our Palmer Penguins endpoint, it’s just a function for filtering the Palmer Penguins dataset that we can access over a network.\nA single API may have many different functions available – each function is available at an endpoint.\nAn endpoint is denoted by a path. So for example, if I had an API available at my-api.com, I might have my penguins function available at the /penguins endpoint and it would be accessible by querying my-api.com/penguins.\n{plumber} and FastAPI both include frameworks for auto-generating documentation and interactive testing facilities for your API using the Swagger framework. Most often, these docs are available on the root path / of your API, though they may default to another path like /__docs__."
  },
  {
    "objectID": "chapters/sec1/1-4-apis.html#comprehension-questions",
    "href": "chapters/sec1/1-4-apis.html#comprehension-questions",
    "title": "4  Designing Your Application Layers",
    "section": "4.3 Comprehension Questions",
    "text": "4.3 Comprehension Questions\n\nWhat are the layers of a three-layer application architecture? What libraries could you use to implement a three-layer architecture in R or Python?\nWhat is the relationship between an R or Python function and an API?\nWhat information goes in each of the following: request method, response code, headers, body, query parameters"
  },
  {
    "objectID": "chapters/sec1/1-4-apis.html#portfolio-exercise-standing-up-a-loosely-coupled-app",
    "href": "chapters/sec1/1-4-apis.html#portfolio-exercise-standing-up-a-loosely-coupled-app",
    "title": "4  Designing Your Application Layers",
    "section": "4.4 Portfolio Exercise: Standing Up a Loosely Coupled App",
    "text": "4.4 Portfolio Exercise: Standing Up a Loosely Coupled App\nFind a public data source you like that’s updated on a regular candence. Some popular ones include weather, public transit, and air travel.\nCreate a job that uses R or Python to access the data and create a job to put it into a database (make sure this is compliant with the data use agreement).\nCreate an API in Plumber (R) or Flask or FastAPI (Python) that allows you to query the data – or maybe create a machine learning model that you can run against new data coming in.\nCreate an app that visualizes what’s coming out of the API using Dash or Streamlit (Python) or Shiny (either R or Python)."
  },
  {
    "objectID": "chapters/sec1/1-5-monitor-log.html#working-with-logs-and-metrics",
    "href": "chapters/sec1/1-5-monitor-log.html#working-with-logs-and-metrics",
    "title": "5  Logging and Monitoring",
    "section": "5.1 Working with logs and metrics",
    "text": "5.1 Working with logs and metrics\nI’m assuming you already have a good sense of how to include helpful tabulations in your literate programming formats, so the rest of this chapter is going to be able logging and monitoring as distinct from just including the right output in your Jupyter Notebook, Quarto document, or R Markdown document.\nMonitoring and logging are somewhat nascent practices for data scientists. Part of the purpose of this chapter is to convince you they shouldn’t be! And to show you that they’re really not that hard.\nEmitting logs can be as simple as just putting print statements throughout your code – but there are much better ways.\nAnd you’re a data scientist – if you’re putting information into your logs, it’s probably not hard to scrape that data into structured metrics data should the need ever arise. That said, the barrier to entry for actually emitting metrics is way lower than you might realize, and most data scientists should probably pay more attention to emitting metrics than they do.\nWhether you need to emit metrics is a more subtle question. It really depends on the criticality of your system. In many data science contexts, you aren’t going to do monitoring on a real-time basis, so emitting metrics that can be used by a real-time monitoring system is less important than for other types of software.\n\n5.1.1 How to write logs\nPython has a standard logging package that is excellent and you should use that. R does not have a standard logging package, but there are a number of excellent CRAN packages for logging. My personal favorite is log4r.\nEvery time you log something, that is a log entry. Every log entry has log data – that would be a description of what’s being logged, perhaps something like “Button A Pressed”. Every entry also has a log level. The log level describes how scary the thing you’re logging is.\nMost logging libraries have 5-7 levels of logging. Both the Python logging library and log4r use these five levels from most to least scary:\n\nCritical: an error so big that the app itself shuts down. For example, if your app cannot run without a connection to an outside service, you might log an inability to connect as a Critical error.\nError: an issue that will make an operation not work, but that won’t bring down your app. In the language of software engineering, you might think of this as a caught exception. An example might be a user submitting invalid input.\nWarn/Warning: an unexpected application issue that isn’t fatal. For example, you might include having to retry doing something or noticing that resource usage is high. If something were to go wrong later, these might be helpful breadcrumbs to look at.\nInfo: something normal happened in the app. These record things like starting and stopping, successfully making database and other connections, and configuration options that are used.\nDebug: a deep record of what the app was doing. The debug log is meant for people who understand the app code to be able to understand what functions ran and with what arguments.\n\nThe nice thing about using a logger as opposed to just print statements is that they make it easy to consistently use these log levels. They also can automatically append other helpful information to each log record – like the timestamp or the active user.\nAs the log record is written, it has to be formatted using a formatter or layout. In most cases, the default log format is in plain text.\nSo if you log “Button pressed” at the info level, your log record might look something like this in the file:\n2022-11-18 21:57:50 INFO Button Pressed\nBut if you’re shipping your logs off to have them consumed by some other service, you might prefer to have a highly structured log file. The most common structured logging format is JSON, though it’s also possible to use something like YAML or XML. If you used JSON logging, the same record might be emitted as:\n{\n  \"time\": \"2022-11-18 21:57:50\",\n  \"level\": \"INFO\", \n  \"data\": \"Button Pressed\"\n}\nOnce your log record is created, it needs to be put somewhere for you to get it later via a handler in logger or an appender in log4r.\nThe first (and most common) way to handle logs is just to append them to a file on disk. Many logging consumers are very comfortable watching a file somewhere and aggregating lines of the log as they are written to the file. If you’re not already familiar with how to consume log files, you’ll learn a lot more about this in Chapter 8 on using the command line.\nIf you are emitting logs to file, you may also want to consider how long those logs stay around. Log rotation is the process of storing logs for a period of time and then deleting the old ones. A common log rotation pattern is to have a 24-hour log file. Each day, the system automatically sets up a new log file and deletes the oldest one (30 days is a typical retention period). The Python logging library does log rotation itself. log4r does not, but there is a Linux library called logrotate that you can use in concert with log4r.1\nIn some cases, you may not want to write to a file on disk. This is most common because you’re running in a Docker container – perhaps in a Kubernetes cluster. As you’ll learn more about in Chapter 9, anything that lives inside a Docker container is ephemeral. This is obviously bad if you’re writing a log that might contain clues for why a Docker container was unexpectedly killed.\nIn that case, you may want to direct log entries somewhere alternative or additional to a log file. Linux provides one facility specifically for receiving normal output called stdout (usually pronounced standard out) and one for errors called stderr (standard error).\nIn a Docker/Kubernetes context, you’ll usually emit your logs inside the running container to stdout/stderr and then have some sort of more permanent service collecting the logs outside.\nLastly, it’s possible you want to do something else completely custom with your logs. This is most common for critical or error logs. For example, you may want to send an email, slack, or text message immediately if your system emits a high-level log message.\nBoth logger and log4r have various ways of sending arbitrary messages to other services.\nThe last piece to logging is configuring your logging environment. When you initialize your logger, you’ll set the default level at which to log events. In log4r, the default level is INFO. For logger, it’s WARNING. This means that, by default, any log less critical than those levels won’t be logged.\nThis is a great use case for environment variables, as explained in Section 1.4. It’s likely that you’ll want to configure different levels of logging in production vs development apps.\n\n\n5.1.2 How to emit metrics\nMonitoring is a much more nascent practice in Data Science than logging. The main place metrics have a footprint is in the MLOps/ModelOps domain. For many data science use cases, emitting metrics may not be necessary.\nThese days, most people use the Prometheus/Grafana stack for open source monitoring of servers and their resources.\nPrometheus is an open source monitoring tool that makes it easy to store metrics data, query that data, and alert based on it. Grafana is an open source dashboarding tool that sits on top of Prometheus to do visualization of the metrics. They are usually used together to do monitoring and visualization of metrics.\nIt’s very easy to monitor the server your data science asset might sit on. Prometheus includes something called a node exporter, which makes it extremely easy to monitor the system resources of your server.\nPython has an official Prometheus client you can use for emitting metrics from a Python asset, and the openmetrics package in R makes it easy to emit metrics from a Plumber API or Shiny app.\nThis means that you can instrument your Python or R asset to emits metrics in a format that Prometheus recognizes, Prometheus watches the metrics and stores them and alerts if necessary, and you have a Grafana dashboard available for visualizing those metrics as needed.\nFor ModelOps in particular, there are also many specialized platforms for monitoring the performance of models over time. The open source vetiver package in R and Python is a model deployment framework that offers built-in facilities for extracting model metrics and putting them in a dashboard (there is currently no prometheus integration).\n\n\n5.1.3 Consuming Logs and Metrics\nOnce you’ve got your asset instrumented and your logging going, you’ll want to use them. If you’re using a professional product like Posit Connect to host your asset, you will probably get some degree of support for finding and reading relevant logs out of the box.\nIf not, you’ll probably be making good use of the skills we’ll review in Chapter 11 to watch logs and parse them.\nYou also can configure the Prometheus and Grafana stack to do both metrics monitoring and log aggregation. Grafana provides a generous free tier that allows you to use Prometheus and Grafana to do your monitoring without having to set up your own server. You can just set up their service and point your app to it.\nThere’s a great Get Started with Grafana and Prometheus doc on the GrafanaLabs website if you want to actually try it out."
  },
  {
    "objectID": "chapters/sec1/1-5-monitor-log.html#comprehension-questions",
    "href": "chapters/sec1/1-5-monitor-log.html#comprehension-questions",
    "title": "5  Logging and Monitoring",
    "section": "5.2 Comprehension Questions",
    "text": "5.2 Comprehension Questions\n\nWhat is the difference between monitoring and logging? What are the two halves of the monitoring and logging process?\nIn general, logging is good, but what are some things you should be careful not to log?\nAt what level would you log each of the following events:\n\nSomeone clicks on a particular tab in your Shiny app.\nSomeone puts an invalid entry into a text entry box.\nAn http call your app makes to an external API fails.\nThe numeric values that are going into your computational function."
  },
  {
    "objectID": "chapters/sec1/1-5-monitor-log.html#portfolio-exercise-adding-logging",
    "href": "chapters/sec1/1-5-monitor-log.html#portfolio-exercise-adding-logging",
    "title": "5  Logging and Monitoring",
    "section": "5.3 Portfolio Exercise: Adding Logging",
    "text": "5.3 Portfolio Exercise: Adding Logging\nUsing the appropriate package for whichever language you wrote in, add robust logging to both the API and the App you built.\nFor an extra challenge, consider trying to emit Prometheus metrics as well and consume them using Prometheus and Grafana."
  },
  {
    "objectID": "chapters/sec2/2-0-sec-intro.html",
    "href": "chapters/sec2/2-0-sec-intro.html",
    "title": "IT/Admin Tools",
    "section": "",
    "text": "The last section was all about how to make use of DevOps technologies as a data scientist. The next section introduces a baseline of IT/Admin knowledge.\nThis section is the bridge between the two.\nThis section is designed to get you comfortable with the tools you need to administer a remote server over SSH if you’ve never done so before. This section includes an intro to the terminal and the command line and how to use SSH to connect to a remote server.\nI think of these tools solidly as IT/Admin tools, though I think facility on the command line can also be useful as a solo data scientist.\nIf you’re comfortable using SSH on a remote server, it may well be that this section is skippable for you. I might recommend skimming – there might be a useful trick or two in here. If you’re not sure, review the questions at the end of each chapter. If you’ve got the answers down you can probably skip the chapter with no issue."
  },
  {
    "objectID": "chapters/sec2/2-1-terminal.html#what-makes-up-the-command-line",
    "href": "chapters/sec2/2-1-terminal.html#what-makes-up-the-command-line",
    "title": "6  The Terminal",
    "section": "6.1 What makes up the command line?",
    "text": "6.1 What makes up the command line?\nIt is possible to spend a lotof time customizing your terminal to be exactly what you like. Some might argue it wouldn’t be the best use of your time to do so.\nSuch people are no fun, and having a terminal that’s super customized to what you like is great. Plus you get to feel like a real hacker.\nOne of the confusing things about customizing your command line is understanding what program you’re actually interacting with and where it’s customized. So here’s a little intro.\nThere are three programs that sit on top of each other when you interact with the command line – the terminal, the shell, and the operating system.\nThe terminal is the visual program where you’ll type in commands. The terminal program you use will dictate the colors and themes available for the window, how tabs and panes work, and the keyboard shortcuts you’ll use to manage them.\nThe shell is the program you’re interacting with as you’re typing in commands. It’s what matches the words you type to actual commands or programs on your system. Depending on which shell you choose, you’ll get different options for autocompletion, options for plugins for things like git, and coloring and theming of the actual text in your terminal.\nThere is some overlap of things you can customize via the terminal vs the shell, so mix and match to your heart’s content.\nLastly, the operating system is what actually runs the commands you’re typing in. So the set of commands available to you will differ by whether you’re using Windows or Mac or Linux.\n\n\n\n\ngraph LR\n    A[A Human] --> |Types| B[Commands]\n    A --> |Opens| E\n    E[Terminal] --> |Opens| C\n    C[Shell] --> |Dispatches| B\n    D[Operating System] --> |Defines the set of| B\n    D[Operating System] --> |Runs| B\n\n\n\n\n\n\n\n\nIn the next few sections of this chapter, we’ll get into how to set up your terminal and shell so that it looks and behaves exactly the way you want.\n\n\n\n\n\n\nNote\n\n\n\nI haven’t used a Windows machine in many years. I’ve collected some recommendations here, but I can’t personally vouch for them the way I can my Mac recommendations."
  },
  {
    "objectID": "chapters/sec2/2-1-terminal.html#choose-your-terminal",
    "href": "chapters/sec2/2-1-terminal.html#choose-your-terminal",
    "title": "6  The Terminal",
    "section": "6.2 Choose your terminal",
    "text": "6.2 Choose your terminal\n\nMacOSWindows\n\n\nIf you’re using a Mac, you can use the built-in terminal app, conveniently called Terminal. It’s fine.\nIf you’re going to be using your terminal more than occasionally, I’d recommend downloading and switching to the the free iTerm2, which adds a bunch of niceties like better theming and multiple tabs.\n\n\nIf you’re using Windows, there are a variety of alternative terminals you can try, but the built-in terminal is the favorite of many users. Experiment if you like, but feel free to stick with the default."
  },
  {
    "objectID": "chapters/sec2/2-1-terminal.html#choosing-and-configuring-your-shell",
    "href": "chapters/sec2/2-1-terminal.html#choosing-and-configuring-your-shell",
    "title": "6  The Terminal",
    "section": "6.3 Choosing and configuring your shell",
    "text": "6.3 Choosing and configuring your shell\n\nMacOSWindows\n\n\nThe default shell for MacOS (and Linux) is called bash. It’s pretty great shell. There’s nothing to really replace bash, but there are bash alternatives that extend bash in various ways.\nThe most popular bash alternatives include zsh, Ksh, and Fish. If you don’t already have a favorite, I recommend zsh.1\nIt has a few advantages over bash out of the box, like better autocompletion. It also has a huge ecosystem of themes and plugins that can make your shell way prettier and more functional. There are plugins that do everything from displaying your git status on the command line to controlling your Spotify playlist.\nThere are two popular plugin managers for zsh – OhMyZsh and Prezto. I prefer and recommend Prezto, but the choice is really up to you.\nI’m not going to go through the steps of installing these tools – there are numerous online walkthroughs and guides that you can google.\nBut it is a little confusing to know what to customize where, so here’s the high level overview if you’ve installed iTerm2, zsh, and prezto. You’ll customize the look of the window and the tab behavior in the iTerm2 preferences and customize the text theme and plugins via prezto. You can mostly skip any customization of zsh in the .zshrc since you’ll be doing that in Prezto.\n\n\nWindows comes with two shells built in, the Command shell (cmd) and the PowerShell.\nThe command shell is older and has been superseded by PowerShell. If you’re just getting started, you absolutely should just work with PowerShell. If you’ve been using Command shell on a Windows machine for a long time, most Command shell command work in PowerShell, so it may be worth switching over.\nOnce you’ve installed PowerShell, many people like customizing it with Oh My Posh."
  },
  {
    "objectID": "chapters/sec2/2-1-terminal.html#comprehension-questions",
    "href": "chapters/sec2/2-1-terminal.html#comprehension-questions",
    "title": "6  The Terminal",
    "section": "6.4 Comprehension Questions",
    "text": "6.4 Comprehension Questions\n\nDraw a mental map that includes the following: terminal, shell, operating system, my laptop"
  },
  {
    "objectID": "chapters/sec2/2-2-ssh.html#how-ssh-works",
    "href": "chapters/sec2/2-2-ssh.html#how-ssh-works",
    "title": "7  Connecting Securely with SSH",
    "section": "7.1 How SSH works",
    "text": "7.1 How SSH works\nSSH allows you to directly access the command line on a remote host from anywhere that can connect to it over a network. It is the main way to administer a server.\nSSH works via the exchange of cryptographic keys. You will create an SSH key, which comes in two parts – the public key and the private key.\n\n\n\n\n\n\nNote\n\n\n\nI believe the terms public key and private key are a little bit of a misnomer. The analogy to the real world is a little clearer by thinking of the private key as the key and the public key as the lock.\nYou’re the only one who has the key, but you can hand copies of the lock around so they can always verify that your key is the real one.\n\n\nAs the name might suggest, you keep the private key secret. The best practice is to never move it once it has been created. You can give the public key out to anywhere you might need to access using SSH. Popular targets include remote servers you’ll need to SSH into as well as remote git hosts, like GitHub.\nSSH works on the basis of public key cryptography, which is really cool. It also defies common sense a little bit – it is a little strange that you create this two-part thing and it’s absolutely fine to hand one half around but really bad if you mix them up.\nA short digression about the mathematics of public key cryptography may help clarify.\nPublic key cryptography relies on mathematical operations that are easy in one direction, but really hard to reverse. This means that if I’ve got the public key, it’s really hard to reverse-engineer the private key, but really easy to check that the private key is right if I’m given it up front.\nAn example of an operation like this is multiplying prime numbers together. Having a public key is just like being told a number – say \\(91\\). Even if you know it’s the product of two primes, it’ll probably take you a few moments to figure out the right primes are \\(7\\) and \\(13\\).\nBut if you already have \\(91\\) and I tell you that the right primes are \\(7\\) and \\(13\\), it’s super quick to check that those are indeed the right ones.\nThe biggest difference between multiplying \\(7 * 13 = 91\\) and modern encryption algorithms is the size of the number. Public key cryptography doesn’t use small numbers like 91. It uses numbers with 91 or 9,191 digits.\nModern encryption methods also use substantially more convoluted mathematical operations than simple multiplication – but the idea is completely the same, and prime numbers are equally important.\nThe point is that SSH public keys are very big numbers, so while someone could try to reverse-engineer the private keys by brute force, it’d take more time than we have left before the heat death of the universe at current computing speeds.\nThis is why you can give your public key to a server or service that you might not fully control. Someone who has your public key can verify that your private key is the one that fits that public key – but it’s basically impossible to reverse engineer the private key with the public key in hand.\nHowever, it is totally possible to compromise the security of an SSH connection by being sloppy with your private keys. So while SSH is cyptographically super secure, the whole system is only as secure as you. Always keep your private keys securely in the place where they were created and share only the public keys."
  },
  {
    "objectID": "chapters/sec2/2-2-ssh.html#practical-ssh-usage",
    "href": "chapters/sec2/2-2-ssh.html#practical-ssh-usage",
    "title": "7  Connecting Securely with SSH",
    "section": "7.2 Practical SSH usage",
    "text": "7.2 Practical SSH usage\nBefore SSH will work, your keypair needs to be created and the public key needs to be shared. There are tons of guides online to creating an SSH key for your operating system – google one when you need it.\n\n\n\n\n\n\nDebugging SSH\n\n\n\nSSH has one of my favorite debugging modes.\nIf something’s not working when you try to connect, just add a -v to your command for verbose mode. If that’s not enough information, add another v for -vv, and even another!\nEvery v you add (up to 3) will make the output more verbose.\n\n\nThe way you register a keypair as valid on a server you control is by creating a user on that server and adding the public key to the end of the .ssh/authorized_keys file inside the user’s their home directory. More on server users and home directories in Chapter 11.\nIf you’re the server admin, you’ll have your users create their SSH keys, share the public keys with you, and you’ll put them into the right place on the server.\nIf you need to SSH from the server to another server or to a service that uses SSH, like GitHub, you’ll create another SSH key on the server and use that public key on the far end of the connection.\nIf you follow standard instructions for creating a key, it will use the default name, probably id_ed25519.1 I’d recommend sticking with the default name if you’ve only got one. This is because the ssh command will just use the keys you’ve created if they have the default name.\nIf you don’t want to use the default name for some reason, you can specify a particular key with the -i flag.\nIf you’re using SSH a lot on the same servers, I’d recommend setting up an SSH config file. You can include usernames and addresses in a config file so instead of typing ssh alexkgold@do4ds-lab.shop I can just type ssh lab.\nA google search should return good instructions for setting up your SSH config when you get there."
  },
  {
    "objectID": "chapters/sec2/2-2-ssh.html#comprehension-questions",
    "href": "chapters/sec2/2-2-ssh.html#comprehension-questions",
    "title": "7  Connecting Securely with SSH",
    "section": "7.3 Comprehension Questions",
    "text": "7.3 Comprehension Questions\n\nUnder what circumstances should you move or share your SSH private key?\nWhat is it about SSH public keys that makes them safe to share?"
  },
  {
    "objectID": "chapters/sec2/2-3-cmd-line.html#the-structure-of-bash-commands",
    "href": "chapters/sec2/2-3-cmd-line.html#the-structure-of-bash-commands",
    "title": "8  Getting comfortable on the Command Line",
    "section": "8.1 The structure of bash commands",
    "text": "8.1 The structure of bash commands\nbash and its derivatives provide small programs that each do one small thing well called a command.\nA command is the program you want to run, usually an abbreviation of the word for what you want to do. For example, the command to list the contents of a directory is ls.\nArguments tell the command what to run on. They come after the command with a space in between. For example, if I want to run ls on the directory /home/alex, I can run ls /home/alex on the command line.\nMany commands have default arguments. For example, ls runs by default on the directory I’m currently in. So if I’m in /home/alex, running ls and running ls /home/alex would return the same thing.\nOptions or flags modify how the command operates. Flags are denoted by having one or more dashes before them. For example, the ls command, which lists files, has the optional flag -l, which indicates that the files should be displayed as a list.\nFlags always come in between the command and any arguments to the command. So, for example, if I want to get the files in /home/alex as a list, I can run ls -l /home/alex or navigate to /home/alex and run ls -l.\nSome flags themselves have arguments. So, for example, if you’re using the -l flag on ls, you can also use the -D flag to format the datetime when the file was last updated.\nSo, for example, running ls -l -D %Y-%m-%dT%H:%M:%S /home/alex will list all the files in /home/alex with the date-time of the last update formatted in ISO 8601 format (which is always the correct format for dates).\nIt’s nice that this structure is standard. You always know that a bash command will be formatted as <command> <flags + flag args> <command args>. The downside is that having the main argument come all the way at the end, after all the flags, can make it really hard to mentally parse commands if you don’t know them super well.\nBecause there can be so many arguments, bash commands can get long. Sometimes you’ll see bash commands split them over multiple lines. You can tell bash you want it to keep reading after a line break by ending the line with a space and a \\. It’s often nice to include one flag or argument per line.\nFor example, here’s that ls command more nicely formatted:\n> ls -l \\\n  -D %Y-%m-%dT%H:%M:%S \\\n  /home/alex\nThis is at least a little easier to parse. There is also help available!\nAll of the flags and arguments for commands can be found in the program’s man page (short for manual). You can access the man page for any command with man <command>. You can scroll the man page with arrow keys and exit with q.\nIf you ever can’t figure out how to quit, ctrl + c will generally quit from any command line situation on Linux, Mac, and Windows.\n\n\n\n\n\n\n\nSymbol\nWhat it is\n\n\n\n\nman\nmanual\n\n\nq\nQuit man pages (and many other situations)\n\n\n\\\nContinue command on new line"
  },
  {
    "objectID": "chapters/sec2/2-3-cmd-line.html#linux-directories-and-files",
    "href": "chapters/sec2/2-3-cmd-line.html#linux-directories-and-files",
    "title": "8  Getting comfortable on the Command Line",
    "section": "8.2 Linux directories and files",
    "text": "8.2 Linux directories and files\nIn Linux, directories define where you can be. A directory is just a container for files and other directories.\nIn Linux, the entire file system is a tree (or perhaps an upside-down tree). The root directory, / is the base of the tree and a / in between two directories means that it’s a sub-directory. So the directory /home/alex is the alex directory, which is contained in home, which is in the root directory /.\nIn Linux, every directory is a sub-directory of / or a sub-directory of a sub-directory of / or…you get the picture. The sequence of directories that defines a location is called a file path.\nEvery Linux command happens at a particular file path – called the working directory. In some cases the commands you’re allowed to run or what they do will vary a lot based on where you are when they run.1\nFile paths can be either absolute – specified relative to the root – or relative to the working directory. Absolute file paths always start with / so they’re easy to identify.\nDepending on what you’re doing, either absolute or relative paths make more sense. In general, absolute file paths make more sense when you want to access the same resource regardless of where the command is run, and relative file paths make more sense when you want to access a resource specific to where you run it.\nAt any time, you can get the full path to your working directory with the pwd command, which is an abbreviation for print working directory. When you’re writing out a file path, the current working directory is at ..\nGoing back to the ls command, you can now see that the default argument to ls is .. You can test this for yourself by comparing the output of ls and ls .. They should be identical.\nAside from / and ., there are two other special directories.\n.. is the parent of the directory you’re in, so you can move to the parent of your current directory using cd .. and to it’s parent with cd ../...\n~ is the home directory of your user (assuming it has one). We’ll get more into what that means in a bit.\n\n\n\n\n\n\nHow does / compare to C:?\n\n\n\nIf you’re a Windows person, you might think this is analogous to C:. You’re not wrong, but the analogy is imprecise.\nIn Linux, everything is a sub-directory of /, irrespective of the configuration of physical or virtual drives that houses the storage. Frequently, people will put extra drives on their server – a process called mounting – and house them at /mnt (short for…you guessed it).\nThat’s different from Windows. In Windows you can have multiple roots, one for each physical or logical disk you’ve got. That’s why your machine may have a D: drive, or if you have network shares, those will often be on M: or N: or P:.\n\n\nAlong with being able to inspect directories, it’s useful to be able to change your working directory with the cd command, short for change directory.\nRecap of commands in this section\n\n\n\n\n\n\n\n\n\nCommand\nWhat it does/is\nHelpful options\nExample\n\n\n\n\n/\nsystem root\n\n\n\n\n.\ncurrent working directory\n\n\n\n\nls\nlist objects in a directory\n-l - format as list\n-a - all (include hidden files)\n$ ls .\n$ ls -la\n\n\npwd\nprint working directory\n\n$ pwd\n\n\ncd\nchange directory\n\n$  cd ~ / D ocuments\n\n\n~\nhome directory of the current user\n\n$ ls ~"
  },
  {
    "objectID": "chapters/sec2/2-3-cmd-line.html#reading-text-files",
    "href": "chapters/sec2/2-3-cmd-line.html#reading-text-files",
    "title": "8  Getting comfortable on the Command Line",
    "section": "8.3 Reading text files",
    "text": "8.3 Reading text files\nBeing comfortable opening and navigating around text files is an essential IT/Admin skill.\ncat is the command to print a file, starting at the beginning.\nSometimes you’ve got a really big file and you want to see just part. less starts at the top with the ability to scroll down. head prints the first few lines and quits. It is especially useful to peer at the beginning of a plain text data file (like csv) as it prints the first few rows and exits – so you can preview the beginning of a very large data file very quickly.\ntail skips right to the end of a file. Log files usually are written so the newest part is last – so much so that “tailing a log file” is a synonym for looking at it. In some cases, you’ll want to tail a file as the process is still running and writing information to the log. You can get a live view of the end of the file using the -f flag (for follow).\nSometimes you want to search around inside a text file. You’re probably familiar with the power of regular expressions (regex) to search for specific character sequences in text strings. The Linux command to do regex searches is grep, which returns results that match the regex pattern you specify.\nThe true power of grep is unlocked in combination with the pipe. The Linux pipe operator – | – takes the output of the previous command and sends it into the next one.\n\n\n\n\n\n\nHaven’t I seen the pipe before?\n\n\n\nThe pipe should feel extremely familiar to R users.\nThe {magrittr} pipe, %>%, has become extremely popular as part of the tidyverse since its introduction in 2013. A base R pipe, |>, was released as a part of R 4.1 in 2021.\nThe original pipe in {magrittr} took inspiration from both the Unix pipe and the pipe operator in the F# programming langauge.\n\n\nA combination I do all the time is to pipe the output of ls into grep when searching for a file inside a directory. So if I was searching for a file whose name contained the word data, that might look something like ls ~/projects/my-project | grep data.\n\n\n\n\n\n\n\n\nCommand\nWhat it does\nNotes + Helpful options\n\n\n\n\ncat\nPrints a file.\n\n\n\nless\nPrints a file, but just a little.\nCan be very helpful to look at a few rows of csv.\nLazily reads lines, so can be much faster than cat for big files.\n\n\ntail\nLook at the end of a file.\nUseful for logs, where the newest part is last.\nThe -f flag is useful for a live view.\n\n\nhead\nLook at the beginning of a file.\nDefaults to 10 lines, can specify a different number with -n <n>.\n\n\ngrep\nSearch a file using regex.\nWriting regex can be a pain. I suggest testing expressions on regex101.com.\nOften useful in combination with the pipe.\n\n\n|\nthe pipe"
  },
  {
    "objectID": "chapters/sec2/2-3-cmd-line.html#copying-moving-and-removing-files-and-directories",
    "href": "chapters/sec2/2-3-cmd-line.html#copying-moving-and-removing-files-and-directories",
    "title": "8  Getting comfortable on the Command Line",
    "section": "8.4 Copying, moving and removing files and directories",
    "text": "8.4 Copying, moving and removing files and directories\nYou can copy a file from one place to another using the cp command. cp leaves behind the old file and adds the new one at the specified location. You can move a file with the mv command, which does not leave the old file behind.\nIf you want to remove a file entirely, you can use the rm command. There is also a version to remove a directory, rmdir.\n\n\n\n\n\n\nWarning\n\n\n\nBe very careful with the rm command.\nUnlike on your desktop there’s no recycle bin! Things that are deleted are instantly deleted forever.\n\n\nIf you want to make a directory, mkdir makes a directory at the specified filepath. mkdir will only work if it’s creating the entire file path specified, so the -p flag can be handy to create only the parts of the path that don’t exist.\nSometimes it’s useful to operate on every file inside a directory. You can get every file that matches a pattern with the wildcard, *. You can also do partial matching with the wildcard to get all the files that match part of a pattern.\nFor example, let’s say I have a /data directory and I want to put a copy of only the .csv files inside into a new sub-directory. I could do the following:\n> mkdir -p /data/data-copy\n> cp /data/*.csv /data/data-copy\n\n\n\n\n\n\n\n\n\nCommand\nWhat it does/is\nNotes + Helpful Options\nExample\n\n\n\n\nrm\nremove – deletes i m mediately and p e rmantenly\n-r - r e cursively remove e verything below a file path\n-f - force - dont ask for each file\n$ r m  - rf o ld_docs/\nBE VERY CAREFUL WITH -rf\n\n\ncp\ncopy\n\n\n\n\nmv\nmove\n\n\n\n\n*\nwildcard\n\n\n\n\nmkdir/ rmdir\nmake/ remove directory\n-p - create any parts that don’t exist"
  },
  {
    "objectID": "chapters/sec2/2-3-cmd-line.html#moving-things-to-and-from-the-server",
    "href": "chapters/sec2/2-3-cmd-line.html#moving-things-to-and-from-the-server",
    "title": "8  Getting comfortable on the Command Line",
    "section": "8.5 Moving things to and from the server",
    "text": "8.5 Moving things to and from the server\nIt’s very common to have a file on your server you want to move to your desktop or vice versa.\nIt’s generally easier to move a single file rather than a whole bunch. The tar command turns a set of files or whole directory into a single archive file, usually with the file suffix .tar.gz. Creating an archive also does some compression when it creates the archive file. The amount depends on the content.\nThe tar command is used to both create and unpack (extract) archive files and telling it which one requires the use of several flags. I never remember them – this is a command I google every time I use it. The flags you’ll use most often are in the cheat sheet below.\nOnce you’ve created an archive file, you’ve got to move it. The scp command is the way to do this. scp – short for secure copy – is basically a combo of SSH and copy.2 scp is particularly nice because it uses the syntax you’re used to from using cp.\nSince scp establishes an SSH connection, you need to make the request to somewhere that is accepting SSH connections. Hopefully your server is accepting SSH connections and your laptop is not.\nYou’ll almost certainly have the experience at some point of being on your server and wanting to scp something to or from your laptop. You need to do the scp command from a regular terminal on your laptop, not one that’s already SSH-ed into your server.\n\n\n\n\n\n\n\n\nCommand\nWhat it does\nNotes + Helpful options\n\n\n\n\ntar\nc ompress/decompress file/directory\nAlmost always used with flags.\nCreate is usually\ntar -czf <archiv e name> <file(s)>\nExtract is usually\ntar -x fv <archive name>\n\n\nscp\nCopy across ssh\nCan use most ssh flags (like -i and -v)"
  },
  {
    "objectID": "chapters/sec2/2-3-cmd-line.html#writing-files-on-the-command-line",
    "href": "chapters/sec2/2-3-cmd-line.html#writing-files-on-the-command-line",
    "title": "8  Getting comfortable on the Command Line",
    "section": "8.6 Writing files on the command line",
    "text": "8.6 Writing files on the command line\nThere will be many situations where writing into a text file will be handy while administering your server – for example, when changing config files. When you’re on the command line, you’ll use a command line tool for writing into those files – meaning you’ll only have your keyboard to navigate, save, and exit.\nThere are times when you want to make files or directories with nothing in them – the touch command makes a blank file at the specified file path.\nYou also may want to take some text and make it into a file. You can do this with the > command. >> does the same thing, but appends it to the end of the file. This works similarly to the pipe, |, where the output of the left-hand side is passed as the input to a file on the right-hand side.\nA common reason you might want to do this is to add something to the end of your .gitignore. You can’t just type a word on the command line and have it treated like a string – so you may need the echo command to have something you type treated as a string.\nFor example, if you want to add your .Rprofile file to your .gitignore, you could do that with echo .Rprofile >> .gitignore.\n\n\n\n\n\n\n\n\nCommand\nWhat it does\nNotes\n\n\n\n\ntouch\nCreates file if doesn’t already exist.\nUpdates timestamp to current time if it does exist\n\n\n>\nOverwrite file contents\nCreates a new file if it doesn’t exist\n\n\n>>\nConcatenate to end of file\nCreates a new file if it doesn’t exist\n\n\n\n\n8.6.1 Command line text editors\nThere are two command line text editors you’ll probably encounter – both extremely powerful text editing tools: nano and vi/vim.3\nYou can open a file in either by typing nano <filename> or vi <filename>. Unfortunately for many newbie Linux Admins it’s extremely easy to get stuck inside a file with no hint of how to get out!\nIn nano there will be helpful prompts along the bottom to tell you how to interact with the file, so you’ll see once you’re ready to go, you can exit with ^x. But what is ^? On most keyboards, you can insert the caret character, ^, by pressing Shift + 6. But that’s not what this is.\nIn this case, the ^ caret is short for Ctrl on Windows and for Command (⌘) on Mac. Phew!\nWhere nano gives you helpful – if obscure – hints, vim leaves you all on your own. It doesn’t even tell you you’re inside vim!\nThis is where many people get stuck and end up having to just exit and start a new terminal session. It’s not the end of the world if you do, but knowing a few vim commands can help you avoid that fate.\nOne of the most confusing things about vim is that you can’t edit the file when you first enter. That’s because vim keybindings were (1) developed before keyboards uniformly had arrow keys and (2) are designed to minimize how much your hands need to move.\nIf you feel like taking the time, learning vim keybindings can make navigating and editing text (code) files easier. Plus it just feels really cool. I recommend spending some time trying. In this section, I’m just going to help you get the minimum amount of vim you need to be safe.\nWhen you enter, you’re in normal mode, which is for navigating through the file. Typing things on your keyboard won’t type into the document, but will do other things.\nPressing the i key activates insert mode. For those of us who are comfortable in a word processor like Word or Google Docs, insert mode will feel very natural. You can type and words will appear and you can navigate with the arrow keys.\nOnce you’re done writing, you can go back to normal mode by pressing the escape key. In addition to navigating the file, normal mode allows you to do file operations like saving and quitting.\nFile operations are prefixed with a colon :. The two most common commands you’ll use are save (write) and quit. You can combine these together, so you can save and quit in one command using :wq.\nSometimes you may want to exit without saving. If you’ve made changes and try to exit with :q, you’ll find yourself in an endless loop of warnings that your changes won’t be saved. You can tell vim you mean it with the exclamation mark and exit using :q!.\n\n\n\n\n\n\n\n\nVim Command\nWhat it does\nNotes + Helpful options\n\n\n\n\n^\nPrefix for file command in nano editor.\nIts the ⌘ or Ctrl key, not the caret symbol.\n\n\ni\nEnter insert mode in vim\n\n\n\nescape\nEnter normal mode in vim.\n\n\n\n:w\nWrite the current file in vim (from normal mode)\nCan be combined to save and quit in one, :wq\n\n\n:q\nQuit vim (from normal mode)\n:q! quit without saving"
  },
  {
    "objectID": "chapters/sec2/2-3-cmd-line.html#comprehension-questions",
    "href": "chapters/sec2/2-3-cmd-line.html#comprehension-questions",
    "title": "8  Getting comfortable on the Command Line",
    "section": "8.7 Comprehension Questions",
    "text": "8.7 Comprehension Questions\n\nIf you don’t know the real commands for them, make up what you think the bash commands might be to do the following. So if you think you’d create a command called cmd with a flag -p and an argument arg, you’d write cmd -p <what p does> <arg>. In the next chapter you’ll get to see how close you got to the real thing:\n\nChange Directories, the only argument is where to go\nMaking a Directory, with an optional flag to make parents as you go. The only argument is the directory to make.\nRemove files, with flags to do so recursively and to force it without checking in first. The only argument is the file or directory to remove."
  },
  {
    "objectID": "chapters/sec2/2-4-docker.html#containers-are-a-packaging-tool",
    "href": "chapters/sec2/2-4-docker.html#containers-are-a-packaging-tool",
    "title": "9  Docker for Data Science",
    "section": "9.1 Containers are a Packaging Tool",
    "text": "9.1 Containers are a Packaging Tool\nLet’s tell a story that might feel familiar. A collaborator sends you a piece of code. You go to run it on your machine and an error pops up Python 3.7 not found. So you spend an hour on Stack Overflow, figuring out how to install version 3.7 of Python.\nThen you try to run the code again, which creates some maps, and this time get an error System library gdal not found. “Augh!” you cry, “Why is there not a way to include all of these dependencies with the code?!?”\nYou have just discovered one of the primary use cases for a container.\nContainers are a way to package up some code with all of its dependencies, making it easy to run the code later, share it with someone else for collaboration, or put it onto a production server – all while being reasonably confident that you won’t ever have to say, “well, it runs on my machine”.\nDocker is by far the most popular open-source containerization platform. So much so that for most purposes container is a synonym for Docker container.1 In this chapter, containers will exclusively refer to Docker containers.\nIn addition to making it easy to get all of the dependencies with an app, Docker also makes it easy to run a bunch of different isolated apps without having them interfere with each other.\nVirtual machines of various sorts have been around since the 1960s, and are still used for many applications. In contrast to a virtual machine, Docker is much more lightweight. Once a container has been downloaded to your machine, it can start up in less than a second.\nThis is why Docker – not the only, or even the first open source containerization system – was the first to hit the mainstream, as much as any esoteric code-development and deployment tool can be said to “hit the mainstream”.\nThis means that – for the most part – anything that can run in a Docker container in one place can be run on another machine with very minimal configuration.\n\n\n\n\n\n\nNote\n\n\n\nThere are exceptions. Until recently, a huge fraction of laptop CPUs were of a particular architecture called x86.\nApple’s recent M1 and M2 chips run on an ARM64 architecture, which had previously been used almost exclusively for phones and tablets. The details aren’t super important, but the upshot is that getting containers working on Apple silicon may not be trivial.\n\n\nDocker doesn’t completely negate the need for other sorts of IT tooling, because you still have to provision the physical hardware somehow, but it does make everything much more self-contained. And if you’ve already got a laptop, you can easily run Docker containers with just a few commands (we’ll get to that below)."
  },
  {
    "objectID": "chapters/sec2/2-4-docker.html#containers-for-data-science",
    "href": "chapters/sec2/2-4-docker.html#containers-for-data-science",
    "title": "9  Docker for Data Science",
    "section": "9.2 Containers for Data Science",
    "text": "9.2 Containers for Data Science\nIn a data science context, there are two main ways you might use containers – as a way to package a development environment for someone else to use, and as a way to package a finished app for archiving, reproducibility, and production.\n\n\n\n\n\n\nThe Data Science Reproducibility Stack\n\n\n\nA reminder from the reproducibility chapter:\nThe data science reproducibility stack generally includes 6 elements:\n\nCode\nData\nR + Python Packages\nR + Python Versions\nOther System Libraries\nOperating System\n\n\n\nIf you’re running RStudio Server or JupyterHub on a centralized server, Docker can be a great way to maintain that server. In my opinion, maintaining a Docker container is one of the easiest ways to start on an infrastructure-as-code journey.\nWe’re not going to get terribly deep into this use case, as creating the overwhelming majority of the work involved is standard IT/Admin tasks for hosting a server - things like managing networking, authentication and authorization, security, and more.\n\n\n\n\n\n\nNote\n\n\n\nIf you’re thinking about hosting a data science workbench in Docker, you should think carefully about whether you want to deal with standalone containers, or whether you’re really looking for container orchestration using Kubernetes.\n\n\nIn this chapter, I’ll suggest trying to stand up RStudio Server in a container on your desktop, but don’t let the ease fool you. The majority of difficulties with administering a server are the same, even if you put your application stack into a Docker container. Section II of this book will have a lot more on those challenges, and I suggest you check it out if you’re interested.\nInstead, we’re going to stick with talking about how actual data scientists would want to use Docker: to archive and share completed data science assets.\n\nIn this pattern, you’ll put your whole reproducibility stack inside the container itself – perhaps minus your data."
  },
  {
    "objectID": "chapters/sec2/2-4-docker.html#container-gotchas",
    "href": "chapters/sec2/2-4-docker.html#container-gotchas",
    "title": "9  Docker for Data Science",
    "section": "9.3 Container Gotchas",
    "text": "9.3 Container Gotchas\nDocker containers are great for certain purposes, but there are also some tradeoffs that it’s worth being aware of.\nThe first is the tradeoff of Docker’s strength: a container only gets access to the resources it has specifically been allowed to access.\nThis is a great feature for security and process isolation, but it means you may run into some issues with networking and access to system resources, like your files. You’ll have to develop a reasonably good mental model of the relationship of the Docker container to the rest of your machine in order to be able to develop effectively.\nIt’s worth noting that in some environments – especially highly-regulated ones – a Docker container may not be a sufficient level of reproducibility. Differences between machines at the physical hardware level could potentially mean that numeric solutions could differ across machines, even with the same container. You probably know if you’re in this kind of environment and you have to maintain physical machines.\nThere are also several antipatterns that using a container could facilitate.\nThe biggest reproducibility headache for most data scientists is managing R and Python package environments. While you can just install a bunch of packages into a container, save the container state, and move on, this really isn’t a good solution.\nIf you do this, you’ve got the last state of your environment saved, but it’s not really reproducible. If you come back next year and need to add a new package, you’ll have no way to do it without potentially breaking the whole environment.\nThe obvious solution is to write down the steps for creating your Docker container – in a file called a Dockerfile. Here, it’s tempting to create a Dockerfile that looks like:\n...\nRUN /opt/R/4.1.0/bin/R install.packages(c(\"shiny\", \"dplyr\"))\n...\nBut this is also completely non-reproducible. Whenever you rebuild your container, you’ll install the newest versions of Shiny and Dplyr afresh, potentially ruining the reproducibility of your code. For that reason, the best move is to still use R- and Python-specific libraries for capturing package state – like renv and rig in R and virtualenv , conda , and pyenv in Python – rather than relying on Docker for that job. There’s more on those topics in the Chapter 2 on environments."
  },
  {
    "objectID": "chapters/sec2/2-4-docker.html#trying-out-docker",
    "href": "chapters/sec2/2-4-docker.html#trying-out-docker",
    "title": "9  Docker for Data Science",
    "section": "9.4 Trying out Docker",
    "text": "9.4 Trying out Docker\nIf you’ve read this far, you probably have a reasonably good mental model of when you might want to use Docker to encapsulate your data science environment or when you might not. The rest of this chapter will be a hands-on intro to using Docker to run a finished app.\n\n9.4.1 Prerequisites\nIn order to get started with Docker, you’ll need to know a few things. The first is that you’ll have to actually have Docker installed on an environment you can use. The easiest way to do this is to install Docker Desktop on your laptop, but you can also put Docker on a server environment.\n\n\n\n\n\n\nNote\n\n\n\nWe’re going to use Docker from a terminal on your machine. I’ll give you all the commands you’ll need, but you need to know how to find and open a terminal.\nIf that’s new to you, you might want to skip ahead and check out the beginning of Chapter 8 so you can at least open a terminal.\n\n\n\n\n9.4.2 Getting Started\nLet’s get started with an example that demonstrates the power of Docker right off the bat.\nOnce you’ve got Docker Desktop installed and running, type the following in your terminal:\ndocker run --rm -d \\\n  -p 8000:8000 \\\n  --name palmer-plumber \\\n  alexkgold/plumber\nOnce you type in this command, it’ll take a minute to pull, extract, and start the container.\nOnce the container starts up, you’ll see a long sequence of letters and numbers. Now, navigate to http://localhost:8000/__docs__/ in your browser (this URL has to be exact!), and you should see the documentation for an R language API that lets you explore the Palmer Penguins data set\nThat was probably pretty uninspiring. It took a long time to download and get started. In order to show the real power of Docker, let’s now kill the container with\ndocker kill palmer-plumber\nYou can check that the container isn’t running by trying to visit that URL again. You’ll get an error.\nLet’s bring the container back up by running the docker run command above again.\nThis time is should be quick – probably less than a second – now that you’ve got the container downloaded. THIS is the power of Docker.\n\nAs you click around, seeing penguin stats and seeing plots, you might notice that nothing is showing up on the command line…but what if I want logs of what people are doing? Or I need to look at the app code?\nYou can get into the container to poke around using the command\ndocker exec -it palmer-plumber /bin/bash\nOnce you’re in, try cat api/plumber.R to look at the code of the running API.\nWhen you need to get out, you can leave by typing exit.\ndocker exec is a general purpose command for executing a command inside a running container. The overwhelming majority of the time I use it, it’s to get a terminal inside a running container so I can poke around.\nYou can spend a lot of time getting deep into why the command works, but just memorizing (or, more likely, repeatedly googling) docker exec -it <container> /bin/bash will get you pretty far.\n\n\n\n\n\n\nNote\n\n\n\nIf you’re used to running things on servers, you might be in the habit of SSH-ing in, poking around, and fixing things that are broken. This isn’t great for a lot of reasons, but it’s a huge anti-pattern in Docker land.\nContainers are stateless and immutable. This means that anything that happens in the container stays in the container – even when the container goes away. If something goes wrong in your running container, you may need to exec in to poke around, but you should fix it by rebuilding and redeploying your image, not by changing the running container.\n\n\nOne nicety of Docker is that it gives you quick access to the most common reason you’d probably exec into the container – looking at logs.\nAfter you’ve clicked around a little in the API, try running:\ndocker logs palmer-plumber\nWe’re done with this container now. Feel free to kill it before you move on.\n\nGreat! We’ve played around with this container pretty thoroughly.\nBefore we get into how this all works, let’s try one more example.\nGo back into your terminal and navigate to a directory you can play around in (the cd command is your friend here, see Chapter 8 if you’re not familiar). Run the following in your terminal:\ndocker run \\\n-v ${PWD}:/project-out \\\nalexkgold/batch:0.1\nIt’ll take a minute to download – this container is about 600Mb. You may need to grant the container access to a directory on your machine when it runs. This container will take a few moments to run. If you go to the directory in file browser, you should be able to open hello.html in your web browser – it should be a rendered version of a Jupyter Notebook.\nThis notebook is just a very basic visualization, but you can see how it’s nice to be able to render a Jupyter Notebook locally without having to worry about making sure you had any of the dependencies installed. This is good both for running on demand, and also for archival purposes.\n\nNow that we’ve got Docker working for you, let’s take a step back, explain what we just did, and dive deeper into how this can be helpful.\nHopefully these two examples are exciting – in the first, we got an interactive web API running like a server on our laptop in just a few seconds – and without installing any of the packages or even a version of R locally. In the second, we rendered a Jupyter Notebook using the quarto library – again, without worrying about downloading it locally."
  },
  {
    "objectID": "chapters/sec2/2-4-docker.html#container-lifecycle",
    "href": "chapters/sec2/2-4-docker.html#container-lifecycle",
    "title": "9  Docker for Data Science",
    "section": "9.5 Container Lifecycle",
    "text": "9.5 Container Lifecycle\nBefore we dig into the nitty-gritty of how that all worked – and how you might change it for your own purposes, let’s spend just a minute clarifying the lifecycle of a Docker container.\nThis image explains the different states a Docker container can be in, and the commands you’ll need to move them around.\n\nA container starts its life as a Dockerfile. A Dockerfile is a set of instructions for how to build a container. Dockerfiles are usually stored in a git repository, just like any other code, and it’s common to build them on push via a CI/CD pipeline.2\nA working Dockerfile gets built into a Docker image with the build command. Images are immutable snapshots of the state of the container at a given time.\nIt is possible to interactively build a container as you go and snapshot to create an image, but for the purposes of reproducibility, it’s generally preferable to build the image from a Dockerfile, and adjust the Dockerfile if you need to adjust the image.\nUsually, the image is going to be the thing that you share with other people, as it’s the version of the container that’s compiled and ready to go.\nDocker images can be shared directly like any other file, or via sharing on an image registry via the push and pull commands.\nIf you’re familiar with git, the mental model for Docker is quite similar. There is a public Docker Hub you can use, and it’s also possible to run private image registries. Many organizations make use of the image registries as a service offerings from cloud providers. The big 3’s are Amazon’s Elastic Container Registry (ECR), Azure Container Registry, and Google Container Registry.\nOnce you’ve got an image downloaded locally, you can run it with the run command. Note that you generally don’t have to pull before running a container, as it will auto-pull if it’s not available.\nNow that you’re all excited, let’s dig in on how the docker run command works, and the command line flags we used here, which are the ones you’ll use most often."
  },
  {
    "objectID": "chapters/sec2/2-4-docker.html#understanding-docker-run",
    "href": "chapters/sec2/2-4-docker.html#understanding-docker-run",
    "title": "9  Docker for Data Science",
    "section": "9.6 Understanding docker run",
    "text": "9.6 Understanding docker run\nAt it’s most basic, all you need to know is that you can run a Docker image locally using the docker run <name> command. However, Docker commands usually use a lot of command line flags – so many that it’s easy to miss what the command actually is.\n\n\n\n\n\n\nNote\n\n\n\nA command line flag is an argument passed to a command line program.\nThere’s a lot more about using command line tools in Chapter 8.\n\n\nLet’s pull apart the two commands we just used, which use the command line flags you’re most likely to need.\n\n9.6.1 Parsing container names\nTo start with, let’s parse the name of the container. In this example, you used two different container names – alexkgold/plumber and alexkgold/batch:0.1. All containers have an id, and they may also have a tag. If you’re using the public DockerHub registry, like I am, container ids are of the form <user>/<name>. This should look very familiar if you already use a git repository.\nIn addition to an id, containers can also have a tag. For example, for the alexkgold/batch image, we specified a version: 0.1. If you don’t specify a tag when pulling or pushing an image, you’ll automatically create or get latest – the newest version of a container that was pushed to the registry.\nUsers often create tags that are relevant to the container – often versions of the software contained within. For example, the rocker/r-ver container, which is a container pre-built with a version of R in it uses tags for the version of R.\nAll these examples use the public DockerHub. Many organizations use a private image registry, in which case you can prefix the container name with the URL of the registry.\n\n\n9.6.2 docker run flags\nIn this section we’re going to go through the docker run flags we used in quite a bit of detail.\n\n\n\n\n\n\nNote\n\n\n\nIf you just want a quick reference later, there’s a cheatsheet in [Appendix @docker-cheat].\n\n\nLet’s first look at how we ran the container with the plumber API in it.\nFor this container, we used the --rm flag, the -d flag, the -p flag with the argument 8000:8000, and the --name flag with the argument plumber-palmer.\nThe --rm flag removes the container after it finishes running. This is nice when you’re just playing around with a container locally because then you can use the same container name repeatedly, but it’s a flag you’ll almost never use in production because it removes everything from the container, including logs.\nYou can check this by running docker kill palmer-plumber to make sure the container is down and then try to get to the logs with docker logs palmer-plumber. But they don’t exist because they got cleaned up!\nFeel free to try running to container without the --rm flag, playing around, killing the container, and then looking at the logs. Before you’re able to bring back another container with the same name, you’ll have to remove the container with docker rm palmer-plumber.\nThe -d flag instructs the container to run in detached mode so the container won’t block the terminal session. You can feel free to run the container attached – but you’ll have to quit the container by aborting the command from inside the terminal (Ctrl + c), or opening another terminal to docker kill the container.\nThe -p flag publishes a port from inside the container to the host machine. So by specifying -p 8000:8000, we’re taking whatever’s available on the port 8000 inside the container and making it available at the same port on the localhost of the machine that’s hosting the container.\nTODO: picture of ports\nPort forwarding is always specified as <host port>:<container port>. Try playing around with changing the values to make the API available on a different port, perhaps 9876. For a more in-depth treatment of ports, see Chapter 12.\nThe --name flag gives our container a name. This is really just a convenience so that you could do commands like docker kill in terms of the container name, rather than the container ID, which will be different for each person who runs the command.\nIn a lot of cases, you won’t bother with a name for the container.\nYou can find container ID using the docker ps command to get the process status. In the case below, I could control the container with the name palmer-plumber, or with the container ID. You can abbreviate container IDs as long as they’re unique – I tend to use the first three characters.\n❯ docker ps                                                         [12:23:13]\n\nCONTAINER ID   IMAGE               COMMAND                  CREATED          STATUS          PORTS                    NAMES\n\n35bd54e44015   alexkgold/plumber   \"R -e 'pr <- plumber…\"   29 seconds ago   Up 28 seconds   0.0.0.0:8000->8000/tcp   palmer-plumber\n\nNow let’s head over to the batch document rendering, where we only used one command line flag -v ${PWD}:/project-out, short for volume. To demonstrate what this argument does, navigate to a new directory on your command line and re-run the container without the argument.\nWait…where’d my document go?\nRemember – containers are completely ephemeral. What happens in the container stays in the container. This means that when my document is rendered inside the container, it gets deleted when the container ends its job.\nBut that’s not what I wanted – I wanted to get the output back out of the container.\nThe solution – making data outside the container available to the container and vice-versa – is accomplished by mounting a volume into the container using the -v flag. Like with mounting a port, the syntax is -v <directory outside container>:<directory inside container>.\n\nThis is an essential concept to understand when working with containers. Because containers are so ephemeral, volumes are the way to get anything from your host machine in, and to persist anything that you want to outlast the lifecycle of the container.\nIn this case, we actually used a variable ${PWD}, which will be evaluated to the current working directory to be the directory project-out inside the container, so the rendered document can be persisted after the container goes away."
  },
  {
    "objectID": "chapters/sec2/2-4-docker.html#build-your-own-with-dockerfiles",
    "href": "chapters/sec2/2-4-docker.html#build-your-own-with-dockerfiles",
    "title": "9  Docker for Data Science",
    "section": "9.7 Build your own with Dockerfiles",
    "text": "9.7 Build your own with Dockerfiles\nSo far, we’ve just been running containers based on images I’ve already prepared for you. Let’s look at how those images were created so you can try building your own.\nA Dockerfile is just a set of instructions that you use to build a Docker image. If you have a pretty good idea how to accomplish something on a running machine, you shouldn’t have too much trouble building a Dockerfile to do the same, as long as you remember two things:\nTODO: Image of build vs run time\n\nThe difference between build time and run time. There are things that should happen at build time – like setting up the versions of R and Python, copying in the code you’ll run, and installing the system requirements. That’s very different from the thing I want to have happen at run time – rendering the notebook or running the API.\nDocker containers only have access to exactly the resources you provide to them at both build and runtime. That means that they won’t have access to libraries or programs unless you give them access, and you also won’t have access to files from your computer unless you make them available.\n\nThere are many different commands you can use inside a Dockerfile, but with just a handful, you’ll be able to build most images you might need.\nHere are the important commands you’ll need for getting everything you need into your images.\n\nFROM – every container starts from a base image. In some cases, like in my Jupyter example, you might start with a bare bones container that’s just the operating system (ubuntu:20.04). In other cases, like in my shiny example, you might start with a container that’s almost there, and all you need to do is to copy in a file or two.\nRUN – run any command as if you were sitting at the command line inside the container. Just remember, if you’re starting from a very basic container, you may need to make a command available before you can run it (like wget in my container below).\nCOPY – copy a file from the host filesystem into the container. Note that the working directory for your Dockerfile will be whatever your working directory is when you run your build command.\n\nOne really nice thing about Docker containers is that they’re built in layers. Each command in the Dockerfile defines a new layer. If you make changes below a given layer in your Dockerfile, rebuilding will be easy, because Docker will only start rebuilding at the layer with changes.\nIf you’re mainly building containers for finished data science assets to be re-run on demand, there’s only one command you need:\n\nCMD - Specifies what command to run inside the container’s shell at runtime. This would be the same command you’d use to run your project from the command line.\n\nIf you do much digging, you’ll probably run into the ENTRYPOINT command, which can take a while to tell apart from CMD. If you’re building containers to run finished data science assets, you shouldn’t need ENTRYPOINT. If you’re building containers to – for example – accept a different asset to run or allow for particular arguments, you’ll need to use ENTRYPOINT to specify the command that will always run and CMD to specify the default arguments to ENTRYPOINT, which can be overridden on the command line.3\nHere’s the Dockerfile I used to build the container for the Jupyter Notebook rendering. Look through it. Can you understand what it’s doing?\n# syntax=docker/dockerfile:1\nFROM ubuntu:20.04\n\n# Copy external files\nRUN mkdir -p /project/out/\n\nCOPY ./requirements.txt /project/\nCOPY ./hello.ipynb /project/\n\n# Install system packages\nRUN apt-get update && apt-get install -y \\\n  wget python3 python3-pip\n\n# Install quarto CLI + clean up\nRUN wget https://github.com/quarto-dev/quarto-cli/releases/download/v0.9.83/quarto-0.9.83-linux-amd64.deb\nRUN dpkg -i ./quarto-0.9.83-linux-amd64.deb\nRUN rm -f ./quarto-0.9.83-linux-amd64.deb\n\n# Install Python requirements\nRUN pip3 install -r /project/requirements.txt\n\n# Render notebook\nCMD cd /project && \\\n  quarto render ./hello.ipynb && \\\n  # Move output to correct directory\n  # Needed because quarto requires relative paths in --output-dir: \n  # https://github.com/quarto-dev/quarto-cli/issues/362\n  rm -rf /project-out/hello_files/ && \\\n  mkdir -p /project-out/hello_files && \\\n  mv ./hello_files/* /project-out/hello_files/ && \\\n  mv ./hello.html /project-out/\nOnce you’ve created your Dockerfile, you build it into an image using docker build -t <image name>. You can then push that to DockerHub or another registry using docker push."
  },
  {
    "objectID": "chapters/sec2/2-4-docker.html#comprehension-questions",
    "href": "chapters/sec2/2-4-docker.html#comprehension-questions",
    "title": "9  Docker for Data Science",
    "section": "9.8 Comprehension Questions",
    "text": "9.8 Comprehension Questions\n\nWhat does using a Docker container for a data science project make easier? What does it make harder?\nDraw a mental map of the relationship between the following: Dockerfile, Docker Image, Docker Registry, Docker Container\nWhen would you want to use each of the following flags for docker run? When wouldn’t you?\n\n-p, --name, -d, --rm, -v\n\nWhat are the most important Dockerfile commands?"
  },
  {
    "objectID": "chapters/sec2/2-4-docker.html#lab-running-a-container-locally",
    "href": "chapters/sec2/2-4-docker.html#lab-running-a-container-locally",
    "title": "9  Docker for Data Science",
    "section": "9.9 Lab: Running a Container Locally",
    "text": "9.9 Lab: Running a Container Locally\nTODO: move palmer penguins example down here\nCONSIDER – should the portfolio example instead be dockerizing the app you created in section 1-3?"
  },
  {
    "objectID": "chapters/sec2/2-4-docker.html#portfolio-example-run-rstudio-server-in-a-container",
    "href": "chapters/sec2/2-4-docker.html#portfolio-example-run-rstudio-server-in-a-container",
    "title": "9  Docker for Data Science",
    "section": "9.10 Portfolio Example: Run RStudio Server in a Container",
    "text": "9.10 Portfolio Example: Run RStudio Server in a Container\nWrite a blog post about getting RStudio Server running in a container on your desktop using the rocker/rstudio container. Can you access your home directory from RStudio Server?\n\nHint 1: You’ll probably need all of the docker run flags you’ve learned and one more – the -e KEY=value flag provides an environment variable to the running container.\nHint 2: The default port for RStudio Server is 8787.\nHint 3: If you’re running a laptop with Apple Silicon (M1 or M2), you may need to try a different container. amoselb/rstudio-m1 worked for me.4\n\n\n9.10.1 Running a service in a container\nConsider reviewing the containers section if you’re not generally familiar with how to run a container.\nIf you want to run a service, like RStudio Server, out of a container, the pattern is very similar to running an interactive app. You’ll find an appropriate container, bring it up, and do some port mapping to make it available to the outside world.\nThe rocker organization makes available a number of containers related to R and RStudio. So if you want a container running RStudio Server on your laptop, it’s as easy as running\ndocker run \\\n  --rm -d \\\n  -p 8787:8787 \\\n  --name rstudio \\\n  rocker/rstudio\nNow, when you go to http://localhost:8787 on your laptop, you should see the the RStudio login screen…but what’s the password? Luckily, the wonderful folks who built the rocker/rstudio container made it easy to supply a password for the default rstudio user.\nWe do this by supplying an environment variable to the container named PASSWORD using the -e flag.\nSo, docker kill rstudio and try again by adding a password when you start:\ndocker run \\\n  --rm -d \\\n  -p 8787:8787 \\\n  --name rstudio \\\n  -e PASSWORD=my-rstudio-pass\n  rocker/rstudio\nNow you should be looking at the RStudio IDE! Hurray!\nThis is great for standing up a quick sandbox…but before you go standing up an RStudio Server on your laptop and spreading it around the world, there are a few things you’ll want to think about.\nIt’s totally possible to run a service like RStudio Server in a docker container, but you’ll need to take all the same steps to make it available to the outside world in terms of hosting it on a server, routing traffic properly, and making sure that you’re using HTTPS to secure your traffic. See chapters XX-XX for more on all that.\nNow, let’s say I wanted to create another user on the server (docker exec)\nBut there’s also one more concern that’s particular to Docker.\nOne of the best things about a Docker container is how ephemeral it is. Things come up in moments, and when they’re gone you don’t have to worry about them. But that’s also very dangerous if you want things to persist.\nThe best way to fix this is to mount external volumes that will maintain the state should the container die or should you want to replace it. We went over how to do that in the last section.\nAt a minimum, you’ll want to mount the user home directories that store all the data and code you can see in RStudio Server. You may also want to mount other bits of state, like wherever you’ve installed your version of R, and the config file you’re using to maintain the server.\nNote that, unlike on a server (where you restart the process of the server), the pattern with a container is generally to kill and restart the container, and let changes come up with the new container."
  },
  {
    "objectID": "chapters/sec3/3-0-sec-intro.html",
    "href": "chapters/sec3/3-0-sec-intro.html",
    "title": "IT/Admin for Data Science",
    "section": "",
    "text": "Data scientists shouldn’t be responsible for server administration. There’s a reason IT Administration is a career all to itself. There’s so much to know about servers, networking, managing users, storage, and more.\nYou’re probably great at writing R or Python code, cleaning and managing data, or building models. Unless you feel like a career switch, managing servers isn’t in your core skillset.\nAnd being responsible for a server is scary – you can unknowingly make choices that cause security vulnerabilities, system instability, and general annoyance.\nThat said, there are good reasons you might find yourself administering a server. Maybe you’re a student who wants to host a project for portfolio purposes, the hobbyist who’s hosting a toy project, or the data science leader who has no choice but to host things for themselves because they just can’t get IT/Admin support.\nI’ve been in all of these situations. Believe, me I feel you.\nThe good news is that if you are in any of those roles, you’ve come to the right place.\nThe first section of this book focused on how to move your data science practices closer to the DevOps ideal by doing Dev better.\nThis section focuses on how to do Ops. In contrast to the first section, which assumed a pretty high level of understanding and focused on best practices, this section is a lot of introductory IT/Admin knowledge. This section assumes you know nothing about how to administer a server other than the tools introduced in ?sec-2-intro.\nThroughout the chapters in this section, you’re going to learn standard patterns for simple administrative tasks for servers and apply those to a data science use-case. By the time you’re done, you’ll be ready to administer a simple server-based environment for doing or deploying data science in the cloud.\nSo what exactly will you learn about in this section?\nWe’ll start with an intro to the cloud – what it is, how it works, and how to make use of it for data science purposes. We’ll then spend a chapter on Linux administration. If you’re using a server, it’s going to be a Linux server, and understanding how to administer a Linux server is worth learning. We’ll then spend three chapters on several facets of networking, including a basic intro, how getting and using a domain name works, and how to secure a server with SSL/HTTPS. Lastly, we’ll get into how to choose the size and type of server to use for your data science purposes.\nEach chapter in this section is accompanied by a lab. If you follow along with the labs in this section, you’ll get an AWS server configured and ready to run RStudio Server, JupyterHub, and serve model predictions out of a Docker Container. While these aren’t all the things you could want to do on a server, once you’ve learned to do these tasks, many other things will be reasonably similar.\nWhat you’ll learn in this section and in the labs should be sufficient for you to do data science on a server for a small group of data scientists who you trust, assuming you’re not using data that is especially sensitive.\nIf this isn’t you – if you actually do have IT/Admin support, I’d still recommend skimming this section, especially Chapters Chapter 12, Chapter 13, and Chapter 14. Being able to understand a little bit about networking will make it way easier to communicate with the IT/Admin group at your organization.\nAnd don’t forget, there’s still section 4 – where you’ll learn more about how to communicate with IT/Admin professionals for issues that really should be left to them.\nLet’s jump in!"
  },
  {
    "objectID": "chapters/sec3/3-1-cloud.html#layers-of-cloud-services",
    "href": "chapters/sec3/3-1-cloud.html#layers-of-cloud-services",
    "title": "10  Getting Started with the Cloud",
    "section": "10.1 Layers of cloud services",
    "text": "10.1 Layers of cloud services\nWhile the basic premise of the cloud is rent a server, there are layers and layers of services on top of that basic premise. Depending on your organization’s needs, it may make sense to rent a very basic service, or a higher level one.\nLet’s talk about cakes to help make the levels clear. This year, I’m planning to bake and decorate a beautiful cake for my friend’s birthday. It’s going to be a round layer cake with white frosting, and it’s going to say “Happy Birthday!” in teal with giant lavender frosting rosettes.\nNow that I’ve decided what I’m making, I have a few different options for how I get there. If I’m a real DIY kind of person, I could buy all the ingredients from the grocery store and make everything from scratch. Or I could buy a cake mix – reducing the likelihood I’ll buy the wrong ingredients or end up with unnecessary leftovers. Or maybe I don’t want to bake at all – I could just buy a premade cake already covered in white frosting and just do the writing and the rosettes.\nThe choice of how much to DIY my friend’s birthday cake and how much to get from a bakery is very much akin to your choices when buying server capacity from the cloud.\nTODO: Image of getting cake w/ IaaS, PaaS, SaaS\nIn the US, a huge fraction of server workloads run on servers rented from one of the “big three clouds” – Amazon Web Services (AWS), Microsoft Azure, and Google Cloud Platform (GCP). There are also numerous smaller cloud providers, many of which target more specific use cases.\nIt’s totally possible to just rent the raw ingredients from these cloud providers – they’ll happily rent you the most basic cloud services like servers, networking, and storage and let you put them together yourself. This kind of cloud service – the equivalent of baking from scratch – is called Infrastructure as a Service (IaaS, pronounced eye-az).\nBut then I’m responsible for managing all of the reproducibility stack – I have to make sure the servers are up to date with new operating systems and security patches, while also ensuring there are modern versions of R and Python on the system, and making sure there are RStudio and Jupyter Notebooks present.\nBut just as you might not want to have to buy eggs, milk, and sugar just to end up with a pretty cake, IT/Admin organizations are increasingly taking advantage of the cloud equivalent of cake mixes and premade blank sheet cakes.\nIf I want to offload more of the work, I can think about a Platform as a Service (PaaS – pronounced pass) solution. This would give me the ability to somehow define an environment and send it somewhere to run without worrying about the underlying servers. For example, I might want to build a docker container with Python and Jupyter Notebook and host it somewhere that autoscales for the number of users and the amount of other resources needed.\nThere are a bunch of different entities you can run in this way, and services to match each. For example, if you want to run a general app, you can use something like AWS’s Elastic Beanstalk to run a user-facing app that dynamically scales (TODO: Azure/GCP equivalents), if you want to run a container, you might use XXXX, and if you want to get a Kubernetes cluster as a service, you might use XXXX. If you want to just run a function, there’s always AWS Lambda.\n\n\n\n\n\n\nDanger\n\n\n\nI am not recommending you run a data science workbench on a service like Elastic Beanstalk. This actually tends not to work very well, for reasons we’ll get into in a bit.\n\n\nIf I want to go one level more abstracted, I might want to go with a Software as a Service (SaaS - pronounced sass) solution. This would be something like one of the cloud providers hosted machine learning environment where you can train, deploy, and monitor machine learning models. AWS has SageMaker, Azure has Azure Machine Learning, and GCP has Vertex AI. And there are organizations that host popular data science tools like Posit.cloud and Saturn Cloud that you can use.\nSometimes you’ll hear people describe PaaS or SaaS solutions called “going serverless”.\nThe first thing to understand about serverless computing is that there is no such thing as serverless computing.\nEvery bit of cloud computation runs on a server - the question is whether you have to deal with the server or if you just deal with a preconfigured service.\nIn this book, we’ll be working exclusively with IaaS services – taking the most basic services and building up a working data science environment. Once you understand how these pieces fit together, you’ll be in a much better place if it turns out your organization wants to leverage more abstracted versions of those services."
  },
  {
    "objectID": "chapters/sec3/3-1-cloud.html#common-services-for-data-science-usage",
    "href": "chapters/sec3/3-1-cloud.html#common-services-for-data-science-usage",
    "title": "10  Getting Started with the Cloud",
    "section": "10.2 Common services for data science usage",
    "text": "10.2 Common services for data science usage\nIf you’re not familiar with cloud provider terminology, it can be very hard to tell what service you might need from a cloud provider, and they don’t really help the matter. It’s very common (especially in AWS) to have many different services that fulfill similar needs, and it can be really hard to concretely tell the difference.\nMaking the issue even more difficult, many companies go out of their way to make their services sound grand and important and don’t just say, “this is a ___ you can rent”.\nIt’s helpful to keep in mind that at the very bottom, all you’re doing is renting servers and storage and managing networking and permissions for the servers and storage. Every other service is just a combination of server, storage, networking, and permissions that comes with software pre-installed or configured to make your life easier.4\nWe’re going to talk through some of the basic services that are offered by each of the three major cloud platforms.\n\n\n\n\n\n\nCloud service naming\n\n\n\nAzure and GCP tend to name their offerings pretty literally, while AWS chooses cutesy names that have, at best, a tangential relationship to the task at hand. I’m going to try to name the services for each of the purposes I’m talking about, but it’s worth noting that feature sets aren’t exactly parallel across cloud providers.\nThis makes AWS names a little harder to learn, but much easier to recall once you’ve learned them. In this section – contrary to standard practice – I’m going to use the common abbreviated names for AWS services and put the full name in parentheses as these are just trivia.\n\n\nRemember, at the bottom of all cloud services are servers and each of cloud service provider has a service that is “just rent me a server”. AWS has EC2 (Elastic Cloud Compute) and Azure has Azure VMs, and Google has Google Compute Engine.\nAlong with servers, there are two main kinds of storage you’ll rent. The first is file storage, where you’ll store things in a file directory like on your laptop. These are AWS’s EBS (Elastic Block Store), Azure Managed Disk, and Google Persistent Disk.\nThe major cloud providers all also have blob (Binary Large Object) storage. Blob storage allows you to store individual objects somewhere and recall them to any other machine that has access to the blob store. The major blob stores are AWS Simple Storage Service (S3), Azure Blob Storage, and Google Cloud Storage.\nThere are also two important networking services for each of the clouds – a way to make a private network and a way to do DNS routing. If you don’t know what these mean, there will be a lot more detail in Chapter 12. For now, it’s enough to know that private networking is done in AWS’s VPC (Virtual Private Cloud), Azure’s Virtual Network, and Google’s Virtual Private Cloud. DNS is done in AWS’s Route 53, Azure DNS, and Google Cloud DNS.\nOnce you’ve got all this stuff up and running, you need to make sure that permissions are set in the right way. AWS has IAM (Identity and Access Management), Azure has Azure Active Directory, and Google has Identity Access Management.\nNow, there are a variety of things you might want to do past these basic tasks of server, storage, networking, and access management. Here are a few more services you’ll likely hear about over time.\n\n\n\n\n\n\n\n\n\nService\nAWS\nAzure\nGCP\n\n\n\n\nKubernetes cluster\nEKS (Elastic Kubernetes Service)\nAKS (Azure Kubernetes Service)\nGKE (Google Kubernetes Engine)\n\n\nRun a function as an API\nLambda\nAzure Functions\nGoogle Cloud Functions\n\n\nDatabase\nRDS/Redshift5\nAzure Database\nGoogle Cloud SQL\n\n\nML Platform\nSageMaker\nAzure ML\nVertex AI"
  },
  {
    "objectID": "chapters/sec3/3-1-cloud.html#lab",
    "href": "chapters/sec3/3-1-cloud.html#lab",
    "title": "10  Getting Started with the Cloud",
    "section": "10.3 Lab",
    "text": "10.3 Lab\nWelcome to the lab portion. In this lab, we’re going to get you up and running with an AWS account and show you how to manage, start, and stop EC2 instances in AWS.\nAt a high level, you can do all of this with any cloud provider. Feel free to try if you prefer Azure or GCP, but the details on how to get started and service names will be different.\nWe will be standing up a server in AWS’s free tier – so there will be no cost involved as long as you haven’t used up all your AWS free tier credits before now.\n\n10.3.1 Login to the AWS Console\nWe’re going to start by logging into AWS. If you already know how, you can skip ahead to standing up an instance.\nIf not, go to https://aws.amazon.com and click Sign In to the Console .\nIf you’ve never set up an AWS account before, click Create a New AWS account and follow the instructions to create an account. Note that even if you’ve got an Amazon account for ordering stuff online and watching movies, an AWS account is separate.\nOnce you’ve logged in, you’ll be confronted by the AWS console. There are a ton of things here – feel free to poke around if you want – come back and continue when you’re ready.\n\n\n10.3.2 Stand up an instance\nAWS’s “rent a server” is called EC2.6 It is the most basic AWS service, and it’s what we’re going to use to get started.\nThere are five things we have to configure before launching the server. I’m not going to walk you through configuring each one on this page, because Amazon is constantly updating the exact layout of the page and the text, but I’ll explain the choices and then you can make them yourself – or even make similar choices if you’re in GCP or Azure.\nThe first choice is instance name and tags. None of this is required – it’s for convenience and organization. I’d suggest you name the server something like do4ds-lab.\nNext, you’ll have to choose the image. All clouds have a concept of server images. In AWS, they’re called AMIs (short for Amazon Machine Image, pronounced like you’re reading the letters individually).\nAn image is the set of software that’s preinstalled when you start the server. Images can range from just a bare operating system to a running RStudio instance that’s suspended in between two computations. There are many paid images that come preinstalled with a bunch of software ready to go.\nSince we’re going to work on configuring the server from the ground up, we’re going to choose an AMI that’s just the operating system. Choose the most basic Ubuntu one. At the time of this writing, that’s using Ubuntu 22.04.\nIf you want to use a different operating system, that’s fine, but you may need to adjust the commands in this chapter and subsequent to match.\nYou’ll have to choose an Instance Type. In AWS, there are two components to instance type – the family and the size. The family is the category of server that you’re using. In AWS, families are denoted by letters and numbers, so there are T2s and T3s, C4s and C5s, R5s and many more.\nWithin each family, there are different sizes. The sizes vary by the family, but generally range from things below small nano to multiples of xlarge like 24xlarge.\nWe’ll get into choosing the right family and size for your team’s use case in Chaper 15.\nFor now, I’d recommend you get the largest server that is free tier eligible, which is a t2.micro with 1 CPU and 1 Gb of memory as of this writing.\n\n\n\n\n\n\nServer sizing for the lab\n\n\n\nA t2.micro with 1 CPU and 1 Gb of memory is a very small server.\nIf all you’re doing is walking through the lab, it should be sufficient, but if you actually want to do any data science work, you’ll need a substantially larger server.\nIt is possible to rack up really large AWS bills, so be careful.\nThat said, a modestly-sized server is still pretty cheap if you’re only putting it up for a short amount of time.\nI’m writing this on a 2021 M1 Macbook Pro with 10 CPUs and 32 Gb of memory. If you wanted that same computational power from an AWS server, it’s roughly comparable to a t3.2xlarge – with 8 CPUs and 32Gb of memory. That server costs is $0.33 an hour. So a full year running full time for an instance is nearly $3,000, but if you’re only running that instance for a little while – say, the few hours it’ll take you to complete this lab – it will probably only be a few dollars.\n\n\nNext, you’ll need to make sure you have a keypair. We’ll get into what this key is and how to use it in Chapter 8. For now, you’ll need to create a keypair if you don’t already have one. I’d suggest naming it do4ds-lab-key because I’ll use that name, so you can just copy/paste commands if you use the same name.\nI’d recommend creating a directory for this lab, perhaps something like do4ds-lab and putting your keypair there. If you’re not going to do that, just make sure you keep track of where you downloaded it.\nWhether you’re on Windows or Mac, download the pem version of the key.\nYou shouldn’t need to change any networking settings from the defaults. Make sure it’s configured to allow SSH traffic.\nYou’ll need to configure storage. By default, the server comes just with root storage. Root storage is where the operating system will be, as well as storing most of the executables you’ll need.\nYou’ll want another volume where you’ll store the actual work you’re doing – data files, code, and more. You can attach volumes quite easily. In AWS, they’re called EBS. While it’s not strictly necessary to configure an extra EBS volume to complete the labs in this book, make sure to add one since it’s necessary if you ever want to stand up an actual environment to do work.\nYou can feel free to look through the Advanced Details, but you shouldn’t need to adjust any of them.\nWhen you go to the summary, it should look something like this, assuming you followed the instructions here:\n\nClick Launch Instance. AWS is now creating a virtual server just for you.\nIf you go back to the EC2 page and click on Instances you can see your instance as it comes up. You may need to remove the filter for State: Running since it’ll take a few moments to be Running.\n\n\n10.3.3 Grab the address of your server\nIf you click on the actual instance ID in blue, you can see all the details of your server.\nThe instance ID and public IP addresses were auto-assigned.\nGrab the Public IPv4 DNS address, which starts with ec2- and ends with amazonaws.com. Copy it somewhere easy to grab. That’s going to be the way we access the server.\nFor example, as I write this, my server has the address ec2-54-159-134-39.compute-1.amazonaws.com. In the commands, I’ll include the variable SERVER_ADDRESS. If you’d like to be able to copy commands verbatim, you can set the variable SERVER_ADDRESS to be your server address using SERVER_ADDRESS=ec2-54-159-134-39.compute-1.amazonaws.com.\n\n\n\n\n\n\nTip\n\n\n\nIf you’re used to R, where it’s best practice to put spaces around =, notice that assigning variables in bash requires no spaces around =.\n\n\n\n\n10.3.4 Stopping or burning it down\nWhenever you’re stopping for the day, you may want to suspend your server so you’re not paying for it overnight or using up your free tier hours.\nYou can suspend an instance in the state it’s in so it can be restarted later. Depending on how much data you’re storing, it may not be free, but storage costs are generally very modest.\nWhenever you want to suspend your instance, go to the EC2 page for your server. Under the Instance State drop down in the upper right, choose Stop Instance.\nAfter a couple minutes the instance will stop and you won’t get charged for it. Before you come back to the next lab, you’ll need to start the instance back up so it’s ready to go.\nIf you want to completely delete the instance at any point, you can choose to Terminate Instance from that same Instance State dropdown."
  },
  {
    "objectID": "chapters/sec3/3-1-cloud.html#comprehension-questions",
    "href": "chapters/sec3/3-1-cloud.html#comprehension-questions",
    "title": "10  Getting Started with the Cloud",
    "section": "10.4 Comprehension Questions",
    "text": "10.4 Comprehension Questions\n\nWhat is the difference between PaaS, IaaS, and SaaS? What’s an example of each that you’re familiar with?\nWhat are the names for AWS’s services for: renting a server, file system storage, blob storage"
  },
  {
    "objectID": "chapters/sec3/3-2-linux-admin.html#linux-is-an-operating-system-with-a-long-history",
    "href": "chapters/sec3/3-2-linux-admin.html#linux-is-an-operating-system-with-a-long-history",
    "title": "11  Basic Linux SysAdmin",
    "section": "11.1 Linux is an operating system with a long history",
    "text": "11.1 Linux is an operating system with a long history\nA computer’s operating system (OS) defines how applications – like Microsoft Word, RStudio, and Minecraft – interact with the underlying hardware to actually do computation. OSes define how files are stored and accessed, how applications are installed and can connect to networks, and more.\nThese days, basically all computers run on one of a few different operating systems – Windows, MacOS, or Linux for laptops and desktops; Windows or Linux for servers, Android (a flavor of Linux) or iOS for phones and tablets, and Linux for other kinds of embedded systems (like ATMs and the chips in your car).\nWhen you stand up a server, you’re going to be choosing from one of a few versions of Linux. If you’re unfamiliar with Linux, the number of choices can seem overwhelming, so here’s a quick primer on the history of operating systems. Hopefully it’ll help it all make sense.\nBefore the early 1970s, the market for computer hardware and software looked nothing like it does now. Computers released in that era had extremely tight linking between hardware and software. There were no standard interfaces between hardware and software, so each hardware manufacturer also had to release the software to use with their machine.\nIn the early 1970s, Bell Labs released Unix – the first operating system.\nOnce there was an operating system, the computer market started looking a lot more familiar to 2020s eyes. Hardware manufacturers would build machines that ran Unix and software companies could write applications that ran on Unix. The fact that those applications would run on any Unix machine was a game-changer.\nIn the 1980s, programmers wanted to be able to work with Unix themselves, but didn’t necessarily want to pay Bell Labs for Unix, so they started writing Unix-like operating systems. Unix-like OSes or Unix clones behaved just like Unix, but didn’t actually include any code from Unix itself.1\nIn 1991, Linus Torvalds – then a 21 year-old Finnish grad student – released Linux, an open source Unix clone via a amusingly nonchalant newsgroup posting.2\nSince then, Linux has seen tremendous adoption. A large majority of the world’s servers run on Linux.3 Along with most of the world’s servers, almost all of the world’s embedded computers – in ATMs, cars and planes, TVs, and most other gadgets and gizmos – run on Linux. If you have an Android phone or a Chromebook – that’s Linux. Basically all of the world’s supercomputers use Linux.\nAs you might imagine, running Linux in so many different places has necessitated the creation of many different kinds of Linux. For example, a full-featured Linux server is going to require a very different operating system than the barebones operating system running on an ATM with extremely modest computational power.\nThese different versions are called distributions (distros for short) of Linux. They have a variety of technical attributes and also different licensing models.\nSome versions of Linux, like Ubuntu, are completely open source. Others, like Red Hat Enterprise Linux (RHEL), are paid. Most paid Linux OSes have closely-related free and open source versions – like CentOS and Fedora for RHEL.4\nMany organizations have a standard Linux distro they use – most often RHEL/CentOS or Ubuntu. Increasingly, organizations deploying in AWS are using Amazon Linux, which is independently maintained by Amazon but was originally a RHEL derivative. There are also some organizations that use SUSE (pronunced soo-suh), which has both open source and enterprise versions."
  },
  {
    "objectID": "chapters/sec3/3-2-linux-admin.html#a-tiny-intro-to-linux-administration",
    "href": "chapters/sec3/3-2-linux-admin.html#a-tiny-intro-to-linux-administration",
    "title": "11  Basic Linux SysAdmin",
    "section": "11.2 A tiny intro to Linux administration",
    "text": "11.2 A tiny intro to Linux administration\nWe’ll get into how to administer a Linux server just below, but before we get there, let’s introduce what you’ll be doing as a Linux server admin. There are three main things you’ll manage as a Linux server admin:\n\nSystem resources Each server has a certain amount of resources available. In particular, you’ve got CPU, RAM, and storage. Keeping track of how much you’ve got of these things and how they’re being used – and making sure no one is gobbling up all the resources – is an important part of system administration.\nNetworking Your server is only valuable if you and others can connect to it, so managing how your server can connect to the environment around it is an important part of Linux administration.\nPermissions Servers generally exist to allow a number of people to access the same machine. Creating users and groups – and managing what they’re allowed to do – is a huge part of server administration.\nApplications Generally you want to do something with your server, so being able to interact with applications that are running, debug issues, and fix things that aren’t going well is an essential Linux admin skill.\n\nWhen you log into a Linux server, you’ll be interacting exclusively via the command line, so all of the commands in this chapter are going to be terminal commands. If you haven’t yet figured out how to open the terminal on your laptop (and gotten it themed and customized so it’s perfect), I’d advise going back to Chapter 8 to get it all configured.5\n\n\n\n\n\n\nWindows, Mac, and Linux\n\n\n\nMacOS is based on BSD, a Unix clone, so any terminal commands you’ve used before on Mac will be very similar to Linux commands.\nWindows, on the other hand, is basically the only popular operating system that isn’t a Unix clone. Over time, the Windows command line has gotten more Unix-like, so the differences aren’t as big as they used to be, but there will be some differences in the exact commands that work on Windows vs Linux.\nThe most obvious difference is the types of slashes used in file paths. Unix-like systems use forward slashes / to denote file hierarchies, while Windows historically uses back slashes \\. These days Windows deals just fine with either."
  },
  {
    "objectID": "chapters/sec3/3-2-linux-admin.html#managing-who-can-do-what",
    "href": "chapters/sec3/3-2-linux-admin.html#managing-who-can-do-what",
    "title": "11  Basic Linux SysAdmin",
    "section": "11.3 Managing who can do what",
    "text": "11.3 Managing who can do what\nWhenever you’re doing something in Linux, you’re doing that thing as a particular user.\nOn any Unix-like system, you can check your active user at any time with the whoami command. For example, here’s what it looks like on my MacBook.\n ❯ whoami                                                       \nalexkgold\nwhoami returns the username of my user.\nUsernames have to be unique on the system – but they’re not the true identifier for a Linux user. A user is uniquely (and permanently) identified by their user id (uid). All other attributes (including username, password, home directory, groups, and more) are malleable, but uid is forever.\nMany of the users on a Linux server correspond to actual humans, but there are more users than that. Most programs that run on a Linux server run as a service account that represent the set of permissions allowed to that program.\nFor example, installing RStudio Server will create a user with username rstudio-server. Then, when rstudio-server goes to do something – start an R session for example – it will do so as rstudio-server.\n\n\n\n\n\n\nA few details on UIDs\n\n\n\nuids are just numbers from 0 to over 2,000,000,000. uids are assigned by the system at the time the user is created. You should probably keep uids below 2,000,000 or so if you are ever assigning uids manually – some programs can’t deal with uids any bigger.\n10,000 is the the lowest uid that’s available for use by a user account. Everything below that is reserved for predefined system accounts or application accounts.\n\n\nIn addition to users, Linux has a notion of groups. A group is a collection of users. Each user has exactly one primary group and can be a member of secondary groups.6 By default, each user’s primary group is the same as their username.\nLike a user has a uid a group has a gid. User gids start at 100.\nYou can see a user’s username, uid, groups, and gid with the id command.\n ❯ id                                                                \nuid=501(alexkgold) gid=20(staff) groups=20(staff),12(everyone),61(localaccounts),79(_appserverusr),80(admin),81(_appserveradm),98(_lpadmin),701(com.apple.sharepoint.group.1),33(_appstore),100(_lpoperator),204(_developer),250(_analyticsusers),395(com.apple.access_ftp),398(com.apple.access_screensharing),400(com.apple.access_remote_ae)\nOn my laptop, I’m a member of a number of different groups.\nThere’s one extra special user – called the admin, root, sudo, or super user. They get the ultra-cool uid 0. That user has permission to do anything on the system. You almost never want to actually log in as the root user. Instead, you make users and add them to the admin or sudo group so that they have the ability to temporarily assume those admin powers.\nThe easiest way to make users is with the useradd command. Once you have a user, you may need to change the password, which you can do at any time with the passwd command. Both useradd and passwd start interactive prompts, so you don’t need to do much more than run those commands.\n\n\n\n\n\n\n\nCommand\nWhat it does\n\n\n\n\nsu <username>\nChange to be a different user.\n\n\nwhoami\nGet username of current user.\n\n\nid\nGet full user + group info on current user.\n\n\npasswd\nChange password.\n\n\nuseradd\nAdd a new user.\n\n\n\n\n11.3.1 File Permissions\nEvery object in Linux is just a file. Every log – file. Every picture – file. Every program – file. Every system setting – file.\nSo determining whether a user can take a particular action is really a question of whether they have the right permissions on a particular file.\n\n\n\n\n\n\nNote\n\n\n\nThe question of who’s allowed to do what – authorization – is an extremely deep one. There’s a chapter all about authorization, how it differs from authentication, and the different ways your IT/Admins might want to manage it later in the book.\nThis is just going to be a high-level overview of basic Linux authorization.\n\n\nThere are three permissions you can have: read, write, and execute. Read means you’re allowed to see the contents of a file, write means you can save a changed version of a file, and execute means you’re allowed to run the file as a program.\nThe execute permission really only makes sense for some kinds of files - what would it mean to execute a csv file? But Linux doesn’t care – you can assign any combination of these three permissions for any file.\nHow are these permissions assigned? Every file has an owner and an owning group.\nSo you can think of permissions in Linux as being assigned in a 3x3 grid. The owner, the owning group, and everyone else can have permissions to read, write, or execute the file.\nTODO: change to graphic\n\n\n\n\nOwner\nGroup\nEveryone Else\n\n\n\n\nRead\n✅/❌\n✅/❌\n✅/❌\n\n\nWrite\n✅/❌\n✅/❌\n✅/❌\n\n\nExecute\n✅/❌\n✅/❌\n✅/❌\n\n\n\nTo understand better, let’s look at the permissions on an actual file.\nRunning ls -l on a directory gives you the list of files in that directory, along with their permissions. The first few columns of the list give you the full set of file permissions – though they can be a little tricky to read.\nSo, for example, here’s a few lines of the output of running ls -l on a python project I have.\n❯ ls -l                                                           \n-rw-r--r--  1 alexkgold  staff     28 Oct 30 11:05 config.py\n-rw-r--r--  1 alexkgold  staff   2330 May  8  2017 credentials.json\n-rw-r--r--  1 alexkgold  staff   1083 May  8  2017 main.py\ndrwxr-xr-x 33 alexkgold  staff   1056 May 24 13:08 tests\nThis readout has the file permissions (a series of ten characters), followed by a number7, then the file’s owner, and the file’s group. Let’s learn how to read these.\nThe file’s owner and group are the easiest to understand. In this case, I alexkgold own all the files, and the group of all the files is staff.\nThe ten-character file permissions are relative to that user and group.\nThe first character indicates the type of file: - for normal and d for a directory.\nThe next nine characters are indicators for the three permissions – r for read, w for write, and x for execute (or - for in place of any of those for not) – first for the user, then for the group, then for any other user on the system.\nSo, for example, my config.py file with permissions of rw-r-r-- indicates the user (alexkgold) can read and write the file, and everyone else – including in the file’s group staff – has read-only permission.\nIn the course of administering a server, you will probably need to change a file’s permissions. You can do so using the chmod command.\nFor chmod, permissions are indicated with only three numbers – one for the user, one for the group, and one for everyone else. The way this works is pretty clever – you just sum up the permissions as follows: 4 for read, 2 for write, and 1 for execute. You can check for yourself, but any set of permissions can be uniquely identified by a number between 1 and 7.8\nSo chmod 765 <filename> would give the user full permissions (4 + 2 + 1), read and write (4 + 2) to the group, and read and execute (4 + 1) to everyone else. This would be a strange set of permissions to give a file, but it’s a perfectly valid chmod command.\n\n\n\n\n\n\nNote\n\n\n\nIf you spend any time administering a Linux server, you almost certainly will at some point find yourself running into a problem and applying chmod 777 out of frustration to rule out a permissions issue.\nI can’t in good faith tell you not to do this – we’ve all been there. But if it’s something important, be sure you change it back once you’re finished figuring out what’s going on.\n\n\nIn some cases you might actually want to change the owner or group of a file. You can change users or groups with either names or ids. You can do so using the chown command. If you’re changing the group, the group name gets prefixed with a colon.\nIn some cases, you might not be the correct user to take a particular action. You might not want to change the file permissions, but instead to change who you are. In that case, you can switch users with the su command.\nSome actions are also reserved for the admin user. For example, let’s take a look at this configuration file:\n ❯ ls -l /etc/config/my-config                      \n-rw-r--r--  1 root  system  4954 Dec  2 06:37 config.conf\nAs you can see, all users can read this file to check the configuration settings. But this file is owned by root, and only the owner has write permissions. So I could run cat config.conf to see it. Or I could go into it with vim config.conf, but I’d find myself stuck if I wanted to make changes.\nSo if I want to change this configuration file, I’d need to temporarily assume my root powers to make changes. Instead of switching to be the root user, I would run sudo vim config.conf and open the file for editing with root permissions.\n\n\n\n\n\n\n\n\nCommand\nWhat it does\nHelpful options + notes\n\n\n\n\nchmod <permissions> <file>\nModifies permissions on a file.\nNumber indicates permissions for user, group, others: add 4 for read, 2 for write, 1 for execute, 0 for nothing.\n\n\nchown <user/group> <file>\nChange the owner of a file.\nCan be used for user or group, e.g. :my-group.\n\n\nsu <username>\nChange active user.\n\n\n\nsudo <command>\nAdopt super user permissions for the following command."
  },
  {
    "objectID": "chapters/sec3/3-2-linux-admin.html#installing-stuff",
    "href": "chapters/sec3/3-2-linux-admin.html#installing-stuff",
    "title": "11  Basic Linux SysAdmin",
    "section": "11.4 Installing Stuff",
    "text": "11.4 Installing Stuff\nThere are several different ways to install programs for Linux, and you’ll see a few of them throughout this book.\nJust as CRAN and PyPI are repositories for R and Python packages, Linux distros also have their own repositories. For Ubuntu, the apt command is used for accessing and installing .deb files from the Ubuntu repositories. For CentOS and RedHat, the yum command is used for installing .rpm files.\n\n\n\n\n\n\nNote\n\n\n\nThe examples below are all for Ubuntu, since that’s what we use in the lab for this book. Conceptually, using yum is very similar, though the exact commands differ somewhat.\n\n\nWhen you’re installing packages in Ubuntu, you’ll often see commands prefixed with apt-get update && apt-get upgrade y. This command makes your machine update the list of available packages it knows about on the server and upgrade everything to the latest version.\nPackages are installed with apt-get install <package>. Depending on which user you are, you may need to prefix the command with sudo.\nYou can also install packages that aren’t from the central package repository. Doing that will generally involve downloading a file directly from a URL – usually with wget and then installing it from the file you’ve downloaded – often with the gdebi command.\n\n\n\n\n\n\n\nCommand\nWhat it does\n\n\n\n\napt-get update  && apt-get upgrade y\nFetch and install upgrades to system packages\n\n\napt-get install <package>\nInstall a system package.\n\n\nwget\nDownload a file from a URL.\n\n\ngdebi\nInstall local .deb file."
  },
  {
    "objectID": "chapters/sec3/3-2-linux-admin.html#debugging-and-troubleshooting",
    "href": "chapters/sec3/3-2-linux-admin.html#debugging-and-troubleshooting",
    "title": "11  Basic Linux SysAdmin",
    "section": "11.5 Debugging and troubleshooting",
    "text": "11.5 Debugging and troubleshooting\nThere are three main resources you’ll need to manage as the server admin – CPU, RAM, and storage space. There’s more on all three of these and how to make sure you’ve got enough in Chapter 15.\nFor now, we’re just going to go over how to check how much you’ve got, how much you’re using, and getting rid of stuff that’s misbehaving.\n\n11.5.1 Storage\nA common culprit for weird server behavior is running out of storage space. There are two handy commands for monitoring the amount of storage you’ve got – du and df. These commands are almost always used with the -h flag to put file sizes in human-readable formats.\ndu, short for disk usage, gives you the size of individual files inside a directory. This can be helpful for finding your largest files or directories if you think you might need to clean up things. It’s particularly useful in combination with the sort command.\nFor example, here’s the result of running du on the chapters directory where the text files for this book live.\n ❯ du -h chapters | sort -h                                      \n 44K    chapters/sec2/images-servers\n124K    chapters/sec3/images-scaling\n156K    chapters/sec2/images\n428K    chapters/sec2/images-traffic\n656K    chapters/sec1/images-code-promotion\n664K    chapters/sec1/images-docker\n1.9M    chapters/sec1/images-repro\n3.4M    chapters/sec1\n3.9M    chapters/sec3/images-auth\n4.1M    chapters/sec3\n4.5M    chapters/sec2/images-networking\n5.3M    chapters/sec2\n 13M    chapters\nSo if I were thinking about cleaning up this directory, I could see that my images-networking directory in sec2 is the biggest single bottom-level directory. If you find yourself needing to find big files on your Linux server, it’s worth spending some time with the help pages for du. There are lots of really useful options.\ndu is useful for identifying large files and directories on a server. df, for disk free, is useful for diagnosing issues that might be a problem for a directory. If you’re struggling to write into a directory – perhaps getting out of space errors, df can help you diagnose.\ndf answers the question – given a file or directory, what device is it mounted on and how full is that device?\nSo here’s the result of running the df command on that same chapters directory.\n ❯ df -h chapters                                                    \nFilesystem     Size   Used  Avail Capacity iused      ifree %iused  Mounted on\n/dev/disk3s5  926Gi  163Gi  750Gi    18% 1205880 7863468480    0%   /System/Volumes/Data\nSo you can see that the chapters folder lives on a disk called /dev/disk3s5 that’s a little less than 1Tb and is 18% full – no problem. On a server this can be really useful to know, because it’s quite easy to switch a disk out for a bigger one in the same spot.\n\n\n\n\n\n\n\n\nCommand\nWhat it does\nHelpful options\n\n\n\n\ndu\nCheck size of files.\nMost likely to be used du  -h  <dir> | sort -h [^3 -2-linux-admin-9]\nAlso useful to combine with head.\n\n\ndf\nCheck storage space on device.\n-h\n\n\n\n\n\n11.5.2 Monitoring processes\nEvery program your computer runs is a process. For example, when you type python on the command line to open a REPL, that’s a process. Running more complicated programs usually involves more than one process.\nFor example, running RStudio involves (at minimum) one process for the IDE itself and one for the R session that it uses in the background. The relationships between these different processes is mostly hidden from you – the end user.\nAs a server admin, finding runaway processes, killing them, and figuring out how to prevent the them from happening again is a pretty common task. Runaway processes usually misbehave by using up the entire CPU, filling up the entire machine’s RAM.\nLike users and groups have ids, each process has a numeric process id (pid). Each process also has an owner – this can be either a service account or a real user. If you’ve got a rogue process, the pattern is to try to find the process and make note of its pid. Then you can immediately end the process by pid with the kill command.\nSo, how do you find a troublesome process?\nThe top command is a good first stop. top shows the top CPU-consuming processes in real time. Here’s the top output from my machine as I write this sentence.\nPID    COMMAND      %CPU TIME     #TH    #WQ  #PORT MEM    PURG   CMPRS PGRP\n0      kernel_task  16.1 03:56:53 530/10 0    0     2272K  0B     0B    0\n16329  WindowServer 16.0 01:53:20 23     6    3717  941M-  16M+   124M  16329\n24484  iTerm2       11.3 00:38.20 5      2    266-  71M-   128K   18M-  24484\n29519  top          9.7  00:04.30 1/1    0    36    9729K  0B     0B    29519\n16795  Magnet       3.1  00:39.16 3      1    206   82M    0B     39M   16795\n16934  Arc          1.8  18:18.49 45     6    938   310M   144K   61M   16934\n16456  Messages     1.7  06:58.27 4      1    603   138M   2752K  63M   16456\n1      launchd      1.7  13:41.03 4/1    3/1  3394+ 29M    0B     6080K 1\n573    diagnosticd  1.4  04:31.97 3      2    49    2417K  0B     816K  573\n16459  zoom.us      1.3  66:38.37 30     3    2148  214M   384K   125M  16459\n16575  UniversalCon 1.3  01:15.89 2      1    131   12M    0B     2704K 16575\nIn most instances, the first three columns are the most useful. You’ve got the name of the command and how much CPU they’re using. Right now, nothing is using very much CPU. If I were to find something concerning – perhaps an R process that is using 500% of CPU – I would want to take notice of its pid to kill it with kill.\n\n\n\n\n\n\nSo much CPU?\n\n\n\nFor top (and most other commands), CPU is expressed as a percent of single core availability. So, on a modern machine (with multiple cores), it’s very common to see CPU totals well over 100%. Seeing a single process using over 100% of CPU is rarer.\n\n\nThe top command takes over your whole terminal. You can exit with Ctrl + c.\nAnother useful command for finding runaway processes is ps aux.9 It lists all processes currently running on the system, along with how much CPU and RAM they’re using. You can sort the output with the --sort flag and specify sorting by cpu with --sort -%cpu or by memory with --sort -%mem.\nBecause ps aux returns every running process on the system, you’ll probably want to pipe the output into head.\nAnother useful way to use ps aux is in combination with grep. If you pretty much know what the problem is – often this might be a runaway R or Python process – ps aux | grep <name> can be super useful to get the pid.\nFor example, here are the RStudio processes currently running on my system.\n > ps aux | grep \"RStudio\\|USER\"                                                                                      [10:21:18]\nUSER               PID  %CPU %MEM      VSZ    RSS   TT  STAT STARTED      TIME COMMAND\nalexkgold        23583   0.9  1.7 37513368 564880   ??  S    Sat09AM  17:15.27 /Applications/RStudio.app/Contents/MacOS/RStudio\nalexkgold        23605   0.5  0.4 36134976 150828   ??  S    Sat09AM   1:58.16 /Applications/RStudio.app/Contents/MacOS/rsession --config-file none --program-mode desktop \n\n\n\n\n\n\nTip\n\n\n\nThe grep command above looks a little weird because I used a little trick. I wanted to keep the header in the output, so the regex I used matches both the header line (USER) and the thing I actually care about (RStudio).\n\n\n\n\n\n\n\n\n\n\nCommand\nWhat it does\nHelpful options\n\n\n\n\ntop\nSee what’s running on the system.\n\n\n\nps aux\nSee all system processes.\nConsider using --sort and pipe into head or grep\n\n\nkill\nKill a system process.\n-9 to force kill immediately\n\n\n\n\n\n11.5.3 Managing networking\nNetworking is a complicated topic, which we’ll approach with great detail in Chapter 12. For now, it’s important to be able to see what’s running on your server that is accessible from the outside world on a particular port.\nThe main command to help you see what ports are being used and by what services is the netstat command. netstat returns the services that are running and their associated ports. netstat is generally most useful with the -tlp flags to show programs that are listening and the programs associated.\nTODO: get netstat example\n\n\n\n\n\n\n\n\nCommand\nWhat it does\nHelpful options\n\n\n\n\nnetstat\nSee ports and services using them.\nUsually used with -tlp\n\n\n\nSometimes you know you’ve got a service running on your machine, but you just can’t seem to get the networking working. It can be useful to access the service directly without having to deal with networking. You can do this with port forwarding, also called tunneling.\nSSH port forwarding allows you to take the output of a port on a remote server, route it through SSH, and display it as if it were on a local port. For example, let’s say I’ve got RStudio Server running on my server. Maybe I don’t have networking set up yet, or I just can’t get it working. If I’ve got SSH to my server working properly, I can double check that the service is working as I expect and the issue really is somewhere in the network.\nI find that the syntax for port forwarding completely defies my memory and I have to google it every time I use it. For the kind of port forwarding you’ll use most often in debugging, you’ll use the -L flag.\nssh -L <local port>:<remote ip>:<remote port> <ssh hostname>\nWhen you’re doing ssh forwarding, local is the place you’re ssh-ed into (aka your server) and the remote is another location – usually your laptop.\nSince the “remote” is my laptop, I almost always want to use localhost as the remote IP, and I usually want to use the same port remotely and locally – unless the local service is on a reserved port.\nSo let’s say I’ve got RStudio Server running on my server at my-ds-workbench.com on port 3939. Then I could run ssh -L 3939:localhost:3939 my-user@my-ds-workbench.com. With this command, I can bypass networking and just access whatever is at port 3939 on my server (hopefully RStudio Workbench!) by just going to localhost:3939 in my laptop’s browser.\n\n\n11.5.4 Understanding PATHs\nLet’s say you want to open R on your command line. Once you’ve got everything properly configured, you can just type R and have it open right up.\n ❯ R                                                       \n\nR version 4.2.0 (2022-04-22) -- \"Vigorous Calisthenics\"\nCopyright (C) 2022 The R Foundation for Statistical Computing\nPlatform: x86_64-apple-darwin17.0 (64-bit)\n\nR is free software and comes with ABSOLUTELY NO WARRANTY.\nYou are welcome to redistribute it under certain conditions.\nType 'license()' or 'licence()' for distribution details.\n\n  Natural language support but running in an English locale\n\nR is a collaborative project with many contributors.\nType 'contributors()' for more information and\n'citation()' on how to cite R or R packages in publications.\n\nType 'demo()' for some demos, 'help()' for on-line help, or\n'help.start()' for an HTML browser interface to help.\nType 'q()' to quit R.\n\n>\nBut how does the operating system know what you mean when you type R? If you’ve been reading carefully, you’ve realized that running the command means opening a particular runnable file, and R isn’t a file path on my system.\nYou can actually just type the complete filename of a runnable binary into your command line. For example, on my MacBook, my version of R is at /usr/bin/local/R, so I could open an R session by typing that full path. Sometimes it can be handy to be precise about exactly which executable you’re opening (I’m looking at you, multiple versions of Python), so you may want to use full paths to executables.\nIf you ever want to check which actual executable is being used by a command, you can use the which command. For example, on my system this is the result of which R.\n ❯ which R                                                    \n/usr/local/bin/R\nMost of the time you don’t want to have to bother with full paths for executables. You want to just type R on the command line and have R open. Moreover, there are cases where functionality relies on another executable being able to find R and run it – think of running RStudio Server, which starts a version of R under the hood.\nThe operating system knows how to find the actual runnable programs on your system via something called the path. When you type R into the command line, it searches along the path to find a version of R it can run.\nYou can check your path at any time by echoing the PATH environment variable with echo $PATH. On my MacBook, this is what the path looks like.\n ❯ echo $PATH                                                      \n/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/System/Cryptexes/App/usr/bin:/usr/bin:/bin:/usr/sbin:/sbin\nLater, when we get into running versions of programs that aren’t the system versions, we may have to append locations to the path so that we can run them easily."
  },
  {
    "objectID": "chapters/sec3/3-2-linux-admin.html#comprehension-questions",
    "href": "chapters/sec3/3-2-linux-admin.html#comprehension-questions",
    "title": "11  Basic Linux SysAdmin",
    "section": "11.6 Comprehension Questions",
    "text": "11.6 Comprehension Questions\n\nCreate a mind map of the following terms: Operating System, Windows, MacOS, Unix, Linux, Distro, Ubuntu\nWhen you initially SSH-ed into your server using ubuntu@$SERVER_ADDRESS, what user were you and what directory did you enter? What about when you used test_user@$SERVER_ADDRESS?\nWhat are the 3x3 options for Linux file permissions? How are they indicated in an ls -l command?\nHow would you do the following?\n\nFind and kill the process IDs for all running rstudio-server processes.\nFigure out which port JupyterHub is running on.\nCreate a file called secrets.txt, open it with vim, write something in, close and save it, and make it so that only you can read it."
  },
  {
    "objectID": "chapters/sec3/3-2-linux-admin.html#lab-a-working-data-science-workbench",
    "href": "chapters/sec3/3-2-linux-admin.html#lab-a-working-data-science-workbench",
    "title": "11  Basic Linux SysAdmin",
    "section": "11.7 Lab: A working Data Science Workbench",
    "text": "11.7 Lab: A working Data Science Workbench\nIn this lab, we’re going to take your server from an empty server that only you can access to one with users and useful software installed and running – even if it’s not yet accessible to the outside world.\n\n11.7.0.1 Step 1: Log on with the .pem key\nThe .pem key you downloaded when you set up the server is the skeleton key – it will automatically let you in with complete admin privileges. In Chapter 11, we’ll set up a user with SSH on the server with more limited permissions.\nIn the meantime, we’re going to use the .pem key to get started on the server, but be extremely careful with the power of the .pem key.\nBecause the keypair is so powerful, AWS requires that you restrict the access pretty severely (more on what that means in Chapter 11). If you try to use the keypair without first changing the permissions, you’ll be unable to, and you’ll get a warning that looks something like this:\n\n\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\n\n\\@ WARNING: UNPROTECTED PRIVATE KEY FILE! \\@\n\n\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\n\nPermissions 0644 for 'do4ds-lab-key.pem' are too open.\n\nIt is required that your private key files are NOT accessible by others.\n\nThis private key will be ignored.\n\nLoad key \"do4ds-lab-key.pem\": bad permissions\n\nubuntu\\@ec2-54-159-134-39.compute-1.amazonaws.com: Permission denied (publickey).\nBefore we can use it to open the server, we’ll need to make a quick change to the permissions on the key.\nSo let’s change the file permissions.\nWe’ll get into the details of how to use these commands in just a minute. For now, you’ll need to open a terminal window, navigate to the directory where the key is and change the file permissions.\nOn my machine that looks like:\n$ cd ~/Documents/do4ds-lab\n$ chmod 600 do4ds-lab-key.pem\nYou can sub in the path to where your key is and the name you used for your key.\nIn your terminal type the following\n$ ssh -i do4ds-lab-key.pem ubuntu@$SERVER_ADDRESS\n\n11.7.0.1.1 SSH on Windows\nFor a long time, Windows didn’t come with a built in SSH client, so you had to use PuTTY to do SSH from a Windows machine. Microsoft brought a native SSH client to Windows 10 in 2015, and it has been enabled by default since 2018.\nIf you run into any trouble using SSH commands on Windows, double check that you’ve enabled the OpenSSH Client.\nType yes when prompted, and you’re now logged in to your server!\n\n\n\n11.7.1 Step 2: Create a non-root user\nThe first thing we’re going to do is create a user so that you can login without running as root all the time. In general, if you’ve got a multitenant server, you’re going to want users for each actual human who’s accessing the system.\nI’m going to use the username test-user. If you want to be able to copy/paste commands from the online book, I’d advise doing the same. If you were creating users based on real humans, I’d advise using their names.\nLet’s create a user using the adduser command. This will walk us through a set of prompts to create a new user with a home directory and a password. Feel free to add any information you want – or to leave it blank – when prompted.\nsudo adduser test-user\nWe want this new user to be able to adopt root privileges. Remember that the way that is determined is whether the user is part of the sudo group.\nsudo usermod -aG sudo test-user\nAs you might have guessed, -aG stands for add to group.\n\n\n11.7.2 Step 3: Add an SSH Key for your user\nThe first thing you should do is check if you have an SSH key on your system (try ls ~/.ssh). If you’ve got one already, I’d recommend just using that key so you don’t have to mess around with multiple keys.\nIf you don’t have one, google how to create an SSH key and do it on your laptop.\nThe first step is going to be adding your public key to the end of the authorized_users file.\nFirst, you need to get your public key to the server. I recommend using scp.10\nFor me, the command looks something like this\nscp -i do4ds-lab-key.pem \\ # scp w/ pem key\n  ~/.ssh/id_ed25519.pub \\ # local public key\n  ubuntu@ec2-54-159-134-39.compute-1.amazonaws.com:/home/ubuntu # where to on server\nNow our public key is on the server, but it’s in the ubuntu user’s home directory. We’re going to add it as an authorized key for the test-user so that we can SSH in as that user.\nHere are the commands to do so:\nssh -i \nubuntu@ip-172-31-2-42:~$ sudo mv /home/ubuntu/id_ed25519.pub /home/test-user/ # move key\nubuntu@ip-172-31-2-42:~$ sudo chown test-user /home/test-user/id_ed25519.pub # give it to test-user\nubuntu@ip-172-31-2-42:~$ su test-user #change user\nPassword:\ntest-user@ip-172-31-2-42:/home/ubuntu$ cd ~ #go to home dir\ntest-user@ip-172-31-2-42:~$ mkdir -p .ssh #create .ssh directory\ntest-user@ip-172-31-2-42:~$ chmod 700 .ssh # Lock directory from other users\ntest-user@ip-172-31-2-42:~$ cat id_ed25519.pub >> .ssh/authorized_keys #add public key to end of authorized_keys file\ntest-user@ip-172-31-2-42:~$ chmod 600 .ssh/authorized_keys #set permissions\nNow we’re all set up with SSH, and you can log in as a normal user from your laptop just using ssh test-user@$SERVER_ADDRESS.\nIf you want to set up an SSH config for this server, I’d advise waiting until we’ve got a permanent URL for it in the next chapter.\nNow that we’re all set up, you should store the pem key somewhere safe and never use it to log in again.\nWhen you ever want to exit SSH and get back to your machine, you can just type exit.\n\n\n11.7.3 Step 4: Install R and Python\nEverything until now has been generic server administration. Now let’s get into some data-science-specific work – setting up R and Python.\n\n\n\n\n\n\nTip\n\n\n\nIf you run into trouble assuming sudo with your new user, try exiting SSH and coming back. Sometimes these changes aren’t picked up until you restart the shell.\n\n\n\n11.7.3.1 Installing R\nThere are a number of ways to install R on your server including installing it from source, from the system repository, or using R-specific tooling.\nYou can use the system repository version of R, but then you just get whatever version of R happens to be current when you run sudo apt-get install R. My preferred option is to use rig, which is an R-specific installation manager.\n\n\n\n\n\n\nNote\n\n\n\nAs of this writing, rig only supports Ubuntu. If you want to install on a different Linux distro, you will have to install R a different way.\nPosit makes R binaries available for a variety of different operating systems, including Ubuntu and RedHat. You can find instructions at https://docs.posit.co/resources/install-r/.\nTODO: confirm that installing into /opt/R works for RStudio Server OSS.\n\n\nThere are good instructions on downloading rig and using it to install R on the GitHub repo – https://github.com/r-lib/rig. Use those instructions to install the current R release on your AWS server.\nOnce you’ve installed R on your server, you can check that it’s running by just typing R into the command line. If that works, you’re good to move on to the next step.\n\n\n\n11.7.4 Step 5: Installing RStudio Server\nOnce R is installed, let’s download and install RStudio Server. This should be a very easy process. The basic process is to install the gdebi package from apt, which is used for installing downloaded packages, download the RStudio Server package, and install it.\nI’m not going to reproduce the commands here because the RStudio Server version numbers change frequently and you probably want the most recent one.\nYou can find the exact commands on the Posit website at https://posit.co/download/rstudio-server/. Make sure to pick the version that matches your operating system. Since you’ve already installed R, you can skip down to the “Install RStudio Server” step.\nOnce you’ve installed, you can check the status with sudo systemctl status rstudio-server. If it says running, you’re good to go!\nBut knowing it’s good to go isn’t nearly as fun as actually trying it. We don’t have a stable public URL for the server yet, so we can’t just access it from our browser. This is a perfect use case for an SSH tunnel.\nBy default, RStudio Server is on port 8787, so we’ll tunnel port 8787 on the server to localhost:8787 on our laptop. The command to do that is ssh -L 8787:localhost:8787 test-user@$SERVER_ADDRESS.\nNow, if you go to localhost:8787 in your browser, you should be able to access RStudio Server and login with the username test-user and password you set on the server.\n\n\n11.7.5 Step 6: Installing JupyerHub + JupyterLab\nRStudio and R are system libraries. So when RStudio runs, it calls and owns the R process that you’ll use inside RStudio Server. In contrast, JupyterHub and JupyterLab are Python programs, so we install them inside a Python installation.\nPeople pretty much only ever install R to do data science. In contrast, Python is one of the world’s most popular programming languages for general purpose computing. Contrary to what you might think, this actually makes configuring Python harder than configuring R.\nThe reason is that your system comes with a version of Python installed, but we don’t want to use that version. Given the centrality of that version to normal server operations, we want to leave it alone. It turns out that installing one or more other versions of Python and then ignoring the system version of Python isn’t totally trivial to do. Then we’re going to want to create a standalone virtual environment that’s just for running JupyterHub so it doesn’t get messed up later.\nTODO: diagram of relationships of system python, DS python, Jupyter Python.\nIt’s very likely that the version of Python on your system is old. Generally we’re going to want to install a newer Python for doing data science work, so let’s start there. As of this writing, Python 3.10 is a relatively new version of Python, so we’ll install that one.\nLet’s start by actually installing Python 3.10 on our system. We can do that with apt.\nTODO: finish these instructions\n> sudo su\n> apt install python3.10-venv\nNow that we’ve installed Python, we can create a standalone virtual environment for running JupyterHub.\n> python3 -m venv /opt/jupyterhub\n> source /opt/jupyterhub/bin/activate\nNow we’re going to actually get JupyterHub up and running inside the virtual environment we just created. JupyterHub produces docs that you can use to get up and running very quickly. If you have to stop for any reason, make sure to come back, assume sudo, and start the JupyterHub virtual environment we created.\nHere were the installation steps that worked for me:\nnpm install -g configurable-http-proxy\napt-get install npm nodejs\npython3 -m pip install jupyterhub jupyterlap notebook\n\nln -s /opt/jupyterhub/bin/jupyterhub-singleuser /usr/local/bin/jupyterhub-singleuser # symlink in singleuser server, necessary because we're using virtual environment\n\njupyterhub\nIf all went well, you’ll now have JupyterHub up and running on port 8000!\nIf you want to confirm, tunnel in with ssh -L 8000:localhost:8000 test-user@$SERVER_ADDRESS.\n\n11.7.5.1 Running JupyterHub as a service\nAs I mentioned above, JupyterHub is a Python process, not a system process. This is ok, but it means that we’ve got to remember the command to start it if we have to restart it, and that it won’t auto restart if it were to fail for any reason.\nA program that runs in the background on a machine, starting automatically, and controlled by systemctl is called a daemon. Since we want JupyterHub to be a daemon, we’re got to add it as a system daemon, which isn’t hard.\nWe don’t need it right now, but it’ll be easier to manage JupyterHub later on from a config file that’s in /etc/jupyterhub.\nLet’s create a default config file and move it into the right place using\n> jupyterhub --generate-config\n> mkdir -p /etc/jupyterhub\n> mv jupyterhub_config.py /etc/jupyterhub\nNow we’ve got to daemon-ize JupterHub. There are two steps – create a file describing the service for the server’s daemon, and then start the service.\nTo start with, end the existing JupyterHub process. If you’ve still got that terminal open, you can do so with ctrl + c. If not, you can use your ps aux and grep skills to find and kill the JupyterHub processes.\nOn Ubuntu, adding a daemon file uses a tool called systemd and is really straightforward.\nFirst, add the following to /etc/systemd/system/jupyterhub.service. If you’re reading this book in hard copy, you can go to the online version to copy/paste or get this file on the book’s Git repo at TODO.\n\n\n/etc/systemd/system/jupyterhub.service\n\n[Unit]\nDescription=Jupyterhub\nAfter=syslog.target network.target\n\n[Service]\nUser=root\nEnvironment=\"PATH=/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/opt/jupyterhub/bin\"\nExecStart=/opt/jupyterhub/bin/jupyterhub -f /etc/jupyterhub/jupyterhub_config.py\n\n[Install]\nWantedBy=multi-user.target\n\nHopefully this file is pretty easy to parse. Two things to notice – the Environment line adds /opt/jupyterhub/bin to the path – that’s where our virtual environment is.\nSecond, the ExecStart line is the startup command and includes our -f /etc/jupyterhub/jupyterhub_config.py – this is the command to start JupyterHub with the config we created a few seconds ago.\nNow we just need to reload the daemon tool so it picks up the new service it has available and start JupyterHub!\nsystemctl daemon-reload\nsystemctl start jupyterhub\nYou should now be able to see that JupyterHub is running using systemctl status jupyterhub and can see it again by tunneling to it.\nTo set JupyterHub to automatically restart when the server restarts, run systemctl enable jupyterhub.\n\n\n\n11.7.6 Step 7: Running a Plumber API in a Container\nIn addition to running a development workbench on your server, you might want to run a data science project. If you’re running a Shiny app, Shiny Server is easy to configure along the lines of RStudio Server.\nHowever, if you put an API in a container like we did in Chapter 9, you might want to deploy that somewhere on your server as a running container.\nThe first step is to install docker on your system with sudo apt-get install docker.io. You can check that you can run docker with docker ps. You may need to adopt sudo privileges to do so.\nOnce we have docker installed, getting the API running is almost trivially easy using the command we used back in Chapter 9 to run our container.\nsudo docker run --rm -d \\\n  -p 8555:8000 \\\n  --name palmer-plumber \\\n  alexkgold/plumber\nThe one change you might note is that I’ve changed the port on the server to be 8555, since we already have JupyterHub running on 8000.\nNow, we can SSH tunnel into our server and view our running API at localhost:8555/__docs__/.\nThis is why people love docker – it’s wildly easy to get something simple running. But getting Docker hardened for production takes a bit more work.\nThe first thing to notice is that there’s no auth on our API. Anyone can hit this API as much as they want if they have the URL. Needless to say, this is not a security best practice unless you intend to host a public service.\nThe other issue is that we haven’t daemonized the API. If we restart the server or the container dies for any reason, it won’t auto-restart.\nIt’s not generally a best practice to daemon-ize a docker container by just putting the run command into systemd. Instead, you should use a container management system that is designed specifically to manage running containers, like Docker Compose or Kubernetes. Getting deeper into those systems is beyond the scope of this book.\n\n\n11.7.7 Questions for Alex\nWe didn’t actually make use of the EBS volume we mounted for home dirs. Should we do that? https://www.tecmint.com/move-home-directory-to-new-partition-disk-in-linux/\n\n\n11.7.8"
  },
  {
    "objectID": "chapters/sec3/3-3-networking.html#tcpip-and-the-mail",
    "href": "chapters/sec3/3-3-networking.html#tcpip-and-the-mail",
    "title": "12  Intro to Computer Networks",
    "section": "12.1 TCP/IP and the mail",
    "text": "12.1 TCP/IP and the mail\nLet’s start by imagining you have a penpal, who lives in an apartment building across the country.1\nLet’s say you’ve got some juicy gossip to share with your penpal. Because this gossip is so juicy, you’re not going to put it on a postcard. You’d write your letter, put it inside an envelope, address the letter to the right spot, and put it in the mail.\nYou trust the postal service to take your letter, deliver it to the correct address, and then your friend will be able to read your letter.\nThe process of actualy getting data from one computer to another is governed by the TCP/IP protocol and is called packet switching.2\nWhen a computer has data to send to another computer, it takes that information and packages it up into a bundle called a packet.3\nThe data in the packet is like the contents of your letter.\nJust as the postal service defines permissible envelope sizes and address formats, the TCP/IP protocol defines what a packet has to look like from the outside, including what constitutes a valid address.\nA uniform resource locator (URL) is the way to address a specific resource on a network.\nA full URL includes 4 pieces of information: \\[\n\\overbrace{\\text{https://}}^\\text{protocol}\\overbrace{\\text{example.com}}^\\text{address}\\overbrace{\\text{:443}}^\\text{port}\\overbrace{\\text{/}}^\\text{resource}\n\\]\nThe protocol starts the URL. It is separated from the rest of the URL by ://.\nEach of the rest of the pieces of the URL is needed to get to a specific resource and has a real-world analog.\nThe address is like the street address of your penpal’s building. It specifies the host where your data should go.4\nA host is any entity on a network that can receive requests and send responses. So a server is a common type of host on a network, but so is a router, your laptop, and a printer.\nThe port is like your friend’s apartment. It specifies which service on the server to address.\nLastly, the resource dictates what resource you want on the server. It’s like the name on the address of the letter, indicating that you’re writing to your penpal, not their mom or sister.\nThis full URL may look a little strange to you. You’re probably used to just putting a standalone domain like \\(google.com\\) into your browser when you want to go to a website. That’s because https, port 443, and / are all defaults, so modern browsers don’t make you specify them.\nBut what if you make it explicit? Try going to https://google.com:443/. What do you get?"
  },
  {
    "objectID": "chapters/sec3/3-3-networking.html#ports-get-you-to-the-right-service",
    "href": "chapters/sec3/3-3-networking.html#ports-get-you-to-the-right-service",
    "title": "12  Intro to Computer Networks",
    "section": "12.2 Ports get you to the right service",
    "text": "12.2 Ports get you to the right service\nA port is a location on a server where a network connection is possible. It’s like the apartment number for the mail. Each port has a unique number 1 to just over 65,000. By default, the overwhelming majority of the ports on the server are closed for security reasons.\nYour computer automatically opens ports to make outgoing connections, but if you want to allow someone to make inbound connections – like to access RStudio or a Shiny app on a server – you need to manually configure and open a port.\nAny program that is running on a server and that you intend to be accessible from the outside is called a service. For example, we set up RStudio, JupyterHub, and a Plumber API as services in the lab in Chapter 11. Each service lives on a unique port on the server.\nSince each service running on a server needs a unique port, it’s common to choose a somewhat random relatively high-numbered port. That makes sure it’s unlikely that the port will conflict with another service running on the server.\nFor example RStudio Server runs on port 8787 by default. According to JJ Allaire, there’s no special meaning to this port. It was chosen because it was easy to remember and not 8888, which some other popular projects had taken.\n\n12.2.1 Special Port Cheatsheet\nAll ports below 1024 are reserved for common server tasks, so you can’t assign services to low-numbered ports.\nThere are also three common ports that will come up over and over. These are handy because if you’re using the relevant service, you don’t have to indicate if it’s using the default port.\n\n\n\nProtocol\nDefault Port\n\n\n\n\nHTTP\n80\n\n\nHTTPS\n443\n\n\nSSH\n22"
  },
  {
    "objectID": "chapters/sec3/3-3-networking.html#assigning-addresses",
    "href": "chapters/sec3/3-3-networking.html#assigning-addresses",
    "title": "12  Intro to Computer Networks",
    "section": "12.3 Assigning addresses",
    "text": "12.3 Assigning addresses\nThe proper address of a server on a network is defined by the Internet Protocol (IP) and is called an IP Address. IP addresses are mapped to human-friendly domains like \\(google.com\\) with the Domain Name Service (DNS).\n\n\n\n\n\n\nNote\n\n\n\nIn this chapter, we’re going to set DNS aside and talk exclusively about IP Addresses.\n?sec-dns-ssl is all about DNS.\n\n\nEvery host on the internet has a public IP address.\nA public IP address is an IP address that is valid across the entire internet. That means that each public IP address is unique across the entire internet. You can think of a public IP address like the public street address of a building.\nBut many hosts are not publicly accessible on the internet. Many are housed in private networks. A private network is one where the hosts aren’t directly accessible to the public. You can think of a host in a private network like a building inside a gated community. You may still be able to get there from the public, but you can’t just walk up to the building from the street. Instead you need to come in through the specific gates that have been permitted and approach only on the roads that are allowed.\nTODO: Image of public IPs like street address, private like cul-de-sac\nThere are many different kinds of private networks. Some are small and enforced by connection to a physical endpoint, like the private network your WiFi router controls that houses your laptop, phone, TV, Xbox, and anything else you allow to connect to your router. In other cases, the network is a software network. Many organizations have virtual private networks (VPNs). In this case, you connect to the network via software. There may be resources you can only connect to inside your VPN and there also might be limitations about what you can go out and get.\nThere are a few different reasons for this public/private network split. The first is security. If you’ve got a public IP address, anyone on the internet can come knock on your virtual front door. That’s actually not so bad. What’s more problematic is that they can go all the way around the building looking for an unlocked side door. Putting your host inside a private network is a way to ensure that people can only approach the host through on the pathways you intend.\nThe second reason is convenience.\nPrivate networks provide a nice layer of abstraction for network addresses.\nYou probably have a variety of network-enabled devices in your home, from your laptop and phone to your TV, Xbox, washing machine, and smart locks. But from the outside, your house has only one public IP address – the address of the router in your home. That means that your router has to keep track of all the devices you’ve got, but they don’t need to register with any sort of public service just to be able to use the internet.\nAs we’ll get into in Chapter 13, keeping track of IP Addresses is best left to machines. If you’re managing a complex private network, it’s really nice to give hostnames to individual hosts. A nice feature of a private hostname compared to a public address is that you don’t have to worry if the hostname is unique across the entire internet – it just has to be unique inside your private network.\n\n12.3.1 Firewalls, allow-lists, and other security settings\nOne of the most basic ways to keep a server safe is to not allow traffic from the outside to come in. Generally that means that in addition to keeping the server ports themselves closed, you’ll also have a firewall up that defaults to all ports being closed.\nIn AWS, the basic level of protection for your server is called the security group. If you remember, we used a default security group in launching your server. When you want to go add more services to your server, you’ll have to open additional ports both on the server and in the security group.\nIn addition to keeping particular ports closed, you can also set your server to only allow incoming traffic from certain IP addresses. This is generally a very coarse way to do security, and rather fragile. For example, you could configure your server to only accept incoming requests from your office’s IP address, but what if someone needs to access the server from home or the office’s IP address is reassigned?\nOne thing that is not a security setting is just using a port that’s hard to guess. For example, you might think, “Well, if I were to put SSH on port 2943 instead of 22, that would be safer because it’s harder to guess!” I guess so, but it’s really just an illusion of security. There are ways to make your server safer. Choosing esoteric port numbers really isn’t it."
  },
  {
    "objectID": "chapters/sec3/3-3-networking.html#how-packets-are-routed",
    "href": "chapters/sec3/3-3-networking.html#how-packets-are-routed",
    "title": "12  Intro to Computer Networks",
    "section": "12.4 How packets are routed",
    "text": "12.4 How packets are routed\nThe way packets travel from one computer to another is called routing. A router is a hardware or software device that route packets.\nYou can think of routers and public networks as existing in trees. Each router knows about the IP addresses downstream of it and also the single upstream default address.5\nTODO: diagram of routers in trees\nFor example, the router in your house just keeps track of the actual devices that are attached to it. So if you were to print something from your laptop, the data would just go to your router and then to your printer.\nOn the other hand, when you look at a picture on Instagram, that traffic has to go over the public network. The default address for your home’s router is probably one owned by your internet service provider (ISP) for your neighborhood. And that router’s default address is probably also owned by your ISP, but for a broader network.\nSo your packet will get passed upstream to a sufficiently general network and then back downstream to the actual address you’re trying to reach.\nMeanwhile, your computer just waits for a response. Once the server has a response to send, it comes back using the same technique. Obviously a huge difference between sending a letter to a penpal and using a computer network is the speed. Where sending a physical letter takes a minimum of days, sending and receiving packets over a network is so fast that the delay is imperceptible when you’re playing a multiplayer video game or collaborating on a document online."
  },
  {
    "objectID": "chapters/sec3/3-3-networking.html#how-to-recognize-an-ip-address",
    "href": "chapters/sec3/3-3-networking.html#how-to-recognize-an-ip-address",
    "title": "12  Intro to Computer Networks",
    "section": "12.5 How to recognize an IP address",
    "text": "12.5 How to recognize an IP address\nYou’ve probably seen IPv4 addresses many times. They’re four blocks of 8-bit fields (numbers between 0 and 255) with dots in between, so they look something like 65.77.154.233.\nIf you do the math, you’ll realize there are “only” about 4 billion of these. While we can stretch those 4 billion IP addresses to many more devices since most devices only have private IPs, we are indeed running out of public IPv4 addresses.\nThe good news is that smart people started planning for this a while ago. In the last few years, adoption of the new IPv6 standard has started. IPv6 addresses are eight blocks of hexadecimal (0-9 + a-f) digits separated by colons, with certain rules that allow them to be shortened, so 4b01:0db8:85a3:0000:0000:8a2e:0370:7334 or 3da4:66a::1 are both examples of valid IPv6 addresses.\nIPv6 will coexist with IPv4 for a few decades and we’ll eventually switch entirely to IPv6. There’s no worry about running out of IPv6 addresses any time soon, because the total quantity of IPv6 addresses is a number with 39 zeroes.\n\n12.5.1 Reserved IP Addresses\nMost IPv4 addresses are freely available to be assigned, but there are a few that you’ll see in particular contexts and it’s useful to know what they are.\nThe first IP address you’ll see a lot is 127.0.0.1, also known as localhost or loopback. This is the way a machine refers to itself.\nFor example, if you open a Shiny app in RStudio Desktop, the app will pop up in a little window along with a notice that says\nListening on http://127.0.0.1:6311\nThat http://127.0.0.1 is indicating that your computer is serving the Shiny app to itself on the localhost address.\nThere are also a few blocks of addresses that are reserved for use on private networks, so they’re never assigned in public.\n\n\n\n\n\n\n\nCode\nMeaning\n\n\n\n\n127.0.0.1\nlocalhost or loopback – the machine that originated the request\n\n\n192.168.x.x\n172.16.x.x.x\n10.x.x.x\nProtected address blocks used for private IP addresses.\n\n\n\nYou don’t really need to remember these, but it’s very likely you’ve seen an address like 192.168.0.1 or 192.168.1.1 if you’ve ever tried to configure a router or modem for your home wifi.\nNow you know why."
  },
  {
    "objectID": "chapters/sec3/3-3-networking.html#basic-network-troubleshooting",
    "href": "chapters/sec3/3-3-networking.html#basic-network-troubleshooting",
    "title": "12  Intro to Computer Networks",
    "section": "12.6 Basic network troubleshooting",
    "text": "12.6 Basic network troubleshooting\nNetworking can be difficult to manage because there are so many layers where it can go awry. Let’s say you’ve configured a service on your server, but you just can’t seem to access it.\nThe ping command can be useful for checking whether your server is reachable on the network. For example, here’s what happens when I ping the domain where this book sits.\n> ping -o do4ds.com                                                                        \nPING do4ds.com (185.199.110.153): 56 data bytes\n64 bytes from 185.199.110.153: icmp_seq=0 ttl=57 time=13.766 ms\n\n--- do4ds.com ping statistics ---\n1 packets transmitted, 1 packets received, 0.0% packet loss\nround-trip min/avg/max/stddev = 13.766/13.766/13.766/0.000 ms\nThis looks great – it sent 1 packet to the server and got one back. That’s exactly what I want. Seeing an unreachable host or packet loss would be an indication that my networking probably isn’t configured correctly somewhere between me and the server. I generally like to use ping with the -o option for sending just one packet – as opposed to continuously trying.\nIf ping fails, it means that my server isn’t reachable. The things I’d want to check is that I have the URL for the server correct, that DNS is configured correctly (see Chapter 13), that I’ve correctly configured any firewalls to have the right ports open (Security Groups in AWS), and that any intermediate networking devices are properly configured (see more on proxies in Chapter 17).\nIf ping succeeds but I still can’t access the server, curl is good to check. curl actually attempts to fetch the website at a particular URL. It’s often useful to use curl with the -I option so it just returns a simple status report, not the full contents of what it finds there.\nFor example, here’s what I get when I curl CRAN from my machine.\n > curl -I https://cran.r-project.org/                                                         \n \nHTTP/1.1 200 OK\nDate: Sun, 15 Jan 2023 15:34:19 GMT\nServer: Apache\nLast-Modified: Mon, 14 Nov 2022 17:33:06 GMT\nETag: \"35a-5ed71a1e393e7\"\nAccept-Ranges: bytes\nContent-Length: 858\nVary: Accept-Encoding\nContent-Type: text/html\nThe important thing here is that first line. The server is returning a 200 HTTP status code, which means all is well. For more on HTTP status codes and how to interpret them, see Chapter 4.\nIf ping succeeds, but curl does not, it means that the server is up at the expected IP address, but the service is not accessible. At that point, you might check whether the right ports are accessible – it’s possible to (for example) have port 443 or 80 accessible on your server, but not the port you actually need for your service. You also might check on the server itself that the service is running and that it is running on the port you think it is.\nIf you’re running inside a container, you should check that you’ve properly configured the port inside container to be forwarded to the outside."
  },
  {
    "objectID": "chapters/sec3/3-3-networking.html#comprehension-questions",
    "href": "chapters/sec3/3-3-networking.html#comprehension-questions",
    "title": "12  Intro to Computer Networks",
    "section": "12.7 Comprehension Questions",
    "text": "12.7 Comprehension Questions\n\nWhat are the 4 components of a URL? What’s the significance of each?\nWhat are the two things a router keeps track of? How does it use each of them?\nAre there any inherent differences between public and private IP addresses?\nWhat is the difference between an IP address and a port?\nLet’s say you’ve got a server at 54.33.115.12. Draw a mind map of what happens when you try to SSH into the server. Your explanation should include the terms: IP Address, port, 22, default address, router, sever."
  },
  {
    "objectID": "chapters/sec3/3-3-networking.html#lab-making-it-accessible-in-one-place",
    "href": "chapters/sec3/3-3-networking.html#lab-making-it-accessible-in-one-place",
    "title": "12  Intro to Computer Networks",
    "section": "12.8 Lab: Making it accessible in one place",
    "text": "12.8 Lab: Making it accessible in one place\n\n\n\n\n\n\nNote\n\n\n\nThis lab is somewhat advanced. If you just wanted to run one service on your server – say just RStudio or just the Plumber API, you could skip this lab and just configure the service to be available on port 80 and/or 443.\n\n\nIn this lab, we’re going to go from having to SSH tunnel to be able to use your data science workbench to making it available over the internet. That means we’re going to have to do 2 things: configure the networking to allow HTTP traffic and configure a real domain for our server.\nRight now RStudio Server is ready to serve traffic inside our server on port 8787, JupyterHub is on 8000, and our Palmer Penguins API is on 8555. But right now nothing can get to them. So the first step is to allow traffic in.\nThe easiest thing we could do would be to just open up ports 8787 and 8000 to the world. It’s not the right answer, but it will “work”.\n\n\n\n\n\n\nTip\n\n\n\nIf you do want to try it to prove to yourself, go to the settings for your server’s security group and just add a custom TCP rule allowing access to ports 8787, 8000, and 8555 from anywhere. If you visit $SERVER_ADDRESS:8787 you should get RStudio, similarly with JupyterHub at $SERVER_ADDRESS:8000, and the API at $SERVER_ADDRESS:8555.\nOk, now close those ports back up so we can do this the right way.\n\n\nIf you did it this way, your users would have to remember these arbitrary ports to use your server. It’s also insecure because there isn’t a good way to secure these ports with SSL.6\nThe common solution to wanting to serve multiple services off of one server is to use a proxy. A proxy is a network device that reroutes traffic in various ways. Chapter 17 has more detail on proxies and why they’re a pain.\nFor the purpose of this lab, I’m going to give you a pre-built proxy to use.\nHere are the steps to take on your server:\n\nInstall nginx with sudo apt install nginx.\nSave a backup of nginx.conf, cp /etc/nginx/nginx.conf /etc/nginx/nginx-backup.conf.7\nEdit the nginx configuration with sudo vim /etc/nginx/nginx.conf and replace it with:\n\n\n\n/etc/nginx/nginx.conf\n\nhttp {\n  \n  \\# Enable websockets (needed for Shiny)\n  map \\$http_upgrade \\$connection_upgrade { \n    default upgrade; '' close; \n  }\n  \n  server { listen 80;\n    \n    location /rstudio/ {\n      # Needed only for a custom path prefix of /rstudio\n      rewrite ^/rstudio/(.*)$ /$1 break;\n      \n      # Use http here when ssl-enabled=0 is set in rserver.conf\n      proxy_pass http://localhost:8787;\n      \n      proxy_http_version 1.1;\n      proxy_set_header Upgrade $http_upgrade;\n      proxy_set_header Connection $connection_upgrade;\n      proxy_read_timeout 20d;\n      \n      # Not needed if www-root-path is set in rserver.conf\n      proxy_set_header X-RStudio-Root-Path /rstudio;\n      \n      # Optionally, use an explicit hostname and omit the port if using 80/443\n      proxy_set_header Host $host:$server_port;\n    }\n    \n    location /jupyter/ {\n      # NOTE important to also set base url of jupyterhub to /jupyter in its config\n      proxy_pass http://127.0.0a.1:8000;\n      \n      proxy_redirect   off;\n      proxy_set_header X-Real-IP $remote_addr;\n      proxy_set_header Host $host;\n      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n      proxy_set_header X-Forwarded-Proto $scheme;\n      \n      # websocket headers\n      proxy_set_header Upgrade $http_upgrade;\n      proxy_set_header Connection $connection_upgrade;\n    }\n    \n    location /palmer/ {\n    #TODO\n    }\n  }\n\n\nTest that your configuration is valid sudo nginx -t.\nStart nginx sudo systemctl start nginx. If you see nothing all is well.\n\nIf you need to change anything, update the config and then restart with sudo systemctl restart nginx.\nThere’s one more thing you’ll have to do, which is to let RStudio and JupyterHub know that they’re on a subpath. Complex web apps like RStudio and JupyterHub frequently send people to a different subpath internally. In order to do so properly, they need to know to prepend all of those requests with the subpath.\nRStudio accepts a header from the proxy that lets it know what path it’s on (see the X-RStudio-Root-Path header line in the nginx config). Jupyter needs to be explicitly told.\nIf you recall, we autogenerated the JupyterHub config with\n jupyterhub --generate-config\n sudo mkdir /etc/jupyterhub\n sudo mv jupyterhub_config.py /etc/jupyterhub\nSo you can go edit it with sudo vim /etc/jupyterhub/jupyterhub_config.py.\nFind the line that reads # c.JupyterHub.bind_url = 'http://:8000'.\n\n\n\n\n\n\nTip\n\n\n\nYou can search in vim from normal mode with / <thing you're searching for>. Go to the next hit with n.\n\n\nUncomment that line and add a /jupyter on the end, so it reads c.JupyterHub.bind_url = 'http://:8000/jupyter'.\nStart jupyterhub with a specific config file.\nWe already daemonized JupyterHub, you restarting it with sudo systemctl restart jupyterhub should pick up the new config."
  },
  {
    "objectID": "chapters/sec3/3-4-dns.html#how-dns-lookups-work",
    "href": "chapters/sec3/3-4-dns.html#how-dns-lookups-work",
    "title": "13  DNS allows for human-readable addresses",
    "section": "13.1 How DNS lookups work",
    "text": "13.1 How DNS lookups work\nIn Chapter 12, we discussed how packets gets routed to successively higher-level routers until there’s one that knows where the packet is going and sends it back down. It turns out that this process essentially happens twice. The first time is to get the IP address for the domain in a process called DNS resolution. The second time is when it actually sends the information to that IP address.\nYou can see the basic structure of a DNS lookup using the terminal command nslookup. Here’s the DNS routing for \\(google.com\\):\n ❯ nslookup google.com                                              \nServer:     192.168.86.1\nAddress:    192.168.86.1#53\n\nNon-authoritative answer:\nName:   google.com\nAddress: 142.251.163.113\nName:   google.com\nAddress: 142.251.163.100\nName:   google.com\nAddress: 142.251.163.138\nName:   google.com\nAddress: 142.251.163.101\nName:   google.com\nAddress: 142.251.163.102\nName:   google.com\nAddress: 142.251.163.139\nThese addresses do change periodically, so they may no longer be valid by the time you read this. But if you want to go do an nslookup yourself, you can visit one of those IP addresses and see that it takes you right to the familiar \\(google.com\\).1\nDNS resolution is actually quite a complex process because every computer in the world needs to be able to resolve a DNS entry to the same IP address in a timely fashion. The simplest form of DNS lookup would be easy – there would just be one nameserver with the top-level domains and then one server for each top-level domain.\nThere are two problems with this theoretical DNS lookup – it wouldn’t be very resilient because of a lack of redundancy and it would be slow. In order to speed things up, the DNS system is highly decentralized.\nAlong with decentralization, DNS resolution relies on a lot of cacheing. When your computer or an intermediate DNS server looks up an IP address for you, it caches it. This is because its likely that if you’ve looked up a domain once, you’re going to do it again soon.\nThis is great if you are using the internet and don’t want to wait for DNS lookups, but when you’re changing the domains on servers you control, there are thousands of public DNS servers that a request could get routed to, and many of them may have outdated cache entries. DNS changes can take up to 24 hours to propagate.\nThat means that if you make a change and it’s not working, you have no idea whether you made a mistake or it just hasn’t propagated yet. It’s very annoying.\nSometimes, using a private browsing window will cause DNS cache refreshes, but not always."
  },
  {
    "objectID": "chapters/sec3/3-4-dns.html#how-dns-is-configured",
    "href": "chapters/sec3/3-4-dns.html#how-dns-is-configured",
    "title": "13  DNS allows for human-readable addresses",
    "section": "13.2 How DNS is configured",
    "text": "13.2 How DNS is configured\nFrom the perspective of someone trying to set up their own website, there’s only one DNS server that matters to you personally – the DNS server for your domain name registrar.\nDomain name registrars are the companies that actually own domains. You can buy or rent one from them in order to have a domain on the internet.\nYour first stop would be a domain name registrar where you’d find an available domain you like and pull out your credit card.\nCosts for domain names vary widely. Buying a meaningless domain in a less popular top-level domain, say ladskfmlsdf.me can cost as little as $3 per year. On the other hand, buying a .com domain that’s a real word or phrase can be a few thousand dollars. There are articles every few years about some major company accidentally allowing their domain name to lapse and ransoming it back for stupid amounts of money.\nConfiguration of DNS is done by way of records, of which there are a menagerie of types you can configure. Luckily, most simple configurations only need CNAME and A records.\nHere’s an imaginary DNS record table for the domain example.com:\n\n\n\nPath/Host\nType\nTarget\n\n\n\n\n@\nA\n143.122.8.32\n\n\nwww\nCNAME\nexample.com\n\n\n*\nA\n143.122.8.33\n\n\n\nLet’s go through how to read this table.\nSince we’re configuring example.com, the paths/hosts in this table are relative to example.com.\nIn the first row we’re configuring an A record to go to the target IP address. A records (or their IPv6 cousin AAAA records) map a domain to an actual IP address. The path @ is a special symbol meaning exact match. So by this configuration, any traffic to example.com will be passed straight through to the specified IP address.\nThe second row deals with traffic to the www subdomain. CNAME records alias sub-domains. They’re most frequently used to map sub-domains to the main domain. Since this is a CNAME record for example.com, this record indicates that traffic to www.example.com should be treated exactly like traffic to example.com. Some domain providers do automatic redirection of www traffic, and so this row may not be necessary in some configurations.\nThe last record uses the wildcard symbol * to send all subdomain traffic that’s not already spoken for – say blog.example.com or info.example.com directly to the IP address specified. In this case, I’m sending all of those subdomains to a different IP address, maybe a 404 (not found) page – or maybe I’m serving all the subdomains off a different server.\nSo what happens is that your query goes through several layers of public DNS servers to get to the DNS entry for your domain name registrar. In many cases, you’ll directly configure your domain name registrar to point to your website or server – but you also can configure the domain name registrar to point at another set of DNS servers you actually control with an NS record.\nIf you’re setting up your own server, this probably isn’t the case, but some large enterprises do run their own private DNS servers."
  },
  {
    "objectID": "chapters/sec3/3-4-dns.html#comprehension-questions",
    "href": "chapters/sec3/3-4-dns.html#comprehension-questions",
    "title": "13  DNS allows for human-readable addresses",
    "section": "13.3 Comprehension Questions",
    "text": "13.3 Comprehension Questions\nTODO"
  },
  {
    "objectID": "chapters/sec3/3-4-dns.html#lab-configuring-dns-for-your-server",
    "href": "chapters/sec3/3-4-dns.html#lab-configuring-dns-for-your-server",
    "title": "13  DNS allows for human-readable addresses",
    "section": "13.4 Lab: Configuring DNS for your server",
    "text": "13.4 Lab: Configuring DNS for your server\n\n\n\n\n\n\nWarning\n\n\n\nIn Chapter 14, we’ll secure your server with an SSL certificate and https. You absolutely should not use your server for anything real until you’ve configured SSL.\n\n\n\n13.4.1 Step 1: Allocate Elastic IP\nIf you’ve restarted your server, you’ve probably noticed that the IP address changed!\nThis can be a real pain if you have to reconfigure your DNS every time. AWS offers a service called Elastic IP that gives you an IP address that won’t change that you can use with your instance, even if you restart it.\nAs of this writing, Elastic IPs are free as long as it’s associated with a single running EC2 instance. You get charged if they’re not active – basically AWS doesn’t want you hoarding Elastic IPs. Great – just make sure to give back the elastic IP if you take down your instance.\n\n\n\n\n\n\nNote\n\n\n\nIf you were doing this for real, and not in a somewhat arbitrary order that’s grouped by topic, it would’ve made sense to set up the elastic IP as soon as you brought the server up so you only would’ve used that IP address.\n\n\nGo to Elastic IP and click allocate. Once it has been allocated, you’ll need to associate it with the existing Instance you created. Choose the instance and the default private IP address.\nNote that once you make this change, your server will no longer be available at its old IP address, so you’ll have to ssh in at the new one. If you have SSH terminals open when you make the change, they will break.\n\n\n13.4.2 Step 2: Buy a domain\nThis part will not be free, but it can be very cheap. The easiest way to do this is via AWS’s Route53 service. You can get domains on Route53 for as little as $9 per year – but there are even cheaper services. For example, I was able to get the domain \\(do4ds-lab.shop\\) for $1.98 for a year on \\(namecheap.com\\).\n\n\n13.4.3 Step 3: Configure DNS\nOnce you’ve got your domain, you have to configure your DNS. You’ll have to create 2 A records – one each for the @ host and the * host pointing to your IP and one for the CNAME at the www with the value being your bare domain.\nSo in NameCheap, my Advanced DNS configuration looks like this:\n\n\n\n13.4.4 Step 4: Wait an annoyingly long time\nNow you just have to be patient. Unfortunately DNS takes time to propagate. After a few minutes (or hours?), your server should be reachable at your domain.\nIf it’s not (yet) reachable, try seeing if an incognito browser works. If it doesn’t, wait some more. When you run out of patience, try reconfiguring everything and check if it works now."
  },
  {
    "objectID": "chapters/sec3/3-5-ssl.html#the-problems-https-solves",
    "href": "chapters/sec3/3-5-ssl.html#the-problems-https-solves",
    "title": "14  You should use SSL/HTTPS",
    "section": "14.1 The problems https solves",
    "text": "14.1 The problems https solves\nFor a long time, most web traffic was just plain http traffic.\nIf you recall, there used to be a lot of warnings around that you shouldn’t just connect to and use the WiFi at your neighborhood Starbucks. That was (mostly) because of the risks of using plain http traffic.\nOver the last 10 years or so, almost all web traffic has migrated to https, particularly since 2015 when Google Chrome began the process of marking any site using plain http as insecure. These days it’s actually pretty safe to use any random WiFi network you want – because of https.\nThe biggest risk with those public WiFi networks was that someone could compromise the router. If the router was compromised, there were two things a bad actor could do.\nhttp has no way to verify that the website you think you’re interacting with is actually that website. So a bad actor could mess with the router’s DNS resolution, and make it so that \\(bankofamerica.com\\) went to a lookalike website to capture your banking login. That’s called a man-in-the-middle attack.\nhttp traffic is also completely unencrypted. So that packet you’re sending would be just as readable by someone who installed spyware on the router that’s passing it along as by the website you intended to send it to. This is called a packet sniffing attack.\nBoth of these risks are defanged when you’re using https rather than plain http.\n\n\n\n\n\n\nWhat doesn’t https do?\n\n\n\nUsing SSL/TLS helps a lot with security – but it’s worth noting that it ends at the domain. Using https validates that you’ve gotten to the real example.com if that’s the website you think you’re visiting. It doesn’t in any way validate who owns example.com, whether they’re reputable, or whether you should trust them.\nThere are higher levels of SSL certification that do validate that, for example, the company that owns google.com is actually the company Google.\n\n\nHere’s the process of connecting to a website that uses https. Before your browser starts exchanging any real data with the website, the website sends over its signature, which is generated with its private certificate.\nYour computer checks this signature with a trusted certificate authority (CA) who has a copy of the site’s public certificate. Once your browser has verified that the signature is valid, you know that you’re actually communicating with \\(bankofamerica.com\\) and you’re not victim to a man-in-the-middle attack.\nOnce the verification process is done, your browser then establishes an encryption routine with the website on the other end. Only then does it start sending real data – now encrypted so that only the website on the other end can read the contents.\n\n\n\n\ngraph TD\n    A[Browser] -->|Request| B[SSL Certificate]\n    B --> |Sent| A\n    C[Certificate Authority] --> |Verifies| B\n    A --> |Consults| C"
  },
  {
    "objectID": "chapters/sec3/3-5-ssl.html#configuring-https",
    "href": "chapters/sec3/3-5-ssl.html#configuring-https",
    "title": "14  You should use SSL/HTTPS",
    "section": "14.2 Configuring https",
    "text": "14.2 Configuring https\nThere are three steps to configuring https on a website you control – getting an SSL certificate, putting the certificate on the server, and making sure the server only accepts https traffic.\nIt used to be that getting an SSL certificate was somewhat of a pain. While it could cost less than $10 per year to get a basic SSL certificate for particular subdomains, getting a wildcard certificate that covered all the subdomains of a root domain could be quite pricey.\nThe main alternative at the time was to use a self-signed cert. With a self-signed cert, you would create a certificate yourself and add the certificate directly to any machine that needed to access that site. This is a pain. You need to share the certificate with everyone and get them to install it on their machine. Certificates do expire and so this would have to be periodically repeated. The alternative is just to ignore the scary warning that you might be the victim of a man-in-the-middle attack – and some software doesn’t even allow that.\nLuckily there’s now another option. With the rise of https everywhere, there’s now an organization called letsencrypt. It’s a free CA that issues basic SSL/TLS certificates for free. They even have some nice tooling that makes it super easy to create and configure your certificate right on your server.\nWhen you’re talking about communications that run entirely inside private networks, you have a choice to make. Large organizations often run their own private CA that verifies all the certificates inside your private network.\nThis is a bit of a hassle, since you’ve got to maintain the CA and configure every host inside the network to trust your private CA. Many organizations decide they’re fine with plain http communication within private networks.\nEither way, once you’ve configured your server with https, you generally want to shut down access via plain http, usually by redirecting http traffic, which comes in on port 80 to come in over https via port 443."
  },
  {
    "objectID": "chapters/sec3/3-5-ssl.html#comprehension-questions",
    "href": "chapters/sec3/3-5-ssl.html#comprehension-questions",
    "title": "14  You should use SSL/HTTPS",
    "section": "14.3 Comprehension Questions",
    "text": "14.3 Comprehension Questions\n\nWhat are the two risks of using plain http and how does https mitigate that?\nWrite down a mental map of how SSL secures your web traffic. Include the following: public certificate, private certificate, certificate authority, encrypted traffic, port 80, port 443"
  },
  {
    "objectID": "chapters/sec3/3-5-ssl.html#lab-configure-ssl",
    "href": "chapters/sec3/3-5-ssl.html#lab-configure-ssl",
    "title": "14  You should use SSL/HTTPS",
    "section": "14.4 Lab: Configure SSL",
    "text": "14.4 Lab: Configure SSL\nWe’re going to use letsencrypt’s certbot utility to automatically generate an SSL certificate, share it with the CA, install it on the server, and even update your nginx configuration.\nFor anyone who’s never dealt with self-signing certificates in the past, let me tell you, the fact that it does this all automatically is magical!\nIn our nginx configuration, we’ll need to add a line certbot will use to know which site to generate the certificate for.\nSomewhere inside the nginx configuration’s server block, add\n\n\n/etc/nginx/nginx.conf\n\nserver_name do4ds-lab.shop www.do4ds-lab.shop;\n\nsubstituting in your domain for mine. Make sure to do both the bare domain and the www subdomain.\nAt that point, you can just install certbot and let it do its thing!\nIf you google “configure nginx with letsencrypt”, there’s a great article by someone at Nginx walking you through the process.\nAs of this writing, that was as simple as running these lines\n> sudo apt-get install certbot python3-certbot-nginx \n> sudo systemctl restart nginx \n> sudo certbot –nginx -d do4ds-lab.shop -d www.do4ds-lab.shop\nBefore you move along, I’d recommend you take a moment and inspect the /etc/nginx/nginx.conf file to look at what certbot added.\nYou’ll notice two things. You’ll notice that the listen 80 is gone from inside the server block. We’re no longer listening for http traffic.\nInstead there’s a listen 443 – the default port for https, and a bunch of stuff that tells nginx where to find the certificate on the server.\nScrolling down a little, there’s a new server block that is listening on 80. This block returns a 301, which you might recall is a redirect code (specifically for a permanent redirect) sending all http traffic to https.\nBefore we exit and test it out, let’s do one more thing. RStudio does a bunch of sending traffic back to itself. For that reason, the /rstudio proxy location also needs to know to upgrade traffic from http to https.\nSo add the following line to the nginx config:\n\n\n/etc/nginx/nginx.conf\n\nproxy_set_header X-Forwarded-Proto https;\n\nThis line adds a header to the traffic the proxy forwards specifying the protocol to be https. RStudio Server uses this header to know that it should send traffic back to itself using https, not http.\nOk, now try it by going to the URL your server is hosted at, and you’ll find that…it’s broken again.\nBefore you read along, think for just a moment. Why is it broken?\nIf your thoughts went to something involving ports and AWS security groups, you’re right!\nBy default, our server was open to SSH traffic on port 22. Since then, we may have opened or closed port 80, 8000, 8787, and/or 8555.\nBut now that we’re exclusively sending https traffic into the proxy on 443 and letting the proxy redirect things elsewhere. So you have to go into the security group settings and change it so there are only 2 rules – one that allows SSH traffic on 22 from anywhere and one that allows https traffic on 443 from anywhere.\n\n\n\n\n\n\nNote\n\n\n\nIf you decided to only configure one service on the server, you could have a much simpler setup. Neither RStudio Server nor Plumber support direct configuration of a SSL certificate, so you would still need a proxy – though it could be a much simpler configuration that just passes traffic at the root / on to your service at a different port.\nJupyterHub can be configured directly with an SSL certificate, so you would just configure JupyterHub to be aware of the SSL certificate and put itself on port 443.\n\n\nNOW you should be able to go to <your-domain>/rstudio and get to RStudio, <your-domain>/jupyter to get to JupyterHub, and <your-domain>/palmer to get the API! Voila!\nAnd you should still be able to SSH into your server on your domain.\nAt this point your server is fully configured. You have a server hosting three real data science services available on a domain of your choosing protected by https. You can really use this server to do real data science work.\nNow, that’s not to say this is fully enterprise-ready. If you’re working on a small team or at a small organization, this may be sufficient. But if you’re working at a large organization, your IT/Admin group is going to have other concerns. Section 4 of this book gets into some of those concerns and how you might discuss them with the relevant teams."
  },
  {
    "objectID": "chapters/sec3/3-6-servers.html#computers-are-just-addition-factories",
    "href": "chapters/sec3/3-6-servers.html#computers-are-just-addition-factories",
    "title": "15  Choosing the right server for you",
    "section": "15.1 Computers are (just) addition factories",
    "text": "15.1 Computers are (just) addition factories\nYou’re probably aware that everything you’ve ever seen on a computer – from this book to your work in R or Python, your favorite internet cat videos, and Minecraft – it’s just 1s and 0s.\nThe theory of why this works is deep and fascinating.1 Luckily, the amount of computational theory you need to understand to be an effective data scientist can be can be summarized in three sentences:\n\nEverything on a computer is represented by a (usually very large) number.\nAt a hardware level the only thing computers do is add these numbers together.\nModern computers add very quickly and very accurately.\n\nThis means that for your day-to-day work, you can think of a computer as just a big factory for doing additions.\nEvery bit of input your computer gets is turned into an addition problem, processed, and the results are reverted back into something we interpret as meaningful.\nThe addition assembly line itself – where the work actually gets done – is referred to as compute. It’s where 2+2 gets turned into 4, and where 345619912 + 182347910 gets turned into 527967822.\nThere are a number of different kinds of compute, but they all basically work the same. The total speed is determined by two factors – the number of conveyor belts (cores) and the speed at which each belt is running (clock speed).\nWe’re going to spend some time considering the three important resources – compute, memory, and storage – you have to allocate and manage on your computer or server. We’re going to spend some time generally exploring what each one is and then get into how you should think about each specifically with regard to data science.\n\n15.1.1 Most computation is on the CPU\nAll computers have a central processing unit (CPU). These days, most consumer-grade laptops have between 4 and 16 cores, and may have additional capabilities that effectively doubles that number. So most laptop CPUs can do between 4 and 32 simultaneous addition problems.\nClock speeds are measured in operations per second or hertz (hz). The cores in your laptop probably run between two and five gigahertz (GHz), which means between 2 and 5 billion operations per second when running at full speed.\nFor decades, many of the innovations in computing were coming from increases in clock speed, but raw increases in speed have fallen off a lot in the last few decades. The clock speeds of consumer-grade chips increased by approximately 10x during the 90s, by 2-3x in the 2000s, and somewhere between not at all and 1.5x in the 2010s.\nBut computers have continued getting a lot faster even as the increase in clock speeds has slowed. The increase has mostly come from increases in the number of cores, better software usage of parallelization, and innovative chip architectures for special-purpose usage.\n\n15.1.1.1 Recommendation 1: Fewer, faster CPU cores\nR and Python are single-threaded. Unless you’re using special libraries for parallel processing, you’ll end up red-lining a single CPU core while the other just look on in silence.\nTherefore for most R and Python work, single core clock speed matters more than the number of cores, and fewer, faster cores are usually preferable to many slower\nYou’re probably not used to thinking about this tradeoff from buying a laptop or phone. The reality is that modern CPUs are pretty darn good and you should just buy the one that fits your budget.\nIf you’re standing up a server, you often do have an explicit choice between more slower cores and fewer faster ones. In AWS, the instance type is what dictates the tradeoff – more on this below.\nIf you’re running a multi-user server, the number of cores you need is really hard to estimate. If you’re doing non-ML tasks like counts and dashboarding or relatively light-duty machine learning I might advise the following:\n\\[\n\\text{n cores} = \\text{1 core per user} + 1\n\\]\nThe spare core is for the server to do its own operations apart from the data science usage. On the other hand, if you’re doing heavy-duty machine learning or parallelizing jobs across the CPU, you may need more cores than that.\n\n\n\n15.1.2 GPU-backed computing\nThe most common special architecture chips is the graphical processing unit (GPU). GPUs are specialized chips used for tasks like editing photo or videos, rendering video game graphics, some kinds of machine learning, and (yes) Bitcoin mining.\nLike a CPU, a GPU is just an addition factory but with a different architecture. A CPU has a few fast cores, so it can only work on a few problems simultaneously, but it does them very fast. In contrast, a GPU takes the opposite approach, with many slower cores. Where a consumer-grade CPU has 4-16 cores, mid-range GPUs have 700-4,000 cores, but each one runs between 1% and 10% the speed of a CPU core.\nFor GPU-centric tasks, the overwhelming parallelism of a GPU is more important than the speed of any individual core, and GPU computation can be dramatically faster. For the purposes of data science, many popular machine learning techniques – including neural networks, XGBoost, and other tree-based models – potentially run much much faster on GPUs relative to CPUs.\nSome machines are also adding other types of specialized chips to do machine learning – though these generally aren’t accessible for training models. For example, iPhone TODO.\n\n15.1.2.1 Recommendation 2: Get a GPU…maybe…\nThe choice of whether you need a GPU to do your work will really depend on what you’re doing and your budget.\nOnly certain kinds of data science tasks are even amenable to GPU-backed acceleration. Many data science tasks can only be done in sequence others can be parallelized, but splitting it into a small number of CPU cores is perfectly adequate. For the most part, the things that will benefit most from GPU computing are training highly parallel machine learning models like a neural network or tree-based models.\nIf you do have one of these use cases, GPU computing can massively speed up your computation – making models trainable in hours instead of days.\nIf you are planning to use cloud resources for your computing, GPU-backed instances are quite pricey, and you’ll want to be careful about only putting those machines up when you’re using them.\nIt’s also worth noting that using a GPU won’t happen automatically. The tooling has gotten good enough that it’s usually pretty easy to set up, but your computer won’t train your XGBoost models on your GPU unless you tell it to do so.\nBecause GPUs are expensive, I generally wouldn’t bother with GPU-backed computing unless you’ve already tried without a GPU and find that it takes too long to be feasible.\n\n\n\n15.1.3 Memory (RAM)\nYour computer’s random access memory (RAM) is its short term storage. In the computer as adding factory analogy, the RAM is like the stock that’s sitting out on the factory floor ready to go right on an assembly line.\nRAM is very fast to for your computer to access, so you can read and write to it very quickly. The downside is that it’s temporary. When your computer turns off, the RAM gets wiped.2\n\n\n\n\n\n\nNote\n\n\n\nYou probably know this, but memory and storage is measured in bytes prefixed by metric prefixes. Common sizes for memory these days are in gigabytes (billion bytes) and terrabytes (trillion bytes). Some enterprise data stores run on the scales of thousands of terrabytes (pettabytes) or even thousands of pettabytes (yottabytes).\n\n\nModern consumer-grade laptops come with somewhere between 4 and 16 Gb of memory.\n\n15.1.3.1 Recommendation 3: Get as much RAM as feasible\nIn most cases, R and Python have to load all of your data into memory. Thus, the size of the data you can use is limited to the size of your machine’s RAM. Most other limits of your machine will just result in things being slower than you’d really want, but trying to load too much data into memory will result in a session crash, and you won’t be able to do your analysis.\n\n\n\n\n\n\nNote\n\n\n\nYou can get around the in-memory limitation by using a database or libraries that facilitate on-disk operations like Apache Arrow or dask.\n\n\nBecause you’ll often be doing some sort of transformation that results in invisible data copies and your computer can’t devote all of its memory, you’ll want to leave plenty of room over your actual data size.\nIt’s easy to say that you’ll always want more RAM, but a rough rule of thumb for whether you’ve got enough is the following:\n\\[\\text{Amount of RAM} = \\text{max amount of data} * 3\\]\nIf you’re thinking about running a multi-user server, you’ll want to think about the maximum simultaneous number of users on the server, the most data each one would want to have loaded into memory and use that for your max amount of data\n\n\n\n15.1.4 Storage (Hard Drive/Disk)\nRelative to the RAM that’s right next to the factory floor, your computer’s storage is like the warehouse in the next building over. It’s much, much slower to get things from storage than RAM, but it’s also permanent once its stored there.\nUp until a few years ago, hard drives were very slow. HDD drives have a bunch of magnetic disks that spin very fast (5,400 and 7,200 RPM are common speeds). Magnetized read/write heads move among the disks and save and read your data.\nWhile 7,200 RPM is very fast, there were still physical moving parts, and reading and writing data was very slow by computational standards.\nIn the last few years, solid-state drives (SSDs) have become more-or-less standard in laptops. SSDs, which are collections of flash memory chips with no moving parts, are up to 15x faster than HDDs. They also can take a wider variety of shapes and sizes, and are more reliable and durable because they have no moving parts. The main drawback is that they’re usually more expensive per byte, but prices are still quite reasonable.\nMany consumer laptops have only an SSD at this point. Some desktops and high-end laptops combine a smaller SSD with a larger HDD.\n\n15.1.4.1 Recommendation 4: Get a lot of storage, it’s cheap\nAs for storage – get a lot – but don’t think about it too hard, because it’s cheap. Both a 1TB SSD and a 4TB HDD are around $100. Storage is cheap enough these days that it is almost always more cost efficient to buy more storage rather than making a highly-paid professional spend their time trying to figure out how to move things around.\nOne litmus test of an IT organization that is well-equipped to support data science is whether they understand this. Smart organizations know that just getting more storage is almost always worth the cost in terms of the time of admins and data scientists.\nIf you’re running a multi-user server, the amount of storage you need depends a lot on your data and your workflows. A reasonable rule of thumb is to choose\n\\[\n\\text{Amount of Storage} = \\text{data saved} + 1Gb * \\text{n users}\n\\]\nOne thing to keep in mind is that you can basically just choose storage to match your data size and add a Gigabyte per person.\nAside from actual data, the amount of space each person needs on the server is small. Code is very small and it’s rare to see R and Python packages take up more than a few dozen Mb per data scientist.\n\n\n\n\n\n\nNote\n\n\n\nIf you’re working with a professional IT admin, they may be concerned about the storage implications of having package copies for each person on their team. I’ve heard this concern a lot from IT/Admins thinking prospectively about running their server and almost never of a case where it’s actually been a problem.\nFinding a person who has more than a few hundred Mb of packages would be very strange indeed.\n\n\nSo it really comes down to how much data you expect to save on the server. In some organizations, each data scientist will save dozens of flat files of a Gb or more for each of their projects. That team would need a lot of storage. In other teams, all the data lives in a database and you basically don’t need anything beyond that 1 Gb per person.\nIf you’re operating in the cloud, this really isn’t an important choice. As you’ll see in the lab, upgrading the amount of storage you have is a really trivial operation, requiring at most a few minutes of downtime. Choose a size you guess will be adequate and add more if you need."
  },
  {
    "objectID": "chapters/sec3/3-6-servers.html#comprehension-questions",
    "href": "chapters/sec3/3-6-servers.html#comprehension-questions",
    "title": "15  Choosing the right server for you",
    "section": "15.2 Comprehension Questions",
    "text": "15.2 Comprehension Questions\n\nThink about the scenarios below – which part of your computer would you want to upgrade to solve the problem?\n\nYou try to load a big csv file into pandas in Python. It churns for a while and then crashes.\nYou go to build a new ML model on your data. You’d like to re-train the model once a day, but it turns out training this model takes 26 hours on your laptop.\nYou design an visualization Matplotlib , and create a whole bunch in a loop, you want to parallelize the operation. Right now you’re running on a t2.small with 1 CPU.\n\nDraw a mind map of the following: CPU, RAM, Storage, Operations Per Second, Parallel Operations, GPU, Machine Learning\nWhat are the architectural differences between a CPU and a GPU? Why does this make a GPU particularly good for Machine Learning?\n15.3 AWS Instance Classes for Data Science\nAWS offers a variety of different EC2 instance types. There are a few different types you’ll probably consider, here’s a quick guide to the types most commonly used for data science purposes.\nWithin each type, there are different sizes available, ranging from nano to 2xl. Instances are denoted by <instance type>.<size>. So, for example, when we put our instance originally on a free tier machine, we put it on a t2.micro.\nIn most cases, going up a size doubles the amount of RAM, the number of cores, and the cost. That isn’t strictly true, especially at some of the more extreme sizes.\n\n\n\n\n\n\n\nInstance Type\nNotes\n\n\n\n\nt3\nThe “standard” configuration. Relatively cheap per core/Gb RAM.\ngood b/c of instance credits, limited size\n\n\nC\nFaster CPUs\n\n\nR\nHigher ratio of RAM to CPU\n\n\nP\nGPU instances, very expensive\n\n\n\n15.4 Lab: Changing Instance Size\n\nOk, now we’re going to experience the real magic of the cloud – flexibility.\nWe’re going to upgrade the size of our server in just a minute or two.\nFirst, let’s confirm what we’ve got available. You can check the number of CPUs you’ve got with lscpu in a terminal. Similarly, you can check the amount of RAM with free -h. This is just so you can prove to yourself later that the instance really changed.\nNow, you can go to the instance page in the AWS console. The first step is to stop (not terminate!) the instance. This means that changing instance type does require some downtime for the instance, but it’s quite limited.\nOnce the instance has stopped, you can change the instance type under Actions > Instance Settings. Then start the instance. It’ll take a few seconds to start the instance.\nAnd that’s it.\nSo, for example, I changed from a t2.micro to a t2.small. Both only have 1 CPU, so I won’t see any difference in lscpu, but running free -h before and after the switch reveals the difference in the total column:\ntest-user@ip-172-31-53-181:~$ free -h\n               total        used        free      shared  buff/cache   available\nMem:           966Mi       412Mi       215Mi       0.0Ki       338Mi       404Mi\nSwap:             0B          0B          0B\ntest-user@ip-172-31-53-181:~$ free -h\n               total        used        free      shared  buff/cache   available\nMem:           1.9Gi       225Mi       1.3Gi       0.0Ki       447Mi       1.6Gi\nSwap:             0B          0B          0B\nI got twice as much RAM!\nThere are some rules around being able to change from one instance type to another…but this is an amazing superpower if you’ve got variable workloads or a team that’s growing. The ability to scale a machine so easily is a game-changer.\nA lot of the things we did – adding the elastic IP and daemonizing JupyterHub are the reason that we’re able to bring it back up so smoothly.\nIt’s similarly easy to resize the EBS volume attached to your server for more storage.\nThere are two caveats worth knowing: You can only automatically adjust volume sizes up, so you’d have to manually transfer all of your data if you ever wanted to scale back down. Second, you’ll have to adjust the Linux filesystem so it knows it can take advantage of the new space available after resizing. I’d recommend AWS’s walkthroughs on how to adjust the Linux filesystem if you do resize your volume."
  },
  {
    "objectID": "chapters/sec4/4-0-sec-intro.html#what-enterprise-it-is-about",
    "href": "chapters/sec4/4-0-sec-intro.html#what-enterprise-it-is-about",
    "title": "Making it Enterprise-Grade",
    "section": "What enterprise IT is about",
    "text": "What enterprise IT is about\nAs a data scientist, your primary concern about your data science environment is that it’s useful. You want to be able to get all the data you want at your fingertips.\nMany data scientists in enterprises find that this desire runs headlong into requirements from their organization’s IT/Admin teams.\nThis can be extremely frustrating, so it’s helpful to understand the concerns enterprise IT/Admins have in mind. Broadly, IT/Admins care about the security and stability of the systems they control.\nGreat IT/Admin teams also care about the usefulness of the system to users (that’s you), but it’s usually a distant third. And there is sometimes a tension here. After all the only system that’s completely secure is the one that doesn’t exist at all.\nBut that’s not always the case. Often, there’s a lot to gain by partnering with the IT/Admin team at your organization. You may be primarily focused on getting stuff done minute-to-minute, but a data science platform that is insecure and allows bad actors to break in and steal data is not useful. And one where you can do what you want but end up crashing the workbench for 50 other users is ultimately self-defeating.\nBalancing security, stability, and usefulness is always about tradeoffs. Great IT/Admin organizations are constantly in conversations with other parts of the organization to figure out the right stance for your organization given the set of tradeoffs you face.\nUnfortunately, many IT/Admin organizations don’t act that way – they act as gatekeepers to the resources you need to do your job. That means you’ll have to figure out how to communicate with those teams, understand what matters to them, help them understand what matters to you, and reach acceptable organizational outcomes.\nYou probably already have a good understanding of how a data science environment can be useful – but what about secure and stable. What do they mean?\nSecurity is about making sure that the right people can interact with the systems they’re supposed to and that unauthorized people can’t.\nIT security professionals think about security in layers. And while you’ve done a good job setting your server up to comply with basic security best practices, there are no layers. That server front door is open to the internet. Literally anyone in the world can come to that authentication page for your RStudio Server or JupyterHub and start trying out passwords. That means you’re just one person choosing the password password away from a bad actor getting access to your server.\nLest you think you’re immune because you’re not an interesting target, there are plenty of bots out there randomly trying to break in to every existing IP address, not because they care about what’s inside, but because they want to co-opt your resources for their own purposes like crypto mining or virtual DDOS attacks on Turkish banks.1\nMoreover, security and IT professionals aren’t just concerned with bad actors from outside (called outsider threat) or even someone internal who decides to steal data or resources (insider threat). They are (or at least should be) also concerned with accidents and mistakes – data that is accidentally permanently deleted is bad the same way stolen data is bad.\nStability is ensuring enterprise-grade systems are around when people need them, and that they are stable during whatever load they face during the course of operating. The importance of stability tends to rise along with the scale of the team and the centrality of their operations to the functioning of your organization.\nIf you’re a team of three data scientists who sit in a room together, it probably won’t be a huge deal if someone accidentally knocks your data science workbench offline for 30 minutes because they tried to run a job that was too big. You’re probably all sitting in the same room and you can learn something from the experience.\nThat’s not the case when you get to enterprise-grade tooling. An enterprise-grade data science workbench probably supports dozens or hundreds of professionals across multiple teams. The server being down isn’t a sorta funny occurrence you can all fix together – it’s a problem that must be fixed immediately – or even better avoided altogether.\nIT/Admins think hard about how to provide resources in a way that avoids having servers go down because they’re hitting resource constraints.\nOne thing that is almost certain to be untrue in an enterprise context is that you’ll have root access as a user of the system.\nThere is no one-size-fits-all (or even most) position for security. Instead, great security teams are constantly articulating and making decisions about tradeoffs."
  },
  {
    "objectID": "chapters/sec4/4-1-os-ds-in-ent.html#data-science-sandboxes",
    "href": "chapters/sec4/4-1-os-ds-in-ent.html#data-science-sandboxes",
    "title": "16  Open Source Data Science in the Enterprise",
    "section": "16.1 Data Science Sandboxes",
    "text": "16.1 Data Science Sandboxes\nIn Chapter 1 on code promotion, we talked about wanting to create separate dev/test/prod environments for your data science assets. In many organizations, your good intentions to do dev/test/prod work for your data science assets are insufficient.\nIn the enterprise, it’s often the case that IT/Admins are required to ensure that your dev work can’t wreak havoc on actual production systems. If you can work with your organization’s IT/Admins to get a place to work, this is a good thing! It’s nice to have a sandbox where you can do whatever you want with no concern of breaking anything real.\nThere are three components to a data science sandbox:\n\nFree read-only access to real data\nRelatively broad access to packages\nA process to promote data science projects into production\n\nMany IT/Admins are familiar with building workbenches for other sorts of software development tasks, so they may believe they understand what you need.\n\n16.1.0.1 Read-Only Data Access\nIn software engineering, the shape of the data in the system matters a lot, but the content doesn’t matter much. If you’re building software to do inventory tracking, you’re perfectly fine testing on data that isn’t of a real inventory as long as it’s formatted the same as real inventory data. That means that a software engineering dev environment doesn’t require access to any real data.\nIT/Admins may believe that providing a standalone or isolated environment for you to work in is sufficient. You’re probably wincing reading this knowing that it’s not the case.\nAs you know, data science isn’t like that. In data science, what you’re doing in dev environments – exploring relationships between variables, visualizing, training models, looks much more different from test and prod than for software engineering. You need a place where you can work with real data, explore relationships in the data, and try out ways of modeling and visualizing the data to see what works.\nIn many cases, creating a data science sandbox with read-only access to data is a great solution. If you’re creating a tool that also writes data, you can write the data only within the dev environment.\nIf you’re able to do this, you can explore the data to your heart’s content without the IT/Admin team having to worry you’re going to mess up production data.\nDepending on your organization, they may also be worried about data exfiltration – people intentionally or accidentally removing data from the sandbox environment. In that case, you may want to run your sandbox without the ability to connect to the internet, sometimes called airgapped or offline. More on how to work in an airgapped environment is in Chapter 17.\n\n\n16.1.0.2 Package Availability\nYour organization may not have any restrictions on the packages you can install and use in your environment. If so, that’s great! You should still use environment as code tooling as we explored in Chapter 2.\nBut other organizations don’t allow free-and-open access to packages inside their environments for security or other reasons. They may have restrictive networking rules that don’t allow you to reach our to CRAN, BioConductor, PyPI, or GitHub to install packages publicly.\nThere may also be rules about having to validate packages – for security and/or correctness – before they can be used in a production context.\nThis is difficult, since the exploratory dev process often involves trying out new packages – is it best to use this modeling package or that, best to use one type of visualization or another. If you can convince your IT/Admins to give you freer access to packages in dev, that’s ideal.\nYou can work with them to figure out what the promotion process will look like. It’s easy to generate a list of the packages you’ll need to promote apps or reports into production with tools like renv or venv. Great collaborations with IT/Admin are possible if you can develop a procedure where you give them package manifests that they can compare to allow-lists and then make those packages available.\n\n\n16.1.0.3 A Promotion Process\nThe last thing you’ll need is a process to promote content out of the dev environment into test or prod. The parameters of this process will vary a lot depending on your organization.\nIn many organizations, data scientists won’t have permissions to deploy into the prod environment – only a small group of admins will have the ability to verify changes and submit them to the prod environment.\nIn this case, a code promotion strategy (see Chapter 1) co-designed with your IT/Admins is the way to go. You want to hand off a fully-baked production asset so all they need to do is put it into the right place, preferably with CI/CD tooling."
  },
  {
    "objectID": "chapters/sec4/4-1-os-ds-in-ent.html#devtestprod-for-admins",
    "href": "chapters/sec4/4-1-os-ds-in-ent.html#devtestprod-for-admins",
    "title": "16  Open Source Data Science in the Enterprise",
    "section": "16.2 Dev/Test/Prod for Admins",
    "text": "16.2 Dev/Test/Prod for Admins\nIn the chapter on environment as code tooling, we discussed how you want to create a version of your package environment in a safe place and then use code to share it with others and promote it into production.\nThe IT/Admin group at your organization probably is thinking about something similar, but at a more basic level. They want to make sure that they’re providing a great experience to you – the users of the platform.\nSo they probably want to make sure they can upgrade the servers you’re using, change to a newer operating system, or upgrade to a newer version of R or Python without interrupting your use of the servers.\nThese are the same concerns you probably have about the users of the apps and reports you’re creating. In this case, I recommend a two-dimensional grid – one for promoting the environment itself into place for IT/Admins and another for promoting the data science assets into production.\nIn order to differentiate the environments, I often call the IT/Admin development and testing area staging, and use dev/test/prod language for the data science promotion process.\nBack in section one, we learned about environments as code – using code to make sure that our data science environments are reproducible and can be re-created as needed.\nThis idea isn’t original – in fact, DevOps has it’s own set of practices and tooling around using code to manage DevOps tasks, broadly called Infrastructure As Code (IaC).\nTo get from “nothing” to a usable server state, there are (at minimum) two things you need to do – provision the infrastructure you need, and configure that infrastructure to do what you want.\nFor example, let’s say I’m standing up a server to deploy a simple shiny app. In order to get that server up, I’ll need to stand up an actual server, including configuring the security settings and networking that will allow the proper people to access the server. Then I’ll need to install a version of R on the server, the Shiny package, and a piece of hosting software like Shiny Server.\nSo, for example, you might use AWS’s CloudFormation to stand up a virtual private cloud (VPC), put an EC2 server instance inside that VPC, attach an appropriately-sized storage unit, and attach the correct networking rules. Then you might use Chef to install the correct software on the server and get your Shiny app up-and-running.\nIn infrastructure as code tooling, there generally isn’t a clear dividing line between tools that do provisioning and tools that do configuration…but most tools lean one way or the other.\nBasically any tool does provisioning will directly integrate into the APIs of the major cloud providers to make it easy to provision cloud servers. Each of the cloud providers also has their own IaC tool, but many people prefer to use other tools when given the option (to be delicate).\nIt’s worth noting that Docker by itself is not an IaC tool. A Dockerfile is a reliable way to re-create the container, but that doesn’t get you all the way to a deployed container that’s reliable. You’ll need to combine a container with a deployment framework like Docker Compose or Kubernetes, as well as a way to stand up the servers that underlie that cluster.\nIt’s also worth noting that IaC may or may not be deployed using CI/CD. It’s generally a good practice to do so, but you can have IaC tooling that’s deployed manually.\nBasically none of these tools will save you from your own bad habits, but they can give you alternatives."
  },
  {
    "objectID": "chapters/sec4/4-1-os-ds-in-ent.html#package-management-in-the-enterprise",
    "href": "chapters/sec4/4-1-os-ds-in-ent.html#package-management-in-the-enterprise",
    "title": "16  Open Source Data Science in the Enterprise",
    "section": "16.3 Package Management in the Enterprise",
    "text": "16.3 Package Management in the Enterprise\nIn the chapter on environments as code, we went in depth on how to manage a per-project package environment that moves around with that project. This is a best practice and you should do it.\nBut in some Enterprise environments, there may be further requirements around how packages get into the environment, which packages are allowed, and how to validate those packages.\n\n16.3.1 Open Source in Enterprise\nOpen Souce software is the water code-first data science swims in. R and Python are open source. Any package you’ll access on CRAN, BioConductor, or PyPI is open source.\nEven if you don’t know the details, you’re probably already a big believer in the power of Free and Open Source Software (FOSS) to help people get stuff done.\nIn the enterprise, it is sometimes the case that people are more acquainted with the risks of open source than the benefits.\nFOSS is defined by a legal license. When you put something out on the internet, you can provide no licensing at all. That means that people can do whatever they want with it, but they also have no reassurances you might not come to sue them later.\nI am not a lawyer and this is not legal advice, but hopefully this is helpful context on the legal issues around FOSS software.\nPutting a license on a piece of software makes clear what is and isn’t allowed with that piece of software.\nThe type of license you’re probably most familiar with is a copyright. A copyright gives the owner exclusivity to do certain things with whatever falls under the copyright. FOSS licenses are the opposite – they guarantee that you’re freely allowed to do what you want with the software, though there may be other obligations as well.\nThe point of FOSS is to allow people to build on each other’s work. So the free in FOSS is about freedom, not about zero cost. As a common saying goes – it means free as in free speech, not free as in free beer.\nThere are many different flavors of open-source licenses, and all of them I’m aware of (even the anti-capitalist one) allows you to charge for access. The question is what you’re allowed to do after you acquire the software.\nBasically all open source licenses guarantee four freedoms. These are the freedom to view and inspect the source code, to run the software, to modify the software, and to redistribute the software as you see fit.\nWithin those parameters, there are many different kinds of open source licenses. The Open Source Initiative lists dozens of different licenses with slightly different parameters, but they fall into two main categories – permissive and copyleft.\nPermissive licenses allow you to do basically whatever you want with the software.\nFor example, the common MIT license allows you to, “use, copy, modify, merge, publish, distribute, sublicense, and/or sell” MIT-licensed software without attribution. The most important implication of this is that you can take MIT-licenses software, incorporate it into some other piece of code, and keep that code completely closed.\nCopyleft or viral licenses require that any derivative works are released under the same license. The idea is that open source software should beget more open source software and not silently be used by big companies to make megabucks.\nThe biggest concern with copyleft licenses is that they might propagate into the private work you are doing inside your company. This concern is especially keen at organizations that themselves sell proprietary software. For example, what if a court were to rule that Apple or Google had to suddenly open source all their software?\nThis is a subtle concern and largely revolves around what it means to include another piece of software. Many organizations deem that inclusion only happens if you were to literally copy/paste open source code into your code. In this view, the things created with open source software are not themselves open source.\nThe reality is that there have been basically no legal cases on this topic and nobody knows how it would shake out if it did get to court, so some organizations err on the side of caution.\nSo one of the concerns around open source software is the mixure of licenses involved. R is released under a copyleft GPL license, Python under a permissive Python Software Foundation (PSF) license, RStudio under a copyleft AGPL, and Jupyter Notebook under a permissive modified BSD. And every single package author can choose a license for themselves.\nSay I create an app or plot in R and then share that plot with the public – is that app or plot bound by the license as well? Do I now have to release my source code to the public? Many would argue no – it uses R, but doesn’t derive from R in any way. Others have concerns that stricter interpretations of copyleft might hold up in court if it were to come to that.\nThere are disagreements on this among lawyers, and you should be sure to talk to a lawyer at your organization if you have concerns.\n\n\n16.3.2 Dealing with Package Restrictions\nChapter 2 on environments as code dealt with how to create a reproducible version of your package library for any given piece of content. But in some enterprises, you won’t be able to freely create a library. Instead, your organization may have restrictions on package installation and will need to gate the packages that are allowed into your environment.\nIn order to enact these restrictions, IT/Admins have to do two things - make sure that public repositories are not available to users of their data science platforms, and use one of these repository tools to manage the set of packages that are available inside their environment. It often takes a bit of convincing, but a good division of labor here is generally that the IT/Admins manage the repository server and what’s allowed into the environment and the individual teams manage their own project libraries.\n\n\n\n\n\n\nAmount of Space for Packages\n\n\n\nWhen admins hear about a package cache per-project, they start getting worried about storage space. I have heard this concern many times from admins who haven’t yet adopted this strategy, and almost never heard an admin say they were running out of storage space because of package storage.\nThe reality is that most R and Python packages are very small, so storing many of them is reasonably trivial.\nAlso, these package storage tools are pretty smart. They have a shared package cache across projects, so each package only gets installed once, but can be used in a variety of projects.\nIt is true that each user then has their own version of the package. Again, because packages are small, this tends to be a minor issue. It is possible to make the package cache one that is shared across users, but the (small) risk this introduces of one user affecting other users on the server is probably not worth the very small cost of provisioning enough storage that this just isn’t an issue.\n\n\nMany enterprises run some sort of package repository software. Common package repositories used for R and Python include Jfrog Artifactory, Sonatype Nexus, Anaconda Business, and Posit Package Manager.\nArtifactory and Nexus are generalized library and package management solutions for all sorts of software, while Anaconda and Posit Package Manager are more narrowly tailored for data science use cases.\nThere are two main concerns that come up in the context of managing packages for the enterprise. The first is how to manage package security vulnerabilities.\nIn this context, the question of how to do security scanning comes up. What exactly security professionals mean by scanning varies widely, and what’s possible differs a good bit from language to language.\nIt is possible to imagine a security scanner that actually reads in all of the code in a package and identifies potential security risks – like usage of insecure libraries, calls to external web services, or places where it accesses a database. The existence of tools at this level of sophistication exist roughly in proportion to how popular the language is and how much vulnerability there is.\nSo javascript, which is both extremely popular and also makes up most public websites, has reasonably well-developed software scanning. Python, which is very popular, but is only rarely on the front end of websites has fewer scanners, and R, which is far less popular has even fewer. I am unaware of any actual code scanners for R code.\nOne thing that can be done is to compare a packaged bit of software with known software vulnerabilities.\nNew vulnerabilities in software are constantly being identified. When these vulnerabilities are made known to the public, the CVE organization attempts to catalog them all. One basic form of security checking is looking for the use of libraries with known CVE records inside of packages.\nThe second thing your organization may care about is the licenses software is released under. They may want to disallow certain licenses – especially aggressive copyleft licenses – from being present in their codebases."
  },
  {
    "objectID": "chapters/sec4/4-1-os-ds-in-ent.html#comprehension-questions",
    "href": "chapters/sec4/4-1-os-ds-in-ent.html#comprehension-questions",
    "title": "16  Open Source Data Science in the Enterprise",
    "section": "16.4 Comprehension Questions",
    "text": "16.4 Comprehension Questions\n\nWhat is the purpose of creating a data science sandbox? Who benefits most from the creation of a sandbox?\nWhy is using infrastructure as code an important prerequisite for doing Dev/Test/Prod?\nWhat is the difference between permissive and copyleft open source licenses? Why are some organizations concerned about using code that includes copyleft licenses?\nWhat are the key issues to solve for open source package management in the enterprise?"
  },
  {
    "objectID": "chapters/sec4/4-2-ent-networks.html#enterprise-networking-terminology",
    "href": "chapters/sec4/4-2-ent-networks.html#enterprise-networking-terminology",
    "title": "17  Enterprise Networking",
    "section": "17.1 Enterprise Networking Terminology",
    "text": "17.1 Enterprise Networking Terminology\nHopefully the analogy of a basic server as an apartment building and an enterprise server as a castle keep behind a drawbridge makes basic sense. But let’s get into the way you’ll actually talk about this with the IT/Admins at your organization – I promise they won’t talk about castles and apartments.\nWhen you stand up a server in the cloud, it’s inside a private network. In AWS, the private network that houses the servers is called a virtual private cloud (VPC), which you probably saw somewhere in the AWS console.\nFor our workbench server, we took that private network and made it public so every server (there was just one) inside our private network also has a public IP address so it was accessible from the internet.\nIn an enterprise configuration you won’t do that. Instead, you’ll take your private network and divide it into subnets – most often two of them.\nNow you’ll take the subnets and put all the stuff you actually care about in a private subnet. Private networks generally host all of the servers that actually do things. Your data science workbench server, your databases, server for hosting shiny apps – all these things should live inside the private network. Nothing in the private subnet will be directly accessible from the public internet.\n\n\n\n\n\n\nDefining private networks and subnets\n\n\n\nPrivate networks and subnets are defined by something called a Classless Inter-Domain Routing (CIDR) block. A CIDR block is basically an IP address range, so a private network is a CIDR block and each subnet is CIDR blocks within the private network’s block.\nEach CIDR block is defined by a starting address and the size of the network. For example, the address 10.33.0.0 and the /26 CIDR defines the block of 64 addresses from 10.33.0.0 to 10.33.0.63.\nLarger CIDR numbers indicate a smaller block, so you could take the 10.33.0.0/26 CIDR and split it into the 10.33.0.0/27 block that includes 10.33.0.0 to 10.33.0.31 and 10.33.0.32/27 for 10.33.0.32 through 10.33.0.63.\nAs you’ve probably guessed, the number of IPs in each CIDR have to do with powers of two. But the rules are hard to remember and there are online calculators if you ever have to figure a block out for yourself.\n\n\nThe only things you’ll put in the public subnet – often called a demilitarized zone (DMZ) – are servers that exist solely for the purpose of relaying traffic back and forth to the servers in your private network. These servers are called proxy servers or proxies – more on them in a moment.\nThis means that the traffic actually coming to your workbench comes only from other servers you control. It’s easy to see why this is more secure.\n[TODO: image of private networks + proxies]\nIn most cases, you’ll have minimum two servers in the DMZ. You’ll usually have one or more proxies to handle the incoming HTTPS traffic that comes in from the outside world. You’ll also usually have a proxy that is just for passing SSH traffic along to hosts in the private network, often called a bastion host or jump box.\nThe other benefit of using a private network for the things you actually care about is that you can manage the IP addresses and hostnames of those servers without having to worry about getting public addresses. If you want to name one of the servers in your private subnet google.com, you can do that (I wouldn’t recommend it), because the only time that name will be used is when traffic is coming past your proxy servers and into the private network.\nThere’s a device sitting on the boundary of all networks that provide translations between private IP addresses and public ones. For your private subnet, you’ll only have an outbound one available. In AWS, it’s called a Network Address Translation (NAT) Gateway. For your private network as a whole, there’ll be another gateway that provides both inbound and outbout support, it’s called an Internet Gateway by AWS.\n\n17.1.1 What proxies do\nAs a data scientist, this may be the first time you’re encountering the term proxy, but for IT/Admins – especially ones who specialize in networking – configuring proxies is an everyday activity.\nProxies can be either in software or hardware. For example, in our workbench server, we installed the nginx software proxy on the same server as our workbench to allow people to go to any of the different services we installed on that server. In enterprise use cases, proxies are most often on standalone pieces of hardware. They may run nginx or Apache – the other popular open source option. Popular paid enterprise options include F5, Citrix, Fortinet, and Cloudflare.\nProxies can deal with traffic coming into the private network, called an inbound proxy or they can deal with traffic going out from the private network, called an outbound proxy.\n\n\n\n\n\n\nNote\n\n\n\nInbound and outbound are not industry standard terms for proxies. The terms you’ll hear from IT/Admins are forward and reverse. Proxies are discussed from the perspective of being inside the network, so forward equals outbound and reverse equals inbound.\nI find it nearly impossible to remember which is which and IT/Admins will absolutely know what you mean with the terms inbound and outbound, so I recommend you use them instead.\n\n\nProxies are usually used for redirection, port management and firewalling.\nRedirection is when the proxy accepts traffic at the public DNS record and passes (proxies) it along to the actual server. One great thing about this configuration is that only the proxy needs to know the real hostname for your server. For example, you could configure the proxy so example.com/rstudio routes to the RStudio Server that’s at my-rstudio-1 inside the private network. If you want to change it to my-rstudio-2 later on, you just change the proxy routing, which is much easier than changing the public DNS record.\nOne advantage of doing redirection is making it easy to manage ports. For example, RStudio Server runs on port 8787 by default. Generally, you don’t want people to have to remember to go to a random port to access RStudio Server so it’s standard practices to keep standard ports (80 for HTTP, 443 for HTTPS, and 22 for SSH) open on the proxy and have the proxy just redirect the traffic coming into it on 443 to 8787 on the server with RStudio Server.\n\n\n\n\n\n\nNote\n\n\n\nFor our workbench server, we did path rewriting and port management in our nginx proxy.\nIf you recall, by the time we were done, our nginx config was set to only allow HTTPS traffic on 443, redirect all HTTP traffic on 80 to HTTPS on 443, and to take traffic at /rstudio to port 8787 on the same server, /jupyter to port 8000, and /palmer to 8555.\n\n\n[TODO: image of path rewriting + load-balancing]\nProxies are additionally sometimes configured to block traffic that isn’t explicitly allowed. In a data science environment, this means that you’ll have to configure the inbound proxy with all the locations you need. If you’ve got an outbound proxy that blocks traffic, you’re in an airgapped/offline situation, and you should see ?sec-offline on that.\nThere are a few other things a proxy may be used for. These use cases are less common in a data science environment.\nSometimes proxies terminate SSL. Because the proxy is the last server that is accessible from the public network, many organizations don’t bother to implement SSL/HTTPS inside the private network so they don’t have to worry about managing SSL certificates inside their private network. This is getting rarer as tooling for managing SSL certificates gets better, but it’s common enough that you might start seeing HTTP addresses if you’re doing server-to-server things inside the private network.\nOccasionally proxies also do authentication. In most cases, proxies pass along any traffic that comes in to where it’s supposed to go. If there’s authentication, it’s often at the server itself.\nSometimes the proxy is actually where authentication happens, so you have to provide the credentials at the edge of the network. Once those credentials have been supplied, the proxy will let you through. Depending on the configuration, the proxy may also add some sort of token or header to your incoming traffic to let the servers inside know that your authentication is good and to pass along identification for authorization purposes.\nTODO: image of auth at proxy\nLastly, there’s a special kind of reverse proxy called a load-balancer. A load-balancer is used to scale a service across a pool of servers on the back end. We’ll get more into how this works in ?sec-scaling."
  },
  {
    "objectID": "chapters/sec4/4-2-ent-networks.html#what-data-science-needs-from-the-network",
    "href": "chapters/sec4/4-2-ent-networks.html#what-data-science-needs-from-the-network",
    "title": "17  Enterprise Networking",
    "section": "17.2 What data science needs from the network",
    "text": "17.2 What data science needs from the network\nAs you’ve probably grasped, enterprise networking can be complex. And your IT/Admin group knows a lot about it. What they don’t know a lot about is the interaction of networking and data science, so it’s helpful for you to be able to clearly state what you need.\n\n\n\n\n\n\nWhat ports do I need?\n\n\n\nOne of the first questions IT/Admins ask is what ports need to be open. Depending on what ports you choose for the services you’re running those ports need to be open.\nThe good news is that almost all traffic for data science purposes is standard HTTP(S) traffic, so it can happily run over 80 or 443 if there are limitations on what ports can be open.\n\n\nOne of the most common issues with data science environments in an enterprise is proxy behavior. If you’re experiencing weird behavior in your data science environment – files failing to upload or download, sessions getting cutoff strangely, or data not transferring right – asking your IT/Admin about whether there are proxies and their behavior should be suspect number one.\nWhen you’re talking to your IT/Admin about the proxies, it’s really helpful to have a good mental model of what traffic might be hitting an inbound proxy and what traffic might be hitting an outbound one.\nAs we went over in Chapter 12, network traffic always operates on a call and response model. So whether your traffic is inbound or outbound is dependent on who makes the call. Inbound means that the call is coming from a computer outside the private network directed to a server inside the private network, and outbound is the opposite.\nTODO: image inbound vs outbound connection\nSo basically, anything that originates on your laptop – including the actual session into the server is an inbound connection, while anything that originates on the server – including everything in code that runs on the server is an outbound connection.\n\n17.2.1 Issues with inbound proxies\nInbound proxies affect the connection you’re making from your personal computer to the server. There are two ways this might affect your experience doing data science on a server.\nIt’s reasonably common for organizations to have settings that limit file sizes for uploads and downloads or implementing timeouts on file uploads, downloads, and sessions. In data science contexts, files tend to be big and session lengths long.\nIf you’re trying to work in a data science context and weird things are happening with file uploads or downloads or sessions ending unexpectedly, checking on inbound proxy settings is a good first step.\nSome data science app frameworks (including Shiny and Streamlit) use a technology called Websockets for maintaining the connection between the user and the app session. Most modern proxies (including those you’ll get from a cloud provider) support Websockets, but some older on-prem proxies don’t and you may have to figure out a workaround if you can’t get Websockets enabled on your proxy.\n\n\n17.2.2 Issues with forward/outbound proxies\nAlmost all enterprise networks have inbound proxies. Outbound ones are somewhat rarer. That’s because outbound proxies limit connections made from inside the network to the outside. It’s obvious why you’d need to protect your data science environment from the entire outside world.\nMany organizations don’t feel the need to limit what external resources people can interact with from inside their firewall, but limitations on outbound access have long been common in highly regulated industries with strong requirements around data security and governance and are becoming increasingly common in many different industries. Many organizations have these proxies to reduce the risk of someone getting in and then being able to exfiltrate valuable resource.\nOrganizations who limit outbound access from their data science environment usually refer to the environment as offline or airgapped. The term airgapped indicates that there is a physical gap – air – between the internet and the environment. It is very rare for this to be the case. In most cases, airgapping is accomplished by putting in an outbound proxy that disallows (nearly) all connections.\nThe good news is that once you’re working on your data science server, you don’t need to go out much. The bad news is that you will have to go out sometimes. It’s important you work with your IT/Admin to develop a plan for how to handle when outbound connectivity is needed.\nHere are the four most common reasons you’ll need to make outbound connections from inside your data science environment.\n\nDownloading Packages Downloading a package from a public repository requires a network connection to that repository. So you’ll need outbound access when you want to install R or Python packages from CRAN, BioConductor, public Posit Package Manager, Conda, PyPI, or GitHub.\nAccessing External Data In most data science work, you’re mostly just working on data from databases or files inside your private network, so you don’t really need access to data or resources outside. On the other hand, if you’re consuming data from public APIs or scraping data from the web, that may require external connections. You also may need an external connection if you’re accessing private data that lives in an external location – for example you might have data in an AWS S3 bucket you need to access from an on-prem workbench or data in Google Sheets that you need to access from AWS.\nSystem Libraries In addition to the R and Python packages, there are also system libraries you’ll need installed, like the versions of R and Python themselves, and other packages used by the system. Generally it’ll be the IT/Admin managing and installing these, so they probably have a strategy for doing it. This comes up in the context of data science if you’re using R or Python packages that are basically just wrappers around system libraries, like the R and Python packages that use the GDAL system library for geospatial work.\nSoftware Licensing If you’re using all open source software, this probably won’t be an issue. But if you’re buying licenses to a professional product, you’ll have to figure out how to activate the software licensing. This usually involves reaching out to servers owned by the software vendor. They should have a method for activating servers that can’t reach the internet, but your IT/Admins will appreciate if you’ve done your homework on this before asking them to activate some new software.\n\nWhat if your organization doesn’t default to allowing all of these things to be available? In some cases, ameliorating these issues is as easy as talking to your IT/Admin and asking them to open the outbound proxy to the right server.\nBefore you go ahead treating your environment as truly offline/airgapped, it’s almost always worth asking if narrow exceptions can be made to a network that is offline/airgapped. The answer may surprise you. Especially if it’s just a URL or two that are protected by HTTPS – for example CRAN, PyPI, or public RStudio Package Manager, it’s generally pretty safe and many organizations are happy to allow-list a limited number of outbound addresses.\nIf not, you’ll have to have a deeper conversation with the IT/Admin.\nYour organization probably has standard practices around managing system libraries and software licenses in their environment.\nExternal data connections and package management are the areas where you’ll have to have a conversation to make them accessible.\nIT/Admins often do not understand how crucial R and Python packages are to doing data science work. It will be on you to make them understand that your offline environment is useless if you can’t come up with a plan to manage packages together.\nThe best plans for offline package operations involve the IT/Admin curating a repository of allowed packages inside the private network using a professional tool like Posit Package Manager, Jfrog Artifactory, or Sonatype Nexus and then giving data scientists free reign to install those packages as needed inside the environment.\nThis can take a lot of convincing. Good luck."
  },
  {
    "objectID": "chapters/sec4/4-2-ent-networks.html#comprehension-questions",
    "href": "chapters/sec4/4-2-ent-networks.html#comprehension-questions",
    "title": "17  Enterprise Networking",
    "section": "17.3 Comprehension Questions",
    "text": "17.3 Comprehension Questions\n\nWhat is the advantage of adopting a more complex networking setup than a server just deployed directly on the internet? Are there advantages other than security?\nDraw a mental map with the following entities: inbound traffic, outbound traffic, proxy, DMZ, private subnet, public subnet, VPC\nOur workbench server has an nginx proxy that redirects inbound traffic on a few different paths to the right port on the same server. Looking at your nginx.conf, what would have to change if you moved each of those services to different servers? Is there anything you’d have to check on the server itself?\nLet’s say you’ve got a private VPC that hosts an instance of RStudio Server, an instance of JupyterHub, and a Shiny Server that has an app deployed. Here are a few examples of traffic – are they outbound, inbound, or within the network?\n\nSomeone connecting to and starting a session on RStudio Server.\nSomeone SFTP-ing an app and packages from RStudio Server to Shiny Server.\nSomeone installing a package to the Shiny Server.\nSomeone uploading a file to JupyterHub.\nA call in a Shiny app using httr2 or requests to a public API that hosts data.\nAccessing a private corporate database from a Shiny for Python app using sqlalchemy.\n\nWhat are the most likely pain points for running a data science workbench that is fully offline/airgapped?"
  },
  {
    "objectID": "chapters/sec4/4-3-auth.html#the-many-flavors-of-auth-or-what-does-sso-mean",
    "href": "chapters/sec4/4-3-auth.html#the-many-flavors-of-auth-or-what-does-sso-mean",
    "title": "18  Logging in with auth",
    "section": "18.1 The many flavors of auth (or what does SSO mean?)",
    "text": "18.1 The many flavors of auth (or what does SSO mean?)\nSingle Sign On (SSO) is a slippery term, so it is almost always necessary to clarify what is meant by the term when you hear it. At some organizations, identity management isn’t centralized at all. This means that usernames and passwords are unique to each service, onboarding and offboarding of users has to be handled independently for each service, and users have to login frequently. In short, it’s often not a great system. This is never referred to as SSO.\n\nMost organizations of a meaningful size have centralized identity management. This means that identities, credentials, authorization, onboarding, and offboarding are handled centrally. However, you may still need to independently login to each system. For example, in this system, every service might take the same username and password as your credentials, but if you go to RStudio Server followed by JupyterHub, you’ll need to provide that username and password independently to each service. This system is often facilitated by PAM, and LDAP/AD. Some organizations call this SSO, because there’s only one set of credentials.\nIn true SSO, users login once and are given a token or ticket.1 Then, when they go to the next service, they don’t have to login again because that service can just look at the token or ticket to do auth for that user. For example, in this system, I could go to RStudio Server and login, and then go to JupyterHub and get in without being prompted again for my password. This type of auth is facilitated by Kerberos, SAML, or OAuth."
  },
  {
    "objectID": "chapters/sec4/4-3-auth.html#auth-techniques",
    "href": "chapters/sec4/4-3-auth.html#auth-techniques",
    "title": "18  Logging in with auth",
    "section": "18.2 Auth Techniques",
    "text": "18.2 Auth Techniques\nIf you have five data scientists in your group, and the only shared resource you have is an RStudio Server instance, you probably don’t need to think terribly hard about auth. It’s pretty straightforward to just make users on a server and give them access to everything.\nBut as organizations get larger with hundreds or thousands of users, there’s constant churn of people joining and leaving. The number of services can creep into the dozens or hundreds and people may have very different authorization levels to different services. Trying to manage auth on the individual services is a nightmare – as is trying to keep that many usernames and passwords straight for users. That is why almost all organizations with more than a few users have centrally managed auth.\n\n18.2.1 You get a permission and you get a permission!\nFor the most part, we think of people being authenticated and authorized into services. However, it’s sometimes useful to consider the broader class of entities that could do auth. There are two common non-human entities that are included in auth systems that are worth considering.\nService Accounts are accounts given to non-human entities when you want it to be able to do something on its own behalf. For example, maybe you’ve got a Shiny app that users use to visualize data that’s in a database. Very often, you don’t want the app to have the same permisions as the app’s author, or to inherit the permissions of the people viewing the app. Instead, you want the app to be able to have permissions to do certain database operations. In that case, you would create a service account to give to the Shiny app that has exactly those permissions.\nThere are also times where it’s useful to go one level up and give permissions to an entire instance or service. In that case, you might assign permissions to an instance. For example, you could make it the case that anyone who is logged into the JupyterHub server is allowed to read from the database.\nInstance permissions are rather broad, and so they are usually only applied when you’ve got multiple resources inside a private network. In that case, authentication and authorization are only done at a single point and authorization is pretty broad.\n\n\n18.2.2 Authorization is kinda hard\nFrom a management perspective, authentication is pretty simple. A person is given a set of credentials, and they have to supply those credentials when prompted to prove they are who they say they are.\nAuthorization is a whole other can of worms. There is a meaningful literature on varieties of authorization and how they work. We’re not going to get too deep into the weeds, other than to define some common terms and how they’re used.\nThe atomic basis for authorization is a permission. Permissions are a binary switch that answers the question is this person allowed to do the thing they are trying to do?2\nThe simplest way of assigning permissions is called an access control list (ACL). In systems that use ACLs, each piece of content has a list of users who are allowed access. Sometimes, ACLs are also assigned to groups, which are simply sets of users – think data-scientists.\n\nOne ACL implementation with which you may be familiar is file permissions on a Linux server. For example, if you have a Mac or are on a Linux server, you can open your terminal, navigate to a directory and do the following:\n$ ls -l\n-rwxr-xr-x   1 alexkgold  staff   2274 May 10 12:09 README.md\nThat first set of characters describes the ACL for the README.md file. The first character - indicates this is a file, as opposed to a directory of files (which would be d). Then there are three sets of 3 characters, rwx, which are short for read, write, and execute, with the first group for the owner, alexkgold, the second group for anyone else in the group staff, and the third set for anyone else.\nSo you can read -rwxr-xr-x as, this is a file that alexkgold can read, write or execute, and anyone else can read or execute, but not edit.\nACLs are pretty intuitive, but it turns out that when you are managing a lot of users across a lot of files, directories, and services, they can get pretty difficult to manage, so many organizations use Role Based Access Control (RBAC).\nRBAC adds a layer of abstraction between users and permissions, which makes it a little harder to understand, but ultimately results in a much more flexible system. In RBAC, permissions are not assigned to individual pieces of content or to users or groups. Instead, permissions are assigned to roles, and roles are given to users or groups.3\n\nThere are also further iterations on the RBAC model, like Attribute Based Access Control (ABAC) or Policy Based Access Control (PBAC) in which there’s a long list of attributes that could be considered for a user to compute their permissions for a given service."
  },
  {
    "objectID": "chapters/sec4/4-3-auth.html#auth-technologies",
    "href": "chapters/sec4/4-3-auth.html#auth-technologies",
    "title": "18  Logging in with auth",
    "section": "18.3 Auth Technologies",
    "text": "18.3 Auth Technologies\n\n18.3.1 Username + Password\nMany pieces of software come with integrated authentication. When you use those system, the product stores encrypted username and password pairs in a database.\nThese setups are often really easy from an admin perspective – you just set up individual users on the server. However, the flip side is that users have one more username and password to remember, which is annoying for them. Moreover, if you have more than a few users, or the system is one of more than a few, it’s hard to manage users on a lot of different systems. It can be a real pain to create accounts on a ton of different systems when a new person joins the organization, or to remove their permissions one-by-one when they leave.\nFor this reason, most IT/Admin organizations strongly prefer using some sort of centralized identity store.\n\n\n18.3.2 PAM\nPluggable Authentication Modules (PAM) is a Linux system for doing authentication. As of this writing, PAM is the default authentication method for both RStudio Server and JupyterHub.\n\nConceptually PAM is pretty straightforward. You install a service on a Linux machine and configure it to use PAM authentication from the underlying host. By default, PAM just authenticates against the users configured on the Linux server, but it can also be configured to use other sorts of “modules” to authenticate against other systems – most commonly LDAP/AD or Kerberos. PAM can also be used to do things when users login – the most common being initializing tokens or tickets to other systems, like a database.\nPAM is often paired with System Security Services Daemon (SSSD), which is most commonly used to automatically create Linux users on a server based on the identities stored in an LDAP/AD instance.\nThough conceptually simple, reading, writing, and managing PAM modules is kinda painful.\n#TODO: Add PAM example\n\n\n18.3.3 LDAP/AD\nLightweight Directory Access Protocol (LDAP) is a relatively old, open, protocol used for maintaining a set of entities and their attributes. To be precise, LDAP is actually a protocol for maintaining and accessing entities and their attributes in a tree. It happens that this is a really good structure for maintaining permissions and roles of users at an organization, and it’s the main thing LDAP is used for.\nActive Directory (AD) is Microsoft’s implementation of LDAP, and is by-far the most common LDAP “flavor” out there. AD so thoroughly owns the LDAP enterprise market, that LDAP is often referred to as LDAP/AD. There are other implementations you may run across, the most common being OpenLDAP.\n\nAzure, Microsoft’s cloud provider, offers an authentication service called Azure Active Directory (AAD). Confusingly, AAD is usually used in combination with SAML, not LDAP.\n\nIt’s worth distinguishing the use of LDAP as an identity store from its use as an authentication technology. As a tree-based database, LDAP is uniquely well-suited to storing the identities, and other attributes of people at the organization. However, as discussed below, using LDAP to authenticate into actual services has security and convenience drawbacks, and many organizations consider it outdated and insecure.\nA lot of organizations are moving away from LDAP for authentication in favor of token-based technologies like SAML or OAuth, but many are keeping LDAP as their identity “source of truth” that is referenced by the SAML or OAuth Identity Provider.\nLDAP has three main disadvantages relative to other technologies. First, LDAP requires that your credentials (username and password, usually) actually be provided to the service you’re trying to use. This is fundamentally insecure relative to a system where your credentials are provided only to the identity provider, and the service just gets a token verifying who you are. In token-based systems, adding additional requirements like MFA or biometrics are easy, as they’re simply added at the IdP layer. In contrast, doing those things in LDAP would require the service to implement them, which usually is not the case, so you’re usually limited to username and password.\nThe second disadvantage of LDAP is that it does not allow for central administration of permissions. LDAP directly records only objects and their attributes. Say, for example, you want only users of a particular group to have access to a certain resource. In LDAP, you would have to specify in that resource that it should only allow in users of that group. This is in contrast to SAML/OAuth, where the authorization is centrally managed.\nLastly, LDAP authentication is based on each service authenticating. Once you authenticate, the service might give you a cookie so that your login persists, but there is no general-purpose token that will allow you to login to multiple services.\n\n18.3.3.1 How LDAP Works\nWhile the technical downsides of LDAP are real, the technical operations of LDAP are pretty straightforward. In short, you try to login to a service, the service collects your username and password, sends it off to the LDAP server, and checks that your username and password are valid.\n\nNote that LDAP is purely for authentication. When you’re using LDAP, authorization has to be handled separately, which is one of the disadvantages.\n\n\n18.3.3.2 Deeper Than You Need on LDAP\nLDAP is a tree-based entity and value store. This means that LDAP stores things and their attributes, which include a name and one or more values. For example, my entry in a corporate LDAP directory might look like this:\ncn: Alex Gold\nmail: alex.gold@example.com\nmail: alex.gold@example.org\ndepartment: solutions\nmobile: 555-555-5555\nobjectClass = Person\nMost of these attributes should be pretty straightforward. cn is short for common name, and is part of the way you look up an entity in LDAP (more on that below). Each entity in LDAP must have an objectClass, which determines the type of entity it is. In this case, I am a Person , as opposed to a device, domain, organizationalRole, or room – all of which are standard objectClasses.\nLet’s say that your corporate LDAP looks like the tree below:\n\n#TODO: make solutions an OU in final\nThe most common way to look up LDAP entities is with their distinguished name (DN), which is the path of names from the point you’re starting all the way back to the root of the tree. In the tree above, my DN would be cn=alex,ou=solutions,dc=example,dc=com.\nNote that you read the DN from right to left to work your way down the tree. Aside from cn for common name, other common fields include ou for organizational unit, and dc for domain component.\n\n\n18.3.3.3 Trying out LDAP\nNow that we understand in theory how LDAP works, let’s try out an actual example.\nTo start, let’s stand up LDAP in a docker container:\n#TODO: update ldif\ndocker network create ldap-net\ndocker run -p 6389:389 \\\n  --name ldap-service \\\n  --network ldap-net \\\n  --detach alexkgold/auth\nldapsearch is a utility that lets us run queries against the LDAP tree. Let’s try it out against the LDAP container we just stood up.\nLet’s say I want to return everything in the subtree under example.org. In that case, I would run ldapsearch -b dc=example,dc=org, where b indicates my search base, which is a dn. But in order to make this actually work, we’ll need to include a few more arguments, including\n\nthe host where the LDAP server is, indicated by -H\nthe bind DN we’ll be using, flagged with -D\nthe bind password we’ll be using, indicated by -w\n\nSince we’re testing, we’re also going to provide the flag -x to use whatever certificate is present on the server. Putting it altogether, along with the commands to reach the docker container, let’s try:\nldapsearch -x -H ldap://localhost:6389 -b dc=example,dc=org -D \"cn=admin,dc=example,dc=org\" -w admin\n\n# extended LDIF\n#\n# LDAPv3\n# base <dc=example,dc=org> with scope subtree\n# filter: (objectclass=*)\n# requesting: ALL\n#\n\n# example.org\ndn: dc=example,dc=org\nobjectClass: top\nobjectClass: dcObject\nobjectClass: organization\no: Example Inc.\ndc: example\n\n# admin, example.org\ndn: cn=admin,dc=example,dc=org\nobjectClass: simpleSecurityObject\nobjectClass: organizationalRole\ncn: admin\ndescription: LDAP administrator\nuserPassword:: e1NTSEF9d3IyVFp6SlAyKy9xT2RsQ0owTDYzR0RzNFo0NUFrQ00=\n\n# search result\nsearch: 2\nresult: 0 Success\n\n# numResponses: 3\n# numEntries: 2\nYou should be able to read what got returned pretty seamlessly. One thing to notice is that the user password is returned, so it can be compared to a password provided. It is encrypted, so it doesn’t appear in plain text.\nNote that ldap is a protocol – so it takes the place of the http you’re used to in normal web operations. Like there’s https, there is also a protocol called LDAPS, which is ldap + tls for the same reason you’ve got https. LDAP is (almost) always running in the same private network as the service, so many organizations don’t require using LDAPS, but others do require it.\nRunning the ldapadmin\ndocker run -p 6443:443 \\\n        --name ldap-admin \\\n        --env PHPLDAPADMIN_LDAP_HOSTS=ldap-service \\\n        --network ldap-net \\\n        --detach osixia/phpldapadmin\ndn for admin cn=admin,dc=example,dc=org pw: admin\nhttps://localhost:6443\n# Replace with valid license\nexport RSC_LICENSE=XXXX-XXXX-XXXX-XXXX-XXXX-XXXX-XXXX\n\n# Run without persistent data and using default configuration\ndocker run -it --privileged \\\n    --name rsc \\\n    --volume $PWD/rstudio-connect.gcfg:/etc/rstudio-connect/rstudio-connect.gcfg \\\n    -p 3939:3939 \\\n    -e RSC_LICENSE=$RSC_LICENSE \\\n    --network ldap-net \\\n    rstudio/rstudio-connect:latest\n\n\n18.3.3.4 Single vs Double Bind\nThere are two different ways to establish a connection between your server and the LDAP server. The first method is called Single Bind. In a single bind authentication, the user credentials are used both to authenticate to the LDAP server, and to query the server.\nIn a Double Bind configuration, there is a separate administrative service account, used to authenticate to the LDAP server. Once authentication is complete, then the user is queried in the system.\nSingle bind configurations are often more limited than double bind ones. For example, in most cases you’ll only be able to see the single user as well as the groups they’re a part of. This can limit application functionality in some cases. On the other hand, there need be no master key maintained on your server, and some admins may prefer it for security reasons.\nWe can see this really concretely. In the example above, you used a double bind by supplying admin credentials to LDAP. Let’s say instead, you just provide a single user’s credentials. In that case, I don’t get anything back if I just do a general search.\nldapsearch -x -H ldap://localhost:6389 -b dc=example,dc=org -D \"cn=joe,dc=engineering,dc=example,dc=org\" -w joe                                       \n# extended LDIF\n#\n# LDAPv3\n# base <dc=example,dc=org> with scope subtree\n# filter: (objectclass=*)\n# requesting: ALL\n#\n\n# search result\nsearch: 2\nresult: 32 No such object\n\n# numResponses: 1\nBut just searching for information about Joe does return his own information.\nldapsearch -x -H ldap://localhost:6389 -b cn=joe,dc=engineering,dc=example,dc=org -D \"cn=joe,dc=engineering,dc=example,dc=org\" -w joe                    32 ✘\n# extended LDIF\n#\n# LDAPv3\n# base <cn=joe,dc=engineering,dc=example,dc=org> with scope subtree\n# filter: (objectclass=*)\n# requesting: ALL\n#\n\n# joe, engineering.example.org\ndn: cn=joe,dc=engineering,dc=example,dc=org\ncn: joe\ngidNumber: 500\ngivenName: Joe\nhomeDirectory: /home/joe\nloginShell: /bin/sh\nmail: joe@example.org\nobjectClass: inetOrgPerson\nobjectClass: posixAccount\nobjectClass: top\nsn: Golly\nuid: test\\joe\nuidNumber: 1000\nuserPassword:: e01ENX1qL01raWZrdk0wRm1sTDZQM0MxTUlnPT0=\n\n# search result\nsearch: 2\nresult: 0 Success\n\n# numResponses: 2\n# numEntries: 1\n\n\n\n18.3.4 Kerberos Tickets\nKerberos is a relatively old ticket-based auth technology. In Kerberos, encrypted tickets are passed around between servers. Because these tickets live entirely on servers under the control of the organization, they are generally quite secure.\nThough Kerberos is freely available, it was widely adopted along with Active Directory, and it’s used almost exclusively in places that are running a lot of Microsoft products. A frequent use of Kerberos tickets is to establish database connections.\nBecause the tickets are passed around from server to server, Kerberos can be used to create a true SSO experience for users.\n\n18.3.4.1 How Kerberos Works\nAll of Kerberos works by sending information to and from the central Kerberos Domain Controller (KDC). In Kerberos, authentication and authorization are handled independently.\nWhen a Kerberos session is initialized, the service sends the users credentials off to the KDC and requests something called the Ticket Granting Ticket (TGT) from the KDC. TGTs have a set expiration period. When they expire, the client has to request an updated TGT. This is one reason why Kerberos is considered quite secure - even if someone managed to steal a TGT, they’d only be able to use it for a little while before it went stale and could be revoked.\nWhen the user wants to actually do something, they send the TGT back to the KDC again and get a session key (sometimes referred to as a service ticket) that allows access to the service, usually with a specified expiration period.\n\n\n\n18.3.4.2 Try out Kerberos\n#TODO\n\n\n\n18.3.5 SAML\nThese days Security Assertion Markup Language (SAML) is probably the most common system that provides true SSO – including single login and centrally-managed permissions. SAML does this by passing around XML tokens.4\nThe way this generally works is that a user attempts to login to a Service Provider (SP). The SP redirects the user to an Identity Provider (IdP), which checks either for a preexisting token in the users browser, or verifies the users credentials. The IdP checks for the user’s authorization to access the SP in question, and sends an authorization token back to the SP.\n\nRelative to LDAP/AD, which is from the early 1990s, SAML is a new kid on the block. SAML 1.0 was introduced in 2002, and SAML 2.0, which is the current standard, came out in 2005. Many large enterprises are switching their systems over to use SAML or have already done so.\nOne superpower of SAML IdPs is that many of them can federate identity management to other systems. So, it’s pretty common for large enterprises to maintain their user base in one or more LDAP/AD system, but actually use a SAML IdP to do authentication and authorization. In fact, this is what Azure Active Directory (AAD), which is Microsoft Azure’s hosted authentication offering does. It is possible to use LDAP/AD with AAD, but most organizations use it with SAML.\nOne of the nice things about SAML is that credentials are never shared directly with the SP. This is one of the ways in which SAML is fundamentally more secure than LDAP/AD – the users credentials are only ever shared with the IdP.\nThere are two different ways logins can occur – starting from the SP, or starting from the IdP.\nIn SAML, the XML tokens that are passed back and forth are called assertions.\n\n18.3.5.1 Try SAML\nWe’re going to use a simple SAML IdP to try out SAML a bit. This container only supports a single SP. Any IdP that might be used in an enterprise environment is going to support many SPs simultaneously.\nLet’s go through the environment variables we’re providing to this docker run command. We’re providing three different arguments:\n\nThe SP_ENTITY_ID is the URL of the\nSP_ASSERTION_CONSUMER_SERVICE is the URL of the SP that is prepared to receive the authorized responses coming back from the SAML IdP.\nSP_SINGLE_LOGOUT_SERVICE is the URL where the SP will receive a logout command once someone has been logged out at the IdP level. Many SPs do not implement single logout.\n\ndocker run --name=saml_idp \\\n-p 8080:8080 \\\n-p 8443:8443 \\\n-e SIMPLESAMLPHP_SP_ENTITY_ID=http://app.example.com \\\n-e SIMPLESAMLPHP_SP_ASSERTION_CONSUMER_SERVICE=http://localhost/simplesaml/module.php/saml/sp/saml2-acs.php/test-sp \\\n-e SIMPLESAMLPHP_SP_SINGLE_LOGOUT_SERVICE=http://localhost/simplesaml/module.php/saml/sp/saml2-logout.php/test-sp \\\n-d kristophjunge/test-saml-idp:1.15\nhttp://localhost:8080/simplesaml\nadmin/secret\n\n\n\n18.3.6 OIDC/OAuth2.0\nOIDC/OAuth is slightly newer than SAML, created in 2007 by engineers at Google and Twitter. OAuth 2.0 – the current standard was released in 2012. If you’re being pedantic, OAuth is a authorization protocol, and OpenID Connect (OIDC) is an authorization protocol that uses OAuth. In most cases, people will just call it OAuth.\n\n#TODO: this picture is bad\nIn an enterprise context, OAuth/OIDC is conceptually very similar to SAML – but instead of passing around XML tokens, it’s based on JSON Web Tokens (JWT, usually pronounced “jot”).\n\nThe fact that JSON is much more human-readable than XML is one of the big advantages of OIDC/OAuth compared to SAML.\n\n#TODO: try it out\n\n18.3.6.1 OAuth/OIDC vs SAML\nFrom a practical perspective, the biggest difference between OAuth/OIDC and SAML is that SAML is quite strict about what SPs are allowed. Each SP needs to be registered at a specific web address that the IdP knows it’s allowed to receive requests from.\nIn contrast, OAuth/OIDC was designed to be used to delegate authentication and authorization to different kinds of services that might be widely available on the internet. If you’ve ever allowed a website to Login with Apple/Google/Facebook/Github, that has been an application of OAuth/OIDC.\nBecause the set of allowable SPs is fixed under SAML, it’s more common in enterprise settings. Some admins consider SAML more secure for that reason as well.\nIn some situations, SAML is used for authentication and OAuth is used for access to other services. Most commonly in the data science world, this can come up when a user logs into a service like RStudio Server and is then authorized to a database using an OAuth JWT.\nResources: https://www.okta.com/identity-101/saml-vs-oauth/ https://www.okta.com/identity-101/whats-the-difference-between-oauth-openid-connect-and-saml/ https://phoenixnap.com/blog/kerberos-authentication https://www.dnsstuff.com/rbac-vs-abac-access-control"
  },
  {
    "objectID": "chapters/sec4/4-3-auth.html#comprehension-questions",
    "href": "chapters/sec4/4-3-auth.html#comprehension-questions",
    "title": "18  Logging in with auth",
    "section": "18.4 Comprehension Questions",
    "text": "18.4 Comprehension Questions\n\nWhat is the difference between authentication and authorization?\nWhat are some different ways to manage permissions? What are the advantages and drawbacks of each?\nWhat is some advantages of token-based auth? Why are most organizations adopting it? Are there any drawbacks?\nFor each of the following, is it a username + password method or a token method? PAM, LDAP, Kerberos, SAML, ODIC/OAuth"
  },
  {
    "objectID": "chapters/sec4/4-4-scaling.html#k8s",
    "href": "chapters/sec4/4-4-scaling.html#k8s",
    "title": "19  Scaling",
    "section": "19.1 Container Deployment + Orchestration",
    "text": "19.1 Container Deployment + Orchestration\nOne tool that comes up increasingly frequently when talking about scaling is Kubernetes (sometimes abbreviated as K8S).4 Kubernetes is the way people orchestrate Docker containers in production settings.5 So basically that it’s the way to put containers into production when you want more than one to interact – say you’ve got an app that separately has a database and a front end in different containers, or, like in this chapter, multiple load-balanced instances of the same containers.\nWhile the operational details of Kubernetes are very different from the horizontal scaling patterns we’ve discussed so far in this chapter, the conceptual requirements are the same.\nTODO: Diagram of K8S\nMany people like Kubernetes because of its declarative nature. If you recall from the section on Infrastructure as Code, declarative code allows you to make a statement about what the thing is you want and just get it, instead of specifying the details of how to get there.\nOf course, in operation this all can get much more complicated, but once you’ve got the right containers, Kubernetes makes it easy to say, “Ok, I want one instance of my load balancer container connected to three instances of my compute container with the same volume connected to all three.”\n\n\n\n\n\n\nKubernetes Tripwire!\n\n\n\nIf you’re reading this and are extremely excited about Kubernetes – that’s great! Kubernetes does make a lot of things easy that used to be hard. Just know, networking configuration is the place you’re likely to get tripped up. You’ve got to deal with networking into the cluster, networking among the containers inside the cluster, and then networking within each container.\nComplicated kubernetes networking configurations are not for the faint of heart.\n\n\nFor individual data scientists, Kubernetes is usually overkill for the type of work you’re doing. If you find yourself in this territory, it’s likely you should try to work with you organization’s IT/Admin group.\nOne of the nice abstraction layers Kubernetes provides is that in Kubernetes, you provide declarative statements of the containers you want to run, and any requirements you have. You separately register actual hardware with the cluster, and Kubernetes takes care of placing the conatiners onto the hardware depending on what you’ve got available.\nIn practice, unless you’re part of a very sophisticated IT organization, you’ll almost certainly use Kubernetes via one of the cloud providers’ Kubernetes clusters as a service. AWS’s is called Elastic Kubernetes Service (EKS).6\nOne really nice thing about using these Kubernetes clusters as a service is that adding more compute power to your cluster is generally as easy as a few button clicks. On the other hand, that also makes it dangerous from a cost perspective.\nIt is possible to define a Kubernetes cluster “on the fly” and deploy things to a cluster in an ad hoc way. I wouldn’t recommend this for any production system. Helm is the standard tool for defining kubernetes deployments in code, and Helmfile is a templating system for Helm.\nSo, for example, if you had a standard “Shiny Server” that was one load balancer containers, two containers each running a Shiny app, and a volume mounted to both, you would define that cluster in Helm. If you wanted to be able to template that Helm code for different clusters, you’d use Helmfile."
  },
  {
    "objectID": "chapters/sec4/4-4-scaling.html#comprehension-questions",
    "href": "chapters/sec4/4-4-scaling.html#comprehension-questions",
    "title": "19  Scaling",
    "section": "19.2 Comprehension Questions",
    "text": "19.2 Comprehension Questions\n\nWhat is the difference between horizontal and vertical scaling? For each of the following examples, which one would be more appropriate?\n\nYou’re the only person using your data science workbench and run out of RAM because you’re working with very large data sets in memory.\nYour company doubles the size of the team that will be working in your data science workbench. Each person will be working with reasonably small data, but there’s going to be a lot more of them.\nYou have a big modeling project that’s too large for your existing machine. The modeling you’re doing is highly parallelizable.\n\nWhat is the role of the load balancer in horizontal scaling? When do you really need a load balancer and when can you go without?\nWhat are the biggest strengths of Kubernetes as a scaling tool? What are some drawbacks?"
  },
  {
    "objectID": "chapters/append/docker-cheatsheet.html#general-docker-commands",
    "href": "chapters/append/docker-cheatsheet.html#general-docker-commands",
    "title": "Appendix A — Docker Cheatsheet",
    "section": "A.1 General Docker Commands",
    "text": "A.1 General Docker Commands\n\n\n\n\n\n\n\n\nCommand\nPurpose\nExample\n\n\n\n\ndocker run\nRun an image as a container\ndocker run me/my-image\n\n\ndocker ps\nList all containers\ndocker ps\n\n\ndocker kill\nKill a container\ndocker kill my-container\n\n\ndocker exec\nRun a command inside a running container\ndocker exec -it /bin/bash\n\n\ndocker build\nBuild a Dockerfile\ndocker built -t me/my-image .\n\n\ndocker logs\nGet logs from a container\ndocker logs my-container\n\n\ndocker pull\nPull a container from a registry\ndocker pull me/my-image\n\n\ndocker push\nPush a container to a registry\ndocker push me/my-image"
  },
  {
    "objectID": "chapters/append/docker-cheatsheet.html#docker-run-command-flags",
    "href": "chapters/append/docker-cheatsheet.html#docker-run-command-flags",
    "title": "Appendix A — Docker Cheatsheet",
    "section": "A.2 docker run command flags",
    "text": "A.2 docker run command flags\n\n\n\n\n\n\n\n\nFlag\nPurpose\nExample\n\n\n\n\n-d\nRun in “detached” mode that doesn’t block your terminal\ndocker run -d ...\n\n\n--rm\nRemove the container on stop\nReminder: don’t use in prod\ndocker run --rm …\n\n\n-p\nPublish ports from container to host\ndocker run -p 8000:8000 …\n\n\n-v\nMount a volume into the container\ndocker run -v $(pwd):/data\n\n\n--name\nGive container a human-friendly name\ndocker run --name my-container\n\n\n\nReminder - -p and -v order is <host>:<container>"
  },
  {
    "objectID": "chapters/append/docker-cheatsheet.html#dockerfile-commands",
    "href": "chapters/append/docker-cheatsheet.html#dockerfile-commands",
    "title": "Appendix A — Docker Cheatsheet",
    "section": "A.3 Dockerfile Commands",
    "text": "A.3 Dockerfile Commands\nThese are the commands that go in a Dockerfile when you’re building it.\n\n\n\n\n\n\n\n\nCommand\nPurpose\nExample\n\n\n\n\nFROM\nIndicate base container\nFROM rocker/r-ver:4.1.0\n\n\nRUN\nRun a command when building\nRUN apt-get update\n\n\nCOPY\nCopy from the working directory into the container\nCOPY . /app/\n\n\nCMD\nSpecify the command to run when the container starts\nCMD quarto render ."
  }
]