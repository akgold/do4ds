[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DevOps for Data Science",
    "section": "",
    "text": "Welcome!\nThis is the website for the book DevOps for Data Science, currently in draft form.\nIn this book, you’ll learn about DevOps conventions, tools, and practices that can be useful to you as a data scientist. You’ll also learn how to work better with the IT/Admin team at your organization, and even how to do a little server administration of your own if you’re pressed into service.\nThis website is (and always will be) free to use, and is licensed under the Creative Commons Attribution-NonCommercial-NoDerivs 3.0 license.\nIf you’d like a physical copy of the book, they will be available once it’s finished!"
  },
  {
    "objectID": "index.html#software-information-and-conventions",
    "href": "index.html#software-information-and-conventions",
    "title": "DevOps for Data Science",
    "section": "Software information and conventions",
    "text": "Software information and conventions\nI used the knitr package [@xie2015] and the quarto package [@quarto] to compile my book.\nPackage names are in bold text (e.g., rmarkdown), and inline code and filenames are formatted in a typewriter font (e.g., knitr::knit('foo.Rmd')). Function names are followed by parentheses (e.g., bookdown::render_book())."
  },
  {
    "objectID": "index.html#about-the-author",
    "href": "index.html#about-the-author",
    "title": "DevOps for Data Science",
    "section": "About the Author",
    "text": "About the Author\nAlex Gold is the Director of Solutions Engineering at Posit, formerly RStudio.\nThe Solutions Engineering team works with Posit’s customers to help them deploy, configure, and use Posit’s professional software and open source tooling in R and Python.\nIn his free time, he enjoys landscaping, handstands, and Tai Chi.\nHe occasionally blogs about data, management, and leadership at alexkgold.space."
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "DevOps for Data Science",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nA lot of people are helping me write this book.\nThis book is published to the web using GitHub Actions from rOpenSci."
  },
  {
    "objectID": "index.html#color-palette",
    "href": "index.html#color-palette",
    "title": "DevOps for Data Science",
    "section": "Color palette",
    "text": "Color palette\nTea Green: #CAFFDO\nSteel Blue: #3E7CB1\nKombu Green: #273c2c\nBright Maroon: #B33951\nSandy Brown: #FCAA67"
  },
  {
    "objectID": "chapters/intro.html#devops-for-agile-software",
    "href": "chapters/intro.html#devops-for-agile-software",
    "title": "Introduction",
    "section": "DevOps for Agile Software",
    "text": "DevOps for Agile Software\nDevOps is a set of cultural norms, practices, and supporting tooling to help make the process of developing and deploying software smoother and lower risk.\nIf that definition strikes you as unhelpfully vague, you’re right.\nLike Agile software development, to which it is closely related, DevOps is a squishy concept. That’s partially because DevOps isn’t a fixed thing. It’s the application of some principles and process ideas to whatever context you’re working in. That malleability is why DevOps works, but also makes it difficult to pin down.\nThis imprecision furthered by the ecosystem of companies enabling DevOps. There are dozens and dozens of companies proselytizing their own particular flavor of DevOps – one that (shocker) reflects the capabilities of whatever product they’re selling.\nBut underneath the industry hype and the marketing jargon, there are some extremely valuable lessons to take from the field.\nTo understand better, let’s go back to the birth of DevOps.\nAs the story goes, the history of software development before the 1990s involved a waterfall development processes. In these processes, software developers worked with clients and customers to fully define requirements for software, plan the whole thing out, and deliver final software months or years later.\nWhen the application was complete, it was hurled over the metaphorical wall from Development to Operations. IT Administrators in the Ops department would figure out the hardware and networking requirements, get it running, and keep it up.\nThroughout the 1990s, software developers observed that delivering software in small units, quickly collecting feedback, and iterating was a more effective model.\nIn 2001, the Manifesto for Agile Software Development was published, giving a name to this philosophy of software development. Agile development ate the world and, basically, all software is now developed using some form of Agile. Agile work styles have extended far beyond software into other domains as well.\nThere are dozens of Agile software development frameworks you might have heard of including Scrum, Kanban, Extreme Programming (XP), and many, many more. One commonality of these frameworks were really focused on software development. What happened once the software was written?\nThe old pattern clearly wouldn’t work. If you were doing new deployments multiple times a week – or even a day – you needed a complementary process to get that software deployed and into production.\nDevOps arose as this discipline, i.e., a way for Dev and Ops to better collaborate on the process that would take software from development into production. It took a little while for the field to be formalized, with the term DevOps coming into common usage around 2010."
  },
  {
    "objectID": "chapters/intro.html#processes-and-people",
    "href": "chapters/intro.html#processes-and-people",
    "title": "Introduction",
    "section": "Processes and People",
    "text": "Processes and People\nThroughout this book, DevOps refers to the knowledge, practices, and tools that make it easier, safer, and faster to put work into production. So, if you’re a software developer (and as a data scientist, you are) you need to be thinking about DevOps.\nMost organizations also have a set of people and roles who have the permission and responsibility for managing their organization’s servers and software. Their titles vary. They might be named Information Technology (IT), SysAdmin, Site Reliability Engineering (SRE), or DevOps.1\nFor simplicity, I’m going to use the term IT/Admin to refer to these people and teams throughout this book.\nFundamentally, DevOps is about creating good patterns for people to use when collaborating on developing and deploying software. Because these patterns vary by organization, DevOps can and should look different at different organizations.\nAs a data scientist, you are the Dev, so a huge part of making DevOps work for you is finding IT/Admin counterparts with whom you can collaborate. In some cases that will be easier than others. Here are three patterns that are almost always red flags – mostly because they make it hard to develop relationships that can sustain the kind of collaboration DevOps requires.\n\nAt some large organizations, IT/Admin functions are split into small atomic units like security, databases, networking, storage, procurement, cloud, and more. This is useful for keeping the scope-of-work manageable for the people in that group, and often results in deep technical expertise. But it also can be slow to get anything done because you’ll need to bring people together from disparate teams.\nSome organizations have chosen to outsource their IT/Admin functions. While the individuals in those outsourced teams are often quite competent, building relationships can be difficult. Outsourced IT/Admin teams are often in India, so it can be hard to find meeting times with American and European teams. Additionally, turnover on projects and systems tends to be high at outsourced IT/Admin organizations. That means that institutional knowledge is thin and relationships can’t be relied on long term.\nSome organizations, especially small or new ones, don’t have an IT/Admin function. At others, the IT/Admins are preoccupied with other tasks and don’t have the capacity to help the data science team. This isn’t a tragedy, but it probably means you’re going to have to become the IT/Admin if you want to get anything done.\n\nWhether your organization has an IT/Admin setup that facilitates DevOps best practices or not, hopefully this book can help you take the first steps towards making your path to production smoother and simpler."
  },
  {
    "objectID": "chapters/intro.html#about-this-book",
    "href": "chapters/intro.html#about-this-book",
    "title": "Introduction",
    "section": "About this book",
    "text": "About this book\nOver the course of engaging with many organizations, I’ve seen which patterns work to grease the path to production for data scientists and which ones tend to impede it.\nMy goal is that this book helps you create data science projects that are easier and simpler to deploy, and that you have the knowledge and skills to get them into production when it’s time.\nTo that end this book is divided into three sections.\nSection 1 is about applying DevOps best practices to a data science context. There’s a lot data scientists can learn from DevOps, but there are important differences between data science and general purpose software engineering that you’ll learn about.\nSection 2 is a walk through of basic concepts in IT Administration that will get you to the point of being able to host and manage your own small server. If you are a hobbyist or have only a small data science team, this might make you able to operate without any IT/Admin support. Even if you do work at an organization with significant IT/Admin support, it will equip you with the vocabulary to talk to the IT/Admins at your organization and some basic skills of how to do IT/Admin tasks yourself.\nSection 3 is about how all of what you learned in Section 2 changes when you go to enterprise scale. If section 2 explains how to do IT/Admin tasks yourself, section 3 is my attempt to explain why you shouldn’t.\n\nComprehension Questions\nEach chapter in this book includes comprehension questions. As you get to the end of the chapter, take a moment to consider these questions. If you feel comfortable answering them, you’ve probably understood the content of the chapter pretty well.\nAlternatively, feel free to jump ahead to them as you’re reading the chapter. If you can already answer them all, you can probably skip that chapter.\n\n\n\n\n\n\nMental Models + Mental Maps\n\n\n\nThroughout the book, I’ll talk a lot about building a mental model of different concepts.\nA mental map is a way to represent mental models.\nIn a mental map, you draw each of the nouns as nodes and connect them with arrows that are labelled to explain the relationship.\nMental maps are a great way to test your mental models, so I’ll suggest them as comprehension questions in many chapters.\nHere’s an example about this book:\n\nNote how every node is a noun and the edges (labels on the arrows) are verbs. It’s pretty simple! But writing down the relationships between entities like this is a great check on understanding.\n\n\n\n\nLabs\nMany chapters also contain labs. The idea of these labs is to give you hands-on experience with the concepts at hand.\nThese labs all tie together. If you follow the labs in this book, you’ll build up a reasonably complete data science platform that includes a place for you to work, a way to store data, and a deployment environment.\nPalmer Penguins is a public dataset meant for demonstrating data exploration and visualization. We’re going to pretend we care deeply about the relationship between penguin bill length and mass and we’re going to build up an entire data science environment dedicated to exploring that relationship.\nThe front end of this environment is going to be a website that contains an app that allows you to get predictions from a machine learning model of a penguin’s mass based on bill length and other features. We’re also going to include pages dedicated to exploratory data analysis and model building on the website.\nOn the backend, we’re going to build a data science workbench on an AWS EC2 instance where we can do this work. It will include RStudio Server and JupyterHub for working. It will additionally host the machine learning model as an API and the Shiny app that appears on the website.\nThe whole thing will get auto-deployed from a git repo using GitHub Actions.\nFrom an architectural perspective, it’ll look something like this:\n\nIf you’re interested in exactly which pieces get completed in each chapter, check out Appendix B.\n\n\nConventions\nThroughout the book, I will italicize terms of art the first time I introduce them as well as the names of other publications. Because so many of the technical terms in this book are usually referred to by abbreviations or acronyms, I’ll use the abbreviation or acronym in the text and include the full term in parentheses the first time it’s mentioned.\nBolding will be reserved for emphasis.\nR, Python, and system package names will be in code font and will have braces around them like {package}. Networking concepts and terms, including URLs, will appear in \\(\\text{equation font}\\).\nVariables that you would replace with your own values will appear in code font inside angled brackets like &lt;your-variable&gt;."
  },
  {
    "objectID": "chapters/intro.html#footnotes",
    "href": "chapters/intro.html#footnotes",
    "title": "Introduction",
    "section": "",
    "text": "I think a lot of DevOps experts would argue that you’re doing DevOps wrong if you have a standalone DevOps team.↩︎"
  },
  {
    "objectID": "chapters/sec1/1-0-sec-intro.html#labs-in-this-section",
    "href": "chapters/sec1/1-0-sec-intro.html#labs-in-this-section",
    "title": "DevOps Lessons for Data Science",
    "section": "Labs in this section",
    "text": "Labs in this section\nEach chapter in this section has a lab so you can get hands-on experience implementing the best practices I propose.\nIf you complete the labs, you’ll have stood up your Palmer Penguins website to explore the relationship between penguin bill length and mass. Your website will include pages on exploratory data analysis and model building. This website will automatically build and deploy based on changes in a git repo.\nBy the end of the section, you’ll also create a Shiny app that visualizes model predictions and an API that hosts the model and provides real-time predictions to the app. Additionally, you’ll get to practice putting that API inside a Docker Container to see how using Docker can make your life easier when moving code around.\nFor more details on exactly what you’ll do in each chapter, see Appendix B."
  },
  {
    "objectID": "chapters/sec1/1-0-sec-intro.html#footnotes",
    "href": "chapters/sec1/1-0-sec-intro.html#footnotes",
    "title": "DevOps Lessons for Data Science",
    "section": "",
    "text": "If you enjoy this introduction, I strongly recommend The Phoenix Project by Gene Kim, Kevin Behr, and George Spafford. It’s a novel about implementing DevOps principles. A good friend described it as, “a trashy romance novel about DevOps”. It’s a very fun read.↩︎"
  },
  {
    "objectID": "chapters/sec1/1-1-env-as-code.html#environments-have-layers",
    "href": "chapters/sec1/1-1-env-as-code.html#environments-have-layers",
    "title": "1  Environments as Code",
    "section": "1.1 Environments have layers",
    "text": "1.1 Environments have layers\nData science environments have three distinct layers that build on each other. Once you understand the layers of an environment, you can think more clearly about what your actual reproducibility needs are, and which environmental layers you need to target putting into code.\nLayers of a Data Science Environments\n\n\n\n\n\n\n\nLayer\nContents\n\n\n\n\nPackages\nR + Python Packages\n\n\nSystem\nR + Python Language Versions\nOther System Libraries\nOperating System\n\n\nHardware\nVirtual Hardware\nPhysical Hardware\n\n\n\nFundamentally, the hardware and software layers should be the responsibility of an IT/Admin. It may be the case that you’re responsible for them as well, but then you’re just fulfilling that role.\nBut as a data scientist, you can and should be responsible for the package layer, and getting this layer right is where the biggest reproducibility bang for your buck lies."
  },
  {
    "objectID": "chapters/sec1/1-1-env-as-code.html#what-is-a-package-environment",
    "href": "chapters/sec1/1-1-env-as-code.html#what-is-a-package-environment",
    "title": "1  Environments as Code",
    "section": "1.2 What is a package environment",
    "text": "1.2 What is a package environment\nIt’s helpful to start by building a mental model of what a package environment is before we get into how to use them well.\nLet’s start with an intro to the different states packages can be in.\n\nIn a repository. You’re probably used to installing packages from a packages-specific repository like PyPI, Conda, CRAN, or BioConductor. These repositories are like the grocery store – the food is packaged up and ready to go, but inert. There’s also lots of variety – repositories hold current and archival versions of tons of different packages.\nIn a library. Once you install the packages you need with install.packages() or pip install or conda install, they’re in your library – the data science equivalent of a pantry. That library is specific to a certain environment. Importantly, libraries can hold – at most – one version of any given package. Libraries can be specific to the project, user, or shared across the system.\nLoaded. Loading a package with a library or import command is like taking the food out of the pantry and putting it on the counter so you can actually cook with it.\n\nIn general, the library is the package environment that’s the most relevant because it’s the part you, as a data scientist, have control over.\nLet’s say you work on one project for a while, installing packages from the repository into your library. Now let’s say you come back after a year of working on other projects or try to share your project with someone else.\nIt’s probable that future you or your colleague you won’t have the right versions and your code will break.\nWhat would’ve been better is if you’d had an environments as code strategy that created a portable environment for each project on your system.\nA successful package Environment as Code setup has two key attributes:\n\nYour package environment is isolated and cannot be disrupted by other activities on the same machine.\nYour package environment can easily be captured and transported elsewhere.\n\nBoth R and Python have great environments as code utilities that make it easy to do both these things.\n\n\n\n\n\n\nR vs Python\n\n\n\nOn a high level, the workflows for these tasks are similar between R and Python. However, it’s worth noting that Python is used all the time by your system, where R is basically only ever installed for data science purposes.\nThat means that most tutorials on using Python start with installing a standalone data science version of Python using a virtual environment or Conda, so you may already be used to using a virtual environment.\nHopefully this chapter will help fill out your mental model of what’s going on.\n\n\nIn R, {renv} is the standard on this front. In Python, there are many different options. In the context of production data science, I recommend {virtualenv}/{venv} and related tools."
  },
  {
    "objectID": "chapters/sec1/1-1-env-as-code.html#using-a-virtual-environment",
    "href": "chapters/sec1/1-1-env-as-code.html#using-a-virtual-environment",
    "title": "1  Environments as Code",
    "section": "1.3 Using a virtual environment",
    "text": "1.3 Using a virtual environment\nUsing a virtual environment tool to create an environment as code is a three-step process. This section will teach you about the process of creating and using a virtual environment – there’s a cheatsheet on the commands for both R and Python at the end of the chapter.\nStep 1: Create standalone package libraries\nEach project should have it’s own {renv}/{venv} library. When you start your project, it should be in a standalone directory that includes everything the project needs – including a virtual environment.\nThis is called a project-oriented workflow. You can do it in either R or Python. The What They Forgot to Teach You About R course (materials available online at rstats.wtf) is a great intro to a project-oriented workflow whether you work in R or Python. The tooling will be somewhat different in Python, but the idea is the same.\n\n\n\n\n\n\nNote\n\n\n\nSome projects include multiple content items – like a project that like an app project backed up by an API and an ETL script. My recommendation is to create one virtual environment for each piece of content.\n\n\nThen, every time you work on the project, you activate the virtual environment and install/activate packages in there.\nOne worry that can arise is that it could take up a lot of space to reinstall these packages over and over again for every project on your system. Don’t worry – {renv} and {venv} are a little cleverer than this. If you’re using the same package multiple times, it does some work behind the scenes using symbolic links to make sure that the package is actually only installed once.\nStep 2: Document environment state\nAs you work inside your virtual environment, you’ll want to document the state of the package library. Both {renv} and {venv} have standard file formats for documenting the packages that are installed in the environment as well as the versions.\nIn {renv}, the file is called a lockfile and it’s a requirements.txt in {venv}.\nSince all this work is occurring in a standalone package environment, you don’t have to worry about what’ll happen if you come back after a break. You’ll still have those same packages to use.\nStep 3: Collaborate or deploy\nWhen you go to share your project, you don’t want to share your actual package libraries. The actual package libraries can be large, so putting just a short lockfile or requirements file into git is definitely preferred. Additionally, package installs are specific to the operating system and the language version you’re using.\nSo if you’re working on a Mac with a particular set of R and/or Python versions and you collaborate or deploy to a Windows or Linux machine, you can’t share the actual package files. Those machines will need to install the required set of packages for themselves.\nSo you just share your lockfile or requirements.txt and your collaborator or deployment process consults that file and installs all the required packages from there."
  },
  {
    "objectID": "chapters/sec1/1-1-env-as-code.html#whats-happening-under-the-hood",
    "href": "chapters/sec1/1-1-env-as-code.html#whats-happening-under-the-hood",
    "title": "1  Environments as Code",
    "section": "1.4 What’s happening under the hood",
    "text": "1.4 What’s happening under the hood\nNow that you understand the basic process of using a virtual environment tool, it might be helpful to understand what’s actually going on.\nWhen you install a package, it installs into a package cache. This is just a directory full of installed packages. When you install a package, it’s installed into that directory. When you activate a library with library or import, it searches that directory and loads the library into the active session.\nIn R, the package cache(s) to be consulted are contained in .libPaths(). In Python it’s sys.path.\nThe key to virtual environments is that they monkey around with the package cache list, so both installation and loading happens from a cache that’s specific to the virtual environment.\nHere’s an example. If I run .libPaths() before and after activating an {renv} environment, the first entry from the .libPaths() call changes from a user-level library /Users/alexkgold to a project-level library /Users/alexkgold/Documents/do4ds/.\n&gt; .libPaths()\n[1] \"/Users/alexkgold/Library/R/x86_64/4.2/library\"                 \n[2] \"/Library/Frameworks/R.framework/Versions/4.2/Resources/library\"\nrenv::activate()\n* Project '~/Documents/do4ds/docker/docker/plumber' loaded. [renv 0.15.5]\n.libPaths()\n[1] \"/Users/alexkgold/Documents/do4ds/docker/docker/plumber/renv/library/R-4.2/x86_64-apple-darwin17.0\"\n[2] \"/Library/Frameworks/R.framework/Versions/4.2/Resources/library\"  \nSimilarly, in Python it looks like this. Note that the “after” version replaces the last line of the sys.path with a project-level library:\n&gt; python3 -m site                                       \nsys.path = [\n    '/Users/alexkgold/Documents/python-examples/dash-app',\n    '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python38.zip',\n    '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8',\n    '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/lib-dynload',\n    '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/site-packages',\n]\nUSER_BASE: '/Users/alexkgold/Library/Python/3.8' (doesn't exist)\nUSER_SITE: '/Users/alexkgold/Library/Python/3.8/lib/python/site-packages' (doesn't exist)\nENABLE_USER_SITE: True\n\n&gt; source .venv/bin/activate                       \n(.venv)\n\n&gt; python3 -m site\nsys.path = [\n    '/Users/alexkgold/Documents/python-examples/dash-app',\n    '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python38.zip',\n    '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8',\n    '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/lib-dynload',\n    '/Users/alexkgold/Documents/python-examples/dash-app/.venv/lib/python3.8/site-packages',\n]\nUSER_BASE: '/Users/alexkgold/Library/Python/3.8' (doesn't exist)\nUSER_SITE: '/Users/alexkgold/Library/Python/3.8/lib/python/site-packages' (doesn't exist)\nENABLE_USER_SITE: False\n(.venv)\nIf you’re on a shared server, you may want to share a package cache across users. This generally isn’t necessary, but can save some space on the server. Both {renv} and venv include settings to allow you to relocate the package cache to a shared location on the server. You’ll need to make sure that all the relevant users have read and write privileges to this location.\n\n\n\n\n\n\nWhy I’m not talking about Conda\n\n\n\nMany data scientists love Conda for managing their Python environments.\nConda is allows you to create a data science environment on your local laptop. It’s especially useful when your machine is locked down and you don’t have root access because it does all of its installation in user space.\nThat’s super useful for working on your laptop, but it’s not a great fit for a production environment. Conda smashes together the language version, the package management, and (sometimes) the system library management. This has the benefit of being conceptually simple and easy-to-use. But I’ve often seen it go awry in production environments, and I generally recommend people use a tool that’s just for package management, like {venv}, as opposed to an all-in-one tool like Conda."
  },
  {
    "objectID": "chapters/sec1/1-1-env-as-code.html#reproducing-the-rest-of-the-stack",
    "href": "chapters/sec1/1-1-env-as-code.html#reproducing-the-rest-of-the-stack",
    "title": "1  Environments as Code",
    "section": "1.5 Reproducing the rest of the stack",
    "text": "1.5 Reproducing the rest of the stack\nAs a data scientist, your responsibility in a production environment ends at responsibly creating and using virtual environments for your packages. That said, there may not be an IT/Admin to take care of the rest for your team.\nLet’s talk a little about how to reproduce the rest of the stack.\nManaging R and Python versions has gotten easier over the years. There are great tools like {rig} in R and {pyenv} in Python that makes it easy to manage the versions of R and Python that are available on a system.\nMany R and Python libraries are wrappers for system libraries, often written in C++ for speed. For example, many popular packages for geospatial analysis are just thin language wrappers that call out to the system libraries. In this case, it might be important to be able to maintain a particular version of the underlying system library to ensure that your code runs at all in the future.\nThere are many tools you can use to record and reproduce the R and Python versions you’re using, the system libraries, and the operating system itself.\nThese days, the clear leader of the pack on this front is Docker. Since its introduction in 2013, it has become an increasingly popular way to create, maintain, and use standalone environments – and for good reason!\n\n\n\n\n\nIn most cases, the only requirement to run any Docker container is having Docker installed. If you put your project in a Docker container and it runs today, you can be reasonably confident that the container itself can run almost anywhere else, irrespective of what else is running on that machine.2\nIn Chapter 6, you’ll learn the basics of how to use Docker as a tool for reproducing a data science project and moving it around."
  },
  {
    "objectID": "chapters/sec1/1-1-env-as-code.html#comprehension-questions",
    "href": "chapters/sec1/1-1-env-as-code.html#comprehension-questions",
    "title": "1  Environments as Code",
    "section": "1.6 Comprehension Questions",
    "text": "1.6 Comprehension Questions\n\nWhy does difficulty increase as the level of required reproducibility increase for a data science project. In your day-to-day work, what’s the hardest reproducibility challenge?\nDraw a mental map of the relationships between the 7 levels of the reproducibility stack. Pay particular attention to why the higher layers depend on the lower ones.\nWhat are the two key attributes of environments as code? Why do you need both of them? Are there cases where you might only care about one?\nDraw a mental map of the relationships between the following: package repository, package library, package, project-level-library, .libPaths() (R) or sys.path(python), lockfile\nWhy is it a bad idea to share package libraries? What’s the best way to collaborate with a colleague using an environment as code? What are the commands you’ll run in R or Python to save a package environment and restore it later?"
  },
  {
    "objectID": "chapters/sec1/1-1-env-as-code.html#lab1",
    "href": "chapters/sec1/1-1-env-as-code.html#lab1",
    "title": "1  Environments as Code",
    "section": "1.7 Lab 1: Create a website with virtual environments",
    "text": "1.7 Lab 1: Create a website with virtual environments\nIn this lab, we’re going to start working on our penguin explorer website. We’re going to create a simple website using Quarto, which is an open source scientific and technical publishing system that makes it easy to render R and Python code into beautiful documents, websites, reports, and presentations.\nIn this lab, we’re going to create pages for a simple exploratory data analysis and model building from the Palmer Penguins dataset. In order to get to practice with both R and Python, I’m going to do the EDA page in R and the modeling in Python.\nYou can follow the instructions on the Quarto site to start a new Quarto project in your editor of choice. You can check it out locally with quarto preview.\nAs we add each of the pages below, don’t forget to add them to your _quarto.yml so quarto knows to render them.\n\n1.7.1 EDA in R\nLet’s add a simple R-language EDA of the Palmer Penguins data set to our website by adding a file called eda.qmd in the root directory of your project.\nBefore you start adding code, create and activate an {renv} environment with renv::init().\nNow, go ahead and do your analysis. Here’s the contents of my eda.qmd.\n\n\neda.qmd\n\n---\ntitle: \"Penguins EDA\"\nformat:\n  html:\n    code-fold: true\n---\n\n## Penguin Size and Mass by Sex and Species\n\n```{r}\nlibrary(palmerpenguins)\nlibrary(dplyr)\nlibrary(ggplot2)\n\ndf &lt;- palmerpenguins::penguins\n```\n\n```{r}\ndf %&gt;%\n  group_by(species, sex) %&gt;%\n  summarise(\n    across(\n      where(is.numeric), \n      \\(x) mean(x, na.rm = TRUE)\n      )\n    ) %&gt;%\n  knitr::kable()\n```\n\n## Penguin Size vs Mass by Species\n\n```{r}\ndf %&gt;%\n  ggplot(aes(x = bill_length_mm, y = body_mass_g, color = species)) +\n  geom_point() + \n  geom_smooth(method = \"lm\")\n```\n\nFeel free to copy this Quarto doc right into your website or to write your own.\nOnce you’ve finished writing your EDA script and checked that it renders nicely into the website, save the doc, and create your lockfile with renv::snapshot().\n\n\n1.7.2 Modeling in Python\nNow let’s build a {scikit-learn} model for predicting penguin weight based on bill length in a Python notebook by adding a model.qmd to the root of our project.\nAgain, you’ll want to create your virtual environment and activate it before you start pip install-ing packages into the environment. Check the cheatsheet if you need help with the specific commands.\nHere’s what’s in my model.qmd, but you should feel free to include whatever you want.\n\n\nmodel.qmd\n\n---\ntitle: \"Model\"\nformat:\n  html:\n    code-fold: true\n---\n\n```{python}\nfrom palmerpenguins import penguins\nfrom pandas import get_dummies\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import preprocessing\n```\n\n## Get Data\n\n```{python}\ndf = penguins.load_penguins().dropna()\n\ndf.head(3)\n```\n\n## Define Model and Fit\n\n```{python}\nX = get_dummies(df[['bill_length_mm', 'species', 'sex']], drop_first = True)\ny = df['body_mass_g']\n\nmodel = LinearRegression().fit(X, y)\n```\n\n## Get some information\n\n```{python}\nprint(f\"R^2 {model.score(X,y)}\")\nprint(f\"Intercept {model.intercept_}\")\nprint(f\"Columns {X.columns}\")\nprint(f\"Coefficients {model.coef_}\")\n```\n\nOnce you’re happy with how the page is working, capture your dependencies in a requirements.txt using pip freeze &gt; requirements.txt on the command line."
  },
  {
    "objectID": "chapters/sec1/1-1-env-as-code.html#cheat-renv-venv",
    "href": "chapters/sec1/1-1-env-as-code.html#cheat-renv-venv",
    "title": "1  Environments as Code",
    "section": "1.8 Cheatsheet: Environments as Code",
    "text": "1.8 Cheatsheet: Environments as Code\n\n1.8.1 Checking library + repository status\n\n\n\n\n\n\n\n\nStep\nR Command\nPython Command\n\n\n\n\nCheck whether library in sync with lockfile.\nre nv::status()\nNone\n\n\n\n\n\n1.8.2 Creating and Using a Standalone Project Library\nMake sure you’re in a standalone project library.\n\n\n\n\n\n\n\n\nStep\nR Command\nPython Command\n\n\n\n\nMake sure you’ve got {r  env}/{venv}.\nin s t a l l  .packages(\"renv\")\nIncluded w/ Python 3.5+\n\n\nCreate a standalone library.\nrenv::init()\np y t  hon -m venv &lt;dir&gt;\nConvention: u s  e .venv for &lt;dir&gt;\n\n\nActivate project library.\nrenv::activate()\nHappens automatically if using projects.\nsou r c e &lt; d  ir&gt; /bin/activate\n\n\nInstall packages as normal.\nins t a l l .  packages(\"&lt;pkg&gt;\")\npyth o n - m  pip install &lt;pkg&gt;\n\n\nSnapshot package state.\nrenv::snapshot()\npip fr e e z e &gt;   requirements.txt\n\n\nExit project environment.\nLeave R project.\ndeactivate\n\n\n\n\n\n1.8.3 Collaborating on someone else’s project\nStart by downloading the project into a directory on your machine.\n\n\n\n\n\n\n\n\nStep\nR Command\nPython Command\n\n\n\n\nMove into project directory.\nset wd  (\"&lt;project-dir&gt;\")\nOr just open R project in RStudio.\ncd &lt;project-dir&gt;\n\n\nCreate project environment.\nrenv::init()\npy thon -m venv &lt;dir&gt;\nRecommend: u se .venv for &lt;dir&gt;\n\n\nEnter project environment.\nHappens automatically or renv::activate()\nsource &lt; dir&gt; /bin/activate\n\n\nRestore packages.\nHappens automatically or renv::restore()\npip install -r   requirements.txt"
  },
  {
    "objectID": "chapters/sec1/1-1-env-as-code.html#footnotes",
    "href": "chapters/sec1/1-1-env-as-code.html#footnotes",
    "title": "1  Environments as Code",
    "section": "",
    "text": "Credit for this analogy goes to my former colleague Sean Lopp – a fount of excellent analogies and overall wonderful co-worker.↩︎\nUsually. Users of Apple’s M1 and M2 chips have run into many issues running containers due to differences in hardware architecture.↩︎"
  },
  {
    "objectID": "chapters/sec1/1-2-proj-arch.html#choose-the-right-type-of-presentation-layer",
    "href": "chapters/sec1/1-2-proj-arch.html#choose-the-right-type-of-presentation-layer",
    "title": "2  Data Project Architecture Guidelines",
    "section": "2.1 Choose the right type of presentation layer",
    "text": "2.1 Choose the right type of presentation layer\nThe presentation layer is the actual thing that will be consumed by your users. A lot of the data flows for your project will be dictated by your presentation layer, so you need to start by figuring out the details of your presentation layer.\nBasically all data science projects fall into one of four categories.\nThe first category is a job. A job matters because it changes something in another system. It might move data around, build a model, or produce plots, graphs, or numbers to be used in a Microsoft Office report.\nFrequently, jobs are written in a SQL-based pipelining tool (dbt has been quickly rising in popularity) or in a .R or .py script.1\nThe second type of data science software is an interactive app. These apps are created in frameworks like Shiny (R or Python), Dash (Python), or Streamlit (Python). In contrast to general purpose web apps, which are for all sorts of purposes, data science web apps are usually used to give non-coders a way to explore data sets and see data insights.\nThe third type is a report. Reports are code you’re turning into an output you care about – like a paper, book, presentation, or website. Reports are the result of rendering an R Markdown doc, Quarto doc, or Jupyter Notebook for people to consume on their computer, in print, or in a presentation. These docs may be completely static (this book is a Quarto doc) or they may have some interactive elements.\n\n\n\n\n\n\nNote\n\n\n\nExactly how much interactivity turns a report into an app is completely subjective. I generally think the distinction is whether there’s a running R or Python process in the background, but it’s not a particularly sharp line.\n\n\nThe fourth type is an API (application programming interface) for machine-to-machine communication. In the general purpose software world, APIs are the backbone of how two distinct pieces of software communicate. In the data science world, APIs are most often used to provide data feeds and on-demand predictions from machine learning models.\nChoosing the right type of presentation layer will make it much easier to design the rest of your project. Here are some guidelines on how to choose a presentation layer.\nIf the results of your software are for machine-to-machine use, you’re thinking about a job or API. It’s a job if it should run in a batched way (i.e. you write a data file or results into a database) and it’s an an API if you want results as queried in real time.\nIf your project is for humans to consume, you’re thinking about creating an app or report, depending on whether you need a live Python or R process on the back-end.\nHere’s a little flow chart for how I think about which of the four things you should build.\n\n\n\n\nflowchart TD\n    A{For human\\n consumption?}\n    B{Static?} \n    C{Lots of data\\n on backend?}\n    D{Should run\\batched?}\n\n    E[\"Report\\n(Static)\"]\n    F[App] \n    G[\"Report\\n(Interactive)\"]\n    H[API]\n    J[Job]\n\n\n    A --&gt;|Yes| B\n    A --&gt;|No| D\n    B --&gt;|Yes| E\n    B --&gt;|No| C\n    C --&gt;|Yes| F\n    C --&gt;|No| G\n    D --&gt;|Yes| H\n    D --&gt;|No| J"
  },
  {
    "objectID": "chapters/sec1/1-2-proj-arch.html#do-less-in-the-presentation-layer",
    "href": "chapters/sec1/1-2-proj-arch.html#do-less-in-the-presentation-layer",
    "title": "2  Data Project Architecture Guidelines",
    "section": "2.2 Do less in the presentation layer",
    "text": "2.2 Do less in the presentation layer\nAs a general rule, data scientists don’t do a great job separating out their presentation layers. It’s not uncommon for me to see apps or reports that are thousands of lines of code, with button definitions, UI bits, and user interaction definitions mixed in among the actual work of the app.\nWith presentation and processing layers that are smushed together, it’s really hard to read your code later or to test or log what’s happening inside your app.\nThe best way to separate the presentation layer is to check if you’ve got anything in your presentation layer that does anything beyond\n\nshowing things to the user\ncollecting interactions from the user\n\nCreating the things that are shown to the user or doing anything with the interactions shouldn’t be in the presentation layer. These should be deferred to the processing layer.\nOnce you’ve identified those things, they should be extracted into functions that are documented and tested – preferably in a package – and use those functions put into standalone scripts.\n\n\n\n\n\n\nTip\n\n\n\nMoving things out of the presentation layer is especially important if you’re writing a Shiny app. You really want to use the presentation layer to do reactive things and move all non-reactive interactions into the processing layer."
  },
  {
    "objectID": "chapters/sec1/1-2-proj-arch.html#aim-for-small-data-in-the-presentation-layer",
    "href": "chapters/sec1/1-2-proj-arch.html#aim-for-small-data-in-the-presentation-layer",
    "title": "2  Data Project Architecture Guidelines",
    "section": "2.3 Aim for small data in the presentation layer",
    "text": "2.3 Aim for small data in the presentation layer\nEverything is easy when your data is small because you can simply load it into your Python or R session as your code starts and never think about it again.\n“Real engineers” may scoff at this pattern, but don’t let their criticism dissuade you. If your data size is small and your project performance is good enough, just read in all of your data and operate on it live. Don’t over-complicate things. These days, this pattern often works well into the range of millions of rows.\nIt may be the case that your data isn’t small – but not all large data is created equal.\nTruly big data can’t fit into the memory on your computer all at once. Data that is actually big is pretty rare for most data science purposes.\nIt’s much more common to encounter medium data. You can load it into memory so it’s not actually big, but it’s big enough that loading it all makes your project’s performance too slow.\nDealing with medium or big data requires being somewhat clever and adopting a design pattern for big data. But being clever is hard.\nSo before you go being clever, it’s worth slowing down and asking yourself a few questions that might let you treat your data as small.\n\n2.3.1 Can I add pre-calculation or use a different project type?\nIf your data is truly big, it’s big. You could always get beefier hardware, but there are limits. But if your data is medium-sized, the thing keeping it from being small isn’t some esoteric hardware issue, its performance.\nAn app requires high performance. Someone is staring at their screen through a 90 second wait is going to think your project stinks.\nBut if you can pre-calculate a lookup table of values – or turn your app into a report that gets re-rendered on a schedule you can turn turn medium or even truly big data into a small data set in the presentation layer.\nThe degree to which you can do this depends a lot on the requirements of your presentation layer.\nTalking to your users and figuring out what cuts of the data they really care about can help you determine whether pre-calculation is feasible or whether you really need to load all the data into the presentation layer.\n\n\n2.3.2 Can I reduce data granularity?\nIf you can pre-calculate results and you’re still hitting performance issues, it’s always worth asking if your data can get smaller.\nLet’s think about a specific project to make this a little clearer.\nSuppose you work for a large retailer and are responsible for creating a dashboard of weekly sales. Your input data is a dataset of every item sold at every store going back for years. Clearly this isn’t naturally small data.\nAs you’re thinking about how to make the presentation layer data smaller, it’s worth keeping in mind that each additional dimension you allow users to cut the data multiplies the amount of data you need in the presentation layer.\nFor example, weekly sales at the department level, requires a lookup table as big as \\(\\text{number of weeks} * \\text{number of stores} * \\text{number of departments}\\). Even with a lot of stores and a lot of departments, you’re probably still squarely in the small data category.\nBut if you have to switch to a daily view, you multiply the amount of data you need by 7. If you break it out across 12 products, your data has to get 12 times bigger. And if you do both, it gets 84 times bigger. It’s not long before you’re back to a big data problem.\nTalking with your users about the tradeoffs between app performance and the number of data dimensions they need can identify opportunities to exclude dimensions and reduce your data size."
  },
  {
    "objectID": "chapters/sec1/1-2-proj-arch.html#big-data-patterns",
    "href": "chapters/sec1/1-2-proj-arch.html#big-data-patterns",
    "title": "2  Data Project Architecture Guidelines",
    "section": "2.4 Adopt a pattern to make big data small",
    "text": "2.4 Adopt a pattern to make big data small\nLet’s say you’ve made your presentation layer as small as possible or you’re trying to do your pre-calculation step to go from big data to small data. You need to figure out how to make your large data smaller.\nThe key insight is that you don’t want to pull all of the data into your Python or R session. Instead, you want to pull in only some of the data.\nHere are a few patterns to consider to make your data smaller. This isn’t an exhaustive list and each of these patterns will only work for some projects, but many can adopt one or more of these patterns.\n\n2.4.1 Be lazy with data pulls\nUp until now, we’ve been assuming that your project pulls in all of the data up front in an eager data pattern. This is often a good first cut at writing an app, as it’s much simpler than doing anything else.\nIf that won’t work for your project, you can try being lazy with your data pulls. In a lazy data pattern, you pull in only the data that’s needed when it’s needed.\nIf your project doesn’t always need all the data – especially if the data it needs depends on what the user does inside a session, it might be worthwhile to pull only exactly the data you need once the user interactions clarify what you need.\n\n\n2.4.2 Sample the data\nFor many tasks, especially machine learning ones, it may be adequate to work on only a sample of the data. In some cases like classification of highly imbalanced classes, it may actually work better to work on a sample of the data rather than the whole data set.\nSampling tends to work well when you’re trying to compute statistical attributes of your datasets. Computing averages or rates and creating machine learning models works just fine on samples of your data. Just be careful to be unbiased with your sampling and consider sampling stratification to make sure one weird sample doesn’t mess with your results.\nBut sampling doesn’t work well on counting tasks – it’s hard to count when you don’t have all the data!\n\n\n2.4.3 Chunk and pull\nIn some cases, there may be natural groups in your data. For example, in our retail dashboard example, it may be the case that we want to compute something by time frame or store or product. In this case, you could pull just that chunk of the data, compute what you need and move on to the next one.\nChunking works well for all kinds of tasks including building machine learning models and creating plots as long as the groups are cleanly separable. When they are, this is an example of an embarrassingly parallel task, which you can easily parallelize in Python or R.\nIf you don’t have distinct chunks in your data, it’s pretty hard to chunk the data.\n\n\n2.4.4 Push work to the data source\nIn most cases, actually transmitting the data from the data source to your project is the most costly step in terms of time. So basically anything you can do before you pull the data out should be done before you pull the data out.\nLet’s say you really have to provide a very high degree of granularity in your weekly sales dashboard. You can at least do any computations them in the data source and just pull the results back, as opposed to loading all the data and doing the computations in Python or R. More on how to do this in Chapter 3.\nThis tends to work quite well when you’re creating simple summary statistics and when your database is reasonably fast. If your data source is slow, or if you’re doing complicated machine learning tasks, you may not be able to push that work off to the data source."
  },
  {
    "objectID": "chapters/sec1/1-2-proj-arch.html#store-intermediate-artifacts-in-the-right-format",
    "href": "chapters/sec1/1-2-proj-arch.html#store-intermediate-artifacts-in-the-right-format",
    "title": "2  Data Project Architecture Guidelines",
    "section": "2.5 Store intermediate artifacts in the right format",
    "text": "2.5 Store intermediate artifacts in the right format\nAs you start breaking your processing layer into the different pieces, you’ll find that you have intermediate artifacts to pass the data from one stage to the next.\nIf all you’re producing is rectangular data frames (or vectors) and you have write access to a database, that’s what you should use.\nBut very often you don’t have write access to a database or you’ve got other sorts of artifacts that you need to save between steps and can’t go into a database, like machine learning models or rendered plots. In that case, you’ll need to choose how to store your data.\nFlat files are data files that can be moved around just like any other file on your computer. You can put them on your computer, and share them through tools like dropbox, google drive, scp, or more.\nThe most common is a comma separated value (csv) file, which is just a literal text file of the values in your data with commas as separators.2 You could open it in a text editor and read it if you wanted to.\nThe advantage of csvs is that they’re completely ubiquitous. Basically every programming language has some way to read in a csv file and work with it.\nOn the downside, csvs are completely uncompressed. That makes them quite large relative to other sorts of files and slow to read and write. Additionally, because csvs aren’t language-specific, complicated data types may not be preserved when saving to csv. For example, dates are often mangled going into a csv file and back.\nThey also can only hold rectangular data, so if you’re trying to save a machine learning model, a csv doesn’t make any sense.\nBoth R and Python have language-specific file types – pickle in Python and rds in R. These are nice because they include some amount of compression and preserve data types when you save a data frame. They also can hold non-rectangular data, which can be great if you want to save a machine learning model.\nIf you don’t have a database but are storing rectangular data, you should strongly consider using DuckDB. Its an in-memory database that’s great for analytics use cases. In contrast to a standard database that runs its own live process, there’s no overhead for setting up DuckDB. You just run it against flat files on disk (usually Parquet files), which you can move around like any other. And unlike a csv, pickle, or rds file, a DuckDB is query-able, so you only load the data you need into memory.\nIt’s hard to stress how cool DuckDB is. Data sets that were big just a few years ago are now medium or even small."
  },
  {
    "objectID": "chapters/sec1/1-2-proj-arch.html#choose-where-based-on-update-frequency",
    "href": "chapters/sec1/1-2-proj-arch.html#choose-where-based-on-update-frequency",
    "title": "2  Data Project Architecture Guidelines",
    "section": "2.6 Choose where based on update frequency",
    "text": "2.6 Choose where based on update frequency\nLet’s say you’ve done your data pre-calculation and have a data set you’re using for the presentation layer. You have to figure out where to keep it.\nWhere you store your data should be dictated by how often the data is updated.\nThe simplest answer is to put it in the presentation bundle, which is the code and assets that make up your presentation layer. For example, let’s say you’re building a simple Dash app, app.py.\nYou could create a project structure like this:\nmy-project/\n├─ app.py\n├─ data/\n│  ├─ my_data.csv\n│  ├─ my_model.pkl\nThis works well only if your data will be updated at the same cadence as the app or report itself. If your project is an annual report that will be rewritten when you update the data, this can work just great.\nBut if your data updates more frequently than your project code, you really want to put the data outside the project bundle.\nThere are a few ways you can do this. The most basic way is just to put the data on a location in your file system that isn’t inside the app bundle.\nBut when it comes to deployment, data on the file system can be complicated. If you’re writing your app and deploying it on the same server, then you can access the same directory. If not, you’ll need to worry about how to make sure that directory is also accessible on the server where you’re deploying your project.\nIf you’re not going to store the flat file on the filesystem and you’re in the cloud, the most common option for where it can go is in blob storage. Blob storage allows you to store and recall things by name.3 Each of the major cloud providers has blob storage – AWS’s has s3 (short for simple storage service), Azure has Azure Blob Store, and Google has Google Storage.\nThe nice thing about blob storage is that it can be accessed from anywhere that has access to the cloud. You can also control access using standard cloud identity management tooling, so you could control who has access using individual credentials or could just say that any request for a blob coming from a particular server would be valid.\nThere are packages in both R and Python for interacting with AWS that are very commonly used for getting access to s3 – {boto3} in Python, and {paws} in R.\nThere’s also the popular {pins} package in both R and Python that basically wraps using blob storage into neater code. It can use a variety of storage backends, including cloud blob storage, networked or cloud drives like Dropbox, Microsoft365 sites, and Posit Connect.\nIf you’re still early in your project lifecycle, a google sheet can be a great way to save and recall a flat file. I wouldn’t recommend a google sheet as a permanent home for data, but it can be a good intermediate step while you’re still figuring out what the right answer is for your pipeline.\nThe primary weakness of a google sheet – that it’s editable by someone who logs in – can also be an asset if that’s something you need."
  },
  {
    "objectID": "chapters/sec1/1-2-proj-arch.html#consider-auth-to-data-up-front",
    "href": "chapters/sec1/1-2-proj-arch.html#consider-auth-to-data-up-front",
    "title": "2  Data Project Architecture Guidelines",
    "section": "2.7 Consider auth to data up front",
    "text": "2.7 Consider auth to data up front\nIf everyone who views your project has the same permissions to see the data, life is easy. You can just allow the project access to the data and check for authorization to view the project.\nBut if you need to provide different data access to different users, you’re much more constrained. First off, you probably need to use an app rather than a report so that you can respond to which user is accessing the app.\nThen you have to figure out how you’re actually going to change data access based on who’s viewing the app.\nSometimes this can be accomplished in the app itself. Many app frameworks pass the username or user groups into the session, and you can write code that changes app behavior based on the user. For example, you can gate access to certain tabs or features of your app based on the user.\nSometimes you’ll actually have to pass database credentials along to the database. If this is the case for you, you’ll need to figure out how to establish the user’s database credentials, how to make sure those credentials stay only in the user’s session, and how those credentials get passed along to the database. This is most commonly done with technologies like Kerberos or OAuth and require coordination with an IT/Admin. More on this topic in Chapter 17."
  },
  {
    "objectID": "chapters/sec1/1-2-proj-arch.html#create-an-api-if-you-need-it",
    "href": "chapters/sec1/1-2-proj-arch.html#create-an-api-if-you-need-it",
    "title": "2  Data Project Architecture Guidelines",
    "section": "2.8 Create an API if you need it",
    "text": "2.8 Create an API if you need it\nIn the case of a general purpose three-layer app, it is almost always the case that the middle tier will be an application programming interface (API). In a data science app, separating processing logic into functions is often sufficient. But if you’ve got a long-running bit of business logic, like training an ML model, it’s often helpful to separate it into an API.\n\n\n\n\n\n\nNote\n\n\n\nYou may have heard the term REST API or REST-ful.\nREST is a set of architectural standards for how to build an API. An API that conforms to those standards is called REST-ful or a REST API.\nIf you’re using standard methods for constructing an API like R’s {plumber} package or {FastAPI} in Python, they’re going to be REST-ful – or at least close enough for standard usage.\n\n\nYou can basically think of an API as a “function as a service”. That is, an API is just one or more functions, but instead of being called within the same process that your app is running or your report is processing, it will run in a completely separate process.\nFor example, let’s say you’ve got an app that allows users to feed in input data and then generate a model based on that data. If you generate the model inside the app, the user will have the experience of pressing the button to generate the model and having the app seize up on them while they’re waiting. Moreover, other users of the app will find themselves affected by this behavior.\nIf, instead, the button in the app ships the long-running process to a separate API, it gives you the ability to think about scaling out the presentation layer separate from the business layer.\nLuckily, if you’ve written functions for your app, turning them into an API is trivial as packages like {fastAPI} and {plumber} let you turn a function into an API with just the addition of some specially-formatted comments."
  },
  {
    "objectID": "chapters/sec1/1-2-proj-arch.html#write-a-data-flow-chart",
    "href": "chapters/sec1/1-2-proj-arch.html#write-a-data-flow-chart",
    "title": "2  Data Project Architecture Guidelines",
    "section": "2.9 Write a data flow chart",
    "text": "2.9 Write a data flow chart\nOnce you’ve figured out the project architecture you need, it can be helpful to write a data flow chart.\nA data flow chart maps the different project components you’ve got into the three parts of the project and documents all the intermediate artifacts you’re creating along the way.\nOnce you’ve mapped your project, figuring out where the data should live and in what format will be much simpler.\nFor example, here’s a very simple data flow chart for the labs in this book. You may want to annotate your data flow charts with other attributes like data types, update frequencies, and where data objects live.\n\n\n\n\nflowchart LR\n    A[Palmer Penguins \\nData Package]\n    B[Model Creation Job] \n    C[Model Serving API]\n    D[Model Explorerer App]\n    E[EDA Report]\n    F[Model Creation Report]\n\n    subgraph Data\n        A\n    end\n\n    subgraph Processing\n        B --&gt;|Model| C\n    end\n\n    subgraph Presentation\n        D\n        E\n        F\n    end\n\n    A --&gt; B\n    B --&gt; F\n    A --&gt; E\n    C --&gt; D"
  },
  {
    "objectID": "chapters/sec1/1-2-proj-arch.html#comprehension-questions",
    "href": "chapters/sec1/1-2-proj-arch.html#comprehension-questions",
    "title": "2  Data Project Architecture Guidelines",
    "section": "2.10 Comprehension Questions",
    "text": "2.10 Comprehension Questions\n\nWhat are the layers of a three-layer application architecture? What libraries could you use to implement a three-layer architecture in R or Python?\nWhat are some questions you should explore to reduce the data requirements for your project?\nWhat are some patterns you can use to make big data smaller?\nWhere can you put intermediate artifacts in a data science project?\nWhat does it mean to “take data out of the bundle”?"
  },
  {
    "objectID": "chapters/sec1/1-2-proj-arch.html#lab2",
    "href": "chapters/sec1/1-2-proj-arch.html#lab2",
    "title": "2  Data Project Architecture Guidelines",
    "section": "2.11 Lab 2: Build the processing layer",
    "text": "2.11 Lab 2: Build the processing layer\nIn the last chapter, we did some EDA of the Palmer Penguins data set and also built an ML model. In this lab, we’re going to take that work we did and turn it into the actual presentation layer for our project.\n\n2.11.1 Step 1: Write the model outside the bundle using {vetiver}\nWhen we originally wrote our model.qmd script in Chapter 1, we didn’t save the model at all.\nIt seems likely that our model will get updated more frequently than our app, so we don’t want to store it in the app bundle. Later on, I’ll show you how to store it in the cloud. For now, I’m just going to store it in a directory on my computer.\nSince\nTo do it, I’m going to use the {vetiver} package, which is an R and Python package to version, deploy, and monitor a machine learning model.\nWe can take our existing model, turn it into a {vetiver} model, and save it to the /data/model folder with\n\n\nmodel.qmd\n\n\n```{python}\nfrom vetiver import VetiverModel\nv = VetiverModel(model, model_name='penguin_model', prototype_data=X)\n```\n\n## Save to Board\n\nIf /data/model doesn’t exist on your machine, you can create it, or use a directory that does exist.\nWhatever path you use, I’d recommend using an absolute file path, rather than a relative one.\n\n\n2.11.2 Step 2: Create an API to serve model predictions\nI’m going to say that I need real-time predictions from my model in this case, so I’ll serve the model from an API.\nAs the point of this lab is to focus on the architecture, I’m just going to use the auto-generation capabilities of {vetiver}. If you’re interested in getting better at writing APIs in general, I encourage you to consult the documentation for {plumber} or {fastAPI}.\nIf you’ve closed your modeling code, you can get your model back from your pin with:\n\nb = pins.board_folder('data/model', allow_pickle_read=True)\nv = VetiverModel.from_pin(b, 'penguin_model')\n\nThen you can auto-generate a {fastAPI} from this model with\n\napp = VetiverAPI(v, check_prototype=True)\n\nYou can run this in your Python session with app.run(port = 8080). You can then access run your model API by navigating to http://localhost:8080 in your browser.\nYou can play around with the front end there, including trying the provided examples."
  },
  {
    "objectID": "chapters/sec1/1-2-proj-arch.html#footnotes",
    "href": "chapters/sec1/1-2-proj-arch.html#footnotes",
    "title": "2  Data Project Architecture Guidelines",
    "section": "",
    "text": "Though I’ll argue in Chapter 4 that you should always use a literate programming tool like Quarto, R Markdown, or Jupyter Notebook.↩︎\nThere are other delimitors you can use. Tab separated value files (tsv) are something you’ll see occasionally.↩︎\nThe term blob is great to describe the thing you’re saving in blob storage, but it’s actually an abbreviation for binary large object. I think that’s very clever.↩︎"
  },
  {
    "objectID": "chapters/sec1/1-3-data-access.html#accessing-and-using-databases",
    "href": "chapters/sec1/1-3-data-access.html#accessing-and-using-databases",
    "title": "3  Using databases and data APIs",
    "section": "3.1 Accessing and using databases",
    "text": "3.1 Accessing and using databases\nDatabases are defined by their query-able interface, usually through structured query language (SQL).\n\n\n\n\n\n\nNote\n\n\n\nThere are many, many different kinds of databases, and choosing the right one for your project is beyond the scope of this book. One recommendation: open source PostgreSQL (Postgres) is a great place to start for most general-purpose data science tasks.\n\n\nRegardless of which database you’re using, you’ll open a connection by creating a connection object at the outset of your code. You’ll then use this object to send SQL queries – either literal ones you’ve written, or the output of a package that generates SQL code, like {sqlalchemy} in Python and {dplyr} in R.\nFor example, in Python you might write the following to connect to a Postgres database:\n\nimport psychopg2\n\ncon = psycopg2.connect()\n\nIn R, it might look like this:\n\ncon &lt;- DBI::dbConnect(RPostgres::postgres())\n\nIn order to develop a mental model for working with databases, let’s reverse engineer this example code.\nPython or R both have standard connection APIs that define operations like connecting and disconnecting, sending queries, and retrieving results.\nIn Python, packages for individual databases like {psychopg2} directly implement the API, which is why the example above calls the connect() method of the {psychopg2} package.\nIn R, the API is split into two parts. The {DBI} package (short for database interface) implements the actual connections. It works with a database driver package, whtich is the first argument to DBI::dbConnect(). Packages that implement the {DBI} interface are called DBI-compliant.\n\n\n\n\n\n\nNote\n\n\n\nThere are Python packages that don’t implement the connections API, and there are non DBI-compliant database connector packages in R. These packages may work for you, but I’d recommend sticking with the standard route.\n\n\nIn a lot of cases, there will be a Python or R package that directly implements your database driver. For example, when you’re connecting to a Postgres database, there are Postgres-specific connectors – {psychopg2} in Python and {RPostgres} in R. For Spark, you’ve got {pyspark} and {sparklyr}.\nIf there’s a package specific to your database, it’s probably faster and may provide additional database-specific functionality than other options.\nIf there isn’t a database-specific package that directly implements the driver, you’ll need to use a generic system driver in concert with a Python or R package that can interface with system drivers.\nIn that case, the example code above might look like in Python\n\nimport pyodbc\n\ncon = pyodbc.connect(\"DSN=MY_DSN\")\n\nIn R, it might look like this:\n\ncon &lt;- DBI::dbConnect(odbc::odbc(), dsn = \"MY_DSN\")\n\nWhile performance sometimes isn’t as good for system drivers, the tradeoff is that IT/Admins can pre-configure details of the connection in a data source name (DSN). If one is pre configured for you, you don’t have to remember – or even learn – the database name, host, and port, even username and password if they’re shared. All you need in your code is the DSN name, which is MY_DSN in the example above.\nSystem drivers come in two main varieties Java Database Connectivity (JDBC) and Open Database Connectivity (ODBC).\nIn Python, {pyodbc} is the main package for using ODBC connections and {JayDeBeApi} for connecting using JDBC. In R, {odbc} is the best package for using system ODBC connections and {RJDBC} is the standard way to use JDBC.\n\n\n\n\n\n\nTip\n\n\n\nIf you’re using R and have the choice between JDBC and ODBC, I strongly recommend ODBC. JDBC requires an extra hop through Java and the {rJava} package, which is painful to configure.1"
  },
  {
    "objectID": "chapters/sec1/1-3-data-access.html#connecting-to-apis",
    "href": "chapters/sec1/1-3-data-access.html#connecting-to-apis",
    "title": "3  Using databases and data APIs",
    "section": "3.2 Connecting to APIs",
    "text": "3.2 Connecting to APIs\nSome data sources come in the form of an API (application programming interface).\nIn the data science world, APIs are most often used to provide data feeds and on-demand predictions from machine learning models.\nIt’s common to have Python or R packages that wrap APIs, so you just write Python or R code without needing to think about the API underneath. The usage of these patterns often looks similar to databases – you create and use a connection object that stores the connection details. If your API has a package like this, you should just use it.\nIf you’re consuming a private API at your organization, a helper package probably doesn’t exist or you may have to write it yourself.\n\n\n\n\n\n\nNote\n\n\n\nThere’s increasingly good tooling to auto-generate packages based on API documentation, so you may never have to write an API wrapper package by hand. It’s still helpful to understand how it works.\n\n\nIf find yourself having to call an API directly, you can use the {requests} package in Python or {httr2} in R.\nThese packages idiomatic R and Python ways to call APIs. It’s worth understanding that they’re purely syntactic sugar. There’s nothing special about calling an API from inside Python or R versus using the curl command on the command line and you can go back and forth as you please.\n\n3.2.1 What’s in an API?\nAPIs are the standard way for two computer systems to communicate with each other. It’s an extremely general term that describes the definition of machine-to-machine communication.\nThe APIs used by data scientists are usually http-based REST-ful APIs. What exactly that means isn’t really important, but the rest of this section just addresses the subset of the wild world of APIs you’re likely to encounter as a data scientist.\nhttp operates on a request-response model. So when you use an API, you send a request to the API and it sends a response back.\nThe best way to learn about a new API is to read the documentation, which will include a lot of details about how to use it. Let’s go through some of the most salient ones.\n\n\n3.2.2 API Endpoints and Paths\nEach request to an API is directed to a specific endpoint. An API can have many endpoints, each of which you can think of like a function in a package. Each endpoint lives on a path, which is where you find that particular endpoint.\nFor example, if you did the lab in Chapter 2 and used {vetiver} to create an API for serving the penguin mass model, you found your API at http://localhost:8080. By default, you went to the root path at / and found the API documentation there.\nAs you scrolled the documentation, there were two endpoints – /ping and /predict. Those paths are relative to the root, so you could access /ping at http://localhost:8080/ping.\n\n\n3.2.3 HTTP verbs\nWhen you make a request over HTTP, you are asking a server to do something. The http verb, also known as the request method, describes the type of operation you’re asking for. Each endpoint has one or more verbs that it knows how to use.\nIf you look at the penguin mass API, you’ll see that /ping is a GET endpoint and /predict is a POST. This isn’t coincidence. I’d approximate that 95% of the API endpoints you’ll use as a data scientist are GET and POST, which respectively fetch information from the server and provide information to the server.\nTo round out the basic http verbs you might see, PUT and PATCH change or update something and DELETE (you guessed it) deletes something. There are more esoteric ones you’ll probably never see.\n\n\n3.2.4 Request parameters and bodies\nLike a function in a package, each endpoint accepts specific arguments in a required format. Again, like a function, some arguments may be optional and some may be required.\nFor GET requests, the arguments are specified via query parameters that end up embedded in the URL after a ?. So if you ever see a URL in your browser that looks like ?first_name=alex&last_name=gold, those are query parameters.\nFor POST, PUT, and PATCH requests, arguments are provided in a body, which is usually formatted as JSON.2 Both {httr2} and {requests} have built-in functionality for converting standard Python and R data types to their JSON equivalents, but it can sometimes take some experimentation to figure out exactly how to match the argument format. Experimenting with conversions using {json} in Python and {jsonlite} in R can be very useful.\n\n\n3.2.5 (Auth) Headers\nYou will need to figure out how to authenticate to the API. The most common forms of authentication are a username and password combination, an API key, or an OAuth token.\nAPI keys and OAuth tokens are often associated with particular scopes. Scopes are permissions to do particular things. For example, an API key might be scoped to have GET access to a given endpoint, but not POST access.\nRegardless of your authentication type, it will be provided in a header to your API call. Your API documentation will tell you how to provide your username and password, API key, or token to the API in a header. Both {requests} and {httr2} provide easy helpers for adding authentication headers and also general ways to set headers if you need to.\nAside from authentication, headers are also used for a variety of different metadata like the type of machine that is sending the request and cookies that are set. You’ll rarely interact directly with these.\n\n\n3.2.6 Request Status Codes\nThe first thing you’ll consult when you get a result back is the status code. Status codes indicate what happened with your request to the server. You always hope to see 200 codes, which indicate a successful response.\nThere are also a two common types of error codes. 4xx codes indicate that there’s a problem with your request and the API couldn’t understand what you were asking. 5xx codes indicate that your request was fine, but some sort of error happened in processing your request.\nThere’s a cheatsheet below with some other codes and what they mean.\n\n\n3.2.7 Response Bodies\nThen there’s the actual contents of the response in the body. You’ll need to turn the body into a Python or R object you can work with.\nMost often, bodies are in JSON and you’ll decode them with {json} or {jsonlite}. Usually JSON is the default and you may be given the option to specify something else if you’ve got a preference.\n\n\n3.2.8 Common API patterns\nHere are a couple of common API patterns that are good to be familiar with:\n\nPagination – many data-feed APIs implement pagination. A paginated API returns only a certain number of results at a time to keep data sizes modest. You’ll need to figure out how to get all the pages back when you make a request.\nJob APIs – HTTP is designed for relatively quick request-response cycles. So if your API kicks off a long-running job, it’s rare to wait until the job is done to get a response. Instead, the API immediately returns an acknowledgement and a job-id which you can use to poll a job-status endpoint to check how things are going and eventually find your result.\nMultiple Verbs – a single endpoint often accepts multiple verbs – for example a GET and a POST at the same endpoint for getting and setting the data that endpoint stores."
  },
  {
    "objectID": "chapters/sec1/1-3-data-access.html#env-vars",
    "href": "chapters/sec1/1-3-data-access.html#env-vars",
    "title": "3  Using databases and data APIs",
    "section": "3.3 Environment variables to secure data connections",
    "text": "3.3 Environment variables to secure data connections\nWhen you take an app to production, authenticating to your data source while keeping your secrets secure is crucial.\nThe single most important thing you can do to secure your credentials is to avoid ever putting credentials in your code. Your username and password or API key should never appear in your code.\nThe simplest way to provide credentials without the values appearing in your code is with an environment variable. Environment variables are set before your code starts – sometimes from completely outside Python or R.\n\n3.3.1 Getting environment variables\nThe power of using an environment variable is that you reference them by name. Just sharing that there’s an environment variable called API_KEY doesn’t reveal anything secret, so if your code just includes environment variables, it’s completely safe to share with others.\n\n\n\n\n\n\nNote\n\n\n\nIt is convention to make environment variable names in all caps with words separated by underscores. The values are always simple character values, though these can be cast to some other type inside R or Python.\n\n\nIn Python, you can read environment variables from the os.environ dictionary or by using os.getenv(\"&lt;VAR_NAME&gt;\"). In R, you can get environment variables with Sys.getenv(\"&lt;VAR_NAME&gt;\").\nIt’s common to provide environment variables directly to functions as arguments, though you can also put the values in normal Python or R variables and use them from there.\n\n\n3.3.2 Setting environment variables\nThe most common way to set environment variables in a development environment is to load secrets from a plain text file. In Python, environment variables are usually set by reading a .env file into your Python session. The {python-dotenv} package is a good choice for doing this.\nR automatically reads the .Renviron file as environment variables and also sources the .Rprofile file, where you can set environment variables with Sys.setenv(). I personally prefer putting everything in .Rprofile for simplicity – but that’s not a universal opinion.\nSome organizations don’t ever want credentials files in plain text. After all, if someone stole a plain text secrets file, there’s nothing to stop them from using them.\nThere are packages in both R and Python called {keyring} that allow you to use the system keyring to securely store environment variables and recall them at runtime.\nSetting environment variables in production is a little harder.\nJust moving your secrets from your code into a different file you push to prod is still bad. And using {keyring} in a production environment is quite cumbersome.\nYour production environment may provide environment management tools. For example, GitHub Actions and Posit Connect both provide you the ability to set secrets that aren’t visible to the users, but are accessible to the code at runtime in an environment variable.\nIncreasingly, organizations are using token-based authorization schemes that just exchange one cryptographically secure token for another, never relying on credentials at all. The tradeoff for the enhanced security is that they can be difficult to implement, likely requiring coordination with an IT/Admin to use technologies like Kerberos or OAuth. There’s more on how to do that in Chapter 17."
  },
  {
    "objectID": "chapters/sec1/1-3-data-access.html#data-connection-packages",
    "href": "chapters/sec1/1-3-data-access.html#data-connection-packages",
    "title": "3  Using databases and data APIs",
    "section": "3.4 Data Connection Packages",
    "text": "3.4 Data Connection Packages\nIt’s very common for organizations to write their own data connector packages in Python or R that include all of the shared connection details so users don’t have to remember them. If everyone has their own credentials, it’s also nice if those packages set standard names for the environment variables so they can be more easily set in production.\nWhether you’re using R or Python, the function in your package should return the database connection object for people to use.\nHere’s an example of what that might look like if you were using a Postgres database from R:\n\n#' Return a database connection\n#'\n#' @param user username, character, defaults to value of DB_USER\n#' @param pw password, character, defaults to value of DB_PW\n#' @param ... other arguments passed to \n#' @param driver driver, defaults to RPostgres::Postgres\n#'\n#' @return DBI connection\n#' @export\n#'\n#' @examples\n#' my_db_con()\nmy_db_con &lt;- function(\n    user = Sys.getenv(\"DB_USER\"), \n    pw = Sys.getenv(\"DB_PW\"), \n    ..., \n    driver = RPostgres::Postgres()\n) {\n  DBI::dbConnect(\n    driver = driver,\n    dbname = 'my-db-name', \n    host = 'my-db.example.com', \n    port = 5432, \n    user = user,\n    password = pw, \n    ...\n  )\n}\n\nNote that the function signature defines default environment variables that will be consulted. If those environment variables are set ahead of time by the user, this code will just work."
  },
  {
    "objectID": "chapters/sec1/1-3-data-access.html#comprehension-questions",
    "href": "chapters/sec1/1-3-data-access.html#comprehension-questions",
    "title": "3  Using databases and data APIs",
    "section": "3.5 Comprehension Questions",
    "text": "3.5 Comprehension Questions\n\nDraw two mental map for connecting to a database. One usinga database driver in a Python or R package vs an ODBC or JDBC driver. You should (at a minimum) include the nodes database package, DBI (R only), driver, system driver, ODBC, JDBC, and database.\nDraw a mental map for using an API from R or Python. You should (at a minimum) include nodes for {requests}/{httr2}, request, http verb/request method, headers, query parameters, body, json, response, and response code.\nHow can environment variables be used to keep secrets secure in your code?"
  },
  {
    "objectID": "chapters/sec1/1-3-data-access.html#lab-3-use-a-database-and-an-api",
    "href": "chapters/sec1/1-3-data-access.html#lab-3-use-a-database-and-an-api",
    "title": "3  Using databases and data APIs",
    "section": "3.6 Lab 3: Use a database and an API",
    "text": "3.6 Lab 3: Use a database and an API\nIn this lab, we’re going to build out both the data layer and the presentation layer for our penguin mass model exploration. We’re going to create an app to explore the model, which will look like this: \nLet’s start by moving the data into a real data layer.\n\n3.6.1 Step 1: Put the data in DuckDB\nLet’s start by moving the data into a DuckDB database and use it from there for the modeling and EDA scripts.\nTo start, let’s load the data.\nHere’s what that looks like in R:\ncon &lt;- DBI::dbConnect(duckdb::duckdb(), dbdir = \"my-db.duckdb\")\nDBI::dbWriteTable(con, \"penguins\", palmerpenguins::penguins)\nDBI::dbDisconnect(con)\nOr equivalently, in Python:\nimport duckdb\nfrom palmerpenguins import penguins\n\ncon = duckdb.connect('my-db.duckdb')\ndf = penguins.load_penguins()\ncon.execute('CREATE TABLE penguins AS SELECT * FROM df')\ncon.close()\nNow that the data is loaded, let’s adjust our scripts to use the database.\nIn R, we are just going to replace our data loading with connecting to the database. Leaving out all the parts that don’t change, it looks like\n\n\neda.qmd\n\n\ncon &lt;- DBI::dbConnect(\n  duckdb::duckdb(), \n  dbdir = \"my-db.duckdb\"\n  )\ndf &lt;- dplyr::tbl(con, \"penguins\")\n\nWe also need to call to DBI::dbDisconnect(con) at the end of the script.\nBecause we wrote our data processing code in {dplyr}, we actually don’t have to change anything. Under the hood, {dplyr} can switch seamlessly to a database backend, which is really cool.\n\n\neda.qmd\n\ndf %&gt;%\n  group_by(species, sex) %&gt;%\n  summarise(\n    across(\n        ends_with(\"mm\") | ends_with(\"g\"),\n      \\(x) mean(x, na.rm = TRUE)\n      )\n    ) %&gt;%\n  dplyr::collect() %&gt;%\n  knitr::kable()\n\nIt’s not necessary, but I’ve added a call to dplyr::collect in line 31. It will be implied if I don’t put it there manually, but it helps make obvious that all the work before there has been pushed off to the database. Only the result of this code is coming back to the R process. Obviously it doesn’t matter for this small dataset, but this would be a huge benefit if the dataset were much larger.\nIn Python, we’re just going to load the entire dataset into memory for modeling, so the line loading the dataset changes to\n\n\nmodel.qmd\n\ncon = duckdb.connect('my-db.duckdb')\ndf = con.execute(\"SELECT * FROM penguins\").fetchdf().dropna()\ncon.close()\n\n\nNow let’s switch to figuring out the connection we’ll need to our processing layer in the presentation layer.\n\n\n3.6.2 Step 2: Call the model API from code\nBefore you start, make sure the API is running on your machine from the last lab.\n\n\n\n\n\n\nNote\n\n\n\nI’m assuming it’s running on port 8080 in this lab. If you’ve put it somewhere else, change the 8080 in the code below to match the port on your machine.\n\n\nIf you want to call the model in code, you can use any http request library. In R you should use httr2 and in Python you should use requests.\nHere’s what it looks like to call the API in Python\n\nimport requests\n\nreq_data = {\n  \"bill_length_mm\": 0,\n  \"species_Chinstrap\": False,\n  \"species_Gentoo\": False,\n  \"sex_male\": False\n}\nreq = requests.post('http://127.0.0.1:8080/predict', json = req_data)\nres = req.json().get('predict')[0]\n\nor equivalently in R\n\nreq &lt;- httr2::request(\"http://127.0.0.1:8080/predict\") |&gt;\n  httr2::req_body_json(\n    list(\n      \"bill_length_mm\" = 0,\n      \"species_Chinstrap\" = FALSE,\n      \"species_Gentoo\" = FALSE,\n      \"sex_male\" = FALSE\n    )\n  ) |&gt;\n  httr2::req_perform()\nres &lt;- httr2::resp_body_json(r)$predict[[1]]\n\nNote that there’s no translation necessary to send the request. The {requests} and{httr2} packages automatically know what to do with the Python dictionary and the R list.\nGetting the result back takes a little more work to find the right spot in the JSON returned. This is quite common.\n\n\n\n\n\n\nNote\n\n\n\nThe {vetiver} package also includes the ability to auto-query a {vetiver} API. I’m not using it here to expose the details of calling an API.\n\n\nNow, let’s take this API-calling code and build the presentation layer around it.\n\n\n3.6.3 Step 3: Build a shiny app\nWe’re going to use the {shiny} package, which is an R and Python package for creating interactive web apps using just Python code. If you don’t know much about {shiny}, you can choose to just blindly follow the examples here, or you could spend some time with the Mastering Shiny book to learn to use it yourself.\nEither way, an app that looks like the picture above would look like this in Python\n\n\napp.py\n\nfrom shiny import App, render, ui, reactive\nimport requests\n\napi_url = 'http://127.0.0.1:8080/predict'\n\napp_ui = ui.page_fluid(\n    ui.panel_title(\"Penguin Mass Predictor\"), \n    ui.layout_sidebar(\n        ui.panel_sidebar(\n            [ui.input_slider(\"bill_length\", \"Bill Length (mm)\", 30, 60, 45, step = 0.1),\n            ui.input_select(\"sex\", \"Sex\", [\"Male\", \"Female\"]),\n            ui.input_select(\"species\", \"Species\", [\"Adelie\", \"Chinstrap\", \"Gentoo\"]),\n            ui.input_action_button(\"predict\", \"Predict\")]\n        ),\n        ui.panel_main(\n            ui.h2(\"Penguin Parameters\"),\n            ui.output_text_verbatim(\"vals_out\"),\n            ui.h2(\"Predicted Penguin Mass (g)\"), \n            ui.output_text(\"pred_out\")\n        )\n    )   \n)\n\ndef server(input, output, session):\n    @reactive.Calc\n    def vals():\n        d = {\n            \"bill_length_mm\" : input.bill_length(),\n            \"sex_Male\" : input.sex() == \"Male\",\n            \"species_Gentoo\" : input.species() == \"Gentoo\", \n            \"species_Chinstrap\" : input.species() == \"Chinstrap\"\n\n        }\n        return d\n    \n    @reactive.Calc\n    @reactive.event(input.predict)\n    def pred():\n        r = requests.post(api_url, json = vals())\n        return r.json().get('predict')[0]\n\n    @output\n    @render.text\n    def vals_out():\n        return f\"{vals()}\"\n\n    @output\n    @render.text\n    def pred_out():\n        return f\"{round(pred())}\"\n\napp = App(app_ui, server)\n\nAnd like this in R\n\n\napp.R\n\nlibrary(shiny)\n\napi_url &lt;- \"http://127.0.0.1:8080/predict\"\n\nui &lt;- fluidPage(\n  titlePanel(\"Penguin Mass Predictor\"),\n\n  # Model input values\n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\n        \"bill_length\",\n        \"Bill Length (mm)\",\n        min = 30,\n        max = 60,\n        value = 45,\n        step = 0.1\n      ),\n      selectInput(\n        \"sex\",\n        \"Sex\",\n        c(\"Male\", \"Female\")\n      ),\n      selectInput(\n        \"species\",\n        \"Species\",\n        c(\"Adelie\", \"Chinstrap\", \"Gentoo\")\n      ),\n      # Get model predictions\n      actionButton(\n        \"predict\",\n        \"Predict\"\n      )\n    ),\n\n    mainPanel(\n      h2(\"Penguin Parameters\"),\n      verbatimTextOutput(\"vals\"),\n      h2(\"Predicted Penguin Mass (g)\"),\n      textOutput(\"pred\")\n    )\n  )\n)\n\nserver &lt;- function(input, output) {\n  # Input params\n  vals &lt;- reactive(\n    list(\n      bill_length_mm = input$bill_length,\n      species_Chinstrap = input$species == \"Chinstrap\",\n      species_Gentoo = input$species == \"Gentoo\",\n      sex_male = input$sex == \"Male\"\n    )\n  )\n\n  # Fetch prediction from API\n  pred &lt;- eventReactive(\n    input$predict,\n    httr2::request(api_url) |&gt;\n      httr2::req_body_json(vals()) |&gt;\n      httr2::req_perform() |&gt;\n      httr2::resp_body_json(),\n    ignoreInit = TRUE\n  )\n\n  # Render to UI\n  output$pred &lt;- renderText(pred()$predict[[1]])\n  output$vals &lt;- renderPrint(vals())\n}\n\n# Run the application\nshinyApp(ui = ui, server = server)\n\nOver the next few chapters, we’re going to implement more architectural best practices for the app, and in [Chapter @env-as-code] we’ll actually go to deployment."
  },
  {
    "objectID": "chapters/sec1/1-3-data-access.html#cheat-http",
    "href": "chapters/sec1/1-3-data-access.html#cheat-http",
    "title": "3  Using databases and data APIs",
    "section": "3.7 HTTP Code Cheatsheet",
    "text": "3.7 HTTP Code Cheatsheet\nAs you work more with http traffic, you’ll learn some of the common codes. Here’s a cheatsheet for some of the most frequent you’ll see.\n\n\n\n\n\n\n\nCode\nMeaning\n\n\n\n\n200\nEveryone’s favorite, a successful response.\n\n\n3xx\nYour query was redirected somewhere else, usually ok.\n\n\n4xx\nErrors with the request\n\n\n400\nBad request. This isn’t a request the server can understand.\n\n\n401 and 403\nUnauthorized or forbidden. Required authentication hasn’t been provided.\n\n\n404\nNot found. There isn’t any content to access here.\n\n\n5xx\nErrors with the server once your request got there.\n\n\n500\nGeneric server-side error. Your request was received, but there was an error processing it.\n\n\n504\nGateway timeout. This means that a proxy or gateway between you and the server you’re trying to access timed out before it got a response from the server."
  },
  {
    "objectID": "chapters/sec1/1-3-data-access.html#footnotes",
    "href": "chapters/sec1/1-3-data-access.html#footnotes",
    "title": "3  Using databases and data APIs",
    "section": "",
    "text": "I have heard that some write operations may be faster with a JDBC driver than an ODBC one. I would argue that if you’re doing enough writing to a database that speed matters, you probably should be using database-specific data loading tools, not just writing from R or Python.↩︎\nIn a lot of cases, people use POST for things that look like GETs to my eyes. The reason is request bodies. GET endpoints only recently started allowing bodies – and it’s still discouraged. In the {vetiver} API example, I think of fetching results from an ML model as a GET-type operation, but it uses a POST because it also uses a body in the query.↩︎"
  },
  {
    "objectID": "chapters/sec1/1-4-monitor-log.html#observing-correctness",
    "href": "chapters/sec1/1-4-monitor-log.html#observing-correctness",
    "title": "4  Logging and Monitoring",
    "section": "4.1 Observing Correctness",
    "text": "4.1 Observing Correctness\nObservability of general purpose software is primarily concerned with the operational qualities of the software. A software engineer wants to know whether their software is using too much RAM or CPU, whether its fast enough, and whether its crashed.\nFor a general purpose software engineer, an uncaught exception that makes the software crash is about as bad as it gets.\nBut for a data scientist there’s something even scarier – an issue that doesn’t result in code failure but yields incorrect answers. Data joins usually complete even if the merge quality is really bad. Model APIs will return a prediction even if the prediction is very, very bad.\nIt’s hard to check the actual correctness of the numbers and figures work work returns because you’re (basically by definition) doing something novel. So you’re basically left putting process metrics in place that can help reveal a problem before it surfaces.\nOne important tool you have in your toolbox is correctly architecting your project. Jobs are generally much easier to check for correctness than presentation layers. By moving as much processing as possible out of the presentation layer and into the data and processing layers, you can make it easier to observe.\nMoreover, you’re already very familiar with tools for literate programming like Jupyter Notebooks, R Markdown Documents, and Quarto Documents.\nOne of my spicier opinions is that all jobs should be in a literate programming format. These tools, when used well, intersperse code, commentary, and output, which is one of the best ways of observing the correctness of a job.\nOn a job, there are three particular things I always monitor.\nThe first is the quality of data joins. Based on the number of rows (or unique ids), you know how many rows should be in the data set after a join. Checking that the joined data matches expectations can reveal many data quality issues just waiting to ruin your day.\nThe second checking is cross-tabulations before and after recoding a categorical variable. Making sure the recode logic does what you think and that the values coming in aren’t changing over time is always worth the effort.\nThe last is goodness-of-fit metrics of an ML model in production. There are many, many frameworks and products for monitoring model quality and model drift once your model is in production. I don’t have strong opinions on these other than that you need to use one if you’ve got a model that’s producing results you hope to rely on."
  },
  {
    "objectID": "chapters/sec1/1-4-monitor-log.html#observing-operations",
    "href": "chapters/sec1/1-4-monitor-log.html#observing-operations",
    "title": "4  Logging and Monitoring",
    "section": "4.2 Observing Operations",
    "text": "4.2 Observing Operations\nNow let’s turn to the issue of observing the operational qualities of your code. The operational qualities of your project are things like the system resources its consuming, the number of users, and user interactions just before an error occurred.\nThe first step to making your app or API observable is to add logging. You may be used to just adding print statements throughout your code. And, honestly, this is far better than nothing. But purpose-built tooling for logging includes ways to apply consistent formats, emit logs in useful ways, and provide visibility into severity of issues.\nThere are great logging packages in both Python and R. Python’s logging package is standard. There is not a standard logging package in R, but I recommend {log4r}.\nThese packages – and basically every other logging package – work very similarly. At the outset of your code, you’ll create and parameterize a log session that persists as long as the Python or R session. When your code does something you want to love, you’ll you’ll use the log session to write log statements. When the log statement runs, it creates a log entry.\nFor example, here’s what logging for an app starting up might look like in Python\n\n\n\napp.py\n\nimport logging\n\n# Configure the log object\nlogging.basicConfig(\n    format='%(asctime)s - %(message)s',\n    level=logging.INFO\n)\n\n# Log app start\nlogging.info(\"App Started\")\n\n\nAnd here’s what that looks like using {log4r}\n\n\n\napp.R\n\n# Configure the log object\nlog &lt;- log4r::logger()\n\n# Log app start\nlog4r::info(log, \"App Started\")\n\n\nWhen the R or Python interpreter hits either of these lines, it will create a log entry that looks something like this:\n2022-11-18 21:57:50 INFO App Started\nLike all log entries, this entry has three components:\n\nThe log metadata is data what the logging library automatically includes on every entry. It is configured when you initialize logging. In the example above, the only metadata is the timestamp. Log metadata can include additional information, like which server you’re running on.\nThe second component is the log level. The log level indicates the severity of the event you’re logging. In the example above the log level was INFO.\nThe last component is the log data, which provides details on the event you want to log – App Started in this case.\n\n\n4.2.1 Understanding log levels\nThe log level indicates how serious the logged event is. Most logging libraries have 5-7 log levels. As you’re writing statements into your code, you’ll have to think carefully about the appropriate logging level for a given event.\nBoth the Python {logging} library and {log4r} have five levels from least to most scary:\n\nDebug: what the code was doing in detail that will only make sense to someone who really knows the code. For example, you might include which function ran and with what arguments in a debug log.\nInfo: something normal happened in the app. Info statements record things like starting and stopping, successfully making database and other connections, and runtime configuration options.\nWarn/Warning: an unexpected application issue that isn’t fatal. For example, you might include having to retry doing something or noticing that resource usage is high. If something were to go wrong later, these might be helpful breadcrumbs to look at.\nError: an issue that will make an operation not work, but that won’t bring down your app. An example might be a user submitting invalid input and the app recovering.\nCritical: an error so big that the app itself shuts down. This is the SOS your app sends as it shuts down. For example, if your app cannot run without a connection to an outside service, you might log an inability to connect as a Critical error.\n\nWhen you initialize your logging session, you’ll set your log level, which is the least critical level you want to see in the session. In development, you probably want to log everything down to the debug level, while that probably isn’t ideal in prod.\n\n\n4.2.2 Configuring log formats and log handling\nWhen you initialize your logging session, you’ll choose where logs will be written and in what format. You’ll configure the format with a formatter or layout and where it goes with a handler or an appender.\nFor most logging frameworks, the default is to emit logs to the console in plain text.\nFor example, a plain text log of an app starting might put this on your console\n2022-11-18 21:57:50 INFO App Started\nYou’ll decide the format of your log based on how you’re planning to consume them.\nPlain text logs is a great choice if humans are going to be directly reading them. If you’re shipping your logs off to an aggregation service, you might prefer to have structured logs.\nThe most common structured logging format is JSON, though YAML and XML are often options. If you used JSON logging, the same record might be emitted as\n{\n  \"time\": \"2022-11-18 21:57:50\",\n  \"level\": \"INFO\", \n  \"data\": \"App Started\"\n}\nWhere your logs go should be determined by where your code is running.\nIn development, printing logs for the console makes it easy to iterate quickly.\nIn production, the most common way to handle logs is to append them to a file. It makes them easy for humans to access and many tools for aggregating and consuming logs are comfortable watching a file and aggregating lines as they are written.\nIf you are emitting logs to file, you may also want to consider how long those logs stay around.\nLog rotation is the process of periodically creating new log files, storing old logs for a set retention period, and deleting files outside that period. A common log rotation pattern is to have a log file that lasts for 24 hours, is retained for 30 days, and is then deleted.\nThe Python {logging} library does log rotation itself. {log4r} does not, but there is a Linux library called logrotate that you can use in concert with {log4r}.1\nIf you’re running in a Docker container you don’t want to write to a file on disk. As you’ll learn more about in Chapter 6, anything that lives inside a Docker container is ephemeral. This is obviously bad if you’re writing a log that might contain clues for why a Docker container was unexpectedly killed.\nIn that case, it’s common practice for a service running in a container to emit logs inside the container and then have some sort of more permanent service collecting the logs outside. This is usually accomplished by sending normal operating logs to go to stdout (usually pronounced standard out) and failures to go to stderr (standard error).\nIt’s also possible you want to do something else completely custom with your logs. This is most common for critical or error logs. For example, you may want to send an email, slack, or text message immediately if your system emits a high-level log message.\nIt’s also very common to have different format and location settings in development vs in production. The most common way to enable different logging configurations in different environments is with config files and environment variables. More on how to use these tools in Chapter 1.\n\n\n4.2.3 Working with Metrics\nThe most common place to see metrics in a data science context is when deploying and monitoring ML models in production. Additionally, monitoring ETL data quality is ripe for more monitoring.\nIf you are going to configure metrics emission or consumption, most modern metrics stacks are built around the open source tools Prometheus and Grafana.\nPrometheus is an open source monitoring tool that makes it easy to store metrics data, query that data, and alert based on it. Grafana is an open source dashboarding tool that sits on top of Prometheus to do visualization of the metrics. They are usually used together to do monitoring and visualization of metrics.\nYou can run Prometheus and Grafana yourself, but Grafana Labs provides a generous free tier for their SaaS service. This is great because you can just set up their service and point your app to it.\nBecause the Prometheus/Grafana stack started out in the DevOps world, they are most optimized to do monitoring of a whole server or fleet of servers – but it’s not hard to use them to monitor things you might care about like data quality, API response times, or other things.\nIf you want to register metrics from your API or app with Prometheus, there is an official Prometheus client in Python and the {openmetrics} package in R makes it easy to register metrics from a Plumber API or Shiny app.\nThere’s a great Get Started with Grafana and Prometheus doc on the Grafana Labs website if you want to actually try it out."
  },
  {
    "objectID": "chapters/sec1/1-4-monitor-log.html#comprehension-questions",
    "href": "chapters/sec1/1-4-monitor-log.html#comprehension-questions",
    "title": "4  Logging and Monitoring",
    "section": "4.3 Comprehension Questions",
    "text": "4.3 Comprehension Questions\n\nWhat is the difference between monitoring and logging? What are the two halves of the monitoring and logging process?\nIn general, logging is good, but what are some things you should be careful not to log?\nAt what level would you log each of the following events:\n\nSomeone clicks on a particular tab in your Shiny app.\nSomeone puts an invalid entry into a text entry box.\nAn http call your app makes to an external API fails.\nThe numeric values that are going into your computational function."
  },
  {
    "objectID": "chapters/sec1/1-4-monitor-log.html#lab-4-an-app-with-logging",
    "href": "chapters/sec1/1-4-monitor-log.html#lab-4-an-app-with-logging",
    "title": "4  Logging and Monitoring",
    "section": "4.4 Lab 4: An App with Logging",
    "text": "4.4 Lab 4: An App with Logging\nLet’s go back to the prediction generator app from the last lab and add a little logging. This is quite easy in both R and Python. In both, we just declare that we’re using the logger and then we put logging statements into our code.\nI decided to log when the app starts, just before and after each request, and an error logger if an HTTP error code comes back from the API.\nWith the logging now added, here’s what the app looks like in R:\n\n\napp.R\n\nlibrary(shiny)\n\napi_url &lt;- \"http://127.0.0.1:8080/predict\"\nlog &lt;- log4r::logger()\n\nui &lt;- fluidPage(\n  titlePanel(\"Penguin Mass Predictor\"),\n\n  # Model input values\n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\n        \"bill_length\",\n        \"Bill Length (mm)\",\n        min = 30,\n        max = 60,\n        value = 45,\n        step = 0.1\n      ),\n      selectInput(\n        \"sex\",\n        \"Sex\",\n        c(\"Male\", \"Female\")\n      ),\n      selectInput(\n        \"species\",\n        \"Species\",\n        c(\"Adelie\", \"Chinstrap\", \"Gentoo\")\n      ),\n      # Get model predictions\n      actionButton(\n        \"predict\",\n        \"Predict\"\n      )\n    ),\n\n    mainPanel(\n      h2(\"Penguin Parameters\"),\n      verbatimTextOutput(\"vals\"),\n      h2(\"Predicted Penguin Mass (g)\"),\n      textOutput(\"pred\")\n    )\n  )\n)\n\nserver &lt;- function(input, output) {\n  log4r::info(log, \"App Started\")\n  # Input params\n  vals &lt;- reactive(\n    list(\n      bill_length_mm = input$bill_length,\n      species_Chinstrap = input$species == \"Chinstrap\",\n      species_Gentoo = input$species == \"Gentoo\",\n      sex_male = input$sex == \"Male\"\n    )\n  )\n\n  # Fetch prediction from API\n  pred &lt;- eventReactive(\n    input$predict,\n    {\n      log4r::info(log, \"Prediction Requested\")\n      r &lt;- httr2::request(api_url) |&gt;\n        httr2::req_body_json(vals()) |&gt;\n        httr2::req_perform()\n      log4r::info(log, \"Prediction Returned\")\n\n      if (httr2::resp_is_error(r)) {\n        log4r::error(log, paste(\"HTTP Error\"))\n      }\n\n      httr2::resp_body_json(r)\n    },\n    ignoreInit = TRUE\n  )\n\n  # Render to UI\n  output$pred &lt;- renderText(pred()$predict[[1]])\n  output$vals &lt;- renderPrint(vals())\n}\n\n# Run the application\nshinyApp(ui = ui, server = server)\n\nAnd in Python:\n\n\napp.py\n\nfrom shiny import App, render, ui, reactive\nimport requests\nimport logging\n\napi_url = 'http://127.0.0.1:8080/predict'\nlogging.basicConfig(\n    format='%(asctime)s - %(message)s',\n    level=logging.INFO\n)\n\napp_ui = ui.page_fluid(\n    ui.panel_title(\"Penguin Mass Predictor\"), \n    ui.layout_sidebar(\n        ui.panel_sidebar(\n            [ui.input_slider(\"bill_length\", \"Bill Length (mm)\", 30, 60, 45, step = 0.1),\n            ui.input_select(\"sex\", \"Sex\", [\"Male\", \"Female\"]),\n            ui.input_select(\"species\", \"Species\", [\"Adelie\", \"Chinstrap\", \"Gentoo\"]),\n            ui.input_action_button(\"predict\", \"Predict\")]\n        ),\n        ui.panel_main(\n            ui.h2(\"Penguin Parameters\"),\n            ui.output_text_verbatim(\"vals_out\"),\n            ui.h2(\"Predicted Penguin Mass (g)\"), \n            ui.output_text(\"pred_out\")\n        )\n    )   \n)\n\ndef server(input, output, session):\n    logging.info(\"App start\")\n\n    @reactive.Calc\n    def vals():\n        d = {\n            \"bill_length_mm\" : input.bill_length(),\n            \"sex_Male\" : input.sex() == \"Male\",\n            \"species_Gentoo\" : input.species() == \"Gentoo\", \n            \"species_Chinstrap\" : input.species() == \"Chinstrap\"\n\n        }\n        return d\n    \n    @reactive.Calc\n    @reactive.event(input.predict)\n    def pred():\n        logging.info(\"Request Made\")\n        r = requests.post(api_url, json = vals())\n        logging.info(\"Request Returned\")\n\n        if r.status_code != 200:\n            logging.error(\"HTTP error returned\")\n\n        return r.json().get('predict')[0]\n\n    @output\n    @render.text\n    def vals_out():\n        return f\"{vals()}\"\n\n    @output\n    @render.text\n    def pred_out():\n        return f\"{round(pred())}\"\n\napp = App(app_ui, server)\n\nNow, if you load up this app locally, you can see the logs of what’s happening stream in as you’re pressing buttons in the app.\nYou can feel free to log whatever you think is helpful – for example, it’d probably be more useful to get the actual error contents if an HTTP error comes back."
  },
  {
    "objectID": "chapters/sec1/1-4-monitor-log.html#footnotes",
    "href": "chapters/sec1/1-4-monitor-log.html#footnotes",
    "title": "4  Logging and Monitoring",
    "section": "",
    "text": "There are two common naming patterns with rotating log files.\nThe first is to have dated log filenames that look like my-log-20221118.log.\nThe other pattern is to keep one file that’s current and have the older ones numbered. So today’s log would be my-log.log, yesterday’s would be my-log.log.1 , the day before my-log.log.2, etc. This second pattern works particularly well if you’re using logrotate with log4r, because then log4r doesn’t need to know anything about the log rotation. It’s just always writing to my-log.log.↩︎"
  },
  {
    "objectID": "chapters/sec1/1-5-deployments.html#separate-the-prod-environment",
    "href": "chapters/sec1/1-5-deployments.html#separate-the-prod-environment",
    "title": "5  Deployments and code promotion",
    "section": "5.1 Separate the prod environment",
    "text": "5.1 Separate the prod environment\nThe bedrock of a good CI/CD process is a production environment that’s actually separate from non-production environments.\nCI/CD is all about easily promoting code into production, but if the boundaries of production are a mushy mess, it’s all too easily to accidentally mess up code that’s in production.\nIn order to structure a smooth pathway to production, software environments are separated into three – dev, test, and prod. Dev is the development environment where new work is produced, test is where the code is tested for performance, usability, and feature completeness, and prod is the production environment. Depending on your organization you might have just a dev and prod or you might have more environments between dev and prod.\n\nThe number and configuration of lower environments will vary according to your organization and its needs. But like Tolstoy said about happy families, all prod environments are alike.\nSome criteria that all good prod environments meet:\n\nThe environment is created using code. For data science, that managing R and Python packages using environments as code tooling, as discussed in Chapter 1.\nChanges happen via a promotion process. The process combines human approvals that the code is ready for production and automations to run tests and do the actual deployment.\nChanges only happen via the promotion process. This means no manual changes to the environment or the code.\n\nRules 1 and 2 are pretty straightforward to follow. But the first time something breaks in your prod environment, you will be sorely tempted to violate rule 3. Don’t do it.\nIf you want to run a data science project that becomes critical to your organization, keeping a pristine prod environment that you can rely on is critical. Re-create the issue in a lower environment to figure out what’s wrong and push changes through your promotion process.\n\n5.1.1 Dev and test environments\nThese guidelines for a prod environment look almost identical to guidelines for general purpose software engineering. It’s in the composition of lower environments that the needs of data scientists diverge from general purpose software engineers.\nAs a data scientist, dev means working in a lab environment like RStudio, Spyder, VSCode, or PyCharm and experimenting with the data. You’re slicing the data this way or that to see if anything meaningful emerges, creating plots to see if they are the right way to show off a finding, and checking whether certain features improve model performance. All this means that it’s basically impossible to do work without real data.\n“Duh”, you say, “Of course it’s silly to do data science on fake data.”\nThis may be obvious to you, but doing dev data science on real data is a very common source of friction with IT/Admins.\nThat’s because this need is unique to data scientists. For general purpose software engineering, a lower environment needs data that is formatted like the real data, but the actual content doesn’t matter.\nFor example, if you’re building an online store, you need dev and test environments where the API calls from the sales system are in the same format as the real data – but you don’t actually care if it’s real data. In fact, you probably want to create some odd-looking cases for testing purposes.\nOne way to help allay these concerns is to create a data science sandbox. A great data science sandbox provides:\n\nRead-only access to real data for experimentation.\nPlaces to write mock data to test out things you’ll write for real in prod.\nBroad access to R and Python packages to experiment with before things go to prod.\n\nWorking with your IT/Admin team to get these things isn’t always easy. One thing to point out is that creating this environment actually makes things more secure. It gives you a place to do development without any fear that you might actually damage production data or services."
  },
  {
    "objectID": "chapters/sec1/1-5-deployments.html#version-control-implements-code-promotion",
    "href": "chapters/sec1/1-5-deployments.html#version-control-implements-code-promotion",
    "title": "5  Deployments and code promotion",
    "section": "5.2 Version control implements code promotion",
    "text": "5.2 Version control implements code promotion\nYou need a way to actually operationalize your code promotion process. If your process says that your code needs testing and review before it’s pushed to prod, you need a place to actually do that. Version control is the tool to make your code promotion process real.\nVersion control is software that allows you to keep the prod version of your code safe, gives contributors their own copy to work on, and hosts tools to manage merging changes back together. These days, git is the industry standard for version control.\nGit is a system for tracking changes in computer files in a project called a repository. Git is open source and freely available. There are a number of different companies that host git repositories. Many of them allow you to host public – and some private - repositories for free and have enterprise products that your organization may pay for. GitHub is by far the most popular git host, but GitLab, Bitbucket, and Azure DevOps are also common.\nThis is not a book on git. If you’re not already comfortable with using local and remote repositories, branching, and merging, the rest of this chapter is going to be completely useless. I recommend you take a break from this book and spend some time learning git.\n\n\n\n\n\n\nHints on Learning Git\n\n\n\nPeople who say git is easy to learn are either lying or have forgotten. I am sorry our industry has standardized on a tool with such terrible ergonomics, but it’s really worth it to learn.\nWhether you’re an R or Python user, I’d recommend starting with a resource designed to teach git to a data science user. My recommendation is to check out HappyGitWithR by Jenny Bryan.\nIf you’re a Python user, some of the specific tooling suggestions won’t apply, but the general principles will be exactly the same.\n\n\nIf you understand git and just need a reminder of some common commands, there is a cheatsheet at the end of the chapter.\nThe precise contours of your code promotion process – and therefore your git policies – are up to you and your organization’s needs. Do you need multiple rounds of review? Can anyone promote something to prod, or just certain people? Is automated testing required?\nYou should make these decisions as part of designing your code promotion process, which you can then enshrine in the configuration of your project’s git repository.\nOne important decision you’ll make is on how to configure the branches of your git repository. Here’s how I’d suggest you do it for production data science projects:\n\nMaintain two long running branches – main is the prod version of your project, and test is a long-running pre-prod version.\nCode can only be promoted to main via a merge from test. Direct pushes to main are not allowed.\nNew functionality is developed in short-lived feature branches that are merged into test when you think they’re ready to go. Once sufficient approvals are granted, the feature branch changes in test are merged into main.\n\nThis framework helps maintain a reliable prod version on the main branch, while also leaving sufficient flexibility to accomplish basically any set of approvals and testing you might want.\nHere’s an example of how this might work. Let’s say you were working on a dashboard and were trying to add a new plot.\nYou would create a new feature branch, perhaps called new_plot to work on the plot. When you were happy with how it looked you would merge the feature branch to test. Depending on your organization’s process, you might be able to merge to test yourself or you might require approval.\nIf your testing turned up a bug, you’d fix the bug in the feature branch, merge the bug fix into test, re-test, and merge to main once you were satisfied.\nHere’s what the git graph for that sequence of events might look like:\n\nOne of the tenets of a good CI/CD practice is that changes are merged frequently and incrementally into production.\nA good rule of thumb is that you want your merges to be the smallest meaningful change that can be incorporated into main in a standalone way.\nCreating feature branches for every word of text you might change is clearly too small. Completely rewriting the dashboard in one merge request is also probably too big.\nThere’s no hard and fast rules here. Knowing the appropriate scope for a single merge is an art – one that can take years to develop. Your best resource here is more senior team members who’ve already figured it out."
  },
  {
    "objectID": "chapters/sec1/1-5-deployments.html#cicd-automates-git-operations",
    "href": "chapters/sec1/1-5-deployments.html#cicd-automates-git-operations",
    "title": "5  Deployments and code promotion",
    "section": "5.3 CI/CD automates git operations",
    "text": "5.3 CI/CD automates git operations\nThe role of git is to make your code promotion process happen. Git allows you to configure requirements for whatever approvals and testing you might need. Your CI/CD tool sits on top of that so that all this merging and branching actually does something.1\nTo be more precise a CI/CD pipeline for a project watches the git repository and does something when certain triggers are met, like a merge to a particular branch or a pull request opening.\nThe most common CI/CD operations are pre-merge checks like spell checking, code linting, and automated testing and post-merge deployments.\nThere are a variety of different CI/CD tools available. Because of the tight linkage between GitHub repos and CI/CD, CI/CD pipelines built right into git providers are very popular.\nGitHub Actions (GHA) was released a few years ago and is eating the world of CI/CD. Depending on your organization and the age of your CI/CD pipeline, you might also see Jenkins, Travis, Azure DevOps, or GitLab.\nIf you’re curious how exactly this works, you’ll get your hands dirty in the lab at the end of the chapter.\n\n5.3.1 Configuring per-environment behavior\nAs you promote an app from dev to test and prod, you probably want behavior to look different across the environments. For example, you might want to switch data sources from a dev database to a prod one, or switch a read-only app into write mode, or use a different level of logging.\nThe easiest way to create per-environment behavior is to write code that behaves differently based on on the value of an environment variable and to set that environment variable in each environment.\nMy recommendation is to use a config file to store the values you want for your environment variables for each environment. My preference is to use YAML to store configuration, but there are different ways it can be done.\n\n\n\n\n\n\nNote\n\n\n\nOnly non-secret configuration settings should go in a config file. Secrets should always be configured using secrets management settings in the tooling you’re using so they don’t appear in plain text.\n\n\nFor example, you could write a project that knows whether to write or not based on the value of the config’s write and which database using the config’s db-path. Then you could use the YAML below to specify which environments write and which ones use which database:\n{yaml filename=\"config.yml\"} dev:   write: false   db-path: dev-db test   write: true prod:   write: true   db-path: prod-db\nYou would set a relevant environment variable so your code pulls the dev configuration in dev, test in test, and prod in prod.\nIn Python there are many different ways to set and read in your a per-environment configuration. If you want to use YAML like in the example above, you could save it as config.yml and use the {yaml} package to read it in as a dictionary, and choose which part of the dictionary to at the start of your script.\nIn R, the {config} package is the standard way to load an environmental configuration from a YAML file. The config::get() function uses the value of the R_CONFIG_ACTIVE environment variable to choose which configuration to use. That means that switching from the dev to the prod version of the app is as easy as making sure you’ve got the correct environment variable set on your system."
  },
  {
    "objectID": "chapters/sec1/1-5-deployments.html#comprehension-questions",
    "href": "chapters/sec1/1-5-deployments.html#comprehension-questions",
    "title": "5  Deployments and code promotion",
    "section": "5.4 Comprehension Questions",
    "text": "5.4 Comprehension Questions\n\nWrite down a mental map of the relationship between the three environments for data science?\nWhy is git so important to a good code promotion strategy? Can you have a code promotion strategy without git?\nWhat is the relationship between git and CI/CD? What’s the benefit of using git and CI/CD together?\nWrite out a mental map of the relationship of the following terms: git, GitHub, CI/CD, GitHub Actions, Version Control"
  },
  {
    "objectID": "chapters/sec1/1-5-deployments.html#lab5",
    "href": "chapters/sec1/1-5-deployments.html#lab5",
    "title": "5  Deployments and code promotion",
    "section": "5.5 Lab 5: Host a website with automatic updates",
    "text": "5.5 Lab 5: Host a website with automatic updates\nIn labs 1 through 4, you’ve created a Quarto website for the penguin model. You’ve got sections on EDA and model building. But it’s still just on your computer.\nIn this lab, we’re going to actually deploy that website to a public site on GitHub and and set up GitHub Actions as CI/CD so the EDA and modeling steps re-render every time we make changes.\nBefore we get into the meat of the lab, there are a few things you have to do on your own. If you don’t know how to do these things, there are plenty of great tutorials online.\n\nCreate an empty public git repo on GitHub.\nConfigure the repo as the remote for your Quarto project directory.\n\nOnce you’ve got the GitHub repo connected to your project, you need to set up the Quarto project to publish via GitHub Actions. There are great directions on how to get that configured on the Quarto website.\nFollowing those instructions will accomplish three things for you:\n\nGenerate a _publish.yml, which is a Quarto-specific file for configuring publishing locations.\nConfigure GitHub Pages to serve your website off a long-running standalone branch called gh-pages.\nGenerate a GitHub Actions workflow file, which will live at .github/workflows/publish.yml.\n\nHere’s the basic GitHub Actions file (or close to it) that the process will auto-generate for you.\n\n\n.github/workflows/publish.yml\n\non:\n  workflow_dispatch:\n  push:\n    branches: main\n\nname: Quarto Publish\n\njobs:\n  build-deploy:\n    runs-on: ubuntu-latest\n    permissions:\n      contents: write\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v2\n\n      - name: Set up Quarto\n        uses: quarto-dev/quarto-actions/setup@v2\n\n      - name: Render and Publish\n        uses: quarto-dev/quarto-actions/publish@v2\n        with:\n          target: gh-pages\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n\nOne of the reasons GitHub Actions has gotten so popular is that the actions defined in a very human-readable YAML file and it’s very likely you can read and understand this without much editorializing. But let’s still go through it in some detail.\nThis particular syntax is unique to GitHub Actions, but the idea is universal to all CI/CD systems – you define a trigger and a job to do when it’s triggered.\nIn GitHub Actions, the on section defines when the workflow occurs. In this case, we’ve configured the workflow only to trigger on a push to the main branch.2 Another common case would be to trigger on a pull request to main or another branch.\nThe jobs section defines what happens.\nWhen your action starts up, it’s in a completely standalone environment. This is actually a great thing – if you can easily specify how to start from zero and get your code running in GitHub actions, you can bet it’ll do the same in prod.\nThe runs-on field specifies exactly where we start, which in this case is the latest version of the Ubuntu and not much else.\nOnce that environment is up, each step in jobs runs sequentially.\nThe most common way to define a step is with uses, which calls a preexisting GitHub Actions step that someone else has written. In some cases, you’ll want to specify variable values using with or environment variables with env.\nTake a close look at how this action uses the GITHUB_TOKEN. That’s an environment secret that’s auto-provisioned for an action. By using it as a variable here, it’s easy to see what happens, but the value is still totally secret.\nNow, if you try to run this, it probably won’t work.\nThat’s because the CI/CD process occurs in a completely isolated environment. This auto-generated action doesn’t including setting up versions of R and Python or the packages to run our EDA and modeling scripts. We have to get that configured before this action will work.\n\n\n\n\n\n\nNote\n\n\n\nIf you read the Quarto documentation, they recommend freezing your computations. Freezing is very useful if you want to render your R or Python code only once and just update the text of your document. You wouldn’t need to set up R or Python in CI/CD and the document would render faster.\nThat said, freezing isn’t an option if you intend the R or Python code to re-run because it’s a job you care about.\nBecause the main point here is to learn about getting environments as code working in CI/CD you should not freeze your environment.\n\n\nFirst, add the commands to install R, {renv}, and the packages for your content to the GitHub Actions workflow.\n\n\n.github/workflows/publish.yml\n\n      - name: Install R\n        uses: r-lib/actions/setup-r@v2\n        with:\n          r-version: '4.2.0'\n          use-public-rspm: true\n\n      - name: Setup renv and install packages\n        uses: r-lib/actions/setup-renv@v2\n        with:\n          cache-version: 1\n        env:\n          RENV_CONFIG_REPOS_OVERRIDE: https://packagemanager.rstudio.com/all/latest\n\n\n\n\n\n\n\nNote\n\n\n\nIf you’re having slow package installs in CI/CD for R, I’d strongly recommend using a repos override like in the example above.\nThe issue is that CRAN doesn’t serve binary packages for Linux, which means really slow installs. You’ve got to direct {renv} to install from Public Posit Package Manager, which does have Linux binaries.\n\n\nYou’ll also need to add a workflow to GitHub Actions to install Python and the necessary Python packages from the requirements.txt.\n\n\n.github/workflows/publish.yml\n\n      - name: Install Python and Dependencies\n        uses: actions/setup-python@v4\n        with:\n          python-version: '3.10'\n          cache: 'pip'\n      - run: pip install jupyter\n      - run: pip install -r requirements.txt\n\nNote that in this case, we run the Python environment restore commands with run rather than uses. Where uses takes an existing GitHub Action and runs it, run just runs the shell command natively.\nOnce you’ve made those changes, try pushing or merging your project to main. If you click on the Actions tab on GitHub you’ll be able to see the Action running.\nIn all honesty, it will probably fail the first time or five. You will almost never get your Actions correct on the first try. Just breathe deeply and know we’ve all been there. You’ll figure it out.\nOnce it finishes, you should be able to see your change reflected on your website.\nOnce it’s up, your website will be available at https://&lt;username&gt;.github.io/&lt;repo-name&gt;."
  },
  {
    "objectID": "chapters/sec1/1-5-deployments.html#cheat-git",
    "href": "chapters/sec1/1-5-deployments.html#cheat-git",
    "title": "5  Deployments and code promotion",
    "section": "5.6 Cheatsheet: Git",
    "text": "5.6 Cheatsheet: Git\n\n\n\n\n\n\n\nCommand\nWhat it Does\n\n\n\n\ngit clone &lt;remote&gt;\nClone a remote repo – make sure you’re using SSH URL.\n\n\ngit add &lt;files/dir&gt;\nAdd files/dir to staging area.\n\n\ngit commit -m &lt;message&gt;\nCommit your staging area.\n\n\ngit push origin &lt;branch&gt;\nPush to a remote.\n\n\ngit pull origin &lt;branch&gt;\nPull from a remote.\n\n\ngit checkout &lt;branch name&gt;\nCheckout a branch.\n\n\ngit checkout -b &lt;branch name&gt;\nCreate and checkout a branch.\n\n\ngit branch -d &lt;branch name&gt;\nDelete a branch."
  },
  {
    "objectID": "chapters/sec1/1-5-deployments.html#footnotes",
    "href": "chapters/sec1/1-5-deployments.html#footnotes",
    "title": "5  Deployments and code promotion",
    "section": "",
    "text": "Strictly speaking, this is not true. There are a lot of different ways to kick off CI/CD jobs. But the right way to do it is to base it on git operations.↩︎\nA completed merge counts as a push.↩︎"
  },
  {
    "objectID": "chapters/sec1/1-6-docker.html#container-lifecycle",
    "href": "chapters/sec1/1-6-docker.html#container-lifecycle",
    "title": "6  Demystifying Docker",
    "section": "6.1 Container lifecycle",
    "text": "6.1 Container lifecycle\nDocker is primarily concerned with the creation, movement, and running of containers. A container is a software entity that packages code and its dependencies down to the operating system. Containers make it possible to have completely different environments coexisting side by side on one physical machine.\n\n\n\n\n\n\nNote\n\n\n\nContainers aren’t the only way to run multiple virtual environments side-by-side – they’re just the most popular.\nDocker Containers also aren’t the only type of container, though they’re often treated that way. You may run across other kinds, like Apptainer (formerly Singularity), which is often used in high performance computing contexts.\n\n\nA Docker Image is an immutable snapshot of a container. When you want to run a container, you pull the image and run it as an instance, which is what you’ll actually interact with.\nImages are most often stored in registries, which are similar to git repositories. The most common registry is Docker Hub, which allows public hosting and private hosting of images in free and paid tiers. Docker Hub includes official images for operating systems and programming languages, as well as many, many community-contributed containers. Some organizations run their own private registries, usually using registry as a service offerings from cloud providers.2\nImages are built from Dockerfiles – the code that defines the image. Dockerfiles are usually stored in a git repository. It’s common to build images in a CI/CD pipeline so changes to the Dockerfile are immediately built and pushed to the registry.\nAll instances run on an underlying machine, called a host. A primary feature – also a liability – of using containers is that they are completely ephemeral. Unless configured otherwise, anything inside an instance when it shuts down vanishes without a trace.\nYou can control Docker Containers from the Docker Desktop app. If you’re going to be using Docker on a server, you’ll mostly be interacting via the command line interface (CLI). All Docker CLI commands are formatted as docker &lt;command&gt;.\nThe graphic below shows the different states for a container and the CLI commands to move from one to another.\n\n\n\n\n\n\n\nNote\n\n\n\nI’ve included docker pull on the graphic for completeness, but you’ll almost never run it. docker run auto-pulls the container(s) it needs.\n\n\nThere’s a cheatsheet for Docker CLI commands at the end of this chapter.\n\n6.1.1 Image Names\nIn order to build, push, pull, or run an image, you’ll need to know which image you’re talking about. Every image has a name that consists of an id and a tag.\n\n\n\n\n\n\nNote\n\n\n\nIf you’re using Docker Hub, container ids take the form &lt;user&gt;/&lt;container name&gt;, so I might have the container alexkgold/my-container. This should look familiar to GitHub users.\nOther registries may enforce similar conventions for ids, or they may allow ids in any format they want.\n\n\nTags are used to specify versions of variants of containers and come after the id and :. For example, the official Python docker image has versions for each version of Python like python:3 as well as variants for different operating systems and a slim version that saves space by excluding recommended packages.\nSome tags, usually used for versions, are immutable. For example, the rocker/r-ver container is a container that is built on Ubuntu and has a version of R built in. There’s a rocker/r-ver:4.3.1, which is a container with R 4.3.1.\nOther tags are relative to the point in time. If you don’t see a tag on a container name, it’s using the default latest. Other common relative tags refer to the current development state of the software inside like devel, release, or stable."
  },
  {
    "objectID": "chapters/sec1/1-6-docker.html#running-containers",
    "href": "chapters/sec1/1-6-docker.html#running-containers",
    "title": "6  Demystifying Docker",
    "section": "6.2 Running Containers",
    "text": "6.2 Running Containers\nThe docker run command runs container images as an instance. You can run docker run &lt;image name&gt; to get a running container. However, most things you want to do with your instance require several command line flags.\nThere are a few flags that are useful to manage how your containers run and get cleaned up.\nThe -name &lt;name&gt; flag names an instance. It’s not required, but can be useful to give the instance a name you can remember or use in code. If you don’t provide a name, each instance gets a random alphanumeric id on start.\nThe -rm flag automatically removes the container after its done. If you don’t use the -rm flag, the container will stick around until you clean it up manually with docker rm. The -rm flag can be useful when you’re iterating quickly – especially because you can’t re-use names until you remove the container.\nThe -d flag will run your container in detached mode. This is useful when you want your container to run in the background and not block your terminal session. It’s useful when running containers in production, but you probably don’t want to use it when you’re trying things out and want to see logs streaming out as the container runs.\n\n6.2.1 Getting information in and out\nWhen a container runs, it is isolated from the host. This is a great feature – it means programs running inside the container can address the container’s filesystem and networking without worrying about the host outside. But it also means that using resources on the host requires explicit declarations as part of the docker run command.\nIn order to get data in or out of a container, you need to mount a shared volume (directory) between the container and host with the -v flag. You specify a host directory and a container directory separated by :. Anything in the volume will be available to both the host and the container at the file paths specified.\nFor example, maybe you’ve got a container that runs a job against data it expects in the /data directory. On your host machine, this data lives at /home/alex/data. You could make this happen with\n{bash filename=Terminal} docker run -v /home/alex/data:/data\nHere’s a diagram of how this works.\n\nSimilarly, if you have a service running in a container on a particular port, you’ll need to map the container port to a host port with the -p flag.\n\n\n6.2.2 Other runtime commands\nIf you want to see what containers you’ve got, docker ps lists them. This is especially useful to get instance ids if you didn’t bother with names.\nTo stop a running container docker stop does so nicely and docker kill terminates a container immediately.\nYou can view the logs from a container with docker logs.\nLastly, if you need to run a command inside a running instance, you can use docker exec. This is most commonly used to access the command line inside the container as if SSH-ing to a server with docker exec -it &lt;container&gt; /bin/bash.\nWhile it’s normal to SSH into a server to poke around, it’s somewhat of an anti-pattern to do the same in a container. In general you should prefer to review logs and adjust Dockerfiles and run commands rather than exec in."
  },
  {
    "objectID": "chapters/sec1/1-6-docker.html#building-images-from-dockerfiles",
    "href": "chapters/sec1/1-6-docker.html#building-images-from-dockerfiles",
    "title": "6  Demystifying Docker",
    "section": "6.3 Building Images from Dockerfiles",
    "text": "6.3 Building Images from Dockerfiles\nA Dockerfile is a set of instructions that you use to build a Docker Image. If you know how to accomplish something from the command line, you shouldn’t have too much trouble building a Dockerfile to do the same.\nOne thing to consider when creating Dockerfiles is that the resulting image is immutable. That means anything you build into the image is forever frozen in time. So you’ll definitely want to set up the versions of R and Python and install system requirements. Depending on the purpose of your container, you may want to copy in code, data, and/or R and Python packages, or you may want to mount those in a volume at runtime.\nThere are many Dockerfile commands. You can review them all in the Dockerfile documentation, but here are the handful that are enough to build most images.\n\nFROM – specify the base image. Usually the first line of the Dockerfile.\nRUN – run any command as if you were sitting at the command line inside the container.\nCOPY – copy a file from the host filesystem into the container.\nCMD - Specify what command to run on the container’s shell when it runs, usually the last line of the Dockerfile.3\n\nEvery Dockerfile command defines a new layer.\nA great feature of Docker is that it only rebuilds the layers it needs to when you make changes. For example, take the following Dockerfile:\nFROM ubuntu:latest\n\nCOPY my-data.csv /data/data.csv\n\nRUN [\"head\", \"/data/data.csv\"]\nLet’s say I wanted to change the head command to tail. Rebuilding this container would be nearly instantaneous because the container would only start rebuilding after the COPY command.\nOnce you’ve created your Dockerfile, you build it into an image using docker build -t &lt;image name&gt; &lt;build directory&gt;. If you don’t provide a tag, the default tag is latest.\nYou can then push the image to DockerHub or another registry using docker push &lt;image name&gt;."
  },
  {
    "objectID": "chapters/sec1/1-6-docker.html#comprehension-questions",
    "href": "chapters/sec1/1-6-docker.html#comprehension-questions",
    "title": "6  Demystifying Docker",
    "section": "6.4 Comprehension Questions",
    "text": "6.4 Comprehension Questions\n\nDraw a mental map of the relationship between the following: Dockerfile, Docker Image, Docker Registry, Docker Container\nWhen would you want to use each of the following flags for docker run? When wouldn’t you?\n\n-p, --name, -d, --rm, -v\n\nWhat are the most important Dockerfile commands?"
  },
  {
    "objectID": "chapters/sec1/1-6-docker.html#lab-putting-an-api-in-a-container",
    "href": "chapters/sec1/1-6-docker.html#lab-putting-an-api-in-a-container",
    "title": "6  Demystifying Docker",
    "section": "6.5 Lab: Putting an API in a Container",
    "text": "6.5 Lab: Putting an API in a Container\nPutting an API into a container is popular way to host them. In this lab, we’re going to put the Penguin Model Prediction API from Chapter 2 into a container.\nIf you’ve never used Docker before, start by installing Docker Desktop on your computer.\nYou should feel free to write your own Dockerfile to put the API in a container. If you want to make it easy, the {vetiver} package, which you’ll remember auto-generated the API for us, can also auto-generate a Dockerfile. Look at the package documentation for details.\nI’m using this code to generate my Dockerfile\nfrom pins import board_folder\nfrom vetiver import prepare_docker\n\nboard = board_folder(\"/data/model\", allow_pickle_read=True)\nprepare_docker(board, \"penguin_model\", \"docker\")\nOnce you’ve generated your Dockerfile, take a look at it. Here’s the one for my model:\n\n\nDockerfile\n\n# # Generated by the vetiver package; edit with care\n# start with python base image\nFROM python:3.9\n\n# create directory in container for vetiver files\nWORKDIR /vetiver\n\n# copy  and install requirements\nCOPY vetiver_requirements.txt /vetiver/requirements.txt\n\n#\nRUN pip install --no-cache-dir --upgrade -r /vetiver/requirements.txt\n\n# copy app file\nCOPY app.py /vetiver/app/app.py\n\n# expose port\nEXPOSE 8080\n\n# run vetiver API\nCMD [\"uvicorn\", \"app.app:api\", \"--host\", \"0.0.0.0\", \"--port\", \"8080\"]\n\nThis auto-generated Dockerfile is very nicely commented, so its easy to follow.\n\n\n\n\n\n\nNote\n\n\n\nThis container follows the best practices from Chapter 2. We’d expect the model to be updated much more frequently than the container itself, so the model isn’t built into the container. Instead, the container knows how to fetch the model using the {pins} package.\n\n\nNow build the container using docker build -t penguin-model ..\nYou can run the container using\ndocker run --rm -d \\\n  -p 8080:8080 \\\n  --name penguin-model \\\n  penguin-model\nIf you go to http://localhost:8080 you’ll find that…it doesn’t work? Why? If you run the container attached (remove the -d from the run command) you’ll get some feedback that might be helpful.\nIn line 15 of the Dockerfile, we copied the app.py in to the container. Let’s take a look at that file to see if we can find any hints.\n\n\napp.py\n\nfrom vetiver import VetiverModel\nimport vetiver\nimport pins\n\n\nb = pins.board_folder('./model', allow_pickle_read=True)\nv = VetiverModel.from_pin(b, 'penguin_model', version = '20230422T102952Z-cb1f9')\n\nvetiver_api = vetiver.VetiverAPI(v)\napi = vetiver_api.app\n\nLook at that (very long) line 6. The API is connecting to a local directory to pull the model. Is your spidey sense tingling? Something about container filesystem vs host filesystem?\nThat’s right – we put our model at /data/model on our host machine. But the API inside the container is looking for /data/model inside the container, which doesn’t exist!\nThis is a case where we need to mount a volume into the container like so\ndocker run --rm -d \\\n  -p 8080:8080 \\\n  --name penguin-model \\\n  -v /data/model:/data/model \\\n  penguin-model-local\nAnd NOW you should be able to get your model up in no time.\n\n6.5.1 Lab Extension\nRight now, logs from the API just stay inside the container instance. But that means that the logs go away when the container does. That’s obviously bad if the container dies because something goes wrong.\nHow might you make sure that the container’s logs get written somewhere more permanent?"
  },
  {
    "objectID": "chapters/sec1/1-6-docker.html#footnotes",
    "href": "chapters/sec1/1-6-docker.html#footnotes",
    "title": "6  Demystifying Docker",
    "section": "",
    "text": "This was truer before the introduction of M-series chips for Macs. Chip architecture differences fall below the level that a container captures, and many popular containers wouldn’t run on new Macs. These issues are getting better over time and will probably fully disappear relatively soon.↩︎\nThe big three container registries are AWS Elastic Container Registry (ECR), Azure Container Registry, and Google Container Registry.↩︎\nYou may also see ENTRYPOINT, which sets the command CMD runs against. Usually the default /bin/sh -c to run CMD in the shell will be the right choice.↩︎"
  },
  {
    "objectID": "chapters/sec2/2-0-sec-intro.html#getting-and-running-a-server",
    "href": "chapters/sec2/2-0-sec-intro.html#getting-and-running-a-server",
    "title": "IT/Admin for Data Science",
    "section": "Getting and running a server",
    "text": "Getting and running a server\nThe most common way to get a server for data science is to rent one from a cloud provider. In order to do data science tasks, many people combine their server with other services from the cloud provider. That’s why Chapter 7 is an introduction to what the cloud is and how you might want to use it for data science purposes.\nUnlike your phone or personal computer, you’ll never touch this cloud server you’ve rented. Instead, you’ll administer the server via a virtual interface from your computer. Moreover, servers generally don’t even have the kind of point-and-click interface you’re used to on your personal devices.\nInstead, you’ll access and manage your server from the text-only command line.That’s why Chapter 8 is all about how to set up the command line on your local machine to make it convenient and ergonomic and how to connect to your server for administration purposes using a technology called SSH.\nUnlike your Apple, Windows, or Android operating systems you’re used to on your personal devices, most servers run the Linux operating system. Chapter 9 will teach you a little about what Linux is and will introduce you to the basics of Linux administration including how to think about files and users on a multi-tenant server.\nBut you’re not interested in just running a Linux server. You want to use it to accomplish data science tasks. In particular, you’ll want to install data science tools like R, Python, RStudio, JupyterHub, and more. So you’ll need to learn how to install, run, and configure applications on your server. That’s why Chapter 10 is about application administration.\nWhen your phone or computer gets slow or you run out of storage, it’s probably time for a new one. But a server is a working machine that can be scaled up or down to accommodate more people or heavier workloads over time. That means that you may have to manage the server’s resources much more actively than your personal devices. That’s why Chapter 11 is all about managing and scaling server resources."
  },
  {
    "objectID": "chapters/sec2/2-0-sec-intro.html#making-it-safely-accessible",
    "href": "chapters/sec2/2-0-sec-intro.html#making-it-safely-accessible",
    "title": "IT/Admin for Data Science",
    "section": "Making it (safely) accessible",
    "text": "Making it (safely) accessible\nUnless you’re doing something really silly, your personal devices aren’t accessible to anyone who isn’t physically touching the device. In contrast, most servers are only useful because they’re addressable on a computer network, perhaps even the open internet.\nMaking a server accessible to people over the internet makes it useful, but it also introduces risk. Many dastardly plans for your personal devices are thwarted because a villain would have to physically steal it to get access. For a server, allowing digital access means there are many more potential threats looking to steal data or hijack your computational resources for nefarious ends. You’ve got to be careful about how you’re providing access to the machine.\nMoreover, risk aside, computer networking is a complicated topic, and making it work right can be somewhat difficult. Following random tutorials on the internet is a great way to eventually get your server working, but have no idea what happened or why it suddenly works.\nThe good news is that it’s not magic. Chapter 12 is all about how computers find each other across a network. Once you understand the basic structure and operations of a computer network, making only the things you intend to be public on your server will be much easier.\nAside from a basic introduction to computer networking, there are two other things you’ll want to configure to make your server safe and accessible. The first is to host your server at a human-friendly URL, which you’ll learn how to configure in Chapter 13. The second is to add SSL/TLS to your server to secure the traffic going to and from your server. You’ll learn how to do that in Chapter 14.\nOnce you’ve finished these chapters, you’ll have a basic understanding of all the main topics in IT/Admin that are likely to come up as you try to administer a simple data science workbench or project hosting platform."
  },
  {
    "objectID": "chapters/sec2/2-0-sec-intro.html#labs-in-this-section",
    "href": "chapters/sec2/2-0-sec-intro.html#labs-in-this-section",
    "title": "IT/Admin for Data Science",
    "section": "Labs in this Section",
    "text": "Labs in this Section\nIn the first section of the book, the labs involved creating a DevOps-friendly data science project. In this section, the labs will revolve around actually putting that project into production.\nYou’ll start by standing up an AWS EC2 instance, configuring your local command line, and connecting to the server via SSH. Once you’ve done that, you’ll learn how to create users on the server and access the server as a particular user.\nAt that point, you’ll be ready to transition into data science work. You’ll add R, Python, RStudio Server, and JupyterHub to your server and get them configured for work. Additionally, you’ll deploy the Shiny App and API you created in the book’s first section onto the server.\nOnce the server itself is configured, you’ll need to configure the server’s networking to make it accessible and secure. You’ll learn how to open the proper ports and configure a proxy to access multiple services on the same server, and you’ll learn to configure DNS records so your server is available at a real URL and SSL so it can all be done securely.\nBy the time you’ve finished the labs in this section, you’ll be able to use your EC2 instance as a data science workbench and add your penguin mass prediction Shiny App to the Quarto website you created in the book’s first section.\nFor more details on what you’ll do in each chapter, see Appendix B."
  },
  {
    "objectID": "chapters/sec2/2-1-cloud.html#the-cloud-is-rental-servers",
    "href": "chapters/sec2/2-1-cloud.html#the-cloud-is-rental-servers",
    "title": "7  The Cloud",
    "section": "7.1 The cloud is rental servers",
    "text": "7.1 The cloud is rental servers\nAt one time, the only way to get servers was to buy physical machines and hire someone to install and maintain them. This is called running the servers on-prem (short for premises). There are some organizations, especially those with highly sensitive data, that still run on-prem servers.\nThe problem is that on-prem servers require a large up-front investment in server hardware and professional capacity. If your company has a use case that only requires a single server or one with uncertain payoff, it probably isn’t worth it to hire someone and buy a bunch of hardware.\nAround the year 2000, Amazon took all the server farms across the company and centralized them so teams would use this central server capacity instead of running their own. Over the next few years, Amazon execs (correctly) realized that other companies and organizations would value this ability to rent server capacity. They launched this “rent a server” business as AWS in 2006.\nThese days, the cloud platform business is enormous – collectively nearly a quarter of a trillion dollars. It’s also highly profitable. AWS was only 13% of Amazon’s revenue in 2021, but a whopping 74% of the company’s profits for that year.2\nAWS is still the biggest cloud platform by a considerable margin, but it’s far from alone. Approximately 2/3 of the market consists of the big three – AWS, Microsoft Azure, and Google Cloud Platform (GCP) with the final third made up of numerous smaller companies.3"
  },
  {
    "objectID": "chapters/sec2/2-1-cloud.html#real-and-fake-cloud-benefits",
    "href": "chapters/sec2/2-1-cloud.html#real-and-fake-cloud-benefits",
    "title": "7  The Cloud",
    "section": "7.2 Real (and fake) cloud benefits",
    "text": "7.2 Real (and fake) cloud benefits\nThe cloud arrived with an avalanche of marketing fluff. Over a decade after the cloud went mainstream, it’s clear that some of the purported benefits are real and some are less so. You can be a much more intelligent cloud consumer if you understand which is which.\nThe most important cloud benefit is flexibility. Moving to the cloud allows you to get a new server or re-scale an existing one in minutes, and you only pay for what you use, often on an hourly basis.4 The risk of incorrectly guessing how much capacity you’ll need is drastically reduced, making it way less risky to get started on a server.\nThe other big benefit of the cloud is that it allows IT/Admin teams to narrow their scope and focus. For most organizations, managing physical servers isn’t part of their core competency and outsourcing that work to a cloud provider is a great choice.\n\n\n\n\n\n\nNote\n\n\n\nOne other dynamic is the incentives of individual IT/Admins. As technical professionals, IT/Admins want evidence on their resumes that they have experience with the latest and greatest technologies, which are generally cloud services these days rather than managing physical hardware.\n\n\nAlong with these very real benefits, the cloud was supposed to enable big time savings relative to on-prem operations. For the most part, that hasn’t materialized.\nThe theory was that the cloud would enable organiations to scale their capacity to match need at any given moment. So even if the hourly price was higher, the organization would turn servers off at night or during slow periods and save money.\nIt turns out that dynamic server scaling loads takes a fair amount of engineering effort and only the most sophisticated IT/Admin organizations have implemented effective autoscaling. And even for the organizations that do autoscale, cloud providers are very good at pricing their products to capture a lot of those savings.\nSome large organizations with stable workloads have actually started doing cloud repatriations – bringing workloads back on-prem for significant cost savings. An a16z study found that for certain organizations, the total cost of repatriated workloads, including staffing, could be 1/3 to 1/2 the cost of using a cloud provider.5\nThat said, even if the cash savings aren’t meaningful, the cloud is a key enabler for many businesses. The ability to start small, focus on what matters, and scale up quickly is more than worth it.\nYou may be interested in buying a physical server or re-purposing an old computer just for fun. You’re in good company; I’ve run Ubuntu Server on more than one old laptop. But if you’re trying to spend more time getting important things done and less time playing, getting a server from the cloud is the way to go."
  },
  {
    "objectID": "chapters/sec2/2-1-cloud.html#understanding-cloud-services",
    "href": "chapters/sec2/2-1-cloud.html#understanding-cloud-services",
    "title": "7  The Cloud",
    "section": "7.3 Understanding cloud services",
    "text": "7.3 Understanding cloud services\nIn the beginning, cloud providers did just one thing – rent you a server. But they didn’t stop there. Instead, they started building layers and layers of services that abstract away more IT/Admin tasks.\nAt the end of the day, all cloud services boil down to “rent me an \\(\\text{X}\\)”. As a data scientist, you should start by decoding “What is the \\(\\text{X}\\)?”\nUnfortunately, cloud marketing materials aren’t usually oriented to the data scientist trying to decide whether to use the services, instead they’re oriented at your boss and your boss’s boss, who wants to hear about benefits of using the service. That can make it difficult to decode what \\(\\text{X}\\) is.\nIt’s helpful to keep in mind that, at the end of the day, every service that isn’t directly renting a server is just renting server that already has certain software pre-installed and configured.6\n\n\n\n\n\n\nLess of serverless computing\n\n\n\nYou might hear people talking about going serverless. There is no such thing as serverless computing. Serverless is a marketing term meant to convey that you don’t have to manage the servers. The cloud provider manages them for you, but they’re still there.\n\n\nCloud services are sometimes grouped into three layers to indicate whether you’re renting a basic computing service or something more complete.\nAn analogy to a more familiar layered object may serve to make things clear. Let’s say you’re throwing a birthday party for a friend. You’re planning to bring a chocolate layer cake with vanilla frosting topped with lavender rosettes and “Happy Birthday!” in teal.7\n\n\n\n\n\n\nBig Three Service Naming\n\n\n\nIn this next section, I’ll mention services for common tasks from the big three. AWS tends use cutesy names that have a tangential relationship to the task at hand. Azure and GCP name their offerings more literally.\nThis makes AWS names a little harder to learn, but much easier to recall once you’ve learned them. A table of all the services mentioned in this chapter is in Appendix C.\n\n\n\n7.3.1 IaaS Offerings\nInfrastructure as a service (IaaS, pronounced eye-ahzz) is the basic rent a server premise from the earliest days of the cloud.\nFrom a data science perspective, a IaaS offering might look like what we’re doing in the lab in this book – acquiring a server, networking, and storage from the cloud provider and assembling it into a data science workbench. This is definitely the best way to learn how to administer a data science environment and it’s the cheapest option, but it’s also the most time-consuming.\nThis would be like choosing to go to the grocery store, buy all the ingredients, and bake and decorate your friend’s cake from scratch.\nAlong with a server, you’ll rent storage and networking to make everything work properly.\nSome common IaaS services you’re likely to use include:\n\nRenting a server from AWS with EC2 (Elastic Cloud Compute), from Azure with Azure VMs, and from GCP with Google Compute Engine Instances.\nUnlike your laptop, rented servers don’t include a hard drive, so you’ll have to attach storage with AWS’s EBS (Elastic Block Store), Azure Managed Disk, or Google Persistent Disk.\nCreating and managing the networking where your servers sit with AWS’s VPC (Virtual Private Cloud), Azure’sVirtual Network, and GCP’s Virtual Private Cloud.\nManaging DNS records via AWS’s Route 53, Azure DNS, and Google Cloud DNS. (More on what this means in Chapter 13.\n\nWhile IaaS means the IT/Admins don’t have to be responsible for physical management of servers, they’re responsible for everything else, including keeping the servers updated and secured. For that reason, many organizations are moving away from IaaS towards something more managed these days.\n\n\n7.3.2 PaaS Offerings\nIn a PaaS (Platform as a Service) solution, you hand off management of the servers, but manage the applications you need via an API specific to that service. From a data science perspective, a PaaS setup might look like hosting a JupyterHub, RStudio, or Posit implementation in EKS, or running an ML API in Lambda.\nIn the cake baking world, PaaS would be like buying a pre-made cake and some tins of frosting and doing only the writing and rosettes yourself.\nOne PaaS service that already came up in the book is blob (Binary Large Object) storage. Blob storage allows you to store individual objects somewhere and recall them to any other machine that has access to the blob store. The major blob stores are AWS’s S3 (Simple Storage Service), Azure Blob Storage, and Google Cloud Storage.\nYou’re also likely to make use of cloud-based database, data lake, and data warehouse offerings. There are numerous different offerings, and the ones that you use will depend a lot on your use case and your organization. The ones I’ve seen used most frequently are RDS or Redshift from AWS, Azure Database, and Google BigQuery. This category also includes a number of offerings from outside the big three, most notably Snowflake and Databricks.\nDepending on your organization, you may also use services that run APIs or applications from containers or machine images like AWS’s ECS (Elastic Container Service), Elastic Beanstalk, or Lambda, Azure’s Container Apps or Functions, or GCP’s App Engine or Cloud Functions.\nIncreasingly, organizations are turning to Kubernetes as a way to host services. (More on that in Chapter 18.) Most organizations who do so use a cloud provider’s Kubernetes cluster as a service: AWS’s EKS (Elastic Kubernetes Service) or Fargate, Azure’s AKS (Azure Kubernetes Service), or GCP’s GKE (Google Kubernetes Engine).\nMany organizations are moving to PaaS solutions for hosting applications for internal use. It takes away the hassle of managing and updating actual servers. On the flipside, these offerings are somewhat less flexible than just renting a server, and some applications don’t run well in these environments.\n\n\n7.3.3 SaaS Offerings\nSaaS (Software as a Service) is where you just rent the end-user software for usage, often on the basis of seats or usage. You’re already used to consumer SaaS software like Gmail, Slack, and Office365.\nThe cake equivalent of SaaS would be just going to a bakery and buying a cake for your friend.\nDepending on your organization, you might use a SaaS data science offering like AWS’s SageMaker, Azure’s Azure ML, or GCP’s Vertex AI or Cloud Workstations.\nThe great thing about SaaS offerings is that you get immediate access to the end-user application and it’s usually trivial (aside from cost) to add more users. IT/Admin configuration is generally limited to hooking up integrations, most often authentication and/or data sources.\nThe tradeoff for this ease is that they’re generally more expensive and you’re at the mercy of the provider for configuration and upgrades.\n\n\n7.3.4 Common Services\nIrrespective of the particular services you want to use, there are a few basic services you’ll almost certainly have to interact with.\nRegardless of what you’re trying to do, if you’re working in the cloud, you have to make sure that the right people have the right permissions. In order to manage these permissions, AWS has IAM (Identity and Access Management), GCP has Identity Access Management, and Azure has Microsoft Entra ID, which was called Azure Active Directory until the summer of 2023. Your organization might integrate these services with a SaaS identity management solution like Okta or OneLogin.\nAdditionally, some cloud services are geographically specific. Each of the cloud providers has split the world into a number of geographic areas, which they all call regions.\nSome services are region-specific and can only interact with other services in that region by default. If you’re doing things yourself, I recommend just choosing the region where you live and putting everything there. Costs and service availability does vary somewhat across region, but it shouldn’t be materially different for what you’re trying to do.\nRegions are subdivided into availability zones (AZs). AZs are subdivisions of regions that are designed to be independent. Some organizations want to run services that span multiple availability zones to provide protection against outages in any particular geography. If you’re running something sophisticated enough to need multi-AZ configuration, you should really be working with a professional IT/Admin."
  },
  {
    "objectID": "chapters/sec2/2-1-cloud.html#comprehension-questions",
    "href": "chapters/sec2/2-1-cloud.html#comprehension-questions",
    "title": "7  The Cloud",
    "section": "7.4 Comprehension Questions",
    "text": "7.4 Comprehension Questions\n\nWhat are two reasons you should consider going to the cloud? What’s one reason you shouldn’t?\nWhat is the difference between PaaS, IaaS, and SaaS? What’s an example of each that you’re familiar with?\nWhat are the names for AWS’s services for: renting a server, file system storage, blob storage"
  },
  {
    "objectID": "chapters/sec2/2-1-cloud.html#introduction-to-labs",
    "href": "chapters/sec2/2-1-cloud.html#introduction-to-labs",
    "title": "7  The Cloud",
    "section": "7.5 Introduction to Labs",
    "text": "7.5 Introduction to Labs\nWelcome to the lab!\nThe point of these exercises is to get you hands on with running servers and get you practicing the things you’re learning in the rest of the book.\nIf you walk through the labs sequentially, you’ll end up with a working data science workbench. It won’t suffice for any enterprise-level requirements, but it’ll be secure enough for a hobby project or even a small team.\nFor this lab, we’re going to use services from AWS, as they’re the biggest cloud provider and the one you’re most likely to run into in the real world. Because we’ll be mostly using IaaS services, there are very close analogs from Azure and GCP should you want to use one of them instead."
  },
  {
    "objectID": "chapters/sec2/2-1-cloud.html#lab-getting-started-with-aws",
    "href": "chapters/sec2/2-1-cloud.html#lab-getting-started-with-aws",
    "title": "7  The Cloud",
    "section": "7.6 Lab: Getting started with AWS",
    "text": "7.6 Lab: Getting started with AWS\nIn this first lab, we’re going to get you up and running with an AWS account and show you how to manage, start, and stop EC2 instances in AWS.\nThe server we’ll stand up will be from AWS’s free tier – so there will be no cost involved as long as you haven’t used up all your AWS free tier credits before now.\n\n\n\n\n\n\nTip\n\n\n\nThroughout the labs, I’ll suggest you name things in certain ways. You can do what you want, but I’ll be consistent with those names, so you can copy commands straight from the book if you use the same name.\nIf you want to follow along in that way, start by creating a standalone directory for this lab, named do4ds-lab.\n\n\n\n7.6.1 Step 1: Login to the AWS Console\nWe’re going to start by logging into AWS at https://aws.amazon.com.\n\n\n\n\n\n\nNote\n\n\n\nAn AWS account is separate from an Amazon account for ordering stuff online and watching movies. You’ll have to create one if you’ve never used AWS before.\n\n\nOnce you’ve logged in, you’ll be confronted by the AWS console. There are a ton of things here. Poke around if you want and then continue when you’re ready.\n\n\n7.6.2 Step 2: Stand up an EC2 instance\nThere are five attributes about your EC2 instance you’ll want to configure. If it’s not mentioned here, just stick with the defaults for now.\nIn particular, just stick with the default Security Group. We’ll get into what they are and how to configure them later.\n\n7.6.2.1 Name + Tags\nInstance name and tags are human-readable labels so you can remember what this instance is. Neither name nor tag are required, but I’d recommend you name the server something like do4ds-lab in case you stand up others later.\nIf you’re doing this at work, there may be tagging policies so that the IT/Admin team can figure out who servers belong to later.\n\n\n7.6.2.2 Image\nAn image is a snapshot of a system and serves as the starting point for your server. AWS’s are called AMIs (Amazon Machine Images). They range from free images of bare operating system to paid images that come bundled with software you might want.\nChoose an AMI that’s just the newest LTS Ubuntu operating system. As of this writing, that’s 22.04. It should say free tier eligible.\n\n\n7.6.2.3 Instance Type\nThe instance type identifies the capability of the machine you’re renting. An instance type is made up of a family and a size. The family is the category of server and is denoted by letters and numbers, so there are T2s and T3s, C4s, R5s, and many more.\nWithin each family, there are different sizes. Possible sizes vary by the family, but generally range from nano to multiples of xlarge like 24.xlarge.\nFor now, I’d recommend you get the largest server that is free tier eligible. As of this writing, that’s a t2.micro with 1 CPU and 1 Gb of memory.\n\n\n\n\n\n\nServer sizing for the lab\n\n\n\nA t2.micro with 1 CPU and 1 Gb of memory is a very small server. For example, your laptop probably has at least 8 CPUs and 16 Gb of memory.\nA t2.micro should be sufficient for finishing the lab, but you’ll need a substantially larger server to do any real data science work.\nLuckily, it’s easy to upgrade cloud server sizes later. More on how, as well as advice on sizing servers for real data science work in Chapter 11.\n\n\n\n\n7.6.2.4 Keypair\nThe keypair is the skeleton key to your server. We’ll get more into how to use and configure it in Chapter 8. For now, create a new keypair. I’d recommend naming it do4ds-lab-key. Download the .pem version and put it in your do4ds-lab directory.\n\n\n7.6.2.5 Storage\nBump up the storage to as much as you can get under the free tier, because why not? As of this writing, that’s 30 Gb.\n\n\n\n7.6.3 Step 3: Start the Server\nIf you followed these instructions, you should now be looking at a summary that lists the operating system, server type, firewall, and storage. Go ahead an launch your instance.\nIf you go back to the EC2 page and click on Instances you can see your instance as it comes up. When it’s up, it will transition to State: Running.\n\n\n7.6.4 Optional: Stop the Server\nWhenever you’re stopping for the day, you may want to suspend your server so you’re not using up your free tier hours or paying for it. You can suspend an instance in the state it’s in so it can be restarted later. Suspended instances aren’t always free, but they’re generally very cheap.\nWhenever you want to suspend your instance, go to the EC2 page for your server. Under the Instance State drop down in the upper right, choose Stop Instance.\nAfter a couple minutes the instance will stop. Before you come back to the next lab, you’ll need to start the instance back up so it’s ready to go.\nIf you want to completely delete the instance at any point, you can choose to Terminate Instance from that same Instance State dropdown."
  },
  {
    "objectID": "chapters/sec2/2-1-cloud.html#lab-put-the-penguins-data-and-model-in-s3",
    "href": "chapters/sec2/2-1-cloud.html#lab-put-the-penguins-data-and-model-in-s3",
    "title": "7  The Cloud",
    "section": "7.7 Lab: Put the penguins data and model in S3",
    "text": "7.7 Lab: Put the penguins data and model in S3\nWhether or not you’re hosting your own server, most data scientists working at an organization that uses AWS will run into S3, AWS’s blob store.\nOne really common thing to store in S3 is an ML model. So we’re going to store the mass prediction model we created in Chapters Chapter 2 and Chapter 3 in an S3 bucket.\n\n7.7.1 Step 1: Create an S3 bucket\nTo start off with, you’ll have to create a bucket, most commonly from the AWS console. I’m naming mine do4ds-lab.\n\n\n7.7.2 Step 2: Push new models to S3\nLet’s change the code in our Quarto doc to push the model into S3 when the model rebuilds, instead of just saving it locally.\nThere are a variety of different ways to access an S3 bucket. The simplest way is by using the AWS CLI on the command line. There are also R and Python packages for interacting with S3 (and other AWS services). The most common are Python’s {boto3} package or R’s {paws} and {aws.s3}.\nRegardless of what tooling you’re using, you’ll generally configure your credentials in three environment variables – AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, and AWS_REGION. You can get the access key and secret access key from the AWS console and you should know the region.\nAs always, when you’re developing in Python or R, I’d recommend putting these into a .env or a config.yml file and loading them from there.\nSince we built the model using {vetiver}, it’s really easy to push the model to S3 just by changing the board type to board_s3 and making sure our credentials are defined in an environment variable.\nIt’ll look something like this.\n\n\nmodel.qmd\n\nfrom pins import board_s3\nfrom vetiver import vetiver_pin_write\n\nboard = board_s3(\"do4ds-lab\", allow_pickle_read=True)\nvetiver_pin_write(board, v)\n\nUnder the hood, {vetiver} is making use of standard R and Python tooling to access an S3 bucket.\nInstead of using credentials, you could configure an instance profile using IAM, so the entire EC2 instance has access to the S3 bucket without needing credentials. Configuring instance profiles is the kind of thing you should work with a real IT/Admin to do.\n\n\n7.7.3 Step 3: Pull the API model from S3\nYou’ll also have to configure the API to load the model from the S3 bucket. Luckily, this is very easy. Just update the script you used to build your Dockerfile so it pulls from the pin in the S3 bucket rather than the local folder.\nNow, the script to build the Dockerfile looks like this:\n---\ntitle: \"Prepare Dockerfile\"\nformat:\n  html:\n    code-fold: true\n---\n\n## Load Environment\n\n```{python}\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nfrom pins import board_s3\nfrom vetiver import vetiver_prepare_docker\n\nboard = board_s3(\"do4ds-lab\", allow_pickle_read=True)\nvetiver_prepare_docker(board, \"penguin_model\")\n```\n\n\n7.7.4 Step 4: Give GitHub Actions S3 credentials\nWe want our model building to correctly push to S3 even when it’s running in GitHub Actions, but GitHub doesn’t have our S3 credentials by default, so we’ll need to provide them.\nWe’re going to declare the variables we need in the Render and Publish step of the Action.\nOnce you’re done, that section of the publish.yml should look something like this.\n\n\n.github/workflows/publish.yml\n\non:\n  workflow_dispatch:\n  push:\n    branches: main\n\nname: Quarto Publish\n\njobs:\n  build-deploy:\n    runs-on: ubuntu-latest\n    permissions:\n      contents: write\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v2\n\n      - name: Set up Quarto\n        uses: quarto-dev/quarto-actions/setup@v2\n\n      - name: Install R\n        uses: r-lib/actions/setup-r@v2\n        with:\n          r-version: '4.2.0'\n          use-public-rspm: true\n\n      - name: Setup renv and install packages\n        uses: r-lib/actions/setup-renv@v2\n        with:\n          cache-version: 1\n        env:\n          RENV_CONFIG_REPOS_OVERRIDE: https://packagemanager.rstudio.com/all/latest\n\n      - name: Install Python and Dependencies\n        uses: actions/setup-python@v4\n        with:\n          python-version: '3.10'\n          cache: 'pip'\n      - run: pip install jupyter\n      - run: pip install -r requirements.txt\n\n      - name: Render and Publish\n        uses: quarto-dev/quarto-actions/publish@v2\n        with:\n          target: gh-pages\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}\n          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n          AWS_REGION: us-east-1\n\nNow, unlike the GITHUB_TOKEN secret, which GitHub Actions automatically provides to itself, we’ll have to provide these secrets to the GitHub interface.\n\n\n7.7.5 Lab Extensions\nYou might also want to put the actual data you’re using into S3. This can be a great way to separate the data from the project, as recommended in Chapter 2.\nPutting the data in S3 is such a common pattern that DuckDB allows you to directly interface with parquet files stored in S3."
  },
  {
    "objectID": "chapters/sec2/2-1-cloud.html#footnotes",
    "href": "chapters/sec2/2-1-cloud.html#footnotes",
    "title": "7  The Cloud",
    "section": "",
    "text": "Yes, that is a Sound of Music reference.↩︎\nhttps://www.visualcapitalist.com/aws-powering-the-internet-and-amazons-profits/↩︎\nhttps://www.statista.com/chart/18819/worldwide-market-share-of-leading-cloud-infrastructure-service-providers/↩︎\nAlthough these days a huge amount of cloud spending is done via annual pre-commitments. The cloud providers offer big discounts for making an up-front commitment, which the organization then spends down over the course of the year.↩︎\nhttps://a16z.com/2021/05/27/cost-of-cloud-paradox-market-cap-cloud-lifecycle-scale-growth-repatriation-optimization/↩︎\nThere are also some wild services that do specific things, like let you rent you satellite ground station infrastructure or do Internet of Things (IoT) workloads. Those services are really cool, but so far outside the scope of this book that I’m fine with talking like they don’t exist.↩︎\nIf you’re planning my birthday party, this is the correct cake configuration.↩︎"
  },
  {
    "objectID": "chapters/sec2/2-2-cmd-line.html#getting-the-command-line-you-want",
    "href": "chapters/sec2/2-2-cmd-line.html#getting-the-command-line-you-want",
    "title": "8  Customizing the command line",
    "section": "8.1 Getting the command line you want",
    "text": "8.1 Getting the command line you want\nAs you get started on the command line, you’ll soon realize that some customization is in order. Maybe the colors aren’t quite right, or you want shortcuts for commands you type a lot, or you want more information in the default display.\nSome people might argue that customizing your command line isn’t the best use of your time and energy. Those people are no fun. Having a command line that behaves exactly as you like will speed up your work and make you feel like a hacker.\nBut as you get started, you’ll soon find yourself neck deep in Stack Overflow posts on how to customize your .bashrc. Or wait, is it the .zshrc? Or…\nThe reason customizing your command line is somewhat confusing is that the command line you interact with is actually two or three programs that sit on top of each other. You can mix-and-match options for each of them and configure them in a variety of ways.\n\n\n\n\n\n\nNotes on operating systems\n\n\n\nI’ve been using the command line in MacOS for many years, so I have strong opinions to share in this chapter.\nI haven’t used a Windows machine in many years. I’ve collected some recommendations, but I can’t personally vouch for them the same way.\nI don’t include Linux recommendations because people who use Linux on their desktop have already gone deep down the customization rabbit hole and don’t need my help wasting their time."
  },
  {
    "objectID": "chapters/sec2/2-2-cmd-line.html#the-terminal",
    "href": "chapters/sec2/2-2-cmd-line.html#the-terminal",
    "title": "8  Customizing the command line",
    "section": "8.2 The terminal",
    "text": "8.2 The terminal\nThe terminal is the GUI where you’ll type in commands. The terminal program you use will dictate the colors and themes available for the window, how tabs and panes work, and the keyboard shortcuts you’ll use to manage them.\nSome programs you might use, like RStudio or VSCode have terminals built into them. If you do basically all your terminal work from one of these environments, you may not need another. But it can be nice to have a standalone terminal program you like.\n\nMacOSWindows\n\n\nI’d recommend against using the built-in terminal app (called Terminal). It’s fine, but there are better options.\nMy personal favorite is the free iTerm2, which adds a bunch of niceties like better theming and multiple tabs.\n\n\nThe built-in terminal is the favorite of many users. There are a variety of alternatives you can try, but feel free to stick with the default."
  },
  {
    "objectID": "chapters/sec2/2-2-cmd-line.html#the-shell",
    "href": "chapters/sec2/2-2-cmd-line.html#the-shell",
    "title": "8  Customizing the command line",
    "section": "8.3 The shell",
    "text": "8.3 The shell\nThe shell is the program that takes the commands you’re typing and runs them. It’s what matches the words you type to actual commands or programs on your system. Depending on which shell you choose, you’ll get different options for plugins and themes.\nThe shell runs anywhere you’ve got a running operating system. So your computer has one shell and your server would have a different one. Even a Docker Container has a shell available. That means that if you do a lot of work on a server, you may need to configure your shell twice – once locally and once on the server.\n\nMacOSWindows\n\n\nThe default shell for MacOS (and Linux) is called bash. I’d advise you to switch it out for zsh, which is the most popular bash alternative.1 Bash alternatives are programs that extend bash with various bells and whistles.\nRelative to bash, zsh has a few advantages out of the box, like better auto-completion. It also has a huge ecosystem of themes to enhance visual appeal and functionality, and plugins that let your command line do everything from displaying your git status to controlling your Spotify playlist.\nI’d recommend looking up instructions for how to install zsh using Homebrew.\n\n\nWindows comes with two shells built in, the Command Shell (cmd) and the PowerShell.\nThe Command Shell is older and has been superseded by PowerShell. If you’re just getting started, you should just work with PowerShell. If you’ve been using Command Shell on a Windows machine for a long time, most Command Shell commands work in PowerShell, so it may be worth switching over."
  },
  {
    "objectID": "chapters/sec2/2-2-cmd-line.html#configuration-management",
    "href": "chapters/sec2/2-2-cmd-line.html#configuration-management",
    "title": "8  Customizing the command line",
    "section": "8.4 Configuration management",
    "text": "8.4 Configuration management\nNow that you’ve got your shell and terminal installed, you’ll want to customize. It is possible to directly customize both zsh and PowerShell. But the best way to configure them is to use a configuration manager for your themes and plugins.\n\nMacOSWindows\n\n\nPrezto is my favorite configuration and plugin manager for zsh. OhMyZsh is also popular and very good. Feel free to choose either, but you can only use one.\nOnce you’ve installed Prezto, you’ve got (at least) three different places you could configure your command line – the iTerm2 preferences, .zshrc, and .zpreztorc. I’d recommend leaving .zshrc alone, customizing the look of the window and the tab behavior in the iTerm2 preferences, and customizing the text theme and plugins via Prezto in the .zpreztorc file.\nI tend to be a pretty light on customization, but I’d recommend looking into git plugins and some of the advanced auto-completion and command history search functionality.\n\n\nMany people like customizing PowerShell with Oh My Posh."
  },
  {
    "objectID": "chapters/sec2/2-2-cmd-line.html#text-editors",
    "href": "chapters/sec2/2-2-cmd-line.html#text-editors",
    "title": "8  Customizing the command line",
    "section": "8.5 Text Editors",
    "text": "8.5 Text Editors\nIf you’re working on your command line a lot, you’ll probably be working inside text editors a fair bit. There are many, many options for text editors and people have strong preferences.\nMac OS’s default text editor is called TextEdit and it’s bad. Don’t use it. Windows users get Notepad, which is somewhat better than TextEdit, but still not the best option out there.\nIf you like, you can just edit text files inside your IDE of choice like VS Code or RStudio. Others may prefer a standalone text editor. The most popular these days are probably Sublime or Notepad++ (Windows Only).\nUnlike with the terminal, there’s no deep configuration here. Install one from the web, configure it as you like, and make sure it’s the default for opening .txt and other files you might want to edit in your system preferences."
  },
  {
    "objectID": "chapters/sec2/2-2-cmd-line.html#secure-server-connections-with-ssh",
    "href": "chapters/sec2/2-2-cmd-line.html#secure-server-connections-with-ssh",
    "title": "8  Customizing the command line",
    "section": "8.6 Secure server connections with SSH",
    "text": "8.6 Secure server connections with SSH\nOne common IT/Admin task is remotely accessing a server from the command line on your machine. SSH – short for Secure (Socket) Shell – is a tool for making a secure connection to another computer over an unsecured network. It’s most often used to interact with a server’s command line from the command line of your computer.\nUsing SSH requires invoking the ssh command line interface from your computer (local host) with a username and the address of the remote host (server). For example, connecting to the server at server.example.com as the user alex\n\n\nTerminal\n\n&gt; ssh alex@server.example.com\n\nOnce you run this command, your terminal will open a session to the terminal of the server.\n\n8.6.1 Understanding SSH keys\nBefore any of this can work, you’ll have to configure your SSH keys, which come in a set called keypair. Each keypair consists of a public key and a private key. You’ll register your public key anywhere you’re trying to SSH to, like a server or git host, but your private key must be treated as a precious secret.\nWhen you use the ssh command, your local machine sends a request to open an SSH session to the remote and includes the private key with the request. The remote host verifies the private key with the public key and opens an encrypted connection.\n\nThis seems weird – how can you verify your secret identity with something that you can just spread around publicly? The answer is public key cryptography, which makes it easy to check whether a proffered private key is valid, but nearly impossible to fabricate a private key from a public key.\n\n\n\n\n\n\nNote\n\n\n\nI wish public and private keys were named differently. Calling the private key the key and the public key the lock makes the intent much clearer. But no one asked me.\n\n\nThe key to public key cryptography is mathematical operations that are easy in one direction but really hard to reverse. As a simple example, think of the number \\(91\\) and its prime factors. Do you know what the prime factors of \\(91\\) are offhand? I do not.\nIt’ll probably take you a few minutes to try a bunch, even if you use a calculator. But if I give you the numbers \\(7\\) and \\(13\\), it’s easy to verify that \\(7 * 13 = 91\\).\nIn this example, the number \\(91\\) would be the public key and the prime numbers \\(7\\) and \\(13\\) together would be the private key. This wouldn’t actually make for very good public key cryptography because it doesn’t take more than a few moments to figure out that \\(7\\) and \\(13\\) are prime factors of \\(91\\).\nIn real public key cryptography, the idea is similar, but the mathematical operations are more complex and the numbers much, much bigger. So much so that it’s basically impossible to break public SSH keys through guessing.\nBut that doesn’t make SSH foolproof. While it’s basically impossible to fabricate a private key, it’s totally possible to steal one. Your private key must be kept secret. The best practice is to never move it from the computer where it was created and to never share them.\nIn summary, do what you want with your public keys, but don’t share your private keys. Don’t share your private keys. Seriously, do not share your private keys."
  },
  {
    "objectID": "chapters/sec2/2-2-cmd-line.html#practical-ssh-usage",
    "href": "chapters/sec2/2-2-cmd-line.html#practical-ssh-usage",
    "title": "8  Customizing the command line",
    "section": "8.7 Practical SSH usage",
    "text": "8.7 Practical SSH usage\nNow that you’ve got an understanding of how SSH works, the steps should be easy to remember.\n\nCreate an SSH keypair on any machine you’ll be SSH-ing from (local host).\nPut the public key anywhere you’ll be SSH-ing to (remote host).\nUse the ssh command to connect.\n\nIf you’re working on a server, you’ll probably create at least two keypairs. One on your personal computer to SSH to the server, and one on the server to access outside services that use SSH, like GitHub.\n\n8.7.1 Step 1: Create Keypair\nYou’ll create a keypair on any server you’re SSH-ing from.\nTo create an SSH keypair, you should just follow a tutorial online. The keypair will have two parts. The one that ends in .pub is – you guessed it – the public key.\nIn most cases, you’ll only create one private key on each machine. If you follow standard instructions for creating a key, it will use the default name, probably id_ed25519.2 Sticking with the default name is great because the ssh command will automatically use them. If you don’t use the default name, you’ll have to specify.\n\n\n\n\n\n\nNote\n\n\n\nRemember, you should never move your private key. If you think the answer to a problem you’re having is to move your private key, think again.\nInstead of moving your private key, create a new private key on the machine where you need to use SSH and register a second public key on the remote.\n\n\nSome organizations require that you have a unique key for each service you’re using to make it easier to swap keys in the event of a breach. If so, you won’t be able to use the default key names.\n\n\n8.7.2 Step 2: Register the public keys\nTo register a public key to SSH into a server, you’ll add the public key to the end of the user’s .ssh/authorized_keys file in their home directory. You’ll have to make sure the permissions on the authorized_keys file are correct. More on that in Chapter 9.\nIf you’re registering with a service, like GitHub.com, there’s probably a text box in the GUI to add an SSH key. Google for instructions on how to do it.\n\n\n8.7.3 Step 3: Use SSH\nTo use SSH, you type ssh &lt;user&gt;@&lt;host&gt;. There are also other commands that can use SSH under the hood, like git or scp.\n\n\n\n\n\n\nFor Windows users\n\n\n\nFor a long time, Windows didn’t support SSH out of the box, so SSH-ing from Windows required a separate utility called PuTTY. More recent versions of Windows support using SSH directly in PowerShell or in Windows Subsystem for Linux (WSL). If SSH isn’t enabled on your machine, Google for instructions.\n\n\nIf you have multiple SSH keys or didn’t use the default flag, you can specify a particular key with the -i flag.\nIf you’re using SSH a lot, I’d recommend setting up an SSH config file. An SSH config file allows you to create aliases that are shortcuts to SSH commands including users, hosts, and other details. So if you had a long SSH command like ssh -i my-ssh-key alex@server.example.com, you could shorten it to ssh alex-server or whatever you want.\nOne annoyance about SSH is that they block the terminal they’re using and will break when your computer goes to sleep. Many people also like using the tmux command line utility with SSH to help solve these issues.\ntmux is a terminal multiplexer, which allows you to manipulate terminal sessions from the command line, including putting sessions into the background and making sessions durable through sleeps and other operations. To be honest, I’m mentioning tmux because lots of people love it, but I’ve found the learning curve too steep for it to come into regular usage for me. Your mileage may vary.\nIf you ever run into trouble using SSH, it has one of my favorite debugging modes. Just add a -v to your command for verbose mode. If that’s not enough information, add another v for more verbosity, and if that’s not enough, just add another v for super verbose mode."
  },
  {
    "objectID": "chapters/sec2/2-2-cmd-line.html#comprehension-questions",
    "href": "chapters/sec2/2-2-cmd-line.html#comprehension-questions",
    "title": "8  Customizing the command line",
    "section": "8.8 Comprehension Questions",
    "text": "8.8 Comprehension Questions\n\nDraw a mental map that includes the following: terminal, shell, theme manager, operating system, my laptop\nUnder what circumstances should you move or share your SSH private key?\nWhat is it about SSH public keys that makes them safe to share?"
  },
  {
    "objectID": "chapters/sec2/2-2-cmd-line.html#lab-login-to-the-server",
    "href": "chapters/sec2/2-2-cmd-line.html#lab-login-to-the-server",
    "title": "8  Customizing the command line",
    "section": "8.9 Lab: Login to the server",
    "text": "8.9 Lab: Login to the server\nIn the last chapter we got your server up and running. In this lab, we’ll use the provided .pem key to log in for the first time.\n\n8.9.1 Step 1: Grab the server address\nFrom the EC2 page, you can click on the instance ID in blue to see all the details of about your server.\nCopy the Public IPv4 DNS address, which starts with \\(\\text{ec2-}\\) and ends with \\(\\text{amazonaws.com}\\). That little icon on the left of the address copies it. You’ll need it throughout the labs. If you lose it, come back here to get it.\n\n\n\n\n\n\nSet a Server Address Variable\n\n\n\nIn the rest of the labs in this book, I’m going to write the commands using the bash variable SERVER_ADDRESS. That means that if you create that variable, you’ll be able to just copy the commands out of the book.\nFor example, as I write this, my server has the address \\(\\text{ec2-54-159-134-39.compute-1.amazonaws.com}\\). So would set my server address variable on my command line with SERVER_ADDRESS=ec2-54-159-134-39.compute-1.amazonaws.com.\nIf you’re used to R or Python, where it’s best practice to put spaces around =, notice that assigning variables in bash requires no spaces around =.\n\n\n\n\n8.9.2 Step 2: Log on with the .pem key\nThe .pem key you downloaded when you set up the server is the private key for a pre-registered keypair that will let you SSH into your server as the admin user (named ubuntu on a Ubuntu system).\nThe .pem key is just an SSH key, so you can SSH to your server with\n\n\nTerminal\n\nssh -i do4ds-lab-key.pem \\\n  ubuntu@SERVER_ADDRESS\n\nWhen you first try this, you’re probably going to get an alert that looks something like this:\n\n\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\n\n\\@ WARNING: UNPROTECTED PRIVATE KEY FILE! \\@\n\n\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\n\nPermissions 0644 for 'do4ds-lab-key.pem' are too open.\n\nIt is required that your private key files are NOT accessible by others.\n\nThis private key will be ignored.\n\nLoad key \"do4ds-lab-key.pem\": bad permissions\n\nubuntu@ec2-54-159-134-39.compute-1.amazonaws.com: Permission denied (publickey).\nBecause the keypair is so powerful, AWS requires that you restrict the access pretty severely. Before you can use it to open the server, you’ll need to change the permissions. We’ll get into permissions in Chapter 9. Until then, you can just change the permissions by navigating to the right directory with the cd command and running chmod 600 do4ds-lab-key.pem.\nOnce you’ve done that, you should be able to login to your machine as the root user. When you want to exit an SSH session and get back to your machine, you can just type exit.\n\n\n8.9.3 Step 3: Create your own SSH key\nYou really shouldn’t use the AWS-provided .pem key to login to your server after the first time. It’s too powerful. Create a normal SSH key using the instructions earlier in this chapter. In the next lab, we’ll get that SSH key configured for your user on the server."
  },
  {
    "objectID": "chapters/sec2/2-2-cmd-line.html#footnotes",
    "href": "chapters/sec2/2-2-cmd-line.html#footnotes",
    "title": "8  Customizing the command line",
    "section": "",
    "text": "zsh is pronounced by just speaking the letters aloud, zee-ess-aitch. Some people might disagree and say it’s zeesh, but they’re not writing this book, are they?↩︎\nThe pattern is id_&lt;encryption type&gt;. ed25519 is the standard SSH key encryption type as of this writing.↩︎"
  },
  {
    "objectID": "chapters/sec2/2-3-linux.html#a-brief-history-of-linux",
    "href": "chapters/sec2/2-3-linux.html#a-brief-history-of-linux",
    "title": "9  Intro to Linux Administration",
    "section": "9.1 A brief history of Linux",
    "text": "9.1 A brief history of Linux\nA computer’s OS defines how applications can interact with the underlying hardware. OSes dictate how files are stored and accessed, how applications are installed, how network connections work, and more.\nYour laptop probably runs MacOS or Windows. Neither of them is Linux. But Linux is dominant basically everywhere else. Because it’s an open source OS and in order to accommodate that huge variety of use cases and contexts, Linux comes in a wide variety of “flavors” that differ by technical attributes and licensing model. These flavors are called distros, short for distributions.\nBefore the early 1970s, the market for computer hardware and software looked nothing like it does now. Computers of that era had extremely tight linkages between hardware and software. For example, there was no Microsoft Word you could use on a Dell machine or an HP machine or an Apple machine.\nInstead, every hardware company was also a software company. If Example Corp’s computer did text editing, it was because Example Corp had written (or commissioned) text editing software specifically for their machine. If Example Corp’s machine could run a game, Example Corp had written that game just for their computer.\nThen, in the early 1970s, researchers funded by AT&T’s Bell Labs released Unix – the first operating system. Now, there was a piece of middleware that sat between the computer hardware and the end-user software.\nAfter the advent of the OS, the computer market started looking a lot more familiar to 2020s eyes. Hardware manufacturers could build machines that ran Unix and software companies could write applications that ran on Unix.\nThe one issue (for everyone but Bell Labs) was that they were paying Bell Labs a lot of money for licenses to Unix. So in the 1980s, programmers started writing Unix-like OSes. These so-called Unix clones behaved just like Unix, but didn’t include any actual Unix code.2\nIn 1991, Linus Torvalds – then a 21 year-old Finnish grad student – released an open source Unix clone called Linux via a amusingly nonchalant newsgroup posting, saying, “I’m doing a (free) operating system (just a hobby, won’t be big and professional like gnu)…Any suggestions are welcome, but I won’t promise I’ll implement them :-).”3\nObviously, the Linux project outgrew that modest newgroup post. At this point there are over 600 Linux distros, reflecting both the natural fragmentation of popular open source projects, as well as disparate requirements for an OS for your car’s infotainment system versus a smartphone versus the controller for your smart thermostat.\nLuckily, you don’t have to know hundreds of distros. For server use, most organizations standardize on one of a handful. The most common open source distros are Ubuntu or CentOS. Red Hat Enterprise Linux (RHEL) is the most common paid distro.4 Many organizations on AWS are using Amazon Linux, which is independently maintained by Amazon but was originally a RHEL derivative.5\nMost individuals who have a choice in the matter prefer Ubuntu. It’s definitely my personal preference. It’s a little simpler and easier to configure than the others.\n\n\n\n\n\n\nA note on Ubuntu Versioning\n\n\n\nUbuntu versions are numbered by the year and month they were released. Most people use the Long Term Support (LTS) releases, which are released in April of even years.\nUbuntu versions have fun alliterative names, so you’ll hear people refer to releases by name or version. As of this writing, most Ubuntu machines are running Bionic (20.04, Bionic Beaver) or Jammy (22.04, Jammy Jellyfish)."
  },
  {
    "objectID": "chapters/sec2/2-3-linux.html#bash-basics",
    "href": "chapters/sec2/2-3-linux.html#bash-basics",
    "title": "9  Intro to Linux Administration",
    "section": "9.2 Bash basics",
    "text": "9.2 Bash basics\nLinux is administered from the command line using bash or a bash alternative like zsh. The philosophy behind bash and its derivatives says that you should be able to accomplish anything you want with small programs invoked via a command. Each command should do just one thing, and complicated things should be accomplished by composing commands – taking the output from one as the input to the next.\nInvoking a command is done by typing the command on the command line and hitting enter. If you ever find yourself stuck in a situation you can’t seem to exit, ctrl + c will quit in most cases.\nHelpfully, most bash commands are an abbreviation of the word for what the command does. Unhelpfully, the letters often seem somewhat random.\nFor example, the command to list the contents of a directory is ls, which sorta makes sense. Over time, you’ll get very comfortable with the commands you use frequently.\nBash commands can be modified to behave the way you need them to.\nCommand arguments provide details to the command. They come after the command with a space in between. For example, if I want to run ls on the directory /home/alex, I can run ls /home/alex on the command line.\nSome commands have default arguments. For example the default argument for the ls command is the current directory. So if I’m in /home/alex, I’d get the same thing from either ls or ls /home/alex.\nOptions or flags modify how the command operates and come between the command and arguments. Flags are denoted by having one or more dashes before them. For example, ls allows the -l flag, which displays the output as a list. So, ls -l /home/alex would get the files in /home/alex as a list.\nSome flags themselves have flag arguments. For example, the -D flag allows specifying a format for how the datetime is displayed from ls -l. So running ls -l -D %Y-%m-%dT%H:%M:%S /home/alex lists all the files in /home/alex with the date-time of the last update formatted in ISO-8601 format, which is always the correct format for dates.\nBash commands are always formatted as &lt;command&gt; &lt;flags + flag args&gt; &lt;command args&gt;.\nIt’s nice that this structure is standard. It’s not nice that the main argument is all the way at the end, because it makes long bash commands hard to read. To make commands more readable, you can break the command over multiple lines and include one flag or argument per line. You can tell bash you want it to continue a command after a line break by ending the line with a space and a \\.\nFor example, here’s that long ls command more nicely formatted:\n\n\nTerminal\n\n&gt; ls -l \\\n  -D %Y-%m-%dT%H:%M:%S \\\n  /home/alex\n\nAll of the flags and arguments for commands can be found in the program’s man page (short for manual). You can access the man page for any command with man &lt;command&gt;. You can scroll the man page with arrow keys and exit with q.\nBash is a real – if ugly – programming language, so you can assign variables with &lt;var-name&gt;=&lt;value&gt; (no spaces allowed) and access them $&lt;var-name&gt;. The bash version of print is echo.\nFor example,\n\n\nTerminal\n\n&gt; MSG=\"Hello World!\"\n&gt; echo $MSG\nHello World!\n\nFor the most part, you’ll write commands directly on the command line. It’s also possible to write and run bash scripts that include conditionals, loops, and functions. Bash scripts usually end in .sh and are most often run with the sh command like sh my-script.sh.\nThe advantage of writing bash scripts is that they can run basically anywhere. The disadvantage of writing bash scripts is that bash is a truly ugly programming language that is hard to debug."
  },
  {
    "objectID": "chapters/sec2/2-3-linux.html#commands-run-on-behalf-of-users",
    "href": "chapters/sec2/2-3-linux.html#commands-run-on-behalf-of-users",
    "title": "9  Intro to Linux Administration",
    "section": "9.3 Commands run on behalf of users",
    "text": "9.3 Commands run on behalf of users\nWhenever a program is running in Linux, it is running as a particular user who can be identified by their username.\nOn any Unix-like system, the whoami command returns the username of the active user. So when I run whoami on my MacBook, I get:\n\n\nTerminal\n\n&gt; whoami                                                       \nalexkgold\n\nUsernames have to be unique on the system – but they’re not the true identifier for a user. A user is uniquely identified by their user id (uid), which maps to all the other user attributes like username, password, home directory, groups, and more. The uid for a user is assigned at the time the user is created and usually don’t need to be changed or specified manually.6\nEach human who accesses a Linux server should have their own account. In addition, many applications create service account users for themselves and run as those users. For example, installing RStudio Server will create a user with username rstudio-server. Then, when RStudio Server goes to do something – start an R session for example – it will do so as rstudio-server.\nUser uids start at 10,000 with those below 10,000 reserved for system processes. There’s also one special user – called the admin, root, sudo, or super user who gets the special uid 0.\nUsers belong to groups, which are collections of one or more users. Each user has exactly one primary group and can be a member of secondary groups.7 By default, each user’s primary group is the same as their username.\nLike a user has a uid, a group has a gid. User gids start at 100.\nYou can see a user’s username, uid, groups, and gid with the id command. On my MacBook, I’m a member of a number of different groups, with the primary group staff.\n\n\nTerminal\n\n&gt; id                                                                \nuid=501(alexkgold) gid=20(staff) groups=20(staff),12(everyone),61(localaccounts),79(_appserverusr),80(admin),81(_appserveradm),98(_lpadmin),701(com.apple.sharepoint.group.1),33(_appstore),100(_lpoperator),204(_developer),250(_analyticsusers),395(com.apple.access_ftp),398(com.apple.access_screensharing),400(com.apple.access_remote_ae)\n\nIf you ever need to add users to a server, the easiest way is with the useradd command. Once you have a user, you may need to change the password, which you can do at any time with the passwd command. Both useradd and passwd start interactive prompts, so you don’t need to do much more than run those commands.\nIf you ever need to alter a user – the most common task being to add a user to a group, you would use the usermod command with the -aG flag."
  },
  {
    "objectID": "chapters/sec2/2-3-linux.html#permissions-dictate-what-users-can-do",
    "href": "chapters/sec2/2-3-linux.html#permissions-dictate-what-users-can-do",
    "title": "9  Intro to Linux Administration",
    "section": "9.4 Permissions dictate what users can do",
    "text": "9.4 Permissions dictate what users can do\nIn Linux, everything you can interact with is just a file. Every log – file. Every picture – file. Every application – file. Every configuration – file.\nSo determining whether a user can take an action is really a question of whether they have the right permissions on a particular file.\nBasic Linux permissions (called POSIX permissions) consist of a 3x3 matrix of read, write, and execute for the owner, the owning group, and everyone else. Read means the user can see the contents of a file, write means the user can save a changed version of a file, and execute means they can run the file as a program.\n\n\n\n\n\n\nNote\n\n\n\nThere are more complex ways to manage Linux permissions. For example, you might hear about Access Control Lists (ACLs). They’re beyond the scope of this book.\nThere is more information on different ways organizations manage users and what they’re allowed to do in Chapter 17, which is all about auth.\n\n\nFor example, here’s a set of permissions that you might have for a program if you wanted anyone to be able to run it, group members to inspect it, and only the owner to change it.\n\nDirectories also have permissions – read allows the user see what’s in the directory, write allows the user to alter what’s in the directory, and execute allows the user to enter the directory.\nFile permissions and directory permissions don’t have to match. For example, a user could have read permissions on a directory, so they could see the names of the files, but not actually have read permissions on any of the files, so they can’t look at the contents.\nWhen you’re working on the command line, you don’t get a little grid of permissions. Instead they’re expressed in one of two ways. The first is the string representation, which is a 10-character string that looks something like -rw-r–r--.\nThe first character indicates the type of file: most often - for normal (file) or d for a directory.\nThe next nine characters are indicators for the three permissions for the user, the group, and everyone else. There will be an r for read, a w for write, and an x for execute or - to indicate that they don’t have the permission.\nSo the permissions in the graphic would be -rwxr-x--x for a file and drwxr-x--x for a directory.\nThe best way to get these permissions is to run the ls -l command.\n\n\nTerminal\n\n&gt; ls -l                                                           \n-rw-r--r--  1 alexkgold  staff     28 Oct 30 11:05 config.py\n-rw-r--r--  1 alexkgold  staff   2330 May  8  2017 credentials.json\n-rw-r--r--  1 alexkgold  staff   1083 May  8  2017 main.py\ndrwxr-xr-x 33 alexkgold  staff   1056 May 24 13:08 tests\n\nEach line starts with the string representation of the permissions followed by the owner and group so you can easily understand who should be able to access that file or directory.\nAll of the files in this directory are owned by alexkgold. Only the owner (alexkgold) has write permission, but everyone has read permission. In addition, there’s a tests directory, with read and execute for everyone and write only for alexkgold.\nIn the course of administering a server, you will probably need to change a file’s permissions. You can do so using the chmod command.\nFor chmod, permissions are indicated as a three digit number, like 600, where the first digit is the permission for the user, the second for the group, and the third for everyone else. To get the right number, you sum the permissions as follows: 4 for read, 2 for write, and 1 for execute. You can check for yourself that any set of permissions is uniquely identified by a number between 1 and 7.8\nSo to implement the permissions from the graphic, you’d want the permission set 751 to give the user full permissions (4 + 2 + 1), read and execute (4 + 1) to the group, and execute only (1) to everyone else.\n\n\n\n\n\n\nNote\n\n\n\nIf you spend any time administering a Linux server, you almost certainly will at some point find yourself running into a problem and applying chmod 777 out of frustration to rule out a permissions issue.\nI can’t tell you not to do this – we’ve all been there. But if it’s something important, be sure you change it back once you’re finished figuring out what’s going on.\n\n\nIn some cases you might actually want to change the owner or group of a file. You can change users and groups with either names or ids. You can do so using the chown command. Changing users just uses the username and changing groups get prefixed with a colon.\nIn some cases, you might not be the correct user to take a particular action. If you want to change the user you are, you can use the su (switch user) command. You’ll be prompted for a password to make sure you’re allowed.\nThe admin or root user has full permissions on every file and there are some actions that only the root user can do. When you need to do root-only things, you usually don’t want to su to be the root user. It’s too powerful. Plus if you have user-level configuration, it all gets left behind.\nInstead, individual users can be granted the power to temporarily assume root privileges without changing to be the root user. This is accomplished by making them members of the admin group. If a user is a member of the admin group, they can prefix commands with sudo to run those commands with root privileges.\nThe name of the admin group varies by distro. In Ubuntu, the group is called sudo."
  },
  {
    "objectID": "chapters/sec2/2-3-linux.html#the-linux-filesystem-is-a-tree",
    "href": "chapters/sec2/2-3-linux.html#the-linux-filesystem-is-a-tree",
    "title": "9  Intro to Linux Administration",
    "section": "9.5 The Linux Filesystem is a tree",
    "text": "9.5 The Linux Filesystem is a tree\nRegardless of which Linux distro you’re running, understanding where to find things on your system is crucial.\nAll of the information available to a computer is indexed by its filesystem, which is made up of directories or folders, which are containers for other directories and for files.\nOn your laptop, you’re probably used to browsing the filesystem with your mouse. On your phone, the filesystem is completely obscured by apps, but it’s there.\nOn a Linux server, the only way to traverse the filesystem is with written commands. Having a good mental model for what the filesystem looks like is, therefore, really important.\nOn Linux, the entire filesystem is a tree (or perhaps an upside-down tree). Every directory is contained in by a parent directory, and may contain one or more children or sub-directories. The root directory, / is the base of the tree and is its own parent. A / in between two directories means that it’s a sub-directory.\n\nEvery directory is a sub-directory of / or a sub-directory of a sub-directory of / or…you get the picture. So the /home/alex file path defines a particular location, which is the alex sub-directory of /home, which is a sub-directory of the root directory, /.\n\n\n\n\n\n\nTip\n\n\n\nIt’s never necessary, but sometimes the tree-like layout for a directory is helpful. The tree utility can show you one. It doesn’t always come pre-installed, so you might have to install it.\n\n\nBecause the entire Linux filesystem is based at /, it doesn’t matter what physical or virtual disks you have attached to your system. They will fall somewhere under the main filesystem (often inside /mnt).\nThis will be familiar to MacOS users, because MacOS is based on an operating system called BSD that, like Linux, is a Unix clone.\nIf you’re familiar with Windows, the Linux filesystem may seem a little strange.\nIn Linux, each computer has exactly one filesystem, which is based at the root, /. Network shares or other types of volumes can be mounted somewhere on the filesystem, often below /mnt, but the fact that they’re on separate drives is obscured from the user.\nIn Windows, each physical or logical disk has its own filesystem with its own root. You’re probably familiar with C: as your main filesystem. Your machine may also have a D: drive. If you’ve got network share drives, they’re likely at M: or N: or P:.\nOne other difference is that Windows uses \\ to separate file path elements rather than /. This used to be a big deal, but newer versions of Windows accept file paths using /.\n\n9.5.1 Working with file paths\nWhenever a command runs, it runs at a particular path in the filesystem, called the working directory. You can get the absolute path to your working directory with the pwd command, which is an abbreviation for print working directory.\nWhen you want a command to run the same irrespective of where it’s run from, it’s best to use an absolute path, which is a path specified relative to the root. They operate the same irrespective of the current working directory. Absolute file paths are easy to recognize because they always start with /.\nSometimes it’s convenient to use a relative file path, which starts at the working directory, denoted by .. So, for example, if I want to access the data subdirectory of the working directory, that would be available at ./data.\nThe working directory’s parent is at ... So, you could see everything in the parent directory of your current working directory with ls .. or its parent with ls ../...\nAll accounts that represent actual humans should have a home directory, which usually livelive inside /home.\nThe home directory and all its contents are owned by the user to whom it belongs. The home directory is the user’s space to store the things they need, including user-specific configuration. When navigating the file system, the user can find their home directory is at ~.\nAlong with being able to inspect directories with ls, it’s useful to be able to change your working directory with the cd command, short for change directory. You can use either absolute or relative file paths with cd. So if you were in /home/alex and wanted to navigate to /home, either cd .. or cd /home would work.\nSome files or directories are hidden so they don’t appear in a normal ls. You know a file or directory is hidden because its name starts with .. Hidden files are usually configuration files that you aren’t manipulating in normal usage. These aren’t secret or protected in any way, they’re just skipped by ls for convenience. If you want to display all files in a directory, including hidden ones, you can use the -a flag (for all) with ls.\nYou’ve already seen a couple of hidden files in this book – your GitHub Action was configured in the .github directory and your Prezto configuration was done in the .zpreztorc file. You might also be familiar with .gitignore, .Rprofile, and .Renviron."
  },
  {
    "objectID": "chapters/sec2/2-3-linux.html#moving-files-and-directories",
    "href": "chapters/sec2/2-3-linux.html#moving-files-and-directories",
    "title": "9  Intro to Linux Administration",
    "section": "9.6 Moving files and directories",
    "text": "9.6 Moving files and directories\nYou will frequently need to change where files are on your system, including copying, deleting, moving, and more.\nYou can copy a file or directory from one place to another using the cp command. cp leaves behind the old file or directory and adds the new one at the specified location. You can use the -r flag to recursively copy everything in a directory.\nYou can move a file with the mv command, which does not leave the old file behind. If you want to remove a file entirely, you can use the rm command. The -r (recursive) flag can be used with rm to remove everything within a directory and the -f (force) flag can skip rm double checking you really want to do this.\n\n\n\n\n\n\nWarning\n\n\n\nBe very careful with the rm command, especially with -rf.\nThere’s no recycle bin. Things that are deleted are instantly deleted forever.\n\n\nIf you want to make a directory, mkdir makes a file path. It can be used with relative or absolute file paths, and can include multiple layers of paths to create. For example, if you’re in /home/alex, you could mkdir project1/data to make a project1 directory and data sub-directory.\nThe mkdir command throws an error if you try to create a path that includes some directories that already exist – for example if project1 already existed in the example above. The -p flag can be handy to create only the parts of the path that don’t exist.\nSometimes it’s useful to operate on every file inside a directory. You can get every file that matches a pattern with the wildcard, *. You can also do partial matching with the wildcard to get all the files that match part of a pattern.\nFor example, let’s say I have a /data directory and I want to put a copy of only the .csv files inside into a new data-copy sub-directory. I could do the following:\n\n\nTerminal\n\n&gt; mkdir -p /data/data-copy\n&gt; cp /data/*.csv /data/data-copy\n\n\n9.6.1 Moving things to and from the server\nIt’s very common to have a file on your server you want to move to your desktop or vice versa. There are a few different ways to move files and directories.\nIf you’re moving multiple files, it’s easier to combine them into a single object and move that. The tar command turns a set of files or whole directory into a single archive file, usually with the file suffix .tar.gz. Creating an archive also does some compression. The amount depends on the content.\nIn my opinion, tar is a rare failure of bash to provide standalone commands for anything you need to do. tar is used to both create and unpack (extract) archive files. Telling it which one requires the use of several flags. You’ll basically never use tar without a bunch of flags and the incantation is hard to remember. I google it every time I use it. The flags you’ll use most often are in the cheatsheet in the appendix.\nYou can move files to or from a server with the scp command. scp – short for secure copy – is basically cp, but with an SSH connection in the middle.9\nSince scp establishes an SSH connection, you need to make the request to somewhere that is accepting SSH connections. That means that whether you’re copying something to or from a server, you’ll run scp from a regular terminal on your laptop, not one that’s already SSH-ed into your server.\nRegular ssh options work with scp, like -i and -v."
  },
  {
    "objectID": "chapters/sec2/2-3-linux.html#pipes-and-redirection",
    "href": "chapters/sec2/2-3-linux.html#pipes-and-redirection",
    "title": "9  Intro to Linux Administration",
    "section": "9.7 Pipes and redirection",
    "text": "9.7 Pipes and redirection\nYou can always copy and paste command outputs or write them to a file, but it can also be helpful to just chain a few commands together. Linux provides a few handy operators you can use to make this easy.\nThe simplest operator is the pipe |, which just takes the output of one command and makes it the input for the next command.\nFor example, you might want to see how many files are in a directory. The wc -l (word count, lines) command counts lines, so you could do ls | wc -l since each file returned by ls is counted as a line.\n\n\n\n\n\n\nCeci n’est pas une pipe?\n\n\n\nThe pipe should feel extremely familiar to R users.\nThe pipe from the {magrittr} package, %&gt;%, was introduced in 2013, and is a popular part of the {tidyverse}.10 The {magrittr} pipe was inspired by both the Unix (Linux) pipe and the pipe operator in the F# programming language.\nDue to its popularity, the pipe |&gt; was formally added to the base R language in R 4.1 in 2021.\n\n\nThere are a few operators that write the output of the left hand side into a file.\nThe &gt; command takes the output of a command on the left and writes it as a new file. If the file you specify already exists, it will be overwritten.\nIf you want to append the new text, rather than overwrite, &gt;&gt; appends to the end of the file. I generally default to &gt;&gt;, because it’ll create a new file if one doesn’t exist, and I usually don’t mean to overwrite what’s there.\nA common reason you might want to do this is to add something to the end of your .gitignore. For example, if you want to add your .env file to your .gitignore, you could do that with echo .env &gt;&gt; .gitignore.11 Another great usage is to add a new public key to your .ssh/authorized_keys file.\nThere are times when you want to make files or directories with nothing in them. The touch command makes a blank file at the specified file path. If you touch a preexisting file, it updates the time the file was last updated without actually making any changes. This can be useful because some applications look at the timestamp on files to see if action is needed."
  },
  {
    "objectID": "chapters/sec2/2-3-linux.html#comprehension-questions",
    "href": "chapters/sec2/2-3-linux.html#comprehension-questions",
    "title": "9  Intro to Linux Administration",
    "section": "9.8 Comprehension Questions",
    "text": "9.8 Comprehension Questions\n\nWhat are the parts of a bash command?\nWhere do commands run? How do you know where they’re going to run or specify a relative path?\nHow can you copy, move, or delete a file? What about to or from a server?\nCreate a mind map of the following terms: Operating System, Windows, MacOS, Unix, Linux, Distro, Ubuntu\nWhat are the 3x3 options for Linux file permissions? How are they indicated in an ls -l command?"
  },
  {
    "objectID": "chapters/sec2/2-3-linux.html#lab-set-up-a-user",
    "href": "chapters/sec2/2-3-linux.html#lab-set-up-a-user",
    "title": "9  Intro to Linux Administration",
    "section": "9.9 Lab: Set up a user",
    "text": "9.9 Lab: Set up a user\nWhen you use your server’s .pem key, you login as the root user, but that’s too much power to acquire on a regular basis. Additionally, since your server is probably for multiple people, you’re going to want to create users for them.\nIn this lab, you’ll create a regular user for yourself and add an SSH key for them so you can directly log in from your personal computer.\n\n9.9.1 Step 1: Create a non-root user\nLet’s create a user using the adduser command. This will walk us through a set of prompts to create a new user with a home directory and a password. Feel free to add any information you want – or to leave it blank – when prompted.\nI’m going to use the username test-user. If you want to be able to copy/paste commands, I’d advise doing the same. If you were creating users based on real humans, I’d advise using their names.\n\n\nTerminal\n\n&gt; adduser test-user\n\nWe want this new user to be able to adopt root privileges so let’s add them to the sudo group with\n\n\nTerminal\n\n&gt; usermod -aG sudo test-user\n\n\n\n9.9.2 Step 2: Add an SSH key for your new user\nLet’s register an SSH key for the new user by adding the key from the last lab to the server user’s authorized_users file.\nFirst, you need to get your public key to the server using scp.\nFor me, the command looks like this\n\n\nTerminal\n\n&gt; scp -i ~/Documents/do4ds-lab/do4ds-lab-key.pem \\ \n  ~/.ssh/id_ed25519.pub \\\n  ubuntu@$SERVER_ADDRESS:/home/ubuntu\n\nNote that I’m copying the public key, but SSH access is still using the server’s .pem key because I don’t have another key registered yet.\nNow the public key is on the server, but it’s in the ubuntu user’s home directory. You’ll need to do the following:\n\nCreate .ssh/authorized_keys in test-user’s home directory.\nCopy the contents of the public key you uploaded into the authorized_keys file (recall &gt;&gt;).\nMake sure the .ssh directory and authorized_keys files are owned by test-user with 700 permissions on .ssh and 600 on authorized_keys.\n\nYou could do this all as the admin user, but I’d recommend switching to being test-user at some point with the su command.\n\n\n\n\n\n\nTip\n\n\n\nIf you run into trouble assuming sudo with your new user, try exiting SSH and coming back. Sometimes these changes aren’t picked up until you restart the shell.\n\n\nOnce you’ve done all this, you should be able to log in from your personal computer with ssh test-user@$SERVER_ADDRESS.\nNow that we’re all set up, you should store the .pem key somewhere safe and never use it to log in again."
  },
  {
    "objectID": "chapters/sec2/2-3-linux.html#footnotes",
    "href": "chapters/sec2/2-3-linux.html#footnotes",
    "title": "9  Intro to Linux Administration",
    "section": "",
    "text": "The remainder are mostly Windows servers. There are a few other OSes you might encounter, like Oracle Solaris. There is a product called Mac Server, but it’s just a program for managing Mac desktops and iOS devices, not a server OS.\nThere are also versions on Linux that run on desktop computers. Despite the best efforts of many hopeful nerds, desktop Linux is pretty much only used by professional computer people.↩︎\nOr at least they weren’t supposed to. There’s an interesting history of lawsuits, especially around whether the BSD OS illegally included Unix code.↩︎\nMore in the History of Linux Wikipedia article.\nPedants will scream that the original release of Linux was just the operating system kernel, not a full operating system like Unix. Duly noted, now go away.↩︎\nRHEL and CentOS are related operating systems, but that relationship has changed a lot in the last few years. The details are somewhat complicated, but most people expect less adoption of CentOS in enterprise settings going forward.↩︎\nAs I’m writing this, Amazon Linux 2 is popular, but Amazon Linux 2023 (AL2023) was recently released. I’d expect AL2023 or it’s successor to be dominant by the time you read this.↩︎\nThe one exception to this is when you’ve got the same user accessing resources across multiple machines. Then the uids have to match. If you’re worrying about this kind of thing, it’s probably time to bring in a professional IT/Admin.↩︎\nDepending on your version of Linux, there may be a limit of 16 groups per user.↩︎\nClever eyes may realize that this is just the base-10 representation of a three-digit binary number.↩︎\nIt’s worth noting that scp is now considered “insecure and outdated”. The ways it is insecure are rather obscure and not terribly relevant for most people. But if you’re moving a lot of data, you may want something faster. If so, I’d recommend more modern options like sftp and rsync. I probably wouldn’t bother if you’re only occasionally scp-ing small files to or from your server.↩︎\nThe title of this callout box is also the tagline for the {magrittr} package.↩︎\nNote that echo is needed so that the .env gets repeated as a character string. Otherwise .env would be treated as a command.↩︎"
  },
  {
    "objectID": "chapters/sec2/2-4-app-admin.html#linux-app-install-config",
    "href": "chapters/sec2/2-4-app-admin.html#linux-app-install-config",
    "title": "10  Application administration",
    "section": "10.1 Linux app install + config",
    "text": "10.1 Linux app install + config\nYou can install applications to Linux from a distro-specific repository, much like the app store for your phone or you can just download the application from the internet and install it locally.\nFor Ubuntu, the apt command is used for interacting with repositories of .deb files. For CentOS and RedHat, the yum command is used for installing .rpm files.\n\n\n\n\n\n\nNote\n\n\n\nThe examples below are all for Ubuntu, since that’s what we use in the lab for this book. Conceptually, using yum is very similar, though the exact commands differ somewhat.\n\n\nIn addition to actually installing packages, apt is also the utility for ensuring the lists of available packages you have are up to date with update and that all packages on your system are at their latest version with upgrade. When you find Ubuntu commands online, it’s common to see them prefixed with apt-get update && apt-get upgrade -y. The -y flag bypasses a manual confirmation step.\nPackages are installed with apt-get install &lt;package&gt;. Depending on which user you are, you may need to prefix the command with sudo.\nSometimes, you may want to install packages that aren’t in the repository for your distro. Doing that will generally involve downloading a file directly from a URL – usually with wget and then installing it from the file you’ve downloaded.\n\n10.1.1 Application configuration\nOnce you’ve installed an application, it will generally require some configuration like the default background color, the set of users allowed in, or the frequency something updates.\nOn your personal computer, you’d probably find the setting in a series of dropdown menus at the top of the screen. On a server, no such menu exists.\nFor applications running on your server, application behavior is usually configured through one or more config files. For applications hosted inside a Docker container, behavior is often configured with environment variables, sometimes in addition to config files.\n\n\n10.1.2 Where to find application files\nLinux applications often use several files located in different locations on the filesystem. Here are some of the ones you’ll use most frequently:\n\n/bin, /opt, /usr/local, /usr/bin - installation locations for software.\n/etc - configuration files for applications.\n/var - variable data, most commonly log files in /var/log or /var/lib.\n\nThis means that on a Linux server, the files for a particular application probably don’t all live in the same directory. Instead, you might run the application from the executable in /opt, configure it with files in /etc, and troubleshoot from logs in /var.\n\n\n10.1.3 Command line configuration with vim and nano\nObviously, if you’re administering applications on a server, you’ll need to spend a fair bit of time editing text files. But how? Unlike on your personal computer, where you click a text file to open and edit it, you’ll need to work with a command line text editor when you’re working on a server.\nThere are two command line text editors you’ll probably encounter: nano and vi/vim.1 While they’re both powerful text editing tools, they can also be intimidating if you’ve never used them before.\nYou can open a file in either by typing nano &lt;filename&gt; or vi &lt;filename&gt;.\nAt this point many newbie command line users find themselves completely stuck, unable to do anything – even just exit and try again. But don’t worry, there’s a way out of this labyrinth.\nIf you opened nano, there will be some helpful-looking prompts at the bottom. You’ll see that once you’re ready to go, you can exit with ^x. But the ^ isn’t actually the caret character. On Windows, ^ is short for Ctrl and on Mac it’s for Command (⌘), so Ctrl + x or ⌘ + x will exit.\nWhere nano gives you helpful – if obscure – hints, a first experience in vim is the stuff of command line nightmares. You’ll type words and they won’t appear onscreen. Instead, you’ll experience dizzying jumps around the page and words and lines of text will disappear without a trace.\nThis is because vim uses the letter keys not just to type, but also to navigate the page and interact with vim itself. You see, vim was created before keyboards uniformly had arrow keys.\nWhile they can be intimidating at first, vim keybindings are worth spending some time learning. Vim includes some powerful shortcuts for moving within and between lines and selecting and editing text. Most IDEs you might use, including RStudio, JupyterLab, and VSCode have vim modes.\nWhen you enter vim, you’re in the (poorly named) normal mode, which is for navigation only. Pressing the i key activates insert mode, which will feel normal for those of us used to arrow keys. Once in insert mode, you can type and words will appear and you can navigate with the arrow keys.\nYou can return to normal mode with the escape key.\nOnce you’ve escaped, you may wish never to return to normal mode, but it’s the only way to save files and exit vim. In order to do file operations, you type a colon, :, followed by the shortcut for what you want to do, and enter. The two most common commands you’ll use are save (write) with w and quit with q. You can combine these together, so you can save and quit in one command using :wq.\nSometimes you may want to exit without saving or you might’ve opened and changed a file you don’t actually have permission to edit. If you’ve made changes and try to exit with :q, you’ll find yourself in an endless loop of warnings that your changes won’t be saved. You can tell vim you mean it with the exclamation mark, !, and exit using :q!."
  },
  {
    "objectID": "chapters/sec2/2-4-app-admin.html#reading-logs",
    "href": "chapters/sec2/2-4-app-admin.html#reading-logs",
    "title": "10  Application administration",
    "section": "10.2 Reading logs",
    "text": "10.2 Reading logs\nOnce your applications are up and running, you may run into issues. Or even if you don’t, you may want to take a look at how things are running.\nMost applications write their logs into somewhere inside the /var directory. Some things will get logged to the main log at /var/log/syslog. Other things may get logged to /var/log/&lt;application name&gt; or /var/lib/&lt;application name&gt;.\nIt’s important to get comfortable with the commands to read text files in order to be able to examine logs (and other files). The commands I use most commonly are:\n\ncat is the basic command to print a file, starting at the beginning.\nless prints a file, starting at the beginning, but only a few lines at a time.\nhead prints only the first few lines and exits. It is especially useful to peer at the beginning of a large file, like a csv file – so you can quickly preview the column heads and the first few values.\ntail prints a file going up from the end. This is especially useful for log files, as the newest logs are appended to the end of a file. This is such a common practice that “tailing a log file” is a common phrase.\n\nSometimes, you’ll want to use the -f flag (for follow) to tail a file with a live view as it updates.\n\n\nSometimes you want to search around inside a text file. You’re probably familiar with the power and hassle of regular expressions (regex) to search for specific character sequences in text strings. The Linux command grep is the main regex command.\nIn addition to searching in text files, grep is often useful in combination with other commands. For example, it’s often useful to put the output of ls into grep to search for a particular file in a big directory using the pipe."
  },
  {
    "objectID": "chapters/sec2/2-4-app-admin.html#running-the-right-commands",
    "href": "chapters/sec2/2-4-app-admin.html#running-the-right-commands",
    "title": "10  Application administration",
    "section": "10.3 Running the right commands",
    "text": "10.3 Running the right commands\nLet’s say you want to open Python on your command line. One option would be to type the complete path to a Python install every time. For example, I’ve got a version of Python in /usr/bin, so /usr/bin/python3 works.\nBut in most cases, it’s nice to just type python3 and have the right version open up.\n\n\nTerminal\n\n&gt; python3\nPython 3.9.6 (default, May  7 2023, 23:32:45) \n[Clang 14.0.3 (clang-1403.0.22.14.1)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; \n\nIn some cases, this isn’t optional. Certain applications will rely on others being available, like RStudio Server needing to find R. Or Jupyter Notebook finding your Python kernels.\nSo how does Linux know where to find those applications and files?\nIf you ever want to check which actual executable is being used by a command, you can use the which command. For example, on my system this is the result of which python3.\n\n\nTerminal\n\n&gt; which python3                                                    \n/usr/bin/python3\n\nThe operating system knows how to find the actual runnable programs on your system via the path. The path is a set of directories that the system knows to search when it tries to run a program. The path is stored in an environment variable, conveniently named PATH.\nYou can check your path at any time with echo $PATH. On my MacBook, this is what the path looks like.\n\n\nTerminal\n\n&gt; echo $PATH                                                      \n/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/System/Cryptexes/App/usr/bin:/usr/bin:/bin:/usr/sbin:/sbin\n\nWhen you install a new piece of software you’ll need to add it two the path. Say I was to install a new version of Python in /opt/python. That’s not on my PATH, so my system wouldn’t be able to find it.\nI can get it on the path in one of two ways – the first option would be to add /opt/python to my PATH every time a terminal session starts usually via a file in /etc or the .zshrc.\nThe other option is to create a symlink to the new application in a directory already on the PATH. A symlink does what it sounds like – creates a way to link to a file from a different directory without moving it. Symlinks are created with the ln command."
  },
  {
    "objectID": "chapters/sec2/2-4-app-admin.html#running-applications-as-services",
    "href": "chapters/sec2/2-4-app-admin.html#running-applications-as-services",
    "title": "10  Application administration",
    "section": "10.4 Running applications as services",
    "text": "10.4 Running applications as services\nOn your personal computer, you probably have programs that start every time your computer does. Maybe this happens for Slack, Microsoft Teams, or Spotify. Such applications that execute on startup and run in the background waiting for some sort of input are called a daemon or a service.\nOn a server, most of the applications are configured to run as a service so they’re ready for users who may not have the permissions to start them. For example, on a data science workbench, you’d want JupyterHub and/or RStudio Server to run as a service.\nIn Linux, the tool to turn a regular application into a daemon is called systemd. Some applications automatically configure themselves with systemd when they’re installed. If your application doesn’t, or you want to alter the startup behavior, most applications have their systemd configuration in /etc/systemd/system/&lt;service name&gt;.service.\nDaemonized services are controlled using the systemctl command line tool.\n\n\n\n\n\n\nNote\n\n\n\nBasically all modern Linux distros have coalesced around using systemd and systemctl. Older systems may not have it installed by default and you may have to install it or use a different tool.\n\n\nThe systemctl command has a set of sub-commands that are useful for working with applications. Providing those commands looks like systemctl &lt;subcommand&gt; &lt;application&gt;. Often systemctl has to be run as sudo, since you’re working with an application for all users of the system.\nThe most useful systemctl commands include status for checking whether a program is running or not, start, stop, and restart for a stop followed by a start. Many applications also support a reload command, which reloads configuration settings without having to actually restart the process. Exactly which settings require a restart vs a reload varies from one application to another.\nIf you’ve changed a service’s systemd configuration, you can load changes with daemon-reload. You also can turn a service on or off for the next time the server starts with enable and disable.\n\n10.4.1 Running Docker containers as a service\nOne of the benefits of running Docker Containers is that they run basically anywhere, so it’s a popular way to quickly run an application on a server. To run them as a service, you’ll need to make sure Docker itself is daemonized and then ensure the container you care about comes up whenever Docker does by setting a restart policy for the container.\nHowever, many Docker services involve coordinating more than one container. If that’s the case, you’ll want to use a system that’s custom-built for managing multiple containers. The most popular are Docker Compose or Kubernetes.\nDocker Compose is a relatively lightweight system where you write a YAML file to describe the containers you need and the relationship between them. You can then use a single command to launch the entire set of Docker containers.\nDocker Compose is fantastic for prototyping systems of Docker containers and for running small-scale Docker-ized deployments on a single server. There are many great resources on Docker Compose online to learn more.\nKubernetes is designed for a similar purpose, but instead of running a handful of containers on one server, Kubernetes is a heavy-duty production system designed to schedule hundreds or thousands of Docker-based workloads across a cluster of many individual servers.\nIn general, I’d recommend sticking with Docker Compose for the work you’re doing. If you’re finding yourself needing the full might of Kubernetes to do what you want, you probably should be working closely with a professional IT/Admin"
  },
  {
    "objectID": "chapters/sec2/2-4-app-admin.html#managing-r-and-python",
    "href": "chapters/sec2/2-4-app-admin.html#managing-r-and-python",
    "title": "10  Application administration",
    "section": "10.5 Managing R and Python",
    "text": "10.5 Managing R and Python\nAs the admin of a data science server, Python and R are some of the most critical software you’ll manage.\nIn general, the easiest path to making many users happy is having a bunch of versions of R and Python installed side-by-side. That way you can allow users to upgrade their version of R or Python as it works for their project, not according to your upgrade schedule.\nI’ve seen this work best by installing R and Python into the /opt/python and /opt/R directory. Users can grab them from there to create project-specific virtual environments. You’ll need to make sure permissions are correct on /opt/python for this to work.\nNow, this isn’t what will happen if you just sudo apt-get install python or sudo apt-get install R, so you’ll have to be a little more clever. My favorite route (though I’m obviously biased) is to install Python and R from the pre-built binaries provided by Posit.\n\n10.5.1 Python-specific considerations\nPython is one of the world’s most popular programming languages for general purpose computing. This actually makes configuring Python harder.\nAlmost every system comes with a system version of Python. This is the version of Python that’s used by the operating system for various tasks. It’s almost always very old and you really don’t want to mess with it.\nSo in order to configure Python on your system, you have to install data science specific versions of Python, get them on the path, and get the system version of Python off the path. This is the source of much frustration trying to get Python up and running, both on servers and your personal computer.\nOn your personal computer, Conda is a great solution for this problem. Conda allows you to install a standalone version of Python in user-space. That means that even if your organization doesn’t let you have admin rights on your computer, you can install and manage versions of Python for development.\nFor the same reason, Conda isn’t a great choice for administering a data science server. You’re not a user – you’re an admin. And giving everyone on the server access to Python versions is generally easier without Conda.\nThis is why it’s a good idea to install data science Python versions into /opt/python. It’s easy to add that one directory to the path, and it’s completely distinct from where the system Python lives.\n\n\n10.5.2 R-specific considerations\nPeople pretty much only ever install R to do data science so it’s generally not a huge deal where you install R and get it on the path. If you want just one version of R, using apt-get install is fine.\nIf you want multiple versions, you’ll need to manually install to /opt/R or use rig, which is an R-specific installation manager. Currently, rig only supports Windows, Mac, and Ubuntu."
  },
  {
    "objectID": "chapters/sec2/2-4-app-admin.html#managing-system-libraries",
    "href": "chapters/sec2/2-4-app-admin.html#managing-system-libraries",
    "title": "10  Application administration",
    "section": "10.6 Managing System Libraries",
    "text": "10.6 Managing System Libraries\nAs an admin, you’ll also have to decide what to do about system packages, which are Linux libraries you install from a Linux repository or the internet.\nMany packages in Python and R don’t do any work themselves. Instead, they’re just language-specific interfaces to system packages. For example, any R or Python library that uses a JDBC database connector will need to make use of Java on your system. And many geospatial libraries make use of system packages like GDAL.\nAs the administrator, you’ll need to make sure you understand what system libraries are needed for the Python and R packages you’re using, and you’ll need to make sure they’re available and on the path.\nFor many of these libraries, it’s not a huge pain. You’ll just install the required library using apt or the system package manager for your distro. In some cases (especially Java), more configuration may be required to make sure that the package you need shows up on the path."
  },
  {
    "objectID": "chapters/sec2/2-4-app-admin.html#comprehension-questions",
    "href": "chapters/sec2/2-4-app-admin.html#comprehension-questions",
    "title": "10  Application administration",
    "section": "10.7 Comprehension Questions",
    "text": "10.7 Comprehension Questions\n\nWhat are two different ways to install Linux applications and what are commands to do so?\nWhat does it mean to daemonize a Linux application? What programs and commands are used to do so?\nHow do you know if you’ve opened nano or vim? How would you exit them if you didn’t mean to?\nWhat are 4 commands to read text files?\nHow would you create a file called secrets.txt, open it with vim, write something in, close and save it, and make it so that only you can read it?"
  },
  {
    "objectID": "chapters/sec2/2-4-app-admin.html#lab-installing-applications",
    "href": "chapters/sec2/2-4-app-admin.html#lab-installing-applications",
    "title": "10  Application administration",
    "section": "10.8 Lab: Installing Applications",
    "text": "10.8 Lab: Installing Applications\nAs we’ve started to administer our server, we’ve mostly been doing very generic server administration tasks. Now let’s set up the applications we need to run a data science workbench and get our API and Shiny app set up for using in our portfolio.\n\n10.8.1 Step 1: Install Python\nLet’s start by installing a data science version of Python so we’re not using the system Python for data science purposes.\nIf you want just one version of Python, you can apt-get install a specific version. As of this writing, Python 3.10 is a relatively new version of Python, so we’ll install that one with\n\n\nTerminal\n\nsudo apt-get install python3.10-venv\n\nOnce you’ve installed Python, you can check that you’ve got the right version by running\n\n\nTerminal\n\npython3 --version\n\nThis method, using apt-get, is great when you just want one version of Python. But if you want multiple versions sitting side by side, you’ll need to do something else. I recommend installing using the instructions on the Posit website, which will involve downloading and installing from .deb files.\n\n\n10.8.2 Step 2: Install R\nSince we’re using Ubuntu, we can use rig. There are good instructions on downloading rig and using it to install R on the rlib/rig GitHub repo. Use those instructions to install the current R release on your server.\nOnce you’ve installed R on your server, you can check that it’s running by just typing R into the command line. If that works, you’re good to move on to the next step. If not, you’ll need to make sure R got onto the path.\n\n\n10.8.3 Step 3: Install JupyerHub + JupyterLab\nJupyterHub and JupyterLab are Python programs, so we’re going to run them from within a Python virtual environment. I’d recommend putting that virtual environment inside /opt/jupyterhub.\nHere are the commands to create and activate a jupyterhub virtual environment in /opt/jupyterhub:\n\n\nTerminal\n\nsudo python3 -m venv /opt/jupyterhub\nsource /opt/jupyterhub/bin/activate\n\nNow we’re going to actually get JupyterHub up and running inside the virtual environment we just created. JupyterHub produces docs that you can use to get up and running very quickly. If you have to stop for any reason, make sure to come back, assume sudo, and start the JupyterHub virtual environment we created.\nNote that because we’re working inside a virtual environment, you may have to use the jupyterhub-singleuser version of the binary.\n\n\n10.8.4 Step 4: Daemonize JupyterHub\nBecause JupyterHub is a Python process, not a system process, it won’t automatically get daemonized, so we’ll have to do it manually.\nWe don’t need it right now, but it’ll be easier to manage JupyterHub later on from a config file that’s in /etc/jupyterhub. In order to do so, activate the jupyterhub virtual environment, create a default JupyterHub config (Google for the command), and move it into /etc/jupyterhub/jupyterhub_config.py.\nNow let’s move on to daemonizing JupyterHub. To start, kill the existing JupyterHub process (consult the cheatsheet in Appendix C if you need help). Since JupyterHub wasn’t automatically daemonized, you’ll have to manually create the systemd file.\nHere’s the file I created in /etc/systemd/system/jupyterhub.service.\n\n\n/etc/systemd/system/jupyterhub.service\n\n[Unit]\nDescription=Jupyterhub\nAfter=syslog.target network.target\n\n[Service]\nUser=root\nEnvironment=\"PATH=/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/opt/jupyterhub/bin\"\nExecStart=/opt/jupyterhub/bin/jupyterhub -f /etc/jupyterhub/jupyterhub_config.py\n\n[Install]\nWantedBy=multi-user.target\n\nThere are two things here worth noticing. The first is the Environment line that adds /opt/jupyterhub/bin to the path – that’s where our virtual environment is.\nThe second is ExecStart line, which provides is the startup command and specifies that JupyterHub should use the config we just created, specified by -f /etc/jupyterhub/jupyterhub_config.py.\nNow, you’ll need to use systemctl to reload the daemon, start JupyterHub, and enable it.\n\n\n10.8.5 Step 5: Install RStudio Server\nYou can find the commands to install RStudio server on the Posit website. Make sure to pick the version that matches your operating system. Since you’ve already installed R, you can skip down to the “Install RStudio Server” step.\nUnlike JupyterHub, RStudio Server does daemonize itself right out of the box, so you can check and control the status with systemctl without any further work.\n\n\n10.8.6 Step 6: Run the penguin API from Docker\nFirst, you’ll have to make sure that Docker itself is available on the system. It can be installed from apt using apt-get install docker.io. You may need to adopt sudo privileges to do so.\nOnce Docker is installed, getting the API running is almost trivially easy using the command we used back in Chapter 6 to run our container.\n\n\nTerminal\n\nsudo docker run --rm -d \\\n  -p 8080:8080 \\\n  --name penguin-model \\\n  alexkgold/penguin-model\n\nThe one change you might note is that I’ve changed the port on the server to be 8080, since JupyterHub runs on 8000 by default.\nOnce it’s up, you can check that it’s running with docker ps.\n\n\n10.8.7 Step 7: Put up the Shiny app\nWe’re going to use Shiny Server to host our Shiny app on the server. Start by moving the app code to the server. I put mine in /home/test-user/do4ds-lab/app by cloning the Git repo.\nAfter that, you’ll need to:\n\nOpen R or Python and rebuild the package library with {renv} or {venv}.\nInstall Shiny Server using the instructions from the Admin Guide.\n\nNote that you can skip steps to install R and/or Python, as well as the {shiny} package as we’ve already done that.\n\nEdit Shiny Server’s configuration file to run the right app.\nStart and enable Shiny Server with systemctl.\n\n\n\n10.8.8 Lab Extensions\nThere are a few things you might want to consider before moving into the next chapter, where we’ll start working on giving this server a stable public URL.\nFirst, we haven’t daemonized the API. Feel free to try Docker Compose or setting a restart policy for the container.\nSecond, neither the API nor the Shiny app will automatically update when we make changes to them. You might want to set up a GitHub Action to do so. For Shiny Server, you’ll need to push the updates to the server and then restart Shiny Server. For the API, you’d need to configure a GitHub action to rebuild the container and push it to a registry. You’d then need to tell Docker on the server to re-pull and restart the container.\nFinally, there’s no authentication in front of our API. Now, the API has pretty limited functionality, so that’s not a huge worry. But if you had an API with more functionality that might be a problem. Additionally, someone could try to flood your API with requests to make it unusable. The most common way to solve this is to buy a product that hosts the API for you or to put an authenticating proxy in front of the API (Google for instructions for NGINX if you want to try it)."
  },
  {
    "objectID": "chapters/sec2/2-4-app-admin.html#footnotes",
    "href": "chapters/sec2/2-4-app-admin.html#footnotes",
    "title": "10  Application administration",
    "section": "",
    "text": "vi is the original fullscreen text editor for Linux. vim is its successor (vim stands for vi improved). I’m not going to worry about the distinction.↩︎"
  },
  {
    "objectID": "chapters/sec2/2-5-scale.html#the-briefest-intro-to-computational-theory",
    "href": "chapters/sec2/2-5-scale.html#the-briefest-intro-to-computational-theory",
    "title": "11  Server Resources and Scaling",
    "section": "11.1 The briefest intro to computational theory",
    "text": "11.1 The briefest intro to computational theory\nYou’re probably aware that everything you’ve ever seen on a computer – from this book to your work in R or Python, your favorite internet cat videos, and Minecraft – it’s just 1s and 0s.\nBut the 1s and 0s aren’t actually the interesting part. The interesting part is that these 1s and 0s don’t directly represent a cat or a book. Instead, the 1s and 0s are binary representations of integers (whole numbers) and the only thing the computer does is add these integers.1\nOnce they’ve been added, they’re reinterpreted back into something meaningful. But the bottom line is that every bit of input your computer gets is turned into an addition problem, processed, and the results are reverted back into something we interpret as meaningful.\nSo, a computer is really just a factory that does addition problems. So as you’re thinking about how to size and scale a server, you’re really thinking about how to optimally design your factory.\nThere are three essential resources a server has – compute, memory, and storage. In this chapter, I’m going to share some mental models I find helpful about how to think of each and some recommendations for getting a data science server."
  },
  {
    "objectID": "chapters/sec2/2-5-scale.html#how-computers-compute",
    "href": "chapters/sec2/2-5-scale.html#how-computers-compute",
    "title": "11  Server Resources and Scaling",
    "section": "11.2 How computers compute",
    "text": "11.2 How computers compute\nThe addition assembly line itself – where the work actually gets done – is referred to as compute. It’s where 2+2 gets turned into 4, and where 345619912 + 182347910 gets turned into 527967822.\nThe main compute that all computers have is the central processing unit (CPU), which completes addition problems in a core.\nThe first attribute that determines the speed of compute is the number of cores, which is like the number of conveyor belts in the factory.\nThese days, most consumer-grade laptops have between 4 and 16 physical cores. Many have software capabilities that effectively double that number, so they can do between 4 and 32 simultaneous addition problems.\nThe second speed-determining attribute is how quickly a single addition problem gets completed by a single core. This is called the single core clock speed. You can think of this as how fast the conveyor belt moves.\nClock speeds are measured in operations per second or hertz (hz). The cores in your laptop probably max out between two and five gigahertz (GHz), which means between 2 billion and 5 billion operations per second.\nFor decades, many of the innovations in computing were coming from increases in single core clock speed, but those have fallen off a lot in the last few decades. The clock speeds of consumer-grade chips increased by approximately 10x during the 90s, by 2-3x in the 2000s, and somewhere between not at all and 1.5x in the 2010s.\nBut computers have continued getting a lot faster even as the increase in clock speeds has slowed. The increase has mostly come from increases in the number of cores, better software usage of parallelization, and faster loading and unloading of the CPU (called the bus)."
  },
  {
    "objectID": "chapters/sec2/2-5-scale.html#recommendation-1-fewer-faster-cpu-cores",
    "href": "chapters/sec2/2-5-scale.html#recommendation-1-fewer-faster-cpu-cores",
    "title": "11  Server Resources and Scaling",
    "section": "11.3 Recommendation 1: Fewer, faster CPU cores",
    "text": "11.3 Recommendation 1: Fewer, faster CPU cores\nR and Python are single-threaded. This means that unless you’re using special libraries for parallel processing, you’ll end up red-lining a single CPU core while the others just look on in silence.\nTherefore for most R and Python work, single core clock speed matters more than the number of cores, and fewer, faster cores are usually preferable to many slower\nYou’re probably not used to thinking about this tradeoff from buying a laptop or phone. The reality is that modern CPUs are pretty darn good and you should just buy the one that fits your budget.\nIf you’re standing up a server, you often do have an explicit choice between more slower cores and fewer faster ones, determined by the instance family.\nIf you’re running a multi-user server, the number of cores you need can be hard to estimate. If you’re doing non-ML tasks like counts and dashboarding or relatively light-duty machine learning I might advise the following:\n\\[\n\\text{n cores} = \\text{1 core per user} + 1\n\\]\nThe spare core is for the server to do its own operations apart from the data science usage. On the other hand, if you’re doing heavy-duty machine learning or parallelizing jobs across the CPU, you may need more cores than this rule of thumb."
  },
  {
    "objectID": "chapters/sec2/2-5-scale.html#how-memory-works",
    "href": "chapters/sec2/2-5-scale.html#how-memory-works",
    "title": "11  Server Resources and Scaling",
    "section": "11.4 How memory works",
    "text": "11.4 How memory works\nYour computer’s random access memory (RAM) is its short term storage. You can think of RAM like the stock that’s sitting out on the factory floor ready to go right on an assembly line and the completed work that’s ready to be shipped.\nRAM is very fast to for your computer to access, so you can read and write to it very quickly. The downside is that it’s temporary. When your computer turns off, the RAM gets wiped.2\n\n\n\n\n\n\nNote\n\n\n\nMemory and storage are measured in bytes with metric prefixes.\nCommon sizes for memory these days are in gigabytes (billion bytes) and terrabytes (trillion bytes). Some enterprise data stores run on the scales of thousands of terrabytes (pettabytes) or even thousands of pettabytes (yottabytes).\n\n\nModern consumer-grade laptops come with somewhere between 4 and 16 Gb of memory."
  },
  {
    "objectID": "chapters/sec2/2-5-scale.html#recommendation-2-get-as-much-ram-as-feasible",
    "href": "chapters/sec2/2-5-scale.html#recommendation-2-get-as-much-ram-as-feasible",
    "title": "11  Server Resources and Scaling",
    "section": "11.5 Recommendation 2: Get as much RAM as feasible",
    "text": "11.5 Recommendation 2: Get as much RAM as feasible\nIn most cases, R and Python have to load all of your data into memory. Thus, the size of the data you can use is limited to the size of your machine’s RAM.\nMost other limits of your machine will just result in things being slower than you’d really want, but trying to load too much data into memory will result in a session crash.\n\n\n\n\n\n\nNote\n\n\n\nIf you’re running into this limitation, go back and think about your project architecture as discussed in Chapter 2. Maybe you can load less data into memory.\n\n\nBecause your computer needs memory for things other than R and Python and because you’ll often be doing transformations that temporarily increase the size of your data, you need more memory than your largest data set.\nIn general, you’ll always want more RAM, but a pretty good rule of thumb is that you’ll be happy if:\n\\[\\text{Amount of RAM} \\ge 3 * \\text{max amount of data}\\]\nIf you’re thinking about running a multi-user server, you’ll need to take a step back to think about how many concurrent users you expect and how much data you expect each one to load."
  },
  {
    "objectID": "chapters/sec2/2-5-scale.html#understanding-storage",
    "href": "chapters/sec2/2-5-scale.html#understanding-storage",
    "title": "11  Server Resources and Scaling",
    "section": "11.6 Understanding storage",
    "text": "11.6 Understanding storage\nStorage, or hard disk/drive, is your computer’s place to put things for the long-term. It’s where applications are installed, and where you save things you want to keep.\nRelative to the RAM that’s right next to the factory floor, your computer’s storage is like the warehouse in the next building over. It’s slower to get things from storage than RAM, but it’s also permanent once its stored there.\nUp until a few years ago, storage was much slower than RAM. Those drives, called HDD drives, had a bunch of spinning magnetic disks with magnetized read/write heads that move among the disks to save and read data.\nWhile they spin very fast – 5,400 and 7,200 RPM are common speeds – there were still physical moving parts, and reading and writing data was very slow by computational standards.\nIn the last few years, solid-state drives (SSDs) have become more-or-less standard in laptops. SSDs, which are collections of flash memory chips with no moving parts are up to 15x faster than HDDs."
  },
  {
    "objectID": "chapters/sec2/2-5-scale.html#recommendation-3-get-lots-of-storage-its-cheap",
    "href": "chapters/sec2/2-5-scale.html#recommendation-3-get-lots-of-storage-its-cheap",
    "title": "11  Server Resources and Scaling",
    "section": "11.7 Recommendation 3: Get lots of storage, it’s cheap",
    "text": "11.7 Recommendation 3: Get lots of storage, it’s cheap\nAs for configuring storage on your server – get a lot – but don’t think about it too hard, because it’s cheap and easy to upgrade. Storage is cheap enough these days that it is almost always more cost efficient to buy more storage rather than making a highly-paid professional spend their time trying to figure out how to move things around.\n\n\n\n\n\n\nNote\n\n\n\nIf the IT/Admins at your organization want you to spend a lot of time deleting things from storage that’s usually a red flag that they aren’t thinking much about how to make the overall organization work more smoothly.\n\n\nIf you’re running a multi-user server, the amount of storage you need depends a lot on your data and your workflows.\nIf you’re not running particularly large data, or most of your data won’t be saved into the server’s storage (generally a good thing), a reasonable rule of thumb is to choose\n\\[\n\\text{Amount of Storage} = \\text{1Gb} * \\text{n users}\n\\]\nIf you’re not saving large data files, the amount of space each person needs on the server is small. Code is very small and it’s rare to see R and Python packages take up more than a few dozen Mb per data scientist.\n\n\n\n\n\n\nNote\n\n\n\nIf you’re working with a professional IT admin, they may be concerned about the storage implications of having package copies for each person on their team if you’re following best practices for running environments as code from Chapter 1. I’ve heard this concern a lot from IT/Admins thinking ahead about running their server and almost never of a case where it’s actually been a problem.\n\n\nIf, on the other hand, you will be saving a lot of data, you’ve got to take that into account. In some organizations, each data scientist will save dozens of flat files of a Gb or more for each of their projects.\nIf you’re operating in the cloud, this really isn’t an important choice. As you’ll see in the lab, upgrading the amount of storage you have is a trivial operation, requiring at most a few minutes of downtime. Choose a size you guess will be adequate and add more if you need."
  },
  {
    "objectID": "chapters/sec2/2-5-scale.html#gpus-are-special-purpose-compute",
    "href": "chapters/sec2/2-5-scale.html#gpus-are-special-purpose-compute",
    "title": "11  Server Resources and Scaling",
    "section": "11.8 GPUs are special-purpose compute",
    "text": "11.8 GPUs are special-purpose compute\nAll computers have a CPU. There are also specialized chips where the CPU can offload particular tasks, the most common of which is the graphical processing unit (GPU). GPUs are architected for tasks like editing photo or videos, rendering video game graphics, some kinds of machine learning, and (yes) Bitcoin mining.\nA GPU is an addition factory just like a CPU, but with the opposite architecture. CPUs have only a handful of cores, but those cores are fast. A GPU takes the opposite approach, with many (relatively) slow cores.\nWhere a consumer-grade CPU has 4-16 cores, mid-range GPUs have 700-4,000 cores, with each one running at only about 1% to 10% the single core clock speed speed of a CPU core.\nThe choice of whether you need a GPU to do your work will really depend on what you’re doing and your budget.\nFor the tasks GPUs are good at, the overwhelming parallelism ends up being more important than the speed of any individual core, and GPU computation can be dramatically faster."
  },
  {
    "objectID": "chapters/sec2/2-5-scale.html#recommendation-4-get-a-gpu-maybe",
    "href": "chapters/sec2/2-5-scale.html#recommendation-4-get-a-gpu-maybe",
    "title": "11  Server Resources and Scaling",
    "section": "11.9 Recommendation 4: Get a GPU, maybe",
    "text": "11.9 Recommendation 4: Get a GPU, maybe\nThe tasks that most benefit from GPU computing are training highly parallel machine learning models like deep learning or tree-based models. If you do have one of these use cases, GPU computing can massively speed up your computation – making models trainable in hours instead of days.\nIf you are planning to use cloud resources for your computing, GPU-backed instances are quite pricey, and you’ll want to be careful about only putting those machines up when you’re using them.\nBecause GPUs are expensive, I generally wouldn’t bother with GPU-backed computing unless you’ve already tried without and find that it takes too long to be feasible.\nIt’s also worth noting that using a GPU won’t happen automatically. The tooling has gotten good enough that it’s usually pretty easy to set up, but your computer won’t train your XGBoost models on your GPU unless you tell it to do so."
  },
  {
    "objectID": "chapters/sec2/2-5-scale.html#assessing-ram-cpu-usage",
    "href": "chapters/sec2/2-5-scale.html#assessing-ram-cpu-usage",
    "title": "11  Server Resources and Scaling",
    "section": "11.10 Assessing RAM + CPU usage",
    "text": "11.10 Assessing RAM + CPU usage\nOnce you’ve chosen your server size and gotten up and running, you’ll want to be able to monitor RAM and CPU for problems.\nAny program that is running is called a process. For example, when you type python on the command line to open a REPL, that starts a single Python process. If you were to start a second terminal session and run python again, you’d have a second Python process.\nComplicated programs often involve multiple interlocking processes. For example, running the RStudio IDE involves (at minimum) one process for the IDE itself and one for the R session that it uses in the background. The relationships between these different processes is mostly hidden from you – the end user.\nAs an admin, you may want to inspect the processes running on your system at any given time. The top command is a good first stop. top shows the top CPU-consuming processes in real time along with a number of other facts about them.\nHere’s the top output from my machine as I write this sentence.\n\n\nTerminal\n\nPID    COMMAND      %CPU TIME     #TH    #WQ  #PORT MEM    PURG   CMPRS PGRP\n0      kernel_task  16.1 03:56:53 530/10 0    0     2272K  0B     0B    0\n16329  WindowServer 16.0 01:53:20 23     6    3717  941M-  16M+   124M  16329\n24484  iTerm2       11.3 00:38.20 5      2    266-  71M-   128K   18M-  24484\n29519  top          9.7  00:04.30 1/1    0    36    9729K  0B     0B    29519\n16795  Magnet       3.1  00:39.16 3      1    206   82M    0B     39M   16795\n16934  Arc          1.8  18:18.49 45     6    938   310M   144K   61M   16934\n\nIn most instances, the first three columns are the most useful. The first column is the unique process id (pid) for that process. You’ve got the name of the process (COMMAND) and how much CPU its using. You’ve also got the amount of memory used a few columns over. Right now, nothing is using very much CPU.\nThe top command takes over your whole terminal. You can exit with Ctrl + c.\n\n\n\n\n\n\nSo much CPU?\n\n\n\nFor top (and most other commands), CPU is expressed as a percent of single core availability. So, on a modern machine with multiple cores, it’s very common to see CPU totals well over 100%. Seeing a single process using over 100% of CPU is rarer.\n\n\nAnother useful command for finding runaway processes is ps aux. It lists a snapshot of all processes running on the system, along with how much CPU or RAM they’re using. You can sort the output with the --sort flag and specify sorting by cpu with --sort -%cpu or by memory with --sort -%mem.\nBecause ps aux returns every running process on the system, you’ll probably want to pipe the output into head. In addition to CPU and Memory usage, ps aux gets you who launched the command and the PID.\nFor example, here are the RStudio processes currently running on my system.\n\n\nTerminal\n\nUSER               PID  %CPU %MEM      VSZ    RSS   TT  STAT STARTED      TIME COMMAND\nalexkgold        23583   0.9  1.7 37513368 564880   ??  S    Sat09AM  17:15.27 /Applications/RStudio.app/Contents/MacOS/RStudio\nalexkgold        23605   0.5  0.4 36134976 150828   ??  S    Sat09AM   1:58.16 /Applications/RStudio.app/Contents/MacOS/rsession --config-file none --program-mode desktop \n\nOne of the times you’ll be most interested in the output of top or ps aux is when something is going rogue on your system and using more resources than you intended. If you have some sense of who started the runaway process or what it it, it can be useful to pipe the output of ps aux into grep.\nFor example, the command to get the output above was ps aux | RStudio.\nIf you’ve got a rogue process, the pattern is to try to find the process and make note of its pid. Then you can immediately end the process by pid with the kill command.\nIf I were to find something concerning – perhaps an R process that is using 500% of CPU – I would want to take notice of its pid to kill it with kill."
  },
  {
    "objectID": "chapters/sec2/2-5-scale.html#examining-at-storage-usage",
    "href": "chapters/sec2/2-5-scale.html#examining-at-storage-usage",
    "title": "11  Server Resources and Scaling",
    "section": "11.11 Examining at storage usage",
    "text": "11.11 Examining at storage usage\nA common culprit for weird server behavior is running out of storage space. There are two handy commands for monitoring the amount of storage you’ve got – du and df.\nThese commands are almost always used with the -h flag to put file sizes in human-readable formats.\ndf, for disk free, shows the capacity left on the device where the directory sits.\nFor example, here’s the result of running the df command on the chapters directory on my laptop that includes this chapter.\n\n\nTerminal\n\n&gt; df -h chapters\nFilesystem     Size   Used  Avail Capacity iused      ifree %iused  Mounted on\n/dev/disk3s5  926Gi  227Gi  686Gi    25% 1496100 7188673280    0%   /System/Volumes/Data\n\nSo you can see that the chapters folder lives on a disk called /dev/disk3s5 that’s a little less than 1Tb and is 25% full – no problem. On a server this can be really useful to know, because it’s quite easy to switch a disk out for a bigger one in the same spot.\nIf you’ve figured out that a disk is full, it’s usually most cost effective to just buy a bigger disk. But sometimes something weird happens. Maybe there are a few exceptionally big files, or you think unnecessary copies are being made.\nIf so, the du command, short for disk usage, gives you the size of individual files inside a directory. It’s particularly useful in combination with the sort command.\nFor example, here’s the result of running du on the chapters directory where the text files for this book live.\n\n\nTerminal\n\n&gt; du -h chapters | sort\n12M chapters\n1.7M    chapters/sec1/images\n1.8M    chapters/sec1\n236K    chapters/images\n488K    chapters/sec2/images-traffic\n5.3M    chapters/sec2/images-networking\n552K    chapters/sec2/images\n6.6M    chapters/sec2\n892K    chapters/append/images-append\n948K    chapters/append\n\nSo if I were thinking about cleaning up this directory, I could see that my sec1/images directory is my biggest single directory. If you find yourself needing to find big files on your Linux server, it’s worth spending some time with the help pages for du. There are lots of really useful options."
  },
  {
    "objectID": "chapters/sec2/2-5-scale.html#running-out-of-resources",
    "href": "chapters/sec2/2-5-scale.html#running-out-of-resources",
    "title": "11  Server Resources and Scaling",
    "section": "11.12 Running out of Resources",
    "text": "11.12 Running out of Resources\nIf you recognize that you’re running out of resources on your current server, you may want to move to something bigger. There are two main reasons servers run out of room.\nThe first reason is because people are running big jobs. This can happen at any scale of organization. There are data science teams of one who have use cases that necessitate terrabytes of data.\nThe second reason is because you have a lot of people using your server. This is generally a feature of big data science teams, irrespective of the size of the workloads.\nEither way, there are two basic options for how to scale your data science workbench. The first is vertical scaling, which is just a fancy way of saying get a bigger server. The second option is horizontal scaling, which means running a whole fleet of servers in parallel and spreading the workload across them.\nAs a data scientist, you shouldn’t be shy about vertically scaling if your budget allows it. The complexity of managing a t3.nano with 2 cores and 0.5 Gb of memory is exactly the same as a C5.24xlarge with 96 cores and 192 Gb of memory. In fact, the bigger one may well be easier to manage, since you won’t have to worry about running low on resources.\nThere are limits to the capacity of vertical scaling. As of this writing, AWS’s general-use instance types max out at 96-128 cores. That can quickly get eaten up by 50 data scientists with reasonably heavy computational demands.\nOnce you’re thinking about horizontal scaling, you’ve got a distributed service problem on your hand, which is inherently difficult. You should almost certainly get an IT/Admin professional involved. See Chapter 18 for more on how to talk to them about it.\n\n11.12.1 AWS Instances for data science\nAWS offers a variety of different EC2 instance types split up by family and size. The family is the category of EC2 instance. Different families of instances are optimized for different kinds of workloads.\nHere’s a table of common instance types for data science purposes:\n\n\n\n\n\n\n\nInstance Type\nWhat it is\n\n\n\n\nt3\nThe “standard” configuration. Relatively cheap. Sizes may be limited.\n\n\nC\nCPU-optimized instances, aka faster CPUs\n\n\nR\nHigher ratio of RAM to CPU\n\n\nP\nGPU instances, very expensive\n\n\n\nWithin each family, there are different sizes available, ranging from nano to multiples of xl. Instances are denoted by &lt;family&gt;.&lt;size&gt;. So, for example, when we put our instance originally on a free tier machine, we put it on a t2.micro.\nIn most cases, going up a size doubles the amount of RAM, the number of cores, and the cost. So you should do some quick math before you stand up a C5.24xlarge or a GPU-based P instance. If your instance won’t be up very long, it may be fine, but make sure you take it down when you’re done lest you rack up a huge bill."
  },
  {
    "objectID": "chapters/sec2/2-5-scale.html#comprehension-questions",
    "href": "chapters/sec2/2-5-scale.html#comprehension-questions",
    "title": "11  Server Resources and Scaling",
    "section": "11.13 Comprehension Questions",
    "text": "11.13 Comprehension Questions\n\nThink about the scenarios below – which part of your computer would you want to upgrade to solve the problem?\n\nYou try to load a big csv file into pandas in Python. It churns for a while and then crashes.\nYou go to build a new ML model on your data. You’d like to re-train the model once a day, but it turns out training this model takes 26 hours on your laptop.\nYou design an visualization Matplotlib , and create a whole bunch in a loop, you want to parallelize the operation. Right now you’re running on a t2.small with 1 CPU.\n\nDraw a mind map of the following: CPU, RAM, Storage, Operations Per Second, Parallel Operations, GPU, Machine Learning\nWhat are the architectural differences between a CPU and a GPU? Why does this make a GPU particularly good for Machine Learning?\nHow would you do the following:\n\nFind all running Jupyter processes that belong to the user alexkgold.\nFind the different disks attached to your server and see how full each one is.\nFind the biggest files in each user’s home directory."
  },
  {
    "objectID": "chapters/sec2/2-5-scale.html#lab-changing-instance-size",
    "href": "chapters/sec2/2-5-scale.html#lab-changing-instance-size",
    "title": "11  Server Resources and Scaling",
    "section": "11.14 Lab: Changing Instance Size",
    "text": "11.14 Lab: Changing Instance Size\nIn this lab, we’re going to upgrade the size of our server. And the best part is that because we’re in the cloud, it’ll take only a few minutes.\n\n11.14.1 Step 1: Confirm current server size\nFirst, let’s confirm what we’ve got available. You can check the number of CPUs you’ve got with lscpu in a terminal. Similarly, you can check the amount of RAM with free -h. This is just so you can prove to yourself later that the instance really changed.\n\n\n11.14.2 Step 2: Change the instance type and bring it back\nNow, you can go to the instance page in the AWS console. The first step is to stop (not terminate!) the instance. This means that changing instance type does require some downtime for the instance, but it’s quite limited.\nOnce the instance has stopped, you can change the instance type under Actions &gt; Instance Settings. Then start the instance. It’ll take a few seconds to start the instance.\n\n\n11.14.3 Step 3: Confirm new server size\nSo, for example, I changed from a t2.micro to a t2.small. Both only have 1 CPU, so I won’t see any difference in lscpu, but running free -h before and after the switch reveals the difference in the total column:\ntest-user@ip-172-31-53-181:~$ free -h\n               total        used        free      shared  buff/cache   available\nMem:           966Mi       412Mi       215Mi       0.0Ki       338Mi       404Mi\nSwap:             0B          0B          0B\ntest-user@ip-172-31-53-181:~$ free -h\n               total        used        free      shared  buff/cache   available\nMem:           1.9Gi       225Mi       1.3Gi       0.0Ki       447Mi       1.6Gi\nSwap:             0B          0B          0B\nI got twice as much RAM!\nThere are some rules around being able to change from one instance type to another, but this is a superpower if you’ve got variable workloads or a team that’s growing. Once you’re done with your larger server, it’s just as easy to scale it back down.\n\n\n11.14.4 Step 4: Upgrade storage (maybe)\nIf you want more storage, it’s similarly easy to resize the EBS volume attached to your server.\nI wouldn’t recommend doing it for this lab, because you can only automatically adjust volume sizes up, so you’d have to manually transfer all of your data if you ever wanted to scale back down.\nIf you do resize the volume, you’ll have to let Linux know so it can resize the filesystem with the new space available. AWS has a great walk through called Extend a Linux filesystem after resizing the volume that I recommend you follow."
  },
  {
    "objectID": "chapters/sec2/2-5-scale.html#footnotes",
    "href": "chapters/sec2/2-5-scale.html#footnotes",
    "title": "11  Server Resources and Scaling",
    "section": "",
    "text": "This was proved in Alan Turing’s 1936 paper on computability. If you’re interested in learning more, I recommend The Annotated Turing: A Guided Tour Through Alan Turing’s Historic Paper on Computability and the Turing Machine by Charles Petzold for a surprisingly readable walkthrough.↩︎\nYou probably don’t experience this personally. Modern computers are pretty smart about dumping RAM onto the hard disk before shutting down, and bringing it back on startup, so you usually won’t notice this happening.↩︎"
  },
  {
    "objectID": "chapters/sec2/2-6-networking.html#understanding-digital-addresses",
    "href": "chapters/sec2/2-6-networking.html#understanding-digital-addresses",
    "title": "12  Intro to Computer Networks",
    "section": "12.1 Understanding digital addresses",
    "text": "12.1 Understanding digital addresses\nYou already use digital addresses all the time in the form of a URL (Uniform Resource Locator).1 A URL fully specifies the network location of a resource and looks like this:\n\\[\\overbrace{\\text{https://}}^\\text{protocol}\\underbrace{\\text{google.com}}_\\text{domain}\\overbrace{\\text{:443}}^\\text{port}\\underbrace{\\text{/}}_\\text{path}\\]\nThis may look a little strange. You’re probably used to using just the domain and maybe a path in your web browser like \\(\\text{google.com}\\) or \\(\\text{google.com/maps}\\). The reason is that you’re usually fine with the default protocol and port, so you may never have realized they’re there.2\nHere’s what each of those four parts are:\n\nThe application layer protocol (often just called the protocol) specifies what type of traffic this is. It’s like agreeing that your letter will be in English or Arabic or Dutch.\nThe domain is a human-readable way of providing the digital street address of the server. We’ll get into the actual address later in the chapter.\nThe port specifies where on the server to direct the traffic. It’s the digital equivalent of the apartment number.\nThe path is a human-friendly way of specifying who you intend the message to go to. It’s like the name of person you’re addressing on your letter.\n\nAs a cloud server admin, most of the networking you’ll do is to ensure someone can just use a domain with a path and access the resources they need from your server."
  },
  {
    "objectID": "chapters/sec2/2-6-networking.html#finding-the-right-digital-address",
    "href": "chapters/sec2/2-6-networking.html#finding-the-right-digital-address",
    "title": "12  Intro to Computer Networks",
    "section": "12.2 Finding the right digital address",
    "text": "12.2 Finding the right digital address\nA domain is the human-readable way of addressing a resource on the internet. But it’s not actually the digital street address of any particular server (host). Instead, hosts are actually identified with an IP Address. When that IP address is valid across the entire internet, it’s a public IP address.\n\n\n\n\n\n\nNote\n\n\n\nIP Addresses are mapped to domains via the Domain Name Service (DNS), which you’ll learn about in Chapter 13\n\n\nNetwork traffic arrives in the form of packets, which are routed to the correct IP address in a process called packet switching. You don’t need to understand any more about packet switching than that it reliable gets packets to the right IP Address.3\nOnce the traffic arrives at the server, it has to get to the right service by getting to the right port. Every computer has just over 65,000 ports. Each port is uniquely identified by a number, and there’s a 1-1 mapping between listening services and open ports. Since you’re probably running no more than a handful of services, the overwhelming majority of the ports are closed at any given time.4\nBy default, HTTP traffic goes to port \\(80\\) and HTTPS traffic goes to port \\(443\\). So if there’s just one service, you would configure the application to listen on port \\(80\\) and/or \\(443\\). Then, when people come in, they’d automatically get to the right service.\nBut sometimes you’ve got multiple services on the server. In that case, each one will need to run on a unique port. However, you don’t want to force users to remember, for example, that they access JupyterHub on port \\(8000\\). Instead, you want them just to know that it’s on \\(\\text{/jupyter}\\).\nA proxy is a piece of software that can be used to map paths to another location. Common proxies you might hear about include the open source NGINX and Apache and the paid F5, which is the developer of NGINX. Depending on the configuration, the proxy can be on the same server or a different one.\nIf you’re using a proxy, you’d put each of the server’s services on a different port and use the proxy to redirect incoming traffic to the right place based on the subpath.\nNow, if you were just to put something on port \\(80\\) on your EC2 instance and try to access it from the web, it still wouldn’t work. That’s because there’s a firewall sitting in front of your EC2 instance, which blocks traffic to all but certain ports. In AWS, the default firewall is the security group.\nIn addition to blocking traffic to arriving at certain ports, firewalls can be restricted to allow access only from certain IP Addresses. This can be used, for example, to only allow access from your office to a server. Unless a particular server will only ever be accessed by other servers with known IP addresses, this is a brittle way to configure security and I generally don’t recommend it.\n\n\n\n\n\n\nTip\n\n\n\nIf you think you’ve configured a service correctly and you just can’t seem to access it, one of the first things to check is whether you’ve got the port open in the security group.\nOne symptom that may indicate a security group issue is if you try to go to your service and it just hangs with no response before eventually timing out.\n\n\n\nOnce the traffic makes it to the right server, through the firewall and proxy, and to the right port, it has to communicate using the right application layer protocol. We’ve been talking exclusively about the HTTP and HTTPS application layer protocols, because web traffic arrives as a series of HTTP GET requests, but there are many other application layer protocols, each with its own default port.\nFor example, you’ve already seen a lot about SSH in this book, which is an application layer protocol for allowing secure login and communication over an unsecured network. SSH defaults to port \\(22\\).\nOther important protocols that you might see in this book or elsewhere are SFTP for file transfers, SMTP for emails, LDAP(S) for auth, and websockets, which are used by Shiny and Streamlit."
  },
  {
    "objectID": "chapters/sec2/2-6-networking.html#recognizing-ip-addresses",
    "href": "chapters/sec2/2-6-networking.html#recognizing-ip-addresses",
    "title": "12  Intro to Computer Networks",
    "section": "12.3 Recognizing IP addresses",
    "text": "12.3 Recognizing IP addresses\nIf you’ve seen an IP address before, it probably was an IPv4 address, which are four blocks of 8-bit fields (numbers between \\(0\\) and \\(255\\)) with dots in between, so they look like \\(64.56.223.5\\).\nIf you do the math, you’ll realize there are “only” about 4 billion of these. There are so many things on the public internet that we are running out of IPv4 addresses. The good news is that smart people started planning for this a while ago and the adoption of the new IPv6 standard started a few years ago.\nIPv6 addresses are eight blocks of hexadecimal (\\(\\text{0-9}\\) and \\(\\text{a-f}\\)) digits separated by colons, with certain rules that allow them to be shortened, so \\(\\text{4b01:0db8:85a3:0000:0000:8a2e:0370:7334}\\) or \\(\\text{3da4:66a::1}\\) are both examples of valid IPv6 addresses. There’s no worry about running out of IPv6 addresses any time soon, because the total quantity of IPv6 addresses is a number with 39 zeroes.\nIPv6 will coexist with IPv4 for a few decades and we’ll eventually switch entirely to IPv6.\nThere are a few special IPv4 addresses it’s worth knowing. You’ll probably see \\(127.0.0.1\\) a lot, which is also known as \\(\\text{localhost}\\) or loopback. This is the way a machine refers to itself.\nFor example, if you open a Shiny app in RStudio Desktop, the app will pop up in a little window along with a notice that says\nListening on http://127.0.0.1:6311\nThat means that the Shiny app is running on the same computer and is available on port \\(6311\\). You can open that location in your browser to view the app as it runs.\nThere are also a few blocks of IPv4 addresses – those that start with \\(192.168\\), \\(172.16\\), and \\(10\\) that are reserved for use on private networks, so they’re never assigned in public."
  },
  {
    "objectID": "chapters/sec2/2-6-networking.html#basic-network-administration",
    "href": "chapters/sec2/2-6-networking.html#basic-network-administration",
    "title": "12  Intro to Computer Networks",
    "section": "12.4 Basic network administration",
    "text": "12.4 Basic network administration\nNetworking can be difficult to manage because there are so many layers. It will frequently happen that you think a service is configured, but you just can’t seem to access it. Here are some basic tools for network debugging.\n\n12.4.1 Browser devtools\nOne of the most useful tools for debugging networking issues can be found in the menus of your web browser. It has developer tools that allow you to inspect the network traffic going back and forth between your machine and a remote.\nThis can be really handy if things are going slowly or if you’re not sure why the page isn’t loading. By inspecting the status codes of different HTTP calls and the time they take, you can develop a pretty good idea of where things might be getting stuck.\n\n\n12.4.2 SSH tunneling/port forwarding\nWhen you start a new EC2 instance in AWS, the default security group opens only port \\(22\\), so only SSH traffic is allowed.\nSo far, you’ve seen SSH used to access the command line on that remote server, but SSH can actually be used to access any port in a process called tunneling or port forwarding.\nWhen you tunnel, you make a port on the remote host available at the same port on \\(\\text{localhost}\\) on your machine. The most common usage is to use your browser to look at what’s available at a specific port on a server via HTTP without configuring the host to accept HTTP traffic on that port.\nYou can create an SSH tunnel to a remote host with\n\n\nTerminal\n\nssh -L &lt;remote port&gt;:localhost:&lt;local port&gt; &lt;user&gt;@&lt;server&gt;\n\nI find that the syntax for port forwarding completely defies my memory and I have to google it every time I use it.5\nSo, for example, if your server were running at \\(64.56.223.5\\) and you has the SSH user test-user, you might forward JupyterHub on port \\(8000\\) with ssh -L 8000:localhost:8000 test-user@64.56.223.5. Once the tunnel is established, you could access JupyerHub in your browser on \\(\\text{localhost:8000}\\).\n\n\n12.4.3 Checking what ports are open\nSometimes you can just forget what ports are open on your machine and for what purposes. Or you want to double check that a configuration change took. In that case, you want to use the netstat command to get the services that are running and their associated ports.\nFor this use, netstat is generally most useful with the -tlp flags to show programs that are listening and the programs associated.\n\n\n12.4.4 Checking if a host is accessible\nThe ping command can be useful for checking whether your server is reachable on the network. For example, the server where this book lives is at \\(185.199.110.153\\). So I can ping that domain to check if it’s accessible.\n&gt; ping -o 185.199.110.153\n\nPING 185.199.110.153 (185.199.110.153): 56 data bytes\n64 bytes from 185.199.110.153: icmp_seq=0 ttl=58 time=23.322 ms\n\n--- 185.199.110.153 ping statistics ---\n1 packets transmitted, 1 packets received, 0.0% packet loss\nround-trip min/avg/max/stddev = 23.322/23.322/23.322/nan ms\nThe -o flag tells ping to try just once as opposed to pinging continuously. The fact that I transmitted and received one packet means that everything is working properly.\nSeeing an unreachable host or packet loss would be an indication that my networking probably isn’t configured correctly somewhere between me and the server. That means it’s time to check that the server is actually up, followed by firewalls (security groups) and proxies. You can also use ping with a domain, so it can also be used to see if DNS is working properly.\nIf ping succeeds but a particular resource is inaccessible, curl is can be useful. curl actually attempts to fetch the website at a particular URL. It’s often useful to use curl with the -I option so it just returns a simple status report, not the full contents of what it finds there.\nFor example, here’s what I get when I curl the website for this book.\n&gt; curl -I https://do4ds.com                                         \n\nHTTP/2 200\nserver: GitHub.com\ncontent-type: text/html; charset=utf-8\nlast-modified: Tue, 04 Jul 2023 16:23:38 GMT\naccess-control-allow-origin: *\netag: \"64a4478a-79cb\"\n...\nThe important thing here is that first line. The server is returning a 200 HTTP status code, which means all is well. If you get something else, take a look at the http code cheatsheet in Appendix C.\nIf ping succeeds, but curl does not, it means that the server is up, but the path or port is incorrect. If you’re running inside a container, you should check that you’ve properly configured the port inside container to be forwarded to the outside."
  },
  {
    "objectID": "chapters/sec2/2-6-networking.html#comprehension-questions",
    "href": "chapters/sec2/2-6-networking.html#comprehension-questions",
    "title": "12  Intro to Computer Networks",
    "section": "12.5 Comprehension Questions",
    "text": "12.5 Comprehension Questions\n\nWhat are the 4 components of a URL? What’s the significance of each?\nAre there any inherent differences between public and private IP addresses?\nDraw a mind map of trying to access the following in your browser. Include the following terms: URL, domain, IP Address, port, path, \\(80\\), \\(443\\), 8000, proxy, server, HTTP, HTTPS, status code, protocol\n\nA Shiny app on a server at \\(\\text{http://my-shiny.com}\\) where Shiny Server is sitting on port \\(80\\).\nJupyterHub on a server at \\(\\text{https://example.com/jupyter}\\) where Jupyter is on port \\(8000\\)."
  },
  {
    "objectID": "chapters/sec2/2-6-networking.html#lab-making-it-accessible-in-one-place",
    "href": "chapters/sec2/2-6-networking.html#lab-making-it-accessible-in-one-place",
    "title": "12  Intro to Computer Networks",
    "section": "12.6 Lab: Making it accessible in one place",
    "text": "12.6 Lab: Making it accessible in one place\nIn this lab, we’re going to set up a proxy to be able to access all of our services over HTTP.\nBut first, you might want to try out accessing the various services where they are.\nYou could either try SSH tunneling to them and seeing them on localhost or you could apply custom TCP rules to your security group to temporarily allow access directly to the services. If you want to try, here’s a reminder of where everything is:\n\n\n\nService\nPort\n\n\n\n\nJupyterHub\n\\(8000\\)\n\n\nRStudio\n\\(8787\\)\n\n\nPenguin model API\n\\(8080\\)\n\n\nShiny App\n\\(3838\\)\n\n\n\nIf you’re through playing and ready to get everything configured, let’s go ahead. If you changed your security group rules, change them back.\n\n12.6.1 Step 1: Configure Nginx\nHonestly, configuring proxies is a somewhat advanced networking topic, and in most cases you’d just put one service per server. But if you want to be able to save money and run everything on one server, you’ll want a proxy.\nConfiguring Nginx is pretty straightforward – you install Nginx, put the configuration file into place, and restart the service to pick up the changes. The hard part is figuring out the right configuration. Configuring proxies can be quite painful, as the configuration is very sensitive to seemingly meaningless syntax issues.\nHere are the steps to configure your proxy on your server for JupyterHub and RStudio Server:\n\nSSH into your server.\nInstall Nginx with sudo apt install nginx.\nSave a backup of the default nginx.conf, cp /etc/nginx/nginx.conf /etc/nginx/nginx-backup.conf.6\nEdit the Nginx configuration with sudo vim /etc/nginx/nginx.conf and replace it with:\n\n\n\n/etc/nginx/nginx.conf\n\nuser www-data;\nworker_processes auto;\npid /run/nginx.pid;\ninclude /etc/nginx/modules-enabled/*.conf;\n\nevents {\n\tworker_connections 768;\n\t# multi_accept on;\n}\n\nhttp {\n\n map $http_upgrade $connection_upgrade {\n    \t\tdefault upgrade;\n    \t\t''      close;\n  }\n\n  server {\n    listen 80;\n\n    location /rstudio/ {\n      # Needed only for a custom path prefix of /rstudio\n      rewrite ^/rstudio/(.*)$ /$1 break;\n\n      # Use http here when ssl-enabled=0 is set in rserver.conf\n      proxy_pass http://localhost:8787;\n\n      proxy_http_version 1.1;\n      proxy_set_header Upgrade $http_upgrade;\n      proxy_set_header Connection $connection_upgrade;\n      proxy_read_timeout 20d;\n\n      # Not needed if www-root-path is set in rserver.conf\n      proxy_set_header X-RStudio-Root-Path /rstudio;\n\n      # Optionally, use an explicit hostname and omit the port if using 80/443\n      proxy_set_header Host $host:$server_port;\n    }\n\n    location /jupyter/ {\n      # NOTE important to also set bind url of jupyterhub to /jupyter in its config\n      proxy_pass http://127.0.0.1:8000;\n\n      proxy_redirect   off;\n      proxy_set_header X-Real-IP $remote_addr;\n      proxy_set_header Host $host;\n      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n      proxy_set_header X-Forwarded-Proto $scheme;\n\n      # websocket headers\n      proxy_set_header Upgrade $http_upgrade;\n      proxy_set_header Connection $connection_upgrade;\n    }\n  }\n}\n\n\n\nTest that your configuration is valid sudo nginx -t.\nStart Nginx with sudo systemctl start nginx. If you see nothing all is well.\n\nIf you need to change anything, update the config and then restart with sudo systemctl restart nginx.\n\n\n12.6.2 Step 2: Open port 80\nNow, if you try to go to your your server’s public IP address or DNS, your browser will spin for a while and nothing will happen. That’s because the AWS security group still only allows SSH access on port \\(22\\). We need to add a rule that will allow HTTP access on port \\(80\\).\nOn the AWS console page for your instance, find the Security section and click into the security group for your instance. You want to add a new inbound HTTP rule that allows access on port \\(80\\) from anywhere. Make sure not to get rid of the rule that allows SSH access on \\(22\\). You still need that one too.\nOnce you do this, you should be able to visit your server address and get the default Nginx landing page.\n\n\n12.6.3 Step 3: Configure your subpaths\nComplex web apps like RStudio and JupyterHub frequently proxy traffic back to themselves. For example, when you launch a Shiny app in RStudio, you’re actually just opening a “headless” browser window that gets proxied back into your session.\nThis works by default when those apps are on the root path \\(\\text{/}\\). That’s not true in this case, so we’ve got to let the services know where they’re actually located.\nConfiguring RStudio Server is already done. The X-RStudio-Root-Path line in the Nginx configuration adds a header to each request coming through the proxy that tells RStudio Server that it’s on the \\(\\text{/rstudio}\\) path.\nJupyterHub needs a configuration update to let it know that it’s on a subpath. Luckily it’s a very simple change. You can edit the Jupyter configuration with\n\n\nTerminal\n\nsudo vim /etc/jupyterhub/jupyterhub_config.py\n\nFind the line that reads # c.JupyterHub.bind_url = 'http://:8000'.\n\n\n\n\n\n\nTip\n\n\n\nYou can search in vim from normal mode with / &lt;thing you're searching for&gt;. Go to the next hit with n.\n\n\nDelete the # to uncomment the line and add the subpath on the end. If you’re using the \\(\\text{/jupyter}\\) subpath and the default \\(8000\\) port, that line will read c.JupyterHub.bind_url = 'http://:8000/jupyter'.\nJupyterHub should pick up the new config when it’s restarted with\n\n\nTerminal\n\nsudo systemctl restart jupyterhub\n\n\n\n12.6.4 Step 4: Try it out!\nNow we should have each service configured on a subpath. RStudio Server at \\(\\text{/rstudio}\\), JupyterHub at \\(\\text{/jupyter}\\). For example, with my server at \\(\\text{64.56.223.5}\\), I can get to RStudio Server at \\(\\text{http://64.56.223.5/rstudio}\\).\nNote that right now, this server is on HTTP, which is not a best practice. In fact, it’s such a bad practice that your browser will probably autocorrect the url to https and you’ll have to manually correct it back to http and ignore some scary warnings. Don’t worry, we’ll fix this in Chapter 14.\n\n\n12.6.5 Lab Extensions\nIf you’ve gone to the root URL for your server, you’ve probably noticed that it’s just the default Nginx landing page, which is not very attractive.\nYou might want to create a landing page with links to the subpath by serving a static html page off of \\(\\text{/}\\). Or maybe you want one of the services at \\(\\text{/}\\) and the others at a different subpath.\nRight now, neither the penguins model API or the Shiny app are available from the outside. You might want to add them to the proxy to make them accessible. I’ll leave that as an exercise for you.\n\n\n\n\n\n\nTip\n\n\n\nIt’s very common to put an API and/or a Shiny app behind a proxy. Googling “Shiny app behind nginx” or “FastAPI with nginx” will yield good results.\n\n\nOne thing to consider is whether the model API should be publicly accessible at all. If the only thing calling it is the Shiny app, maybe it shouldn’t be?"
  },
  {
    "objectID": "chapters/sec2/2-6-networking.html#footnotes",
    "href": "chapters/sec2/2-6-networking.html#footnotes",
    "title": "12  Intro to Computer Networks",
    "section": "",
    "text": "URLs are a subset of a broader category called Uniform Resource Identifiers (URIs), which look like a URL and are used to identify a resource by may not be a valid address. I mention them only because you may run across them in certain contexts, like configuring SSO.↩︎\nDifferent resources divide URLs into somewhere between three and seven parts. I think these four are the most useful for this chapter’s purpose.↩︎\nHow packet switching works isn’t super relevant, but it is cool. Packets are addressed with their target IP address when they’re first created and are passed off to the router to which the computer is connected. Routers are arranged in a tree-style hierarchy. What’s clever is that each router only keeps track of downstream addresses and a single upstream default address. So the packet gets passed upstream until it hits a router that knows about the target IP address and then back downstream to the host.↩︎\nPorts are also used for outbound communication. Computers know how to automatically open ports for outbound communication and specify that’s where the response should come, so we’re not going to get into them here.↩︎\nAs you might guess from this complicated syntax, you can do a lot more than this with SSH tunneling, but this is most often what I use it for.↩︎\nThis is generally a good practice before you start messing with config files. Bad configuration is usually preferable to a service that can’t start at all because you’ve messed up the config so badly. It happens.↩︎"
  },
  {
    "objectID": "chapters/sec2/2-7-dns.html#basics-of-dns-and-domains",
    "href": "chapters/sec2/2-7-dns.html#basics-of-dns-and-domains",
    "title": "13  DNS allows for human-readable addresses",
    "section": "13.1 Basics of DNS and domains",
    "text": "13.1 Basics of DNS and domains\nWhen you create or launch a website, you’ll purchase (rent, really) a domain like \\(\\text{do4ds.com}\\) from a domain name registrar. Purchasing a domain gives you the right to attach that domain to an IP Address.\nWhen someone comes to visit your website, their computer resolves the IP Address via a DNS lookup against public DNS nameservers. So when you purchase a domain, you register the association between your domain and the IP Address with the DNS nameservers so users can look them up.\nA complete domain is called a Fully-Qualified Domain Name (FQDN) and consists of three parts:\n\\[\n\\overbrace{\\text{blog}}^{\\text{Subdomain}}.\\overbrace{\\underbrace{\\text{example}}_{\\text{Domain Name}}.\\underbrace{\\text{com}}_{\\text{Top-Level Domain}}}^{\\text{Root Domain}}\n\\]\nWhen you get a domain from a registrar, the thing you’ll actually rent is the root domain. You can choose any root domain you want, as long as it’s not already taken. Domain names are unique only within top-level domains, so you might be able to get \\(\\text{example.fun}\\) even if someone else owns \\(\\text{example.com}\\).\n\n\n\n\n\n\nTop-level domains\n\n\n\nWhen the web first launched, there were only a limited number of top-level domains like \\(\\text{.com}\\), \\(\\text{.org}\\), and \\(\\text{.net}\\). ICANN, the group that controls how domains are assigned, controlled them all.\nIn 2013, ICANN decided to allow people and organizations to register their own top-level domains. That’s why there’s been an explosion in websites at top level domains like \\(\\text{.io}\\), \\(\\text{.ai}\\), and \\(\\text{.fun}\\) in the last decade or so.\nI think it’d be fun to have my own top-level domain, so it’s unfortunate that owning a top-level domain is not something to do on a whim. In 2012, the initial application fee was $185,000.\n\n\nSubdomains are a way to specify a part of a domain, usually to signify to users that its for a distinct purpose. You can generally register as many subdomains as you want against a root domain."
  },
  {
    "objectID": "chapters/sec2/2-7-dns.html#how-dns-is-configured",
    "href": "chapters/sec2/2-7-dns.html#how-dns-is-configured",
    "title": "13  DNS allows for human-readable addresses",
    "section": "13.2 How DNS is configured",
    "text": "13.2 How DNS is configured\nDNS is configured by giving the proper IP Address information to the domain name registrar where you bought your domain. There’s often some minor configuration on the server side as well to let the server know where it lives.\nMost places you might serve a website from have instructions on how to do configure it you can google. The point of this section is just to make you comfortable with whatever instructions you find in your googling.\nThere are a variety of different domain name registrars. Each of the big 3 has their own and there are a number of others including Namecheap (my personal favorite as it is indeed, cheap), Cloudflare, and GoDaddy.\nCosts for domain names vary widely. Buying a meaningless domain in a less popular top-level domain can cost as little as a few dollars per year. For example, I was able to get the domain \\(\\text{do4ds-lab.shop}\\) for $1.98 for a year on Namecheap.\nOn the other hand, buying a \\(\\text{.com}\\) domain that’s a real word or phrase can be thousands of dollars. There are articles every few years about some major company accidentally allowing their domain name to lapse and ransoming it back for stupid amounts of money.\nOnce you’ve purchased your domain, you need to configure the public DNS records of your domain to point to the IP Address you want. Configuration of DNS is done by way of records. Records map a path or host to a target.\nDepending on the type of mapping, records fall into a number of different categories, but there are only a few you’re likely to see:\n\nA records (or their IPv6 cousin AAAA records) map a domain to an actual IP Address.\nCNAME records alias subdomains to another record.\nNS records tells the DNS server to forward the request on to another namespace server. This is usually only used by big organizations that run their own domain name servers for their subdomains.\n\nWhen you go to configure DNS with your domain name registrar, you’ll configure the records in a record table. Here’s an imaginary DNS record table for the domain \\(\\text{example.com}\\):\n\n\n\nPath/Host\nType\nTarget\n\n\n\n\n\\(\\text{@}\\)\nA\n\\(\\text{64.56.223.5}\\)\n\n\n\\(\\text{www}\\)\nCNAME\n\\(\\text{example.com}\\)\n\n\n\\(\\text{blog}\\)\nA\n\\(\\text{114.13.56.77}\\)\n\n\n\\(\\text{*}\\)\nA\n\\(\\text{64.56.223.5}\\)\n\n\n\nThe first row provides an A record for the special \\(\\text{@}\\) symbol meaning exact match. So by this configuration, any traffic to \\(\\text{example.com}\\) will be passed straight through to the specified IP Address.\nThe second row deals with traffic to the \\(\\text{www}\\) subdomain. Since this is a CNAME record, this record indicates that traffic to \\(\\text{www.example.com}\\) should be treated exactly like traffic to the bare \\(\\text{example.com}\\). Some domain providers do automatic redirection of \\(\\text{www}\\) traffic, and so this row may not be necessary in some configurations.\nThe next record sends the \\(\\text{blog}\\) subdomain to a completely different IP Address. This is a common configuration when the subdomain might be owned by a completely different group than the main website or is served from a different server.\nThe last record uses the wildcard symbol \\(\\text{*}\\) to send all subdomain traffic that’s not already spoken for back to the main IP Address.\nOther than the \\(\\text{www}\\) subdomain, which stands for world wide web, and is generally routed to the same place as the bare root domain, using subdomains and choosing between subdomains and paths is entirely about organizing how users experience your website and what’s easiest for your organization to maintain.\n\n\n\n\n\n\n\\(\\text{www}\\) is just a subdomain\n\n\n\nWhen the internet was first started, it seemed like it might be important to differentiate the subdomain where the website would live from, for example, the email domain people at that organization would use.\nThat turned out not really to be the case and these days \\(\\text{www}\\) and the bare root domain are generally used interchangeably.\n\n\nOnce you’ve configured your DNS records, you will need to wait an annoyingly long time to see if you did it correctly.\nWhen your computer does a DNS lookup, there are often at least three nameservers involved. First, your computer talks to a resolver, which is a server that keeps track of where the nameservers are. Then you’re routed to the correct nameserver for the top-level domain, which routes you to the nameserver for your actual domain.\nAnd then there are various geographic repeats of this whole set of servers to make sure your query doesn’t get slowed down by having to travel too far.\nThat means that when you update a DNS record, it can take some time to propagate to all these servers. The other issue is that your computer, and each of these servers, tries to do as few lookups as possible, because they’re slow.\nWhen you configure a DNS record, you configure something called the TTL (time to live). Each level of DNS lookup, may cache the results of looking up a domain to avoid having to ask the next level. How long that cache lasts is configured by the TTL.\nThis is great if you are using the internet and don’t want to wait for DNS lookups, but when you’re changing the domains on servers you control, there are many DNS servers that a request could get routed to, and many of them may have outdated cache entries. That’s why DNS changes can take up to 24 hours to propagate.\nThat means that if you make a change and it’s not working, you have no idea whether you made a mistake or it just hasn’t propagated yet. It’s very annoying."
  },
  {
    "objectID": "chapters/sec2/2-7-dns.html#comprehension-questions",
    "href": "chapters/sec2/2-7-dns.html#comprehension-questions",
    "title": "13  DNS allows for human-readable addresses",
    "section": "13.3 Comprehension Questions",
    "text": "13.3 Comprehension Questions\n\nWhat are the parts of a fully-qualified domain name? How does each of them get created?\nHow does your computer find the IP Address for a domain? Why could it sometimes be wrong?\nWhat are the different kinds of DNS records you’re likely to use?"
  },
  {
    "objectID": "chapters/sec2/2-7-dns.html#lab-configuring-dns-for-your-server",
    "href": "chapters/sec2/2-7-dns.html#lab-configuring-dns-for-your-server",
    "title": "13  DNS allows for human-readable addresses",
    "section": "13.4 Lab: Configuring DNS for your server",
    "text": "13.4 Lab: Configuring DNS for your server\nIn the last lab, we configured the server so that all of the services were served off of one single port that redirected to various subpaths.\nNow we want to get a real, memorable domain for the server so that you and your users don’t have to remember some random ec2- domain or an IP Address. In this lab, we’ll configure DNS records for our server so it’s available at a real domain.\n\n13.4.1 Step 1: Allocate an Elastic IP\nEC2 instances get assigned a public IP when they are started. The issue is that every time the server stops that IP is released and a new one is assigned when it comes back up. This means you’d have to change your DNS record every time you temporarily take your server offline – no good.\nLuckily, AWS has a great solution here called Elastic IP. which gives you a stable public IP Address you can move from one instance to another as you wish.\n\n\n\n\n\n\nPaying for Elastic IPs\n\n\n\nElastic IPs are free as long as they’re attached to a running EC2 instance. You pay when they’re not in use to discourage hoarding them.\nIf you do take your server down for a short time, it’s no big deal. As of this writing, it’s 12 cents per day for a single IP. But do make sure to release the Elastic IP if/when you take your server down permanently.\n\n\nTo set up your Elastic IP, find it in the AWS console and allocate an address. Then you will associate your Elastic IP as the default public IP Address for your instance.\nNote that once you make this change, your server will no longer be available at its old IP Address, so you’ll have to SSH in at the new one. If you have SSH terminals open when you make the change, they will break.\n\n\n\n\n\n\nNote\n\n\n\nNext time you stand up a server, you should start by giving it an Elastic IP so you are immediately using it’s permanent IP Address. The labs in this book are ordered to promote learning, not the right order to configure things.\n\n\n\n\n13.4.2 Step 2: Buy a domain\nYou can buy a domain from any of the many domain purchasing services on the web. This won’t be free, but many domains are very cheap.\nThe easiest place to buy a domain is via AWS’s Route53 service, but you can feel free to use another provider. I usually use Namecheap just because all of the domains I own are there.\n\n\n13.4.3 Step 3: Configure DNS\nOnce you’ve got your domain, you have to configure your DNS. You’ll have to create 2 A records – one each for the \\(\\text{@}\\) host and the \\(\\text{*}\\) host pointing to your IP and one for the CNAME at the \\(\\text{www}\\) with the value being your bare domain.\nExactly how you configure this will depend on the domain name provider you choose. In NameCheap, you configure this via a table under Advanced DNS, which looks like this.\n\n\n\nType\nHost\nValue\nTTL\n\n\n\n\nA Record\n\\(\\text{*}\\)\n\\(\\text{64.56.223.5}\\)\nAutomatic\n\n\nCNAME Record\n\\(\\text{www}\\)\n\\(\\text{do4ds-lab.shop}\\)\nAutomatic\n\n\nA Record\n\\(\\text{@}\\)\n\\(\\text{64.56.223.5}\\)\nAutomatic\n\n\n\nI would recommend sticking with the default for TTL.\n\n\n13.4.4 Step 4: Wait an annoyingly long time\nNow you just have to be patient. Unfortunately DNS takes time to propagate. After a few minutes (or hours?), your server should be reachable at your domain.\nIf it’s not (yet) reachable, try seeing if an incognito browser works because that sidesteps the browser level of caching. If it doesn’t, wait some more. When you run out of patience, try reconfiguring everything and check if it works now.\n\n\n\n\n\n\nTip\n\n\n\nWe still haven’t configured HTTPS, so you’ll need to manually input the URL as \\(\\text{http://}\\), because your browser will otherwise assume it’s HTTPS.\n\n\n\n\n13.4.5 Step 5: Add the Shiny app to your site\nNow that the Shiny app is at a stable URL, let’s put it on our site so people can look at our penguin size prediction model. I put the app at the subpath \\(\\text{/penguins}\\), so it’s now at \\(\\text{http://do4ds-lab.shop/penguins}\\).\nWe’re going to use something called an iFrame, which lets you embed one website inside another. An iFrame is a basic HTML construct and it’s easy to put one in a Quarto site.\n\n\n\n\n\n\nNote\n\n\n\nOnce you change your website to go over HTTPS in the next section, you’ll have to adjust the iFrame URL as well.\n\n\nIn Quarto, you can just add an html block to a any document and it will get loaded in automatically. I want the app on the landing page of my site, index.qmd. So I’ve added a block that looks like:\n\n\nindex.qmd\n\n&lt;iframe width=\"780\" height=\"500\" src=\"http://do4ds-lab.shop/penguins/\" title=\"Penguin Model Explorer\"&gt;&lt;/iframe&gt;"
  },
  {
    "objectID": "chapters/sec2/2-7-dns.html#footnotes",
    "href": "chapters/sec2/2-7-dns.html#footnotes",
    "title": "13  DNS allows for human-readable addresses",
    "section": "",
    "text": "This is really a very shallow intro to DNS. If you want to go a little deeper, I highly recommend Julia Evans’s zines on a variety of technical topics, including DNS. You can find them at wizardzines.com.↩︎"
  },
  {
    "objectID": "chapters/sec2/2-8-ssl.html#what-https-does",
    "href": "chapters/sec2/2-8-ssl.html#what-https-does",
    "title": "14  You should use SSL/HTTPS",
    "section": "14.1 What HTTPS does",
    "text": "14.1 What HTTPS does\nHTTPS is the same as HTTP, but secured with a technology called SSL/TLS (secure sockets layer/transport layer security). SSL/TLS works by configuring the resource to provide an SSL certificate upon demand, which is used to verify the site’s identity and establish an encrypted session.\n\n\n\n\n\n\nSSL vs TLS\n\n\n\nTLS is actually what’s in use these, but you’ll mostly talk about SSL. TLS is the successor to SSL, but the configuration is identical, so most people didn’t update how they talk about it.\n\n\nYou use HTTPS constantly. Go to a website in your browser and look for a little lock icon near the search bar. That little lock indicates that the domain is secured using HTTPS. If you click on it, you can get more information about the site’s SSL certificate.\nIf you’re of a certain age, you may recall warnings that you shouldn’t use the WiFi at your neighborhood Starbucks. The issue was twofold.\nHTTP has no way to verify that the website you think you’re interacting with is actually that website. So a bad actor could put up a fake WiFi network that makes \\(\\text{bankofamerica.com}\\) resolve to a lookalike website to capture your banking information. That’s called a man-in-the-middle attack.\nAnd even if they didn’t do that, they could inspect the traffic going back and forth and just read whatever you’re sending over the web in what’s called a packet sniffing attack.\nIn 2015, Google Chrome began the process of marking any site using plain HTTP as insecure, which led to nearly complete adoption of HTTPS across the internet. The risk of both of these kinds of attacks has been neutered and it’s actually pretty safe to use any random WiFi network you want these days – because of HTTPS.\nAs a website administrator, securing your website or server with HTTPS is one of the most basic things you can do to make sure your website traffic is safe. You should always configure HTTPS for a public website – full stop.\nIt’s worth noting that this SSL/TLS security can be applied to a number of different application layer protocols, including (S)FTP and LDAP(S). You may run across these depending on your organization. In any case, the SSL/TLS part works the same and all that changes is what’s inside the secure digital envelope."
  },
  {
    "objectID": "chapters/sec2/2-8-ssl.html#how-ssltls-works",
    "href": "chapters/sec2/2-8-ssl.html#how-ssltls-works",
    "title": "14  You should use SSL/HTTPS",
    "section": "14.2 How SSL/TLS works",
    "text": "14.2 How SSL/TLS works\nSSL/TLS uses public key encryption (remember, we learned about that in Chapter 8) to do two things – validate that the site you’re visiting is the site you intend and encrypt the traffic back and forth to the site.\nTo set up SSL for a website, you create or acquire an SSL certificate, which has a public and a private component (sound familiar?).1 Then, verify the public certificate with a trusted Certificate Authority (CA) and put the private certificate in the right place on the website.2\nThen, when you go to access that resource, the first thing your machine asks for is a signature. The site uses its private key to generate the signature and your machine verifies the signature against it’s internal trusted CA store.\nNow your machine knows it’s communicating with the right host on the other side and you’re not falling victim to a man-in-the-middle attack.\nOnce the verification process is done, your machine and the remote on the other side create temporary session keys to establish encryption with the website on the other end.3 Only then does it start sending real data, now encrypted securely inside a digital envelope."
  },
  {
    "objectID": "chapters/sec2/2-8-ssl.html#getting-and-using-ssl-certificates",
    "href": "chapters/sec2/2-8-ssl.html#getting-and-using-ssl-certificates",
    "title": "14  You should use SSL/HTTPS",
    "section": "14.3 Getting and using SSL certificates",
    "text": "14.3 Getting and using SSL certificates\nWhen you buy a computer, it comes configured out of the box to trust a small number of official CAs. So if you’re a website wanting to get a certificate, you generally want to get it from one of those CAs.\nThis used to be kinda a pain. The CAs charged to issue certificates. While it was only $10 per year to for a basic SSL certificate, they typically would only cover a single subdomain. A wildcard certificate to cover all the subdomains of a root domain was expensive enough to discourage hobbyists.\nIf you wanted a free certificate, your only option was to use a self-signed certificate, which you’d create similarly to creating an SSH key. This was a pain because you had to manually add the public certificate to the CA store of every machine that would be accessing the site, and then re-add it when the certificate expired.4\nLuckily there’s now another option. For most small organizations, I recommend getting a free SSL certificate from the nonprofit CA Let’s Encrypt. They even have some nice tooling that makes it super easy to create and configure your certificate right on your server.\nFor most organizations, using a public CA to get SSL on public-facing resources is sufficient and use plain HTTP inside private networks. Some large organizations want to encrypt their private traffic as well and run their own private CA. If this is the case, your organization’s policies will make it clear. This can be a pain, because you’ve got to make sure every host inside the network trusts the private CA.\nOnce you’ve configured SSL/TLS, you generally want to only allow HTTPS traffic to your site. You’ll accomplish this by redirecting all HTTP traffic on port \\(80\\) to come in via HTTPS on port \\(443\\).\nSome web applications support configuring a certificate directly, while others only accept HTTP traffic, so you’d need to do SSL termination with a proxy in front of the application."
  },
  {
    "objectID": "chapters/sec2/2-8-ssl.html#comprehension-questions",
    "href": "chapters/sec2/2-8-ssl.html#comprehension-questions",
    "title": "14  You should use SSL/HTTPS",
    "section": "14.4 Comprehension Questions",
    "text": "14.4 Comprehension Questions\n\nWhat are the two risks of using plain HTTP and how does HTTPS mitigate them?\nWrite down a mental map of how SSL secures your web traffic. Include the following: public certificate, private certificate, certificate authority, encrypted traffic, port 80, port 443"
  },
  {
    "objectID": "chapters/sec2/2-8-ssl.html#lab-configure-ssl",
    "href": "chapters/sec2/2-8-ssl.html#lab-configure-ssl",
    "title": "14  You should use SSL/HTTPS",
    "section": "14.5 Lab: Configure SSL",
    "text": "14.5 Lab: Configure SSL\nWe’re going to use Let’s Encrypt’s certbot utility to automatically generate an SSL certificate, share it with the CA, install it on the server, and even update your NGINX configuration.\nIf you’ve never had to manually configure SSL in the past, let me tell you, this is magical!\n\n14.5.1 Step 1: Follow instructions to add SSL for NGINX\nUsing Let’s Encrypt to add an SSL certificate to NGINX configuration is a super common task. As of this writing, there’s a great blog post entitled Using Free Let’s Encrypt SSL/TLS Certificates with NGINX. I’d encourage you to look for that article (or something similar) and follow the steps there.\nAt a high level, what you’ll do is\n\nConfigure the NGINX configuration to know what domain its on.\nInstall certbot on the system.\nRun certbot to get the certificate, apply it to the server, and update the NGINX configuration.\n\nBefore you move along, I’d recommend you take a moment and inspect the /etc/nginx/nginx.conf file to look at what certbot added.\nRelative to the old version, you’ll notice two things. First, the line that read listen 80 is gone from the server block because we’re no longer listening for HTTP traffic. In it’s place, there’s now a listen 443 along with a bunch of stuff that tells NGINX where to find the certificate on the server.\nScrolling down a little, there’s a new server block that is listening on \\(80\\). This block returns a 301 status code (permanent redirect) and sends traffic to HTTPS on \\(443\\).\n\n\n14.5.2 Step 2: Let RStudio Server know it’s on HTTPS\nBefore we exit and test it out, let’s do one more thing. As mentioned when we configured NGINX the first time, RStudio Server does a bunch of proxying traffic back to itself, so it needs to know that it’s on HTTPS.\nYou can let RStudio Server know that it’s on HTTPS by adding a header to all traffic letting RStudio Server know the protocol is HTTPS. You can add this line to your nginx.conf:\n\n\n/etc/nginx/nginx.conf\n\nproxy_set_header X-Forwarded-Proto https;\n\nOk, now try to visit RStudio Server at your URL, and you’ll find that…it’s broken again.\nBefore you read along, think for just a moment. Why is it broken?\n\n\n14.5.3 Step 3: Configure security groups\nIf your thoughts went to something involving ports and AWS security groups, you’re right!\nBy default, our server was open to SSH traffic on port \\(22\\). Since then, we may have opened or closed port \\(80\\), \\(8000\\), \\(8080\\), \\(8787\\), and/or \\(3838\\).\nBut now that we’re exclusively sending HTTPS traffic into the proxy on \\(443\\) and letting the proxy redirect things elsewhere. So you have to go into the security group settings and change it so there are only 2 rules – one that allows SSH traffic on \\(22\\) and one that allows HTTPS traffic on \\(443\\).\nIt’s up to you whether you want to leave port \\(80\\) open. If you do, it will redirect people to HTTPS on \\(443\\). If you close it entirely, people who come to port \\(80\\) will be blocked and will eventually get a timeout. If people are used to coming to the server via HTTP, it might be nice to leave \\(80\\) open so they get a nice redirect experience instead of getting confusingly blocked.\n\n\n14.5.4 Step 4: We did it!\nThis is the end of the labs in this book.\nAt this point your server is fully configured. You have three real data science services available on a domain of your choosing protected by HTTPS and you can SSH in to do admin work.\nTake a moment to celebrate. It’s very cool to be able to stand up and administer your own data science workbench. If you’re working at a small organization or are a hobbyist, you can really use this server to do real data science work.\nBut this server isn’t enterprise-ready. If you work at a large organization or one with stringent security or privacy rules, your IT/Admin group is going to have concerns. Read on to learn more about what they are and why they’re completely valid."
  },
  {
    "objectID": "chapters/sec2/2-8-ssl.html#footnotes",
    "href": "chapters/sec2/2-8-ssl.html#footnotes",
    "title": "14  You should use SSL/HTTPS",
    "section": "",
    "text": "Like with SSL, this makes more sense if you think key where you see private and lock where you see public.↩︎\nThe CA verifies the certificate by signing it. Your machine just keeps public certificates for the CAs. Then, when it gets an SSL certificate that’s signed by one of those CAs, it can validate that the CA actually stamped this certificate as valid.↩︎\nUnlike the asymmetric encryption used by SSL and SSH for the public key encryption, the session keys are symmetric, so they work the same in both directions.↩︎\nYou also could skip that step, in which case you got the session encryption benefits of SSL/TLS, but not the verification.↩︎"
  },
  {
    "objectID": "chapters/sec3/3-0-sec-intro.html#what-enterprise-it-is-about",
    "href": "chapters/sec3/3-0-sec-intro.html#what-enterprise-it-is-about",
    "title": "Making it Enterprise-Grade",
    "section": "What enterprise IT is about",
    "text": "What enterprise IT is about\nAs a data scientist, your primary concern about your data science environment is that it’s useful. You want to be able to get all the data you want at your fingertips.\nMany data scientists in enterprises find that this desire runs headlong into requirements from their organization’s IT/Admin teams.\nThis can be extremely frustrating, so it’s helpful to understand the concerns enterprise IT/Admins have in mind. Broadly, IT/Admins care about the security and stability of the systems they control.\nGreat IT/Admin teams also care about the usefulness of the system to users (that’s you), but it’s usually a distant third. And there is sometimes a tension here. After all the only system that’s completely secure is the one that doesn’t exist at all.\nBut that’s not always the case. Often, there’s a lot to gain by partnering with the IT/Admin team at your organization. You may be primarily focused on getting stuff done minute-to-minute, but a data science platform that is insecure and allows bad actors to break in and steal data is not useful. And one where you can do what you want but end up crashing the workbench for 50 other users is ultimately self-defeating.\nBalancing security, stability, and usefulness is always about tradeoffs. Great IT/Admin organizations are constantly in conversations with other parts of the organization to figure out the right stance for your organization given the set of tradeoffs you face.\nUnfortunately, many IT/Admin organizations don’t act that way – they act as gatekeepers to the resources you need to do your job. That means you’ll have to figure out how to communicate with those teams, understand what matters to them, help them understand what matters to you, and reach acceptable organizational outcomes.\nYou probably already have a good understanding of how a data science environment can be useful – but what about secure and stable. What do they mean?\nSecurity is about making sure that the right people can interact with the systems they’re supposed to and that unauthorized people can’t.\nIT security professionals think about security in layers. And while you’ve done a good job setting your server up to comply with basic security best practices, there are no layers. That server front door is open to the internet. Literally anyone in the world can come to that authentication page for your RStudio Server or JupyterHub and start trying out passwords. That means you’re just one person choosing the password password away from a bad actor getting access to your server.\nLest you think you’re immune because you’re not an interesting target, there are plenty of bots out there randomly trying to break in to every existing IP Address, not because they care about what’s inside, but because they want to co-opt your resources for their own purposes like crypto mining or virtual DDOS attacks on Turkish banks.1\nMoreover, security and IT professionals aren’t just concerned with bad actors from outside (called outsider threat) or even someone internal who decides to steal data or resources (insider threat). They are (or at least should be) also concerned with accidents and mistakes – data that is accidentally permanently deleted is bad the same way stolen data is bad.\nStability is ensuring enterprise-grade systems are around when people need them, and that they are stable during whatever load they face during the course of operating. The importance of stability tends to rise along with the scale of the team and the centrality of their operations to the functioning of your organization.\nIf you’re a team of three data scientists who sit in a room together, it probably won’t be a huge deal if someone accidentally knocks your data science workbench offline for 30 minutes because they tried to run a job that was too big. You’re probably all sitting in the same room and you can learn something from the experience.\nThat’s not the case when you get to enterprise-grade tooling. An enterprise-grade data science workbench probably supports dozens or hundreds of professionals across multiple teams. The server being down isn’t a sorta funny occurrence you can all fix together – it’s a problem that must be fixed immediately – or even better avoided altogether.\nIT/Admins think hard about how to provide resources in a way that avoids having servers go down because they’re hitting resource constraints.\nOne thing that is almost certain to be untrue in an enterprise context is that you’ll have root access as a user of the system.\nThere is no one-size-fits-all (or even most) position for security. Instead, great security teams are constantly articulating and making decisions about tradeoffs."
  },
  {
    "objectID": "chapters/sec3/3-0-sec-intro.html#labs-in-this-section",
    "href": "chapters/sec3/3-0-sec-intro.html#labs-in-this-section",
    "title": "Making it Enterprise-Grade",
    "section": "Labs in this Section",
    "text": "Labs in this Section\nThere are no labs in this section. This section is about how to interact with a professional IT/Admin group. Since it’s about work you really shouldn’t undertake yourself, there are no labs for you to DIY these things.\nIf you want to go deeper into any of the topics, there are plenty of great resources online – a google search should turn them up quickly"
  },
  {
    "objectID": "chapters/sec3/3-0-sec-intro.html#footnotes",
    "href": "chapters/sec3/3-0-sec-intro.html#footnotes",
    "title": "Making it Enterprise-Grade",
    "section": "",
    "text": "Yes, these both really happened.↩︎"
  },
  {
    "objectID": "chapters/sec3/3-1-os-ds-in-ent.html#data-science-sandboxes",
    "href": "chapters/sec3/3-1-os-ds-in-ent.html#data-science-sandboxes",
    "title": "15  Open Source Data Science in the Enterprise",
    "section": "15.1 Data Science Sandboxes",
    "text": "15.1 Data Science Sandboxes\nIn Chapter 5 on code promotion, we talked about wanting to create separate dev/test/prod environments for your data science assets. In many organizations, your good intentions to do dev/test/prod work for your data science assets are insufficient.\nIn the enterprise, it’s often the case that IT/Admins are required to ensure that your dev work can’t wreak havoc on actual production systems. If you can work with your organization’s IT/Admins to get a place to work, this is a good thing! It’s nice to have a sandbox where you can do whatever you want with no concern of breaking anything real.\nThere are three components to a data science sandbox:\n\nFree read-only access to real data\nRelatively broad access to packages\nA process to promote data science projects into production\n\nMany IT/Admins are familiar with building workbenches for other sorts of software development tasks, so they may believe they understand what you need.\n\n15.1.0.1 Read-Only Data Access\nIn software engineering, the shape of the data in the system matters a lot, but the content doesn’t matter much. If you’re building software to do inventory tracking, you’re perfectly fine testing on data that isn’t of a real inventory as long as it’s formatted the same as real inventory data. That means that a software engineering dev environment doesn’t require access to any real data.\nIT/Admins may believe that providing a standalone or isolated environment for you to work in is sufficient. You’re probably wincing reading this knowing that it’s not the case.\nAs you know, data science isn’t like that. In data science, what you’re doing in dev environments – exploring relationships between variables, visualizing, training models, looks much more different from test and prod than for software engineering. You need a place where you can work with real data, explore relationships in the data, and try out ways of modeling and visualizing the data to see what works.\nIn many cases, creating a data science sandbox with read-only access to data is a great solution. If you’re creating a tool that also writes data, you can write the data only within the dev environment.\nIf you’re able to do this, you can explore the data to your heart’s content without the IT/Admin team having to worry you’re going to mess up production data.\nDepending on your organization, they may also be worried about data exfiltration – people intentionally or accidentally removing data from the sandbox environment. In that case, you may want to run your sandbox without the ability to connect to the internet, sometimes called airgapped or offline. More on how to work in an airgapped environment is in Chapter 16.\n\n\n15.1.0.2 Package Availability\nYour organization may not have any restrictions on the packages you can install and use in your environment. If so, that’s great! You should still use environment as code tooling as we explored in Chapter 1.\nBut other organizations don’t allow free-and-open access to packages inside their environments for security or other reasons. They may have restrictive networking rules that don’t allow you to reach our to CRAN, BioConductor, PyPI, or GitHub to install packages publicly.\nThere may also be rules about having to validate packages – for security and/or correctness – before they can be used in a production context.\nThis is difficult, since the exploratory dev process often involves trying out new packages – is it best to use this modeling package or that, best to use one type of visualization or another. If you can convince your IT/Admins to give you freer access to packages in dev, that’s ideal.\nYou can work with them to figure out what the promotion process will look like. It’s easy to generate a list of the packages you’ll need to promote apps or reports into production with tools like renv or venv. Great collaborations with IT/Admin are possible if you can develop a procedure where you give them package manifests that they can compare to allow-lists and then make those packages available.\n\n\n15.1.0.3 A Promotion Process\nThe last thing you’ll need is a process to promote content out of the dev environment into test or prod. The parameters of this process will vary a lot depending on your organization.\nIn many organizations, data scientists won’t have permissions to deploy into the prod environment – only a small group of admins will have the ability to verify changes and submit them to the prod environment.\nIn this case, a code promotion strategy (see Chapter 5) co-designed with your IT/Admins is the way to go. You want to hand off a fully-baked production asset so all they need to do is put it into the right place, preferably with CI/CD tooling."
  },
  {
    "objectID": "chapters/sec3/3-1-os-ds-in-ent.html#devtestprod-for-admins",
    "href": "chapters/sec3/3-1-os-ds-in-ent.html#devtestprod-for-admins",
    "title": "15  Open Source Data Science in the Enterprise",
    "section": "15.2 Dev/Test/Prod for Admins",
    "text": "15.2 Dev/Test/Prod for Admins\nIn the chapter on environment as code tooling, we discussed how you want to create a version of your package environment in a safe place and then use code to share it with others and promote it into production.\nThe IT/Admin group at your organization probably is thinking about something similar, but at a more basic level. They want to make sure that they’re providing a great experience to you – the users of the platform.\nSo they probably want to make sure they can upgrade the servers you’re using, change to a newer operating system, or upgrade to a newer version of R or Python without interrupting your use of the servers.\nThese are the same concerns you probably have about the users of the apps and reports you’re creating. In this case, I recommend a two-dimensional grid – one for promoting the environment itself into place for IT/Admins and another for promoting the data science assets into production.\nIn order to differentiate the environments, I often call the IT/Admin development and testing area staging, and use dev/test/prod language for the data science promotion process.\nBack in section one, we learned about environments as code – using code to make sure that our data science environments are reproducible and can be re-created as needed.\nThis idea isn’t original – in fact, DevOps has it’s own set of practices and tooling around using code to manage DevOps tasks, broadly called Infrastructure As Code (IaC).\nTo get from “nothing” to a usable server state, there are (at minimum) two things you need to do – provision the infrastructure you need, and configure that infrastructure to do what you want.\nFor example, let’s say I’m standing up a server to deploy a simple shiny app. In order to get that server up, I’ll need to stand up an actual server, including configuring the security settings and networking that will allow the proper people to access the server. Then I’ll need to install a version of R on the server, the Shiny package, and a piece of hosting software like Shiny Server.\nSo, for example, you might use AWS’s CloudFormation to stand up a virtual private cloud (VPC), put an EC2 server instance inside that VPC, attach an appropriately-sized storage unit, and attach the correct networking rules. Then you might use Chef to install the correct software on the server and get your Shiny app up-and-running.\nIn infrastructure as code tooling, there generally isn’t a clear dividing line between tools that do provisioning and tools that do configuration…but most tools lean one way or the other.\nBasically any tool does provisioning will directly integrate into the APIs of the major cloud providers to make it easy to provision cloud servers. Each of the cloud providers also has their own IaC tool, but many people prefer to use other tools when given the option (to be delicate).\nIt’s worth noting that Docker by itself is not an IaC tool. A Dockerfile is a reliable way to re-create the container, but that doesn’t get you all the way to a deployed container that’s reliable. You’ll need to combine a container with a deployment framework like Docker Compose or Kubernetes, as well as a way to stand up the servers that underlie that cluster.\nIt’s also worth noting that IaC may or may not be deployed using CI/CD. It’s generally a good practice to do so, but you can have IaC tooling that’s deployed manually.\nBasically none of these tools will save you from your own bad habits, but they can give you alternatives."
  },
  {
    "objectID": "chapters/sec3/3-1-os-ds-in-ent.html#package-management-in-the-enterprise",
    "href": "chapters/sec3/3-1-os-ds-in-ent.html#package-management-in-the-enterprise",
    "title": "15  Open Source Data Science in the Enterprise",
    "section": "15.3 Package Management in the Enterprise",
    "text": "15.3 Package Management in the Enterprise\nIn the chapter on environments as code, we went in depth on how to manage a per-project package environment that moves around with that project. This is a best practice and you should do it.\nBut in some Enterprise environments, there may be further requirements around how packages get into the environment, which packages are allowed, and how to validate those packages.\n\n15.3.1 Open Source in Enterprise\nOpen Souce software is the water code-first data science swims in. R and Python are open source. Any package you’ll access on CRAN, BioConductor, or PyPI is open source.\nEven if you don’t know the details, you’re probably already a big believer in the power of Free and Open Source Software (FOSS) to help people get stuff done.\nIn the enterprise, it is sometimes the case that people are more acquainted with the risks of open source than the benefits.\nFOSS is defined by a legal license. When you put something out on the internet, you can provide no licensing at all. That means that people can do whatever they want with it, but they also have no reassurances you might not come to sue them later.\nI am not a lawyer and this is not legal advice, but hopefully this is helpful context on the legal issues around FOSS software.\nPutting a license on a piece of software makes clear what is and isn’t allowed with that piece of software.\nThe type of license you’re probably most familiar with is a copyright. A copyright gives the owner exclusivity to do certain things with whatever falls under the copyright. FOSS licenses are the opposite – they guarantee that you’re freely allowed to do what you want with the software, though there may be other obligations as well.\nThe point of FOSS is to allow people to build on each other’s work. So the free in FOSS is about freedom, not about zero cost. As a common saying goes – it means free as in free speech, not free as in free beer.\nThere are many different flavors of open-source licenses, and all of them I’m aware of (even the anti-capitalist one) allows you to charge for access. The question is what you’re allowed to do after you acquire the software.\nBasically all open source licenses guarantee four freedoms. These are the freedom to view and inspect the source code, to run the software, to modify the software, and to redistribute the software as you see fit.\nWithin those parameters, there are many different kinds of open source licenses. The Open Source Initiative lists dozens of different licenses with slightly different parameters, but they fall into two main categories – permissive and copyleft.\nPermissive licenses allow you to do basically whatever you want with the software.\nFor example, the common MIT license allows you to, “use, copy, modify, merge, publish, distribute, sublicense, and/or sell” MIT-licensed software without attribution. The most important implication of this is that you can take MIT-licenses software, incorporate it into some other piece of code, and keep that code completely closed.\nCopyleft or viral licenses require that any derivative works are released under the same license. The idea is that open source software should beget more open source software and not silently be used by big companies to make megabucks.\nThe biggest concern with copyleft licenses is that they might propagate into the private work you are doing inside your company. This concern is especially keen at organizations that themselves sell proprietary software. For example, what if a court were to rule that Apple or Google had to suddenly open source all their software?\nThis is a subtle concern and largely revolves around what it means to include another piece of software. Many organizations deem that inclusion only happens if you were to literally copy/paste open source code into your code. In this view, the things created with open source software are not themselves open source.\nThe reality is that there have been basically no legal cases on this topic and nobody knows how it would shake out if it did get to court, so some organizations err on the side of caution.\nSo one of the concerns around open source software is the mixure of licenses involved. R is released under a copyleft GPL license, Python under a permissive Python Software Foundation (PSF) license, RStudio under a copyleft AGPL, and Jupyter Notebook under a permissive modified BSD. And every single package author can choose a license for themselves.\nSay I create an app or plot in R and then share that plot with the public – is that app or plot bound by the license as well? Do I now have to release my source code to the public? Many would argue no – it uses R, but doesn’t derive from R in any way. Others have concerns that stricter interpretations of copyleft might hold up in court if it were to come to that.\nThere are disagreements on this among lawyers, and you should be sure to talk to a lawyer at your organization if you have concerns.\n\n\n15.3.2 Dealing with Package Restrictions\nChapter 1 on environments as code dealt with how to create a reproducible version of your package library for any given piece of content. But in some enterprises, you won’t be able to freely create a library. Instead, your organization may have restrictions on package installation and will need to gate the packages that are allowed into your environment.\nIn order to enact these restrictions, IT/Admins have to do two things - make sure that public repositories are not available to users of their data science platforms, and use one of these repository tools to manage the set of packages that are available inside their environment. It often takes a bit of convincing, but a good division of labor here is generally that the IT/Admins manage the repository server and what’s allowed into the environment and the individual teams manage their own project libraries.\n\n\n\n\n\n\nAmount of Space for Packages\n\n\n\nWhen admins hear about a package cache per-project, they start getting worried about storage space. I have heard this concern many times from admins who haven’t yet adopted this strategy, and almost never heard an admin say they were running out of storage space because of package storage.\nThe reality is that most R and Python packages are very small, so storing many of them is reasonably trivial.\nAlso, these package storage tools are pretty smart. They have a shared package cache across projects, so each package only gets installed once, but can be used in a variety of projects.\nIt is true that each user then has their own version of the package. Again, because packages are small, this tends to be a minor issue. It is possible to make the package cache one that is shared across users, but the (small) risk this introduces of one user affecting other users on the server is probably not worth the very small cost of provisioning enough storage that this just isn’t an issue.\n\n\nMany enterprises run some sort of package repository software. Common package repositories used for R and Python include Jfrog Artifactory, Sonatype Nexus, Anaconda Business, and Posit Package Manager.\nArtifactory and Nexus are generalized library and package management solutions for all sorts of software, while Anaconda and Posit Package Manager are more narrowly tailored for data science use cases.\nThere are two main concerns that come up in the context of managing packages for the enterprise. The first is how to manage package security vulnerabilities.\nIn this context, the question of how to do security scanning comes up. What exactly security professionals mean by scanning varies widely, and what’s possible differs a good bit from language to language.\nIt is possible to imagine a security scanner that actually reads in all of the code in a package and identifies potential security risks – like usage of insecure libraries, calls to external web services, or places where it accesses a database. The existence of tools at this level of sophistication exist roughly in proportion to how popular the language is and how much vulnerability there is.\nSo javascript, which is both extremely popular and also makes up most public websites, has reasonably well-developed software scanning. Python, which is very popular, but is only rarely on the front end of websites has fewer scanners, and R, which is far less popular has even fewer. I am unaware of any actual code scanners for R code.\nOne thing that can be done is to compare a packaged bit of software with known software vulnerabilities.\nNew vulnerabilities in software are constantly being identified. When these vulnerabilities are made known to the public, the CVE organization attempts to catalog them all. One basic form of security checking is looking for the use of libraries with known CVE records inside of packages.\nThe second thing your organization may care about is the licenses software is released under. They may want to disallow certain licenses – especially aggressive copyleft licenses – from being present in their codebases."
  },
  {
    "objectID": "chapters/sec3/3-1-os-ds-in-ent.html#comprehension-questions",
    "href": "chapters/sec3/3-1-os-ds-in-ent.html#comprehension-questions",
    "title": "15  Open Source Data Science in the Enterprise",
    "section": "15.4 Comprehension Questions",
    "text": "15.4 Comprehension Questions\n\nWhat is the purpose of creating a data science sandbox? Who benefits most from the creation of a sandbox?\nWhy is using infrastructure as code an important prerequisite for doing Dev/Test/Prod?\nWhat is the difference between permissive and copyleft open source licenses? Why are some organizations concerned about using code that includes copyleft licenses?\nWhat are the key issues to solve for open source package management in the enterprise?"
  },
  {
    "objectID": "chapters/sec3/3-2-ent-networks.html#enterprise-networking-terminology",
    "href": "chapters/sec3/3-2-ent-networks.html#enterprise-networking-terminology",
    "title": "16  Enterprise Networking",
    "section": "16.1 Enterprise Networking Terminology",
    "text": "16.1 Enterprise Networking Terminology\nREORGED FROM 2.6\nIn order to be accessible to traffic from the internet, a host must have a public IP Address, which is unique across the entire internet.\nBut many, many more hosts don’t have apublic IP Address. Instead, they live only on a private network and have only a private IP Address. For example, your phone, personal computer, and smart fridge don’t (or shouldn’t) have a public IP Address. Instead, these devices register with your home router and the router assigns and keeps track of their private IP Addresses. Only your router has a public IP Address, so return mail has somewhere to go.\nThis system drastically simplifies the topology of the web. Instead of thinking of the web as an enormous set of public streets, it’s more like a series of gated communities and the public IP Address servers only need to keep track of those front gates.\nHopefully the analogy of a basic server as an apartment building and an enterprise server as a castle keep behind a drawbridge makes basic sense. But let’s get into the way you’ll actually talk about this with the IT/Admins at your organization – I promise they won’t talk about castles and apartments.\nWhen you stand up a server in the cloud, it’s inside a private network. In AWS, the private network that houses the servers is called a virtual private cloud (VPC), which you probably saw somewhere in the AWS console.\nFor our workbench server, we took that private network and made it public so every server (there was just one) inside our private network also has a public IP Address so it was accessible from the internet.\nIn an enterprise configuration you won’t do that. Instead, you’ll take your private network and divide it into subnets – most often two of them.\nNow you’ll take the subnets and put all the stuff you actually care about in a private subnet. Private networks generally host all of the servers that actually do things. Your data science workbench server, your databases, server for hosting shiny apps – all these things should live inside the private network. Nothing in the private subnet will be directly accessible from the public internet.\n\n\n\n\n\n\nDefining private networks and subnets\n\n\n\nPrivate networks and subnets are defined by something called a Classless Inter-Domain Routing (CIDR) block. A CIDR block is basically an IP address range, so a private network is a CIDR block and each subnet is CIDR blocks within the private network’s block.\nEach CIDR block is defined by a starting address and the size of the network. For example, the address 10.33.0.0 and the /26 CIDR defines the block of 64 addresses from 10.33.0.0 to 10.33.0.63.\nLarger CIDR numbers indicate a smaller block, so you could take the 10.33.0.0/26 CIDR and split it into the 10.33.0.0/27 block that includes 10.33.0.0 to 10.33.0.31 and 10.33.0.32/27 for 10.33.0.32 through 10.33.0.63.\nAs you’ve probably guessed, the number of IPs in each CIDR have to do with powers of two. But the rules are hard to remember and there are online calculators if you ever have to figure a block out for yourself.\n\n\nThe only things you’ll put in the public subnet – often called a demilitarized zone (DMZ) – are servers that exist solely for the purpose of relaying traffic back and forth to the servers in your private network. These servers are called proxy servers or proxies – more on them in a moment.\nThis means that the traffic actually coming to your workbench comes only from other servers you control. It’s easy to see why this is more secure.\n[TODO: image of private networks + proxies]\nIn most cases, you’ll have minimum two servers in the DMZ. You’ll usually have one or more proxies to handle the incoming HTTPS traffic that comes in from the outside world. You’ll also usually have a proxy that is just for passing SSH traffic along to hosts in the private network, often called a bastion host or jump box.\nThe other benefit of using a private network for the things you actually care about is that you can manage the IP Addresses and hostnames of those servers without having to worry about getting public addresses. If you want to name one of the servers in your private subnet google.com, you can do that (I wouldn’t recommend it), because the only time that name will be used is when traffic is coming past your proxy servers and into the private network.\nThere’s a device sitting on the boundary of all networks that provide translations between private IP Addresses and public ones. For your private subnet, you’ll only have an outbound one available. In AWS, it’s called a Network Address Translation (NAT) Gateway. For your private network as a whole, there’ll be another gateway that provides both inbound and outbout support, it’s called an Internet Gateway by AWS.\n\n16.1.1 What proxies do\nAs a data scientist, this may be the first time you’re encountering the term proxy, but for IT/Admins – especially ones who specialize in networking – configuring proxies is an everyday activity.\nProxies can be either in software or hardware. For example, in our workbench server, we installed the nginx software proxy on the same server as our workbench to allow people to go to any of the different services we installed on that server. In enterprise use cases, proxies are most often on standalone pieces of hardware. They may run nginx or Apache – the other popular open source option. Popular paid enterprise options include F5, Citrix, Fortinet, and Cloudflare.\nProxies can deal with traffic coming into the private network, called an inbound proxy or they can deal with traffic going out from the private network, called an outbound proxy.\n\n\n\n\n\n\nNote\n\n\n\nInbound and outbound are not industry standard terms for proxies. The terms you’ll hear from IT/Admins are forward and reverse. Proxies are discussed from the perspective of being inside the network, so forward equals outbound and reverse equals inbound.\nI find it nearly impossible to remember which is which and IT/Admins will absolutely know what you mean with the terms inbound and outbound, so I recommend you use them instead.\n\n\nProxies are usually used for redirection, port management and firewalling.\nRedirection is when the proxy accepts traffic at the public DNS record and passes (proxies) it along to the actual server. One great thing about this configuration is that only the proxy needs to know the real hostname for your server. For example, you could configure the proxy so example.com/rstudio routes to the RStudio Server that’s at my-rstudio-1 inside the private network. If you want to change it to my-rstudio-2 later on, you just change the proxy routing, which is much easier than changing the public DNS record.\nOne advantage of doing redirection is making it easy to manage ports. For example, RStudio Server runs on port 8787 by default. Generally, you don’t want people to have to remember to go to a random port to access RStudio Server so it’s standard practices to keep standard ports (80 for HTTP, 443 for HTTPS, and 22 for SSH) open on the proxy and have the proxy just redirect the traffic coming into it on 443 to 8787 on the server with RStudio Server.\n\n\n\n\n\n\nNote\n\n\n\nFor our workbench server, we did path rewriting and port management in our nginx proxy.\nIf you recall, by the time we were done, our nginx config was set to only allow HTTPS traffic on 443, redirect all HTTP traffic on 80 to HTTPS on 443, and to take traffic at /rstudio to port 8787 on the same server, /jupyter to port 8000, and /palmer to 8080.\n\n\n[TODO: image of path rewriting + load-balancing]\nProxies are additionally sometimes configured to block traffic that isn’t explicitly allowed. In a data science environment, this means that you’ll have to configure the inbound proxy with all the locations you need. If you’ve got an outbound proxy that blocks traffic, you’re in an airgapped/offline situation.\nThere are a few other things a proxy may be used for. These use cases are less common in a data science environment.\nSometimes proxies terminate SSL. Because the proxy is the last server that is accessible from the public network, many organizations don’t bother to implement SSL/HTTPS inside the private network so they don’t have to worry about managing SSL certificates inside their private network. This is getting rarer as tooling for managing SSL certificates gets better, but it’s common enough that you might start seeing HTTP addresses if you’re doing server-to-server things inside the private network.\nOccasionally proxies also do authentication. In most cases, proxies pass along any traffic that comes in to where it’s supposed to go. If there’s authentication, it’s often at the server itself.\nSometimes the proxy is actually where authentication happens, so you have to provide the credentials at the edge of the network. Once those credentials have been supplied, the proxy will let you through. Depending on the configuration, the proxy may also add some sort of token or header to your incoming traffic to let the servers inside know that your authentication is good and to pass along identification for authorization purposes.\nTODO: image of auth at proxy\nLastly, there’s a special kind of reverse proxy called a load-balancer. A load-balancer is used to scale a service across a pool of servers on the back end. We’ll get more into how this works in Chapter 18."
  },
  {
    "objectID": "chapters/sec3/3-2-ent-networks.html#what-data-science-needs-from-the-network",
    "href": "chapters/sec3/3-2-ent-networks.html#what-data-science-needs-from-the-network",
    "title": "16  Enterprise Networking",
    "section": "16.2 What data science needs from the network",
    "text": "16.2 What data science needs from the network\nAs you’ve probably grasped, enterprise networking can be complex. And your IT/Admin group knows a lot about it. What they don’t know a lot about is the interaction of networking and data science, so it’s helpful for you to be able to clearly state what you need.\n\n\n\n\n\n\nWhat ports do I need?\n\n\n\nOne of the first questions IT/Admins ask is what ports need to be open. Depending on what ports you choose for the services you’re running those ports need to be open.\nThe good news is that almost all traffic for data science purposes is standard HTTP(S) traffic, so it can happily run over 80 or 443 if there are limitations on what ports can be open.\n\n\nOne of the most common issues with data science environments in an enterprise is proxy behavior. If you’re experiencing weird behavior in your data science environment – files failing to upload or download, sessions getting cutoff strangely, or data not transferring right – asking your IT/Admin about whether there are proxies and their behavior should be suspect number one.\nWhen you’re talking to your IT/Admin about the proxies, it’s really helpful to have a good mental model of what traffic might be hitting an inbound proxy and what traffic might be hitting an outbound one.\nAs we went over in Chapter 12, network traffic always operates on a call and response model. So whether your traffic is inbound or outbound is dependent on who makes the call. Inbound means that the call is coming from a computer outside the private network directed to a server inside the private network, and outbound is the opposite.\nTODO: image inbound vs outbound connection\nSo basically, anything that originates on your laptop – including the actual session into the server is an inbound connection, while anything that originates on the server – including everything in code that runs on the server is an outbound connection.\n\n16.2.1 Issues with inbound proxies\nInbound proxies affect the connection you’re making from your personal computer to the server. There are two ways this might affect your experience doing data science on a server.\nIt’s reasonably common for organizations to have settings that limit file sizes for uploads and downloads or implementing timeouts on file uploads, downloads, and sessions. In data science contexts, files tend to be big and session lengths long.\nIf you’re trying to work in a data science context and weird things are happening with file uploads or downloads or sessions ending unexpectedly, checking on inbound proxy settings is a good first step.\nSome data science app frameworks (including Shiny and Streamlit) use a technology called Websockets for maintaining the connection between the user and the app session. Most modern proxies (including those you’ll get from a cloud provider) support Websockets, but some older on-prem proxies don’t and you may have to figure out a workaround if you can’t get Websockets enabled on your proxy.\n\n\n16.2.2 Issues with forward/outbound proxies\nAlmost all enterprise networks have inbound proxies. Outbound ones are somewhat rarer. That’s because outbound proxies limit connections made from inside the network to the outside. It’s obvious why you’d need to protect your data science environment from the entire outside world.\nMany organizations don’t feel the need to limit what external resources people can interact with from inside their firewall, but limitations on outbound access have long been common in highly regulated industries with strong requirements around data security and governance and are becoming increasingly common in many different industries. Many organizations have these proxies to reduce the risk of someone getting in and then being able to exfiltrate valuable resource.\nOrganizations who limit outbound access from their data science environment usually refer to the environment as offline or airgapped. The term airgapped indicates that there is a physical gap – air – between the internet and the environment. It is very rare for this to be the case. In most cases, airgapping is accomplished by putting in an outbound proxy that disallows (nearly) all connections.\nThe good news is that once you’re working on your data science server, you don’t need to go out much. The bad news is that you will have to go out sometimes. It’s important you work with your IT/Admin to develop a plan for how to handle when outbound connectivity is needed.\nHere are the four most common reasons you’ll need to make outbound connections from inside your data science environment.\n\nDownloading Packages Downloading a package from a public repository requires a network connection to that repository. So you’ll need outbound access when you want to install R or Python packages from CRAN, BioConductor, public Posit Package Manager, Conda, PyPI, or GitHub.\nAccessing External Data In most data science work, you’re mostly just working on data from databases or files inside your private network, so you don’t really need access to data or resources outside. On the other hand, if you’re consuming data from public APIs or scraping data from the web, that may require external connections. You also may need an external connection if you’re accessing private data that lives in an external location – for example you might have data in an AWS S3 bucket you need to access from an on-prem workbench or data in Google Sheets that you need to access from AWS.\nSystem Libraries In addition to the R and Python packages, there are also system libraries you’ll need installed, like the versions of R and Python themselves, and other packages used by the system. Generally it’ll be the IT/Admin managing and installing these, so they probably have a strategy for doing it. This comes up in the context of data science if you’re using R or Python packages that are basically just wrappers around system libraries, like the R and Python packages that use the GDAL system library for geospatial work.\nSoftware Licensing If you’re using all open source software, this probably won’t be an issue. But if you’re buying licenses to a professional product, you’ll have to figure out how to activate the software licensing. This usually involves reaching out to servers owned by the software vendor. They should have a method for activating servers that can’t reach the internet, but your IT/Admins will appreciate if you’ve done your homework on this before asking them to activate some new software.\n\nWhat if your organization doesn’t default to allowing all of these things to be available? In some cases, ameliorating these issues is as easy as talking to your IT/Admin and asking them to open the outbound proxy to the right server.\nBefore you go ahead treating your environment as truly offline/airgapped, it’s almost always worth asking if narrow exceptions can be made to a network that is offline/airgapped. The answer may surprise you. Especially if it’s just a URL or two that are protected by HTTPS – for example CRAN, PyPI, or public RStudio Package Manager, it’s generally pretty safe and many organizations are happy to allow-list a limited number of outbound addresses.\nIf not, you’ll have to have a deeper conversation with the IT/Admin.\nYour organization probably has standard practices around managing system libraries and software licenses in their environment.\nExternal data connections and package management are the areas where you’ll have to have a conversation to make them accessible.\nIT/Admins often do not understand how crucial R and Python packages are to doing data science work. It will be on you to make them understand that your offline environment is useless if you can’t come up with a plan to manage packages together.\nThe best plans for offline package operations involve the IT/Admin curating a repository of allowed packages inside the private network using a professional tool like Posit Package Manager, Jfrog Artifactory, or Sonatype Nexus and then giving data scientists free reign to install those packages as needed inside the environment.\nThis can take a lot of convincing. Good luck."
  },
  {
    "objectID": "chapters/sec3/3-2-ent-networks.html#comprehension-questions",
    "href": "chapters/sec3/3-2-ent-networks.html#comprehension-questions",
    "title": "16  Enterprise Networking",
    "section": "16.3 Comprehension Questions",
    "text": "16.3 Comprehension Questions\n\nWhat is the advantage of adopting a more complex networking setup than a server just deployed directly on the internet? Are there advantages other than security?\nDraw a mental map with the following entities: inbound traffic, outbound traffic, proxy, DMZ, private subnet, public subnet, VPC\nOur workbench server has an nginx proxy that redirects inbound traffic on a few different paths to the right port on the same server. Looking at your nginx.conf, what would have to change if you moved each of those services to different servers? Is there anything you’d have to check on the server itself?\nLet’s say you’ve got a private VPC that hosts an instance of RStudio Server, an instance of JupyterHub, and a Shiny Server that has an app deployed. Here are a few examples of traffic – are they outbound, inbound, or within the network?\n\nSomeone connecting to and starting a session on RStudio Server.\nSomeone SFTP-ing an app and packages from RStudio Server to Shiny Server.\nSomeone installing a package to the Shiny Server.\nSomeone uploading a file to JupyterHub.\nA call in a Shiny app using httr2 or requests to a public API that hosts data.\nAccessing a private corporate database from a Shiny for Python app using sqlalchemy.\n\nWhat are the most likely pain points for running a data science workbench that is fully offline/airgapped?"
  },
  {
    "objectID": "chapters/sec3/3-3-auth.html#how-itadmins-think-about-auth",
    "href": "chapters/sec3/3-3-auth.html#how-itadmins-think-about-auth",
    "title": "17  Auth in Enterprise",
    "section": "17.1 How IT/Admins think about auth",
    "text": "17.1 How IT/Admins think about auth\nLet’s start with an analogy to the real world. Imagine all the different services your company’s IT/Admin team manages as rooms in a building. Email, databases, data science workbenches, social media accounts, HR systems, and more. Each one is a room in the building.\nIn order to comply with the principle of least privilege, people are given access only to the rooms they need. Now imagine you’re the one who has to figure out how to keep it all secure.\nWhen someone tries to enter a room, you’ll need a way to know who’s asking – to ascertain and validate their identity. This process is called authentication.\nBut just knowing who they are is insufficient. Remember, not everyone gets to access every room. So you’ll also need a way to check whether they have permission to access that room. That process is called authorization.\nTogether, the whole system of authenticating and authorizing people to take actions is called auth.\nMany organizations start out simply. They just add IT systems one at a time and use their built-in auth functionality. That’d be like posting a guard at the door to each room who don’t talk to each other.\nEach guard would issue some sort of credential that can be used to verify that the person approaching is who they say they are. For computer systems, the most common type of credential is a username and password combination.\nIncreasingly, organizations are moving towards other kinds of credentials instead of or in addition to usernames and passwords, including biometrics like fingerprints and facial identification, multi-factor codes or a push notification on your phone, or ID cards of some sort.\nBut just having each guard keep their own list of who’s allowed in and hand out their own credential is kinda a mess. As someone walking around the building, you’ve gotta keep all your credentials straight. Moreover, adding new people to the different systems or removing old people is a huge pain. You’ve gotta run around and tell each guard and then the guard has to revoke their credential.\nThis is a common scenario in small organizations without much central IT/Admin expertise. They tend to just use whatever auth comes with the systems they’re adopting.\nIn the labs in section 3, this is how we configured our server. We created a standalone server with standalone credentials. And while that’s secure enough on its own, it doesn’t integrate with any central IT/Admin capabilities.\nObviously, among enterprise organizations, this pattern doesn’t fly. If you’re only a team of 20 and you have 5 systems, it’s ok to metaphorically run around to each room. But a team of 2,000 or 20,000 with 50 or 500 systems can’t be managed this way.\nSo let’s start thinking about how we could make the management of this system simpler and easier.\n\n17.1.1 Centralizing user management with LDAP/AD\nThe first thing would be to standardize on a single set of credentials. No more letting each guard issue their own. You’ll create a central security booth where you’ll manage all the credentials. Whenever someone approaches a door, the guard will radio in the credential, you’ll verify that all is well and tell the guard, “yep, that’s Heather!”\nThis makes life easier for someone using the system since they have only one set of credentials. It’s also somewhat more secure. We know now that all of the doors are using a similar level of security and if someone loses their card or gets it stolen, we just have to do one trade.\nThis is basically the situation that many companies have been in from the mid 1990s onwards and many still use.\nAn open protocol called Lightweight Directory Access Protocol (LDAP – pronounced ell-dap) and the Microsoft implementation of LDAP called Active Directory (AD) allow organizations to maintain central lists of users and the groups to which they belong.\nThey configure their systems to query their LDAP/AD servers. LDAP/AD would sends back information on the user including their username and groups and the service could use that information to authorize the user.\nThis is a huge improvement over each system having its own set of credentials, but there are still three big issues.\nFirst, we haven’t done anything to make authorization easier. You’re now centrally verifying identities, but each guard still needs to maintain their own list of who’s allowed into their room. Changing those lists still requires running around to each room, which is time-consuming and error-prone.\nSecond, just how much do you trust those guards? The guards are radioing in the credentials. There’s nothing to stop them from writing them down for themselves.\nLastly, it’s kinda a pain. Having to re-authenticate at every door is a pain. If you’re logging into a lot of different systems, you’re going to have to authenticate dozens of times a day.\nOver time, these problems have only gotten keener. A few decades ago, most companies had only a few systems and they were probably all on-premise systems, reducing both the hassle of changes and the risk of credentials actually being entered to those systems.\nThese days, even modestly-sized organizations have dozens or hundreds of IT systems and many of them are SaaS services. That means the risk of sharing credentials with those services is much higher.\nWouldn’t it be nice if there were a way to solve all these problems? A solution would allow for centralized authorization, never sharing credentials with the guards, and authenticating only once.\nEnter the world of Single-Sign On (SSO).\n\n\n17.1.2 Single Sign On (SSO)\nOk, so let’s revamp the system one more time.\nNow, when someone enters the building, they’ll stop at the central security office and provide their credentials. In exchange, they’ll get a building access pass that’s unique to them and they cannot share.\nMoreover, you’ll equip each room with a machine to swipe a building access pass. Swiping the access pass sends a request back to the central security office with the person’s name and the room number. You can check whether they’re allowed in and send back the allow signal if so and the red disallow signal if not.\nNow, the guards don’t need to know anything about the people approaching and don’t need to be trusted to do anything other than appropriately respond to the allow/disallow indicator.\nMoreover, you can now manage authorization in the central security office. No more running around to each room when you need to onboard or offboard someone or change their role.\nThis is basically how SSO works – your building access pass is a token in your computer’s web browser, and the allow/disallow decision comes from the centralized auth management system.\nNow, SSO isn’t a description of a technology – it’s a description of a user and administrator experience. People have been making SSO work for a long time, but two main standards for doing SSO have arisen in the past 15 years or so.1\nThe most common option in enterprises is Security Assertion Markup Language (SAML), which is an XML-based standard.2 The current standard, SAML 2.0 was finalized in 2005.\nThe other option is Open Identity Connect (OIDC)/OAuth2.0, which used JSON-based tokens.3 It’s slightly newer than SAML, originally created by engineers at Google and Twitter in 2007. OAuth2 – the current standard – was released in 2012.\nFor nearly all SAML or OAuth implementations, organizations use an external identity manager. The most common ones to use are Okta, OneLogin, and Azure Active Directory (AAD).4\nThese days, almost all enterprises are moving quickly towards implementing SSO using either SAML or OAuth (or both).\nThere’s a lot more detail on the technical details of how LDAP, OAuth, SAML, and more work in Appendix A.\n\n\n17.1.3 Managing permissions\nIrrespective of what kind of auth technology you’re using, you have to actually manage permissions somehow. It’s worth learning a little about the options.\nThe simplest option is just to maintain a list of who’s allowed to enter each room. This is called an Access Control List (ACL).5\nACLs are a simple kind of permissions management that make a lot of intuitive sense.\nBut, as you can imagine, if your building has hundreds of rooms, that’s a lot of different rooms to keep individual lists for. Additionally, if you have permissions changing a lot, having to change individual user permissions is a pain.\nInstead, you might want to create a role that has certain permissions and then assign people to that role. For example, maybe there’s the role of manager, who has access to certain rooms. There might be another role that’s executive who has access to different rooms. Managing permissions in this way is called Role Based Access Control (RBAC), and many organizations have a requirement to be able to implement RBAC to adopt a new system.\n\nRBAC has the advantage of allowing more flexibility in creating roles and managing them relative to ACLs. You can also see how it’d be relatively simple to hook up something like your centralized HR database to an RBAC system by mapping actual users to a set of roles that is appropriate for them.\nBut RBAC has its own drawbacks. RBAC can result in role explosion over time – if people need specialized permissions, it’s often easier to create tons and tons of special roles rather than figure out how to harmonize them into a system.\nIt also can’t accommodate certain kinds of highly specific permissions scenarios. For example, what if you have a situation where room 4242 should only allow managers except from 9-12 on Tuesdays when it should allow maintenance services? That’s not something RBAC can accommodate.\nIf you have the need for even more granular permissions than RBAC can provide, you can create rules based on the room, the person, and the environment. This highly-flexible system is called Attribute Based Access Control (ABAC). The downside of ABAC is that it can be a real pain to set up because it is so powerful.\nTODO: ABAC diagram (mapping room 4242 to person/manager to environment)\nThe most well-known ABAC system is the AWS Identity and Access Management (IAM) system. If you’ve ever been utterly befuddled by applying permissions to resources in AWS, you can thank the complexity of ABAC. That complexity is the tradeoff for the very high degree of flexibility ABAC provides."
  },
  {
    "objectID": "chapters/sec3/3-3-auth.html#data-science-auth-concerns",
    "href": "chapters/sec3/3-3-auth.html#data-science-auth-concerns",
    "title": "17  Auth in Enterprise",
    "section": "17.2 Data Science Auth Concerns",
    "text": "17.2 Data Science Auth Concerns\nThe first and simplest concern that comes up for a new data science workbench is that it needs to support whatever auth mechanism your IT/Admin group has decided your organization is going to use. Hopefully you now have some context on what those various mechanisms are.\nThese days, most data science tools you might want to use support all of the standard SSO mechanisms – though many of them reserve that functionality for paid tiers.\n\n17.2.1 Data Access\nThe second concern that comes up is how to get access to data. In our building metaphor, this would be like sending a note from one room asking for a resource in another room.\nThis kind of access is actually easier to configure in a non-SSO configuration. If your database can be accessed with a username and password, it’s the equivalent of being able to just put your request on a piece of paper and append your credentials. The guard can verify the credentials and send back the proper information.\nIt’s worth noting that this configuration is still quite common. Most organizations have implemented SSO for their external-facing services, but internal data sources are often available with username and password auth. This is mostly because relatively few databases have implemented SSO configurations. I expect this will change quickly in the next few years.\nIn an SSO configuration, getting access to data is kinda complicated. Remember, you’re sitting in a room and you’ve got your building access pass, which you can’t give to anyone, and you can’t get access to another room without swiping your pass at the door.\nThe main way this is accomplished is by giving people the ability to remotely acquire the data room access token without actually going there with the building access pass. Depending on the tool you’re using and the type of technology being used, it may be able to automatically do this exchange for you or you may have to do it manually in your code.\nThe first technology used to authenticate to data services is a Windows-based system called a Kerberos ticket. Kerberos is a relatively old – but highly secure system. Kerberos is mainly used in Windows-based environments, so it’s mostly used when accessing a Microsoft SQL Server.\nIf your organization uses Kerberos for database access, you’ll need to work with your organization’s IT/Admin group to get everything configured.\nThe second way to accomplish SSO into a data source is to pass along an JSON Web Token (JWT, pronounced jot). JWTs are the technology that underlies OAuth/OIDC. They’re accepted by relatively few databases these days, but I expect that will change in the next few years.\nFor more technical details on how this experience is implemented, and some of the technical difficulties you may run into, see the appendix section on OAuth and SAML.\nThe last way to access a data source seamlessly is via integration with cloud IAM services. This will only work if you’re trying to access a cloud resource from another cloud resource. So you could allow access from an EC2 instance to an S3 bucket, but not from your on-prem compute cluster to an S3 bucket.\nThe patterns for doing this vary a lot by cloud and by the data science tooling you’re using. Configuring a cloud instance to “just know” who’s making a request is possible, but requires some savvy with managing cloud services.\n\n\n17.2.2 Service Accounts\nThe last concern that comes up a lot is what entity is doing the access. In workbench environments, it’s common for humans to have credentials to data sources and to login as themselves. However, when those resources go to production, it’s very common to introduce service accounts.\nA service account is a non-human entity that has permissions of its own. For example, maybe you’ve got a Shiny app that users use to visualize data that’s in a database. Very often, you don’t want the app to have the same permisions as the app’s author, or to inherit the permissions of the people viewing the app.\nOne important reason you might want this is that you want to manage the permissions of the app itself even if the author were to leave the company or change roles.\nInstead, you want the app to be able to have permissions to do certain database operations. In that case, you would create a service account to give to the Shiny app that has exactly those permissions.\nThere’s no particular magic to service accounts, but you’ll need to figure out how to make your app or report run as the right user and have the correct service account credentials."
  },
  {
    "objectID": "chapters/sec3/3-3-auth.html#comprehension-questions",
    "href": "chapters/sec3/3-3-auth.html#comprehension-questions",
    "title": "17  Auth in Enterprise",
    "section": "17.3 Comprehension Questions",
    "text": "17.3 Comprehension Questions\n\nWhat is the difference between authentication and authorization?\nWhat are some different ways to manage permissions? What are the advantages and drawbacks of each?\nWhat is some advantages of token-based auth? Why are most organizations adopting it? Are there any drawbacks?\nFor each of the following, is it a username + password method or a token method? PAM, LDAP, Kerberos, SAML, ODIC/OAuth"
  },
  {
    "objectID": "chapters/sec3/3-3-auth.html#footnotes",
    "href": "chapters/sec3/3-3-auth.html#footnotes",
    "title": "17  Auth in Enterprise",
    "section": "",
    "text": "Aside from the options below, some organizations use a system called Kerberos to accomplish SSO. This is quite rare.↩︎\nXML is a markup language, much like HTML. The good thing about XML is that it’s very flexible. The bad thing is that it’s relatively hard for a human to easily read.↩︎\nTechnically, OIDC is an authentication standard and OAuth is an authorization standard. You’ll usually just hear it all referred to as OAuth.↩︎\nYes, AAD is used for SAML/OAuth, not for LDAP. It’s confusing.↩︎\nStandard Linux permissions (POSIX permissons) that were discussed in Chapter 9 are basically a special case of ACLs. ACLs allow setting individual-level permissions for any number of users and groups, as opposed to the one owner, one group, and everyone else permissions set for POSIX.\nLinux distros now have support for ACLs on top of the standard POSIX permissions.↩︎"
  },
  {
    "objectID": "chapters/sec3/3-4-ent-servers.html#managing-complexity-with-infrastructure-as-code",
    "href": "chapters/sec3/3-4-ent-servers.html#managing-complexity-with-infrastructure-as-code",
    "title": "18  Enterprise Server Management",
    "section": "18.1 Managing complexity with Infrastructure as Code",
    "text": "18.1 Managing complexity with Infrastructure as Code\nHere’s where you don’t want to be an an enterprise IT/Admin. You stand up a server for the data science team in a known good state – with versions of R and Python and the applications they need. But then, over time, more stuff gets installed. Maybe they needed some more versions of R. And that package they installed required some updated system libraries.\nIf you just ssh in and do these things — or let the data science team do it themselves, you’ll quickly lose track of what the current state of the server is. You can’t just tear down the server and stand it back up, because none of the changes you’ve made are documented. In DevOps language, the server has become stateful – and that’s a no-no.\nNow, when you’ve just got one server like this, it’s annoying, but not the end of the world. If you’re an IT/Admin organization with 40 or 400 servers it is much more troublesome. And moreover, if you have many servers that need to be configured identically – for example because they’re treated as one cluster – this is impossible. In many enterprises, this whole though experiment would just never fly because production servers can’t be changed. They’re validated from top to bottom for compliance or security reasons and then have to be completely re-validated if any changes are needed.\nIT/Admin organizations want to avoid server state drift and make sure the environment is always in a known good state. If it ever needs to be torn down or moved, it should be a painless activity to re-create the exact same environment with a minimum of fuss.\nThere’s a DevOps adage to encompass this need to manage servers as a herd, rather than individually – servers should be cattle, not pets.\nTo restart an entire server environment from nothing, there are two general steps. Provisioning is about creating and configuring the (virtualized) hardware including the servers and networking configuration. Once the servers are provisioned, they need to be configured including installing things on them and getting them running.\nIf you’re thinking of restarting an environment from scratch, you need to\n\nCreate the networking environment for servers (VPC)\nStand up the actual servers and networking facilities\nConfigure servers (create users, groups, mount storage, etc)\nConfigure networking hardware (ports, firewalls, etc)\nInstall system libraries and operating system dependencies\nInstall R and/or Python\nInstall application layer (RStudio Server, JupyterLab, Shiny Server, etc)\nConfigure application layer (write config files)\nConfigure application layer networking (SSL, DNS)\nStart application\n\nAs an R or Python-loving data scientist, you know that the best way to create this kind of setup is with code. Much like you can use packages like renv and venv to document the state of your R or Python environment and restore it somewhere else (see Chapter 1), IT/Admin professionals use Infrastructure as Code (IaC) tooling to stand up and manage their servers.\nIt’s not always the case that organizations need to put all of this in code. For example, while servers should be re-create-able in code, it’s quite rare that you need to re-create a VPC, so it may well be the case that step remains manual.\nEither way, some names you’ll hear a lot if you’re looking into IaC are Terraform, Ansible, CloudFormation (AWS’s IaC tool), Chef, Puppet, and Pulumi. The other option, which we’ll discuss in more detail below, is using Docker in Kubernetes.\n\n\n\n\n\n\nNo Code IaC\n\n\n\nThere are also ways to manage enterprise servers that don’t involve IaC. These usually involve writing extensive “run-books” for how to configure servers or point-and-click graphical interfaces for server administration. If your spidey sense is tingling that this probably isn’t nearly as good, you’re right. Enterprise IT/Admin organizations that don’t use IaC tooling is definitely a red flag.\n\n\nWe’re not going to get into any more detail on how these IaC tools work – there are books on all of them you should feel free to read if you’re interested. But now you’ll understand what you’re hearing when the IT/Admin team says they need to write a Terraform script or a Chef recipe to be able to stand up a particular service.\n\n18.1.1 Dev/Test/Prod for IT/Admins\nLike you want a Dev/Test/Prod setup for your data science projects as discussed in Chapter 5, IT/Admins usually use a Dev/Test/Prod setup for themselves. They want an environment to test changes before promoting them for testing and eventually into production. In ideal states, even changes in Dev are only attempted with IaC tools to make sure it’s easy to move forward once the changes are final.\nIn enterprises, upgrades and migrations are major to-dos. Planning for a “lift-and-shift” to the cloud is a multi-year affair in many organizations. Even just upgrading operating systems can be a major undertaking.\nFor enterprises, I recommend a two-dimensional Dev/Test/Prod setup where the IT/Admin group make changes to the platform in a staging environment.1 Data scientists never get access to the Staging environments and do all their work in the IT/Admin prod environment.\n\nLooking back to Chapter 5, it should be obvious that the best way to move servers and applications from staging to prod is using IaC and CI/CD to make sure that changes to code always make it into production at the right time."
  },
  {
    "objectID": "chapters/sec3/3-4-ent-servers.html#enterprise-scale-and-stability",
    "href": "chapters/sec3/3-4-ent-servers.html#enterprise-scale-and-stability",
    "title": "18  Enterprise Server Management",
    "section": "18.2 Enterprise scale and stability",
    "text": "18.2 Enterprise scale and stability\nNeeding to scale data science environments for large jobs is a common need across large and small organizations. But the enterprise need to support many data scientists often requires a different kind of approach to scaling.\nIn Chapter 11, we discussed how just making a single server bigger with vertical scaling can take you a long way. But there are limits. For example, I was recently working with a large customer of Posit’s who needed to support up to 2,000 data scientists in their environment.\nAt that scale, there is no server that is nearly large enough. The way to accomplish this kind of scale isto spread the workload out across many servers with horizontal scaling.\nAdditionally, when you’re operating at that scale, the stability of the system gets really important. When you’ve got 2,000 data scientists using your platform, each hour of lost time costs more than $100,000.2\nFor some organizations, this necessitates the creation of a disaster recovery plan. This may mean that there are frequent snapshots of the state of the server (often nightly) so the IT/Admins can just roll back to a previous known good state in the event of a failure. Sometimes it also means that there is actually a copy of the server waiting on standby to be activated at all times.\nOther times, there are stiffer requirements such that nodes in the cluster could fail and the users wouldn’t be meaningfully affected. This requirement for limited cluster downtime is often called high availability. High availability is not a description of a particular technology or technical approach – though it is sometimes treated as such. High availability is a description of a desired outcome for a cluster and different organizations have different definitions.\n\n\n\n\n\n\nMeasuring Uptime\n\n\n\nDowntime limits are often measured in nines of uptime. Nines of uptime refers to the proportion of the time that the service is guaranteed to be online out of a year. So a one-nine service is guaranteed to be up 90% of the time, allowing for 36 days of downtime a year. A five-nine service is up for 99.999% of the time, allowing for only about 5 1/4 minutes of downtime a year.\n\n\nHorizontal scaling and high availability are different of requirements with different implications. Depending on your organization, you may have horizontal scaling requirements or high availability requirements or both.\nOne option, where you have horizontal scaling requirements, but not high availability is to just put different users or team on their own server. The upside to this is that the team gets ownership of the server. The downside is fragmentation and that the team has to manage your own servers. In organizations that allow this kind of fragmentation, each team is usually on their own to support those servers and you’re basically back in non-enterprise land.\nIn enterprises, this usually isn’t the way things get done. Most enterprises want to run one centralized service that everyone in the company – or at least across a large group – come to.\nIf you’re trying to run a large centralized service, it’s going to be run as a load-balanced cluster. In this context, you don’t want to think about individual servers. Instead, you want to manage the cluster as a whole and make it really easy to add individual servers, called nodes.\nIn order to go from a single server to a cluster, you add two requirements – there needs to be one front door to all the servers, called a load-balancer – and any state users care about needs to move from the individual server to a shared location all of the nodes can access. Usually that state is stored in a database or network attached storage (NAS), which is just a filesystem that lives on its own server.\n\nIf you are a solo data scientist reading this – please do not try to run your own data science cluster. When you undertake load balancing, you’ve taken on a distributed systems problem and those are inherently difficult. When done for scaling reasons, it is almost always worthwhile to exhaust the part of vertical scaling where costs grow linearly with compute before undertaking horizontal scaling.\nIf your organization has high availability requirements, it’s worth considering that just adding more nodes may not be sufficient. As the requirements for high availability get steeper, the engineering cost to make sure the service really is that resilient rise exponentially. In addition to considering what to do if a node goes offline, you also have to have backup for the load-balancer and the database/NAS, as well as any other part of the system.\nIn fact, it’s totally possible to make your system less stable by just doing horizontal scaling in one spot without thinking through the implications.\n\n18.2.1 Load-balancing requirements\nLoad-balancers are just a special kind of proxy that routes sessions among the nodes in the cluster. A simple load-balancer just rotates sending traffic to the different nodes using a round-robin configuration.\nAt a minimum, your load-balancer has to know what nodes are accepting traffic. This is accomplished by configuring a health check/heartbeat endpoint for your product. A health check is a feature of an application. The load-balancer periodically pings the health check. If it gets a response, it knows it can send traffic to that node. If no response comes back, it treats that node as unhealthy and doesn’t send traffic there.\nIt is sometimes necessary to do more complicated load-balancing that pays attention to how loaded different nodes are. Additionally, some products feature internal load-balancers, so it really doesn’t matter what your load-balancer does.\nOne other feature that may come up is sticky sessions or sticky cookies. For stateful applications – like Shiny apps – you want to get back to the same node in the cluster so you can resume a previous session. In most load-balancers, this is a simple option you can just turn on.\nThere are a few different ways to configure load balancing for servers. The first is called active/active. This just means that all of the servers are online all of the time. So if I have two RStudio Server instances, they’re both accepting traffic all the time.\nIn active/passive configurations, you have two or more servers, with one set accepting traffing all the time, and the other set remaining inert until or unless the first set goes offline. This is sometimes called a blue/green or red/black configuration.\nPeople often really like this configuration if they have high requirements for uptime, and want to be able to do upgrades to the system behind the scenes and then just cut the traffic over at some point without an interruption in service. It is a nice idea. It is often very hard to pull off.\nFor an application, there are basically two ways it can store data – in a database or in files on the filesystem. Now, this isn’t data you need to access, but it’s needed for the internal workings of the application. For example, it may need to save log information or counts of the number of active users.\nWhen you’re in a single-server configuration, all this can just go somewhere on the server with the application. Very often the database is just a sqlite file that sits in a file on the filesystem.\nBut when you go to to a load-balanced cluster, all of the nodes need symmetric access to read and write. You need an actual database (like postgres) for the application to use. Additionally, anything on the filesystem now needs to move off of an individual server onto a networked system that all the nodes can access.\n\n\n18.2.2 Accommodating different kinds of workloads\nWhen you’re doing horizontal-scaling, people often want to accomodate a variety of workloads. For example, they might want to be able to incorporate several different sizes of jobs that might require a few different sized nodes. Or maybe they want to run a mixture of GPU-backed and non-GPU workloads in the cluster and want to reserve GPU nodes for the jobs that need it.\nIn general, it’s not trivial to set up a single cluster that can support different kinds of workloads, and it’s often easier to set up, for example, a standalone GPU cluster.\nThese days, people are often reaching for Kubernetes for clusters that need to support varies workloads. While it’s true that Kubernetes can theoretically support different kinds of nodes within one cluster, this often isn’t trivial to configure.\nOften, a better option for data science workloads is to use use a high-performance computing (HPC) framework. HPC is particularly appropriate when you need very large machines. In Kubernetes, it is not possible to schedule pods larger than the size of the actual nodes in the cluster. In contrast, most HPC frameworks allow you to combine an arbitrary number of nodes into what acts like a single machine with thousands or tens of thousands of nodes.\nFor example, Slurm is an HPC framework that support multiple queues for different sets of machines. AWS has a service called ParallelCluster that allows users to easily set up a Slurm cluster – and with no additional cost relative to the cost of the underlying AWS hardware.\n\n\n18.2.3 Adding Autoscaling\nSometimes people also undertake horizontal scaling to be able to do autoscaling. The idea here is that the organization could maintain a small amount of “always-on” capacity and scale out other capacity as needed to maintain costs. This is possible – but it requires nontrivial engineering work.\nIn particular, autoscaling a data science workbench down is quite hard. The main reason for this is that many autoscaling routines assume you can easily move someone’s job from one node to another just keeping track of the long-term state. This is a bad assumption for a data science workbench and autoscaling a data science workbench downwards is a difficult challenge.\nLet’s think about a relatively stateless workload. For example, let’s imagine a search engine – every time you put in a search, it spins up a job, does your search, and then spins down.\nIf you come back in a minute or two, it can just spin up another job. It only needs to remember your last query, but that’s a pretty simple bit to state to pass around. If you’re on a different node, no big deal!\nThat’s not the case with a data science workbench. Many autoscaling frameworks these days assume that applications are mostly stateless. In a stateless applications, every interaction is standalone – there’s very little path dependence. The line between statefulness and statelessness are pretty blurry, but working inside a development environment like RStudio or a Jupyter Notebook is about as stateful as it gets. You have long-term state like the files you need and your preferences and short-term state like the recent commands you’ve typed and the packages loaded into your R and Python environment."
  },
  {
    "objectID": "chapters/sec3/3-4-ent-servers.html#dockerkubernetes-in-enterprise",
    "href": "chapters/sec3/3-4-ent-servers.html#dockerkubernetes-in-enterprise",
    "title": "18  Enterprise Server Management",
    "section": "18.3 Docker/Kubernetes in Enterprise",
    "text": "18.3 Docker/Kubernetes in Enterprise\nOriginally created at Google and released in 2014, Kubernetes is the way to run production services out of Docker containers.3 It is an open source project. If you’re confused by the name, apparently Kubernetes is an ancient Greek word for “helmsman”.\nMany organizations are moving towards using Docker and Kubernetes for their server-based infrastructure. Kubernetes solves all three of the key enterprise IT/Admin challenges with running servers because you manage a single cluster, you can scale rather effortlessly, and app-level administration is extremely simple.\nThe elegance of using Kubernetes is that it completely separates provisioning and configuration challenges. The main unit of analysis in Kubernetes is a pod, which is the term for a Docker container that is running in Kubernetes.\nThe elegance of Kubernetes is that you create a cluster of a certain number of nodes and separately request a certian number of pods with a certain amount of horsepower. Kubernetes takes care of scheduling the pods on the nodes.\nTODO: Graphic of Kubernetes\nThis is amazing, because unlike running services on a regular VM, you just make sure you’ve got enough horsepower in the cluster and then all the app-level requirements go in the container. Then when you declare how many pods you want, you don’t have to worry about what’s going on with each of the nodes in the cluster because it is just running Kubernetes and Docker.\nThis really is extremely powerful – it’s pretty easy to tell Kubernetes, “I want one instance of my load balancer container connected to three instances of my Workbench container with the same storage volume connected to all three.”\nIn practice, unless you’re part of a very sophisticated IT organization, you’ll almost certainly use Kubernetes via one of the cloud providers’ Kubernetes clusters as a service. AWS’s is called Elastic Kubernetes Service (EKS).4\nOne really nice thing about using these Kubernetes clusters as a service is that adding more compute power to your cluster is generally as easy as a few button clicks. On the other hand, that also makes it dangerous from a cost perspective.\nIt is possible to define a Kubernetes cluster “on the fly” and deploy things to a cluster in an ad hoc way. But then you’re back out of IaC world. You can use any standard IaC tooling to stand up a Kubernetes cluster in any of the three major clouds. Once you’ve got the cluster up, Helm is the standard tool for defining what’s running in the cluster.\nThere’s a reason Kubernetes is taking the world by storm – it makes managing complicated enterprise workloads way easier. For example, let’s say you want to stand up a new service in a Kubernetes cluster using IaC. I listed 10 steps above for doing this in a standard server-based configuration. For Kubernetes, there are only three steps:\n\nProvision the cluster itself using IaC or add nodes to an existing cluster (usually trivially easy)\nGet the Docker containers to run in the cluster (often provided by vendors)\nDeclare what container configuration you want in the environment using Helm\n\nOne really nice thing about this is that it’s really clear where the state lies. The applications themselves are installed in the Docker containers and because the containers themselves are ephemeral, state has to be mounted in from somewhere external.\nRelative to more stateful server-based configuration, Kubernetes-based configurations rest a lot more heavily on environment variables, so a lot of configuring application setup is the same as configuring the environment variables in the running container.\nIn the extreme version, some enterprises are going to “we just run a single kubernetes cluster for our entire organzation”. In my opinion, this will someday be a great pattern, but it’s still a little immature. In particular, Kubernetes kinda makes the underlying assumption that all of the resources for the cluster are shared across the cluster. There are ways to use namespaces and other things – but not all Kubernetes resources can be split into namespaces, and managing a Kubernetes cluster for multiple use cases or groups isn’t at all trivial.\nIt’s easy to look at the power of Kubernetes and think it will make everything easy. This is not the case. While the high-level outlines of Kubernetes are super appealing, it is still a complex tool and a data science workbench is a particularly difficult fit for Kubernetes.\nIn many organizations, adopting Kubernetes is synonymous with trying to do autoscaling. As previously discussed, autoscaling a data science workbench is a particularly difficult task – and Kubernetes is particularly friendly to stateless workloads. Autoscaling in Kubernetes with a data science workbench really requires a highly competent Kubernetes admin.\nNetworking in Kubernetes can also be quite complicated. For anything that lives fully in Kubernetes, like your Workbench nodes and load-balancer, it’s quite simple. But getting things into and out of the Kubernetes cluster – like the filesystem you probably need to mount in and accessing databases is a real challenge.\nAll this to say – these are solvable problems for an experienced Kubernetes admin. But they will probably need some guidance around specific requirements for a data science workbench."
  },
  {
    "objectID": "chapters/sec3/3-4-ent-servers.html#comprehension-questions",
    "href": "chapters/sec3/3-4-ent-servers.html#comprehension-questions",
    "title": "18  Enterprise Server Management",
    "section": "18.4 Comprehension Questions",
    "text": "18.4 Comprehension Questions\n\nWhat is the difference between horizontal and vertical scaling? For each of the following examples, which one would be more appropriate?\n\nYou’re the only person using your data science workbench and run out of RAM because you’re working with very large data sets in memory.\nYour company doubles the size of the team that will be working in your data science workbench. Each person will be working with reasonably small data, but there’s going to be a lot more of them.\nYou have a big modeling project that’s too large for your existing machine. The modeling you’re doing is highly parallelizable.\n\nWhat is the role of the load balancer in horizontal scaling? When do you really need a load balancer and when can you go without?\nWhat are the biggest strengths of Kubernetes as a scaling tool? What are some drawbacks?"
  },
  {
    "objectID": "chapters/sec3/3-4-ent-servers.html#footnotes",
    "href": "chapters/sec3/3-4-ent-servers.html#footnotes",
    "title": "18  Enterprise Server Management",
    "section": "",
    "text": "I call it staging because I call the data science environments Dev/Test/Prod. You’ll have to work out language with your IT/Admin group.↩︎\nA low-end estimate of $100,000 fully-loaded cost for one data scientist FTE and 2,000 hours is $50 per hour.↩︎\nIf you are pedantic, there are other tools for deploying Docker containers like Docker Swarm and Kubernetes is not limited to Docker containers. But those are corner cases.↩︎\nIf you aren’t using EKS, Azures AKS, or Google’s GKE, the main other competitor is Oracle’s OpenShift, which some organizations have running in on-prem Kubernetes clusters.↩︎"
  },
  {
    "objectID": "chapters/append/auth.html#user-database",
    "href": "chapters/append/auth.html#user-database",
    "title": "Appendix A — Auth Technologies",
    "section": "A.1 User Database",
    "text": "A.1 User Database\nMany pieces of software come with integrated authentication. When you use those systems, the product stores encrypted username and password pairs in a database.\nThese setups are often really easy from an admin perspective – you just set up individual users on the server.\nHowever, the flip side is that users have one more username and password to remember, which is annoying for them. Moreover, if you have more than a few users, or the system is one of more than a few, it’s hard to manage users on a lot of different systems. It can be a real pain to create accounts on a ton of different systems when a new person joins the organization, or to remove their permissions one-by-one when they leave.\nFor this reason, most IT/Admin organizations strongly prefer using some sort of centralized identity store."
  },
  {
    "objectID": "chapters/append/auth.html#linux-accounts",
    "href": "chapters/append/auth.html#linux-accounts",
    "title": "Appendix A — Auth Technologies",
    "section": "A.2 Linux Accounts",
    "text": "A.2 Linux Accounts\nMany pieces of software – especially data science workbenches – are able to look at the server they’re sitting on and use the accounts on the server themselves.\nPluggable Authentication Modules (PAM) is the system that allows Linux-based authentication. As of this writing, PAM is the default authentication method for both RStudio Server and JupyterHub.\n\nConceptually PAM is pretty straightforward. You install a service on a Linux machine and configure it to use PAM authentication from the underlying host.\nBy default, PAM just authenticates against the users configured on the Linux server, but it can also be configured to use other modules to authenticate against other systems – most commonly LDAP/AD or Kerberos. PAM can also be used to do things when users login – the most common being initializing tokens or tickets to other systems, like a database.\nPAM is often paired with System Security Services Daemon (SSSD) to automatically create Linux users on a server based on the identities stored in an LDAP/AD instance.\nThough conceptually simple, reading, writing, and managing PAM modules is quite painful. Additionally, as more services move to the cloud, there isn’t necessarily an underlying Linux host where identities live and PAM is generally considered a legacy technology."
  },
  {
    "objectID": "chapters/append/auth.html#ldapad",
    "href": "chapters/append/auth.html#ldapad",
    "title": "Appendix A — Auth Technologies",
    "section": "A.3 LDAP/AD",
    "text": "A.3 LDAP/AD\nFor many years, Microsoft’s Lightweight Directory Access Protocol (LDAP) implementation called Active Directory (AD) was basically the standard in enterprise authentication. As a credential-based system, it is increasingly being retired in favor of token-based systems like SAML and OAuth2.0.\nWhen you login to a service with LDAP, it uses the ldapsearch command to to look you up. LDAP is a protocol, like HTTP. In addition to the query, LDAP sends a set of bind credentials to validate that it’s allowed to be looking things up in the server.\nLDAP can be configured in single-bind mode, where it uses the user’s credentials as the bind credentials. More often, LDAP is configured in double-bind mode, where there is a dedicated service account for binding.\n\nTo give a little more color, let’s look at the results from an ldapsearch against an LDAP server.\nHere’s what my entry in a corporate LDAP directory might look like this:\ncn: Alex Gold\nmail: alex.gold@example.com\nmail: alex.gold@example.org\ndepartment: solutions\nmobile: 555-555-5555\nobjectClass: Person\nYou’ll note that LDAP sends back a lot of information! This is how authorization is done with LDAP – the service is configured to inspect the items that come back from the ldapsearch and do authorization accordingly. This makes it possible to do authorization, but it’s not nearly as tidy as with a token-based system.\nOne of the things LDAP excels at relative to token-based systems is the ability to do lookups ahead of time. Because LDAP is a database, you can do a search for users – for example to make accounts for them – before they show up. In contrast, token-based systems don’t know anything about you until you show up for the first time with a valid token.\nAn example where this matters is if you have a service that itself controls authorization. For example, in Posit Connect, you can assign authorization to see certain apps and reports inside Connect. If you’re using LDAP, you can run a search against the server and add them before they ever log in. If you’re using a token-based system, you’ve gotta wait for them to show up. There are ways around this, but they generally don’t come out-of-the-box."
  },
  {
    "objectID": "chapters/append/auth.html#sec-kerberos",
    "href": "chapters/append/auth.html#sec-kerberos",
    "title": "Appendix A — Auth Technologies",
    "section": "A.4 Kerberos Tickets",
    "text": "A.4 Kerberos Tickets\nKerberos is a relatively old token-based auth technology for use among different enterprise servers. In Kerberos, encrypted tokens called Kerberos Tickets are passed between the servers in the system.\nThough Kerberos is freely available, it was widely adopted along with Active Directory, and it’s used almost exclusively in places that are running a lot of Microsoft products. The most frequent use of Kerberos tickets is to establish connections to Microsoft databases.\nKerberos works by sending information to and from the central Kerberos Domain Controller (KDC).\nWhen a Kerberos session is initialized, the service sends the users credentials off to the KDC and requests something called the Ticket Granting Ticket (TGT) from the KDC. TGTs have a set expiration period. When they expire, the client has to request an updated TGT.\nThis is one reason why Kerberos is considered quite secure - even if someone managed to steal a TGT, they’d only be able to use it for a little while before it went stale and could be revoked.\nWhen the user want to access a service, they send the TGT back to the KDC again and get a session key (sometimes referred to as a service ticket) that allows access, usually with a specified expiration period.\n\nIt’s worth reiterating that Kerberos is used exclusively inside a corporate network. This is one reason it’s still considered secure, even though it’s old. On the other hand, because everything has to live inside the network, it doesn’t work well for providing access to services outside the network, like SaaS software. For that reason, Kerberos is considered a legacy tool."
  },
  {
    "objectID": "chapters/append/auth.html#oauth-saml",
    "href": "chapters/append/auth.html#oauth-saml",
    "title": "Appendix A — Auth Technologies",
    "section": "A.5 Modern systems: OAuth + SAML",
    "text": "A.5 Modern systems: OAuth + SAML\nThese days, most organizations are quickly moving towards implementing a modern token-based authentication system through SAML and/or OAuth.1\nThe way both systems work is that you go to login to a service and are redirected to the SAML/OAuth system to seek a token that will let you in. Assuming all goes well, you’re granted a token and go back to the service to go do your work.\nThere are a few meaningful differences between SAML and OAuth that may help in conversations about these systems with Enterprise IT folks.\nMost of these differences stem from the origin of the technology. The current SAML 2.0 standard was finalized in 2005 – roughly coinciding with the beginning of the modern era of the web. Facebook was started just the prior year.\nOAuth is the next generation. It was started in 2006 and the current 2.0 standard was finalized in 2013.\nWhere SAML was designed to replace older enterprise auth technologies like LDAP/AD and Kerberos, OAuth was designed with the web in mind. People were trying to solve the problem of how to avoid having usernames and passwords for each individual site you might access without giving those sites the usernames and passwords of more sensitive sites.\nYou might’ve used SAML today if you logged into a SSO service for work. In contrast, you’ve almost certainly used OAuth today. Any time you’ve used a Login with Google/Facebook/Twitter/GitHub flow – that’s OAuth.\nWhile OAuth is the default on the web, it’s still somewhat newer in an enterprise context. Increasingly, cloud services are adopting OAuth based auth patterns and many popular database providers are adding the ability to use OAuth.\nSAML is XML-based while OAuth uses JSON Web Tokens (JWT). In both cases, the tokens are usually cryptographically signed. SAML tokens may be encrypted, while JWTs are usually unencrypted and signed. This makes OAuth somewhat easier to debug as JSON is easier to read than XML.\nSAML is a user-centric authentication tool, while OAuth is an application-centric authorization tool. The difference is subtle, but meaningful. For example, let’s say you’re trying to access your organization’s data science workbench. If I was using SAML, I would acquire a token that says “the bearer of this token is Alex Gold”, and it would be up to the workbench to know what to do with that. In contrast, an OAuth token would say, “the bearer of this token should get these specific permissions to Alex Gold’s stuff in this application”. That means that OAuth can be used for authorization at a much more granular level.\nImplementing SSO to the “front door” of a data science service, like your data science workbench or hosting platform is pretty standard these days. Right now, the industry is strongly heading in the direction of “second hop” auth. The idea is that when this works right, you log into the front door of your data science server with SAML or OAuth and then access to data sources like databases, APIs, and cloud storage is handled automatically on the back end – no need for users to (insecurely) input credentials inside the data science environment.2\nTODO: image of passthrough auth\nVery often, IT/Admins describe this experience as “passthrough” auth. The issue is that SAML and OAuth tokens can’t be passed through to another service. SAML tokens are specific to the service provider who asked for them and OAuth tokens describe particular authorizations for the bearer to a particular service.\nIn contrast, a JWT can be issued to any entity and every single request made is accompanied by the OAuth JWT. For example, if you’ve accessed Google Sheets or another Google service from R or Python, you’ve gone through the JWT acquisition dance and then attached that JWT to every request to prove it’s authorized. Similarly, cloud IAM experiences are based on passing JWTs around.\nThis means that there’s always a need to go back to trade in an existing token for a different one for the service you actually want to access. This is almost always an OAuth JWT. Increasingly, data science services are implementing these flows themselves so it does happen automatically, but in some cases you’ll have to figure out how to do the token exchange manually.\nThese days, many organizations are caught in the middle of this work - they are probably using SAML to access many systems and may have mandates to move towards credential-free communication to data sources, but very few have fully implemented such a system.\nHere are quick overviews on how both SAML and OAuth work.\nIn SAML, the service you’re accessing (token consumer) is called the service provider (SP) and the entity issuing the token is called the SAML identity provider (IdP). Most SAML tooling allows you start at either the IdP or the SP.\nIf you start at the SP, you’ll get redirected to the IdP. The IdP will verify your credentials. If all is good, it will put a SAML token in your browser. The SAML token contains claims – information provided by the SAML IdP.\n\nThe OAuth flow is a little more parsimonious. In OAuth, the service you’re trying to visit is called the resource server and the token issuer is the authorization server. When you try to access a service, the service knows to look for a JWT that includes specific claims against a set of scopes defined ahead of time.\nFor example, you may claim read access against the scope of events on Alex’s calendar.\nIf you don’t have that token, you’ll need to go to the authorization server to get it. Unlike in SAML where action is all occurring via HTTP redirects and the SAML token must live in the person’s browser, OAuth makes no assumptions about how this flow happens.\nThe authorization server knows how to accept requests for a token and the resource server knows how to accept them. The process of requesting and getting a token can happen in a number of different ways that might include browser redirects and caches, but also could be done entirely in R or Python.\nThis is why OAuth is used for service-to-service communication."
  },
  {
    "objectID": "chapters/append/auth.html#footnotes",
    "href": "chapters/append/auth.html#footnotes",
    "title": "Appendix A — Auth Technologies",
    "section": "",
    "text": "Modern OAuth is actually OAuth2.0. Whenver I refer to OAuth, I mean OAuth 2.0.↩︎\nWhile OAuth is an authorization technology, there is an authentication technology, Open ID Connect (OIDC) built on top of OAuth. In practice, people use the term OAuth to refer to both OAuth proper and OIDC.↩︎"
  },
  {
    "objectID": "chapters/append/lab-map.html",
    "href": "chapters/append/lab-map.html",
    "title": "Appendix B — Lab Map",
    "section": "",
    "text": "This section aims to clarify the relationship between the assets you’ll make in each portfolio exercise and labs in this book.\n\n\n\n\n\n\n\nChapter\nLab Activity\n\n\n\n\nChapter 1: Environments as Code\nCreate a Quarto side that uses {renv} and {venv} to create standalone R and Python virtual environments, create a page on the website for each.\n\n\nChapter 3: Data Architecture\nMove data into a DuckDB database.\n\n\nChapter 2: Project Architecture\nCreate an API that serves a Python machine-learning model using {vetiver} and {fastAPI}. Call that API from a Shiny App in both R and Python.\n\n\nChapter 4: Logging and Monitoring\nAdd logging to the app from Chapter 2.\n\n\nChapter 5: Code Promotion\nPut a static Quarto site up on GitHub Pages using GitHub Actions that renders the project.\n\n\nChapter 6: Docker\nPut API from Chapter 2 into Docker container.\n\n\nChapter 7: Servers\nStand up an EC2 instance. Put model from [Chapter @sec-proj-arch] into an S3 bucket.\n\n\nChapter 8: Command Line\nLog into your server with a .pem key and create an SSH key.\n\n\nChapter 9: Linux Admin\nCreate a user on the server and add the SSH key from Chapter 8.\n\n\nChapter 10: App Admin\nAdd R, Python, RStudio Server, JupyterHub, palmer penguin fastAPI + App to the server.\n\n\nChapter 11: Servers\nResize servers.\n\n\nChapter 12: Networking\nSet up a proxy (NGINX) to reach all services from the web.\n\n\nChapter 13: DNS\nAdd a real URL to the EC2 instance. Put the Shiny app into an iFrame on the Quarto site.\n\n\nChapter 14: SSL\nAdd SSL/HTTPS to the EC2 instance."
  },
  {
    "objectID": "chapters/append/cheatsheets.html#cheat-docker",
    "href": "chapters/append/cheatsheets.html#cheat-docker",
    "title": "Appendix C — Cheatsheets",
    "section": "C.1 Docker",
    "text": "C.1 Docker\n\nC.1.1 Docker CLI Commands\n\n\n\n\n\n\n\n\n\nStage\nCommand\nWhat it does\nNotes and helpful options\n\n\n\n\nBuild\ndocker build &lt;directory&gt;\nBuilds a directory containing a Dockerfile into an image.\n-t &lt;name:tag&gt; provide a name to the container. tag is optional, defaults to latest.\n\n\nMove\ndocker push &lt;image&gt;\nPush a container to a registry.\n\n\n\nMove\ndocker pull &lt;image&gt;\nPull a container from a registry.\nRarely used because run pulls the container if needed.\n\n\nRun\ndocker run &lt;image&gt;\nRun a container.\nSee flags in next table.\n\n\nRun\ndocker stop &lt;container&gt;\nStop a running container.\ndocker kill can be used if stop fails.\n\n\nRun\ndocker ps\nList containers.\nUseful to get running container id to do things to it.\n\n\nRun\ndocker exec &lt;container&gt; &lt;command&gt;\nRun a command inside a running container.\nBasically always used to open a shell in a container with docker exec -it &lt;container&gt; /bin/bash\n\n\nRun\ndocker logs &lt;container&gt;\nViews logs for a container.\n\n\n\n\n\n\nC.1.2 Flags for docker run\n\nReminder: Order for -p and -v is &lt;host&gt;:&lt;container&gt;\n\n\n\n\n\n\n\nFlag\nEffect\nNotes\n\n\n\n\n--name &lt;name&gt;\nGive a name to the running container.\nOptional. Will be auto-assigned if not provided.\n\n\n--rm\nRemove container when its stopped.\nDon’t use in production. You probably want to inspect failed containers.\n\n\n-d\nDetach container (don’t block the terminal).\nAlmost always used in production.\n\n\n-p &lt;port&gt;:&lt;port&gt;\nPublish port from inside container to outside.\nNeeded if you want to access an app or API running inside the container.\n\n\n-v &lt;dir&gt;:&lt;dir&gt;\nMount volume into the container.\n\n\n\n\n\n\nC.1.3 Dockerfile Commands\nThese are the commands that go in a Dockerfile when you’re building it.\n\n\n\n\n\n\n\n\nCommand\nPurpose\nExample\n\n\n\n\nFROM\nIndicate base container.\nFROM rocker/r-ver:4.1.0\n\n\nRUN\nRun a command when building.\nRUN apt-get update\n\n\nCOPY\nCopy from build directory into the container.\nCOPY . /app/\n\n\nCMD\nSpecify the command to run when the container starts.\nCMD quarto render ."
  },
  {
    "objectID": "chapters/append/cheatsheets.html#cloud-services",
    "href": "chapters/append/cheatsheets.html#cloud-services",
    "title": "Appendix C — Cheatsheets",
    "section": "C.2 Cloud Services",
    "text": "C.2 Cloud Services\n\n\n\n\n\n\n\n\n\nService\nAWS\nAzure\nGCP\n\n\n\n\nKubernetes cluster\nEKS (Elastic Kubernetes Service) or Fargate\nAKS (Azure Kubernetes Service)\nGKE (Google Kubernetes Engine)\n\n\nRun a container /application\nECS (Elastic Container Service) or Elastic Beanstalk\nAzure Container Apps\nGoogle App Engine\n\n\nRun an API\nLambda\nAzure Functions\nGoogle Cloud Functions\n\n\nDatabase\nRDS or Reds hift[^2-1-cloud-8]\nAzure Database\nGoogle Cloud SQL\n\n\nML Platform\nSageMaker\nAzure ML\nVertex AI"
  },
  {
    "objectID": "chapters/append/cheatsheets.html#cheat-cli",
    "href": "chapters/append/cheatsheets.html#cheat-cli",
    "title": "Appendix C — Cheatsheets",
    "section": "C.3 Command Line",
    "text": "C.3 Command Line\n\nC.3.1 General Command Line\n\n\n\nSymbol\nWhat it is\n\n\n\n\nman &lt;command&gt;\nOpen manual for command\n\n\nq\nQuit the current screen\n\n\n\\\nContinue bash command on new line\n\n\nctrl + c\nQuit current execution\n\n\n\n\n\nC.3.2 Linux Navigation\n\n\n\n\n\n\n\n\nCommand\nWhat it does/is\nNotes + Helpful options\n\n\n\n\n/\nSystem root or file path separator\n\n\n\n.\ncurrent working directory\n\n\n\n..\nParent of working directory\n\n\n\n~\nHome directory of the current user\n\n\n\nls &lt;dir&gt;\nList objects in a directory\n-l - format as list\n-a - all (include hidden files that start with .)\n\n\npwd\nPrint working directory\n\n\n\ncd &lt;dir&gt;\nChange directory\nCan use relative or absolute paths\n\n\n\n\n\nC.3.3 Reading Text Files\n\n\n\n\n\n\n\n\nCommand\nWhat it does\nNotes + Helpful options\n\n\n\n\ncat &lt;file&gt;\nPrint a file from the top.\n\n\n\nless &lt;file&gt;\nPrint a file, but just a little.\nCan be very helpful to look at a few rows of csv.\nLazily reads lines, so can be much faster than cat for big files.\n\n\nhead &lt;file&gt;\nLook at the beginning of a file.\nDefaults to 10 lines, can specify a different number with -n &lt;n&gt;.\n\n\ntail &lt;file&gt;\nLook at the end of a file.\nUseful for logs where the newest part is last.\nThe -f flag is useful to follow for a live view.\n\n\ng r    e p &lt;expression&gt;\nSearch a file using regex.\nWriting regex can be a pain. I suggest testing expressions on regex101.com.\nOften useful in combination with the pipe.\n\n\n|\nThe pipe\n\n\n\n\n\n\nC.3.4 Manipulating Files\n\n\n\n\n\n\n\n\nCommand\nWhat it does/is\nNotes + Helpful Options\n\n\n\n\nrm &lt;path&gt;\nRemove\n-r - recursively remove everything below a file path\n-f - force - don’t ask for each file\nBe very careful, it’s permanent\n\n\nc  p &lt;from&gt; &lt;to&gt;\nCopy\n\n\n\nm  v &lt;from&gt; &lt;to&gt;\nMove\n\n\n\n*\nWildcard\n\n\n\nmkdir/rmdir\nMake/remove directory\n-p - create any parts of path that don’t exist\n\n\n\n\n\nC.3.5 Move things to/from server\n\n\n\n\n\n\n\n\nCommand\nWhat it does\nNotes + Helpful options\n\n\n\n\ntar\nCreate/extract archive file\nAlmost always used with flags.\nCreate is usually\ntar -czf &lt; a  r c hive name&gt; &lt;file(s)&gt;\nExtract is usually\nt  a r  -xfv &lt;archive name&gt;\n\n\nscp\nSecure copy via ssh\nRun on laptop to server\nCan use most ssh flags (like -i and -v)\n\n\n\n\n\nC.3.6 Write files from the command line\n\n\n\n\n\n\n\n\nCommand\nWhat it does\nNotes\n\n\n\n\ntouch\nCreates file if doesn’t already exist.\nUpdates last updated to current time if it does exist.\n\n\n&gt;\nOverwrite file contents\nCreates a new file if it doesn’t exist\n\n\n&gt;&gt;\nConcatenate to end of file\nCreates a new file if it doesn’t exist\n\n\n\n\n\nC.3.7 Command Line Text Editors (Vim + Nano)\n\n\n\n\n\n\n\n\nCommand\nWhat it does\nNotes + Helpful options\n\n\n\n\n^\nPrefix for file command in nano editor.\nIts the ⌘ or Ctrl key, not the caret symbol.\n\n\ni\nEnter insert mode (able to type) in vim\n\n\n\nescape\nEnter normal mode (navigation) in vim.\n\n\n\n:w\nWrite the current file in vim (from normal mode)\nCan be combined to save and quit in one, :wq\n\n\n:q\nQuit vim (from normal mode)\n:q! quit without saving"
  },
  {
    "objectID": "chapters/append/cheatsheets.html#cheat-file",
    "href": "chapters/append/cheatsheets.html#cheat-file",
    "title": "Appendix C — Cheatsheets",
    "section": "C.4 Cheatsheet: Users and Permissions",
    "text": "C.4 Cheatsheet: Users and Permissions\n\nC.4.1 Users\n\n\n\n\n\n\n\nCommand\nWhat it does\n\n\n\n\nsu &lt;username&gt;\nChange to be a different user.\n\n\nwhoami\nGet username of current user.\n\n\nid\nGet full user + group info on current user.\n\n\npasswd\nChange password.\n\n\nuseradd\nAdd a new user.\n\n\nu sermod &lt;username&gt;\nModify user username. -aG &lt;group&gt; adds to a group\n\n\n\n\n\nC.4.2 Permissions\n\n\n\n\n\n\n\n\nCommand\nWhat it does\nHelpful options + notes\n\n\n\n\nchm o d &lt;permissi ons&gt; &lt;file&gt;\nModifies permissions on a file.\nNumber indicates permissions for user, group, others: add 4 for read, 2 for write, 1 for execute, 0 for nothing.\n\n\ncho w n  &lt;user/gr oup&gt; &lt;file&gt;\nChange the owner of a file.\nCan be used for user or group, e.g. :my-group.\n\n\nsu  &lt;username&gt;\nChange active user to username.\n\n\n\nsud o &lt;command&gt;\nAdopt super user permissions for the following command."
  },
  {
    "objectID": "chapters/append/cheatsheets.html#cheat-ssh",
    "href": "chapters/append/cheatsheets.html#cheat-ssh",
    "title": "Appendix C — Cheatsheets",
    "section": "C.5 Cheatsheet: ssh",
    "text": "C.5 Cheatsheet: ssh\nssh &lt;user&gt;@&lt;host&gt;\n\n\n\n\n\n\n\n\nFlag\nWhat it does\nNotes\n\n\n\n\n-v\nVerbose, good for debugging.\nAdd more vs as you please, -vv or -vvv.\n\n\n-i\nChoose identity file (private key)\nNot necessary with default key names."
  },
  {
    "objectID": "chapters/append/cheatsheets.html#cheat-admin",
    "href": "chapters/append/cheatsheets.html#cheat-admin",
    "title": "Appendix C — Cheatsheets",
    "section": "C.6 Cheatsheet: Linux Admin Commands",
    "text": "C.6 Cheatsheet: Linux Admin Commands\n\nC.6.1 Users\n\n\n\n\n\n\n\nCommand\nWhat it does\n\n\n\n\nsu &lt;username&gt;\nChange to be a different user.\n\n\nwhoami\nGet username of current user.\n\n\nid\nGet full user + group info on current user.\n\n\npasswd\nChange password.\n\n\nuseradd\nAdd a new user.\n\n\nuse   r m od &lt;username&gt;\nModify user username. -aG &lt;group&gt; adds to a group\n\n\n\n\n\nC.6.2 Permissions\n\n\n\n\n\n\n\n\nCommand\nWhat it does\nHelpful options + notes\n\n\n\n\nch m od &lt;per m i s s ions&gt; &lt;file&gt;\nModifies permissions on a file.\nNumber indicates permissions for user, group, others: add 4 for read, 2 for write, 1 for execute, 0 for nothing.\n\n\nc h ow n  &lt;us e r / g roup&gt; &lt;file&gt;\nChange the owner of a file.\nCan be used for user or group, e.g. :my-group.\n\n\ns u &lt;username&gt;\nChange active user to username.\n\n\n\ns u do &lt;command&gt;\nAdopt super user permissions for the following command.\n\n\n\n\n\n\nC.6.3 Install applications (Ubuntu)\n\n\n\n\n\n\n\nCommand\nWhat it does\n\n\napt-get u p d a t e && apt-get upgrade -y\nFetch and install upgrades to system packages\n\n\na p t-get install &lt;package&gt;\nInstall a system package.\n\n\nwget\nDownload a file from a URL.\n\n\ngdebi\nInstall local .deb file.\n\n\n\n\n\nC.6.4 Storage\n\n\n\n\n\n\n\n\nCommand\nWhat it does\nHelpful options\n\n\n\n\ndf\nCheck storage space on device.\n-h for human readable file sizes.\n\n\ndu\nCheck size of files.\nMost likely to be used as d  u - h &lt;dir&gt; | sort -h\nAlso useful to combine with head.\n\n\n\n\n\nC.6.5 Processes\n\n\n\n\n\n\n\n\nCommand\nWhat it does\nHelpful options\n\n\n\n\ntop\nSee what’s running on the system.\n\n\n\nps aux\nSee all system processes.\nConsider using --sort and pipe into head or grep\n\n\nkill\nKill a system process.\n-9 to force kill immediately\n\n\n\n\n\nC.6.6 Networking\n\n\n\n\n\n\n\n\nCommand\nWhat it does\nHelpful options\n\n\n\n\nnetstat\nSee ports and services using them.\nUsually used with -tlp, for tcp listening applications, including pid\n\n\nssh - L  &lt;port&gt;:&lt;ip&gt;: &lt;port&gt; &lt;host&gt;\nPort forwards a remote port on host to local.\nChoose local port to match remote port. Remote ip is usually localhost.\n\n\n\n\n\nC.6.7 The path\n\n\n\n\n\n\n\nCommand\nWhat it does\n\n\n\n\nwhich &lt;command&gt;\nFinds the location of the binary that runs when you run command.\n\n\n\n`ln -s &lt; lo cation to link&gt;\n\n&lt; l ocation of symlink&gt;`\n\n\nCreates a symlink from file at location to link to location of symlink."
  },
  {
    "objectID": "chapters/append/cheatsheets.html#cheat-ports",
    "href": "chapters/append/cheatsheets.html#cheat-ports",
    "title": "Appendix C — Cheatsheets",
    "section": "C.7 Cheatsheet: IP Addresses and Ports",
    "text": "C.7 Cheatsheet: IP Addresses and Ports\n\nC.7.1 Special IP Addresses\n\n\n\n\n\n\n\nCode\nMeaning\n\n\n\n\n127.0.0.1\nlocalhost or loopback – the machine that originated the request\n\n\n192.168.x.x\n172.16.x.x.x\n10.x.x.x\nProtected address blocks used for private IP addresses.\n\n\n\n\n\nC.7.2 Special Ports\nAll ports below 1024 are reserved for server tasks and cannot be assigned to admin-controlled services.\n\n\n\nProtocol/Application\nDefault Port\n\n\n\n\nHTTP\n80\n\n\nHTTPS\n443\n\n\nSSH\n22\n\n\nPostgreSQL\n5432\n\n\nRStudio Server\n\n\n\nShiny Server\n\n\n\nJupyterHub"
  },
  {
    "objectID": "chapters/append/cheatsheets.html#cheatsheet-debugging-tips",
    "href": "chapters/append/cheatsheets.html#cheatsheet-debugging-tips",
    "title": "Appendix C — Cheatsheets",
    "section": "C.8 Cheatsheet: Debugging Tips",
    "text": "C.8 Cheatsheet: Debugging Tips\nIf things are going poorly on your server, here are some steps you might take to try to troubleshoot.\n\nC.8.1 Check storage usage\nA common culprit for weird server behavior is running out of storage space. There are two handy commands for monitoring the amount of storage you’ve got – du and df. These commands are almost always used with the -h flag to put file sizes in human-readable formats.\ndf, for disk free, shows the capacity left on the device where the directory sits.\nFor example, here’s the result of running the df command on the chapters directory on my laptop that includes this chapter.\n ❯ df -h chapters                                                    \nFilesystem     Size   Used  Avail Capacity iused      ifree %iused  Mounted on\n/dev/disk3s5  926Gi  163Gi  750Gi    18% 1205880 7863468480    0%   /System/Volumes/Data\nSo you can see that the chapters folder lives on a disk called /dev/disk3s5 that’s a little less than 1Tb and is 18% full – no problem. On a server this can be really useful to know, because it’s quite easy to switch a disk out for a bigger one in the same spot.\nIf you’ve figured out that a disk is full and need to figure out where the culprits are, the du can be useful. du, short for disk usage, gives you the size of individual files inside a directory. It’s particularly useful in combination with the sort command.\nFor example, here’s the result of running du on the chapters directory where the text files for this book live.\n ❯ du -h chapters | sort -h                                      \n 44K    chapters/sec2/images-servers\n124K    chapters/sec3/images-scaling\n156K    chapters/sec2/images\n428K    chapters/sec2/images-traffic\n656K    chapters/sec1/images-code-promotion\n664K    chapters/sec1/images-docker\n1.9M    chapters/sec1/images-repro\n3.4M    chapters/sec1\n3.9M    chapters/sec3/images-auth\n4.1M    chapters/sec3\n4.5M    chapters/sec2/images-networking\n5.3M    chapters/sec2\n 13M    chapters\nSo if I were thinking about cleaning up this directory, I could see that my images-networking directory in sec2 is the biggest single bottom-level directory. If you find yourself needing to find big files on your Linux server, it’s worth spending some time with the help pages for du. There are lots of really useful options.\ndu is useful for identifying large files and directories on a server.\n\n\nC.8.2 Look at what’s running\nA running program is a process. For example, when you type python on the command line to open a REPL, that starts a single Python process. If you were to start a second terminal session and run python again, you’d have a second Python process.\nComplicated programs often involves multiple than one process. For example, running the RStudio IDE involves (at minimum) one process for the IDE itself and one for the R session that it uses in the background. The relationships between these different processes is mostly hidden from you – the end user.\nAs a server admin, finding runaway processes, killing them, and figuring out how to prevent the them from happening again is a pretty common task. Runaway processes usually misbehave by using up the entire CPU or filling up the entire machine’s RAM.\nLike users and groups have ids, each process has a numeric process id (pid). Each process also has an owner – this can be either a service account or a real user. If you’ve got a rogue process, the pattern is to try to find the process and make note of its pid. Then you can immediately end the process by pid with the kill command.\nSo, how do you find a troublesome process?\nThe top command is a good first stop. top shows the top CPU-consuming processes in real time. Here’s the top output from my machine as I write this sentence.\nPID    COMMAND      %CPU TIME     #TH    #WQ  #PORT MEM    PURG   CMPRS PGRP\n0      kernel_task  16.1 03:56:53 530/10 0    0     2272K  0B     0B    0\n16329  WindowServer 16.0 01:53:20 23     6    3717  941M-  16M+   124M  16329\n24484  iTerm2       11.3 00:38.20 5      2    266-  71M-   128K   18M-  24484\n29519  top          9.7  00:04.30 1/1    0    36    9729K  0B     0B    29519\n16795  Magnet       3.1  00:39.16 3      1    206   82M    0B     39M   16795\n16934  Arc          1.8  18:18.49 45     6    938   310M   144K   61M   16934\n16456  Messages     1.7  06:58.27 4      1    603   138M   2752K  63M   16456\n1      launchd      1.7  13:41.03 4/1    3/1  3394+ 29M    0B     6080K 1\n573    diagnosticd  1.4  04:31.97 3      2    49    2417K  0B     816K  573\n16459  zoom.us      1.3  66:38.37 30     3    2148  214M   384K   125M  16459\n16575  UniversalCon 1.3  01:15.89 2      1    131   12M    0B     2704K 16575\nIn most instances, the first three columns are the most useful. You’ve got the name of the process (COMMAND) and how much CPU its using. Right now, nothing is using very much CPU. If I were to find something concerning – perhaps an R process that is using 500% of CPU – I would want to take notice of its pid to kill it with kill.\n\n\n\n\n\n\nSo much CPU?\n\n\n\nFor top (and most other commands), CPU is expressed as a percent of single core availability. So, on a modern machine with multiple cores, it’s very common to see CPU totals well over 100%. Seeing a single process using over 100% of CPU is rarer.\n\n\nThe top command takes over your whole terminal. You can exit with Ctrl + c.\nAnother useful command for finding runaway processes is ps aux. It lists a snapshot of all processes running on the system, along with how much CPU and RAM they’re using. You can sort the output with the --sort flag and specify sorting by cpu with --sort -%cpu or by memory with --sort -%mem.\nBecause ps aux returns every running process on the system, you’ll probably want to pipe the output into head.\nAnother useful way to use ps aux is in combination with grep. If you pretty much know what the problem is – often this might be a runaway R or Python process – ps aux | grep &lt;name&gt; can be super useful to get the pid.\nFor example, here are the RStudio processes currently running on my system.1\n &gt; ps aux | grep \"RStudio\"                                                                                      [10:21:18]\nUSER               PID  %CPU %MEM      VSZ    RSS   TT  STAT STARTED      TIME COMMAND\nalexkgold        23583   0.9  1.7 37513368 564880   ??  S    Sat09AM  17:15.27 /Applications/RStudio.app/Contents/MacOS/RStudio\nalexkgold        23605   0.5  0.4 36134976 150828   ??  S    Sat09AM   1:58.16 /Applications/RStudio.app/Contents/MacOS/rsession --config-file none --program-mode desktop \n\n\nC.8.3 Make sure the right ports are open\nNetworking is a complicated topic, which we’ll approach with great detail in Chapter 12. Most often, you’ll want to check that the application you’re running is actually accessible to the outside world, assuming you want it to be.\nThe main command to help you see what ports are being used and by what services is the netstat command. netstat returns the services that are running and their associated ports. netstat is generally most useful with the -tlp flags to show programs that are listening and the programs associated.\nSometimes you know you’ve got a service running on your machine, but you just can’t seem to get the networking working. It can be useful to access the service directly without having to deal with networking.\nSSH port forwarding allows you to take the output of a port on a remote server, route it through SSH, and display it as if it were on a local port. For example, let’s say you’ve got RStudio Server running on my server but the web interface isn’t working. If you’ve got SSH working properly, you can double check that the service is working and the issue really is networking.\nI find that the syntax for port forwarding completely defies my memory and I have to google it every time I use it. For the kind of port forwarding you’ll use most often in debugging, you’ll use the -L flag to get a remote port locally.\nssh -L &lt;local port&gt;:&lt;remote ip&gt;:&lt;remote port&gt; &lt;ssh hostname&gt;\nThe local would be your laptop and the remote your server, so if you had RStudio Server running on a server on port 3939. Then you could run ssh -L 3939:localhost:3939 my-user@my-ds-workbench.com. To get whatever is at port 3939 on the server (hopefully RStudio Workbench!) by going to localhost:3939 in the laptop’s browser.\nMost often, you’ll use the same port locally and on the remote and the remote ip will be localhost.\n\n\nC.8.4 Check your path\nLet’s say you want to open Python on your command line. One option would be to type the complete path to a Python install every time. For example, I’ve got a version of Python in /usr/bin, so /usr/bin/python3 works.\nBut in most cases, it’s nice to just type python3 and have the right version open up.\n$ python3\nPython 3.9.6 (default, May  7 2023, 23:32:45) \n[Clang 14.0.3 (clang-1403.0.22.14.1)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; \nIn some cases, this isn’t optional. Certain applications will rely on others being available, like RStudio needing to find R. Or Jupyter Notebook finding your Python kernels.\nSo how does the operating system know where to find those applications and files?\nIf you ever want to check which actual executable is being used by a command, you can use the which command. For example, on my system this is the result of which python3.\n ❯ which python3                                                    \n/usr/bin/python3\nThe operating system knows how to find the actual runnable programs on your system via the path. The path is a set of directories that the system knows to search when it tries to run a path. The path is stored in an environment variable conveniently named PATH.\nYou can check your path at any time by echoing the PATH environment variable with echo $PATH. On my MacBook, this is what the path looks like.\n ❯ echo $PATH                                                      \n/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/System/Cryptexes/App/usr/bin:/usr/bin:/bin:/usr/sbin:/sbin\nWhen you install a new piece of software you’ll need to add it two the path. Say I was to install a new version of Python in /opt/python. That’s not on my PATH, so my system wouldn’t be able to find it.\nI can get it on the path in one of two ways – the first option would be to add /opt/python to my PATH every time a terminal session starts usually via a file in /etc or the .zshrc. The other option is to create a symlink to the new application in a directory already on the PATH. A symlink does what it sounds like – creates a way to link to a file from a different directory without moving it.\nSymlinks are created with the ln command."
  },
  {
    "objectID": "chapters/append/cheatsheets.html#comprehension-questions",
    "href": "chapters/append/cheatsheets.html#comprehension-questions",
    "title": "Appendix C — Cheatsheets",
    "section": "C.9 Comprehension Questions",
    "text": "C.9 Comprehension Questions\n\nHow would you do the following?\n\nFind and kill the process IDs for all running rstudio-server processes.\nFigure out which port JupyterHub is running on.\nCreate a file called secrets.txt, open it with vim, write something in, close and save it, and make it so that only you can read it."
  },
  {
    "objectID": "chapters/append/cheatsheets.html#footnotes",
    "href": "chapters/append/cheatsheets.html#footnotes",
    "title": "Appendix C — Cheatsheets",
    "section": "",
    "text": "This command actually cuts off the header line of the table. What I actually ran was ps aux | grep \"RStudio\\|USER\".↩︎"
  }
]