[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DevOps for Data Science",
    "section": "",
    "text": "I used the knitr package (Xie 2015) and the quarto package (quarto?) to compile my book.\nPackage names are in bold text (e.g., rmarkdown), and inline code and filenames are formatted in a typewriter font (e.g., knitr::knit('foo.Rmd')). Function names are followed by parentheses (e.g., bookdown::render_book()).\n\n\n\nAlex Gold leads the Solutions Engineering team at RStudio.\nHe works with customers of RStudio’s professional software to help them deploy, configure, and use RStudio’s professional software and open source tooling in R and Python.\nIn his free time, he enjoys landscaping, handstands, and Tai Chi.\nHe occasionally blogs about data, management, and leadership at alexkgold.space.\n\n\n\nA lot of people are helping me write this book.\nThis book is published to the web using GitHub Actions from rOpenSci.\n\n\n\nTea Green: CAFFDO Steel Blue: 3E7CB1 Kombu Green: 273c2c Bright Maroon: B33951 Sandy Brown: FCAA67\n\n\n\n\nXie, Yihui. 2015. Dynamic Documents with R and Knitr. 2nd ed. Boca Raton, Florida: Chapman; Hall/CRC. http://yihui.name/knitr/."
  },
  {
    "objectID": "chapters/intro.html",
    "href": "chapters/intro.html",
    "title": "Introduction",
    "section": "",
    "text": "At some point, most data scientists reach the point where they want to show their work to others. But the skills and tools to deploy data science are completely different from the skills and tools needed to do data science.\n> If you’re a data scientist who wants to get your work in front of the right people, this book aims to equip you with **all the technical things you need to know that aren’t data science.**\nHopefully, once you’ve read this book, you’ll understand how to deploy your data science, whether you’re building a DIY deployment system or trying to work with your organization’s IT/DevOps/SysAdmin/SRE group to make that happen."
  },
  {
    "objectID": "chapters/intro.html#moving-data-science-to-a-server",
    "href": "chapters/intro.html#moving-data-science-to-a-server",
    "title": "Introduction",
    "section": "Moving Data Science to A Server",
    "text": "Moving Data Science to A Server\nIn recent years, as data science has become more central to organizations, many have been moving their operations off of individual contributors’ laptops and onto centralized servers. Depending on your organization, the centralization of data science operations can make your life way easier – or it can be kinda a bummer.\nServer migrations can work well regardless of whether they’re instigated by the data science or the IT organization. The biggest determinant is how well the data science and IT/DevOps teams can collaborate.\nData scientists are good at manipulating and using data, but most have little expertise in SysAdmin work, and aren’t really that interested. On the flip side, IT/DevOps organizations usually don’t really understand data science workflows, the data science development process, or how data scientists use R and Python.\nOften, migrations to a server are instigated by the data scientists themselves – usually because they’ve run out of horsepower on their laptops. If you, or one of your teammates, enjoys and is good as SysAdmin work, this can be a great situation! You get the hardware you need for your project quickly and with minimal interference.\nOn the other hand, most data scientists don’t really want to be SysAdmins, and these systems are often fragile, isolated from other corporate systems, and potentially susceptible to security vulnerabilities.\nOther organizations are moving to servers as well, but led by the IT group. For many IT groups, it’s way easier to maintain a centralized server environment, as opposed to helping each data scientist maintain their own environment on their laptop.\nHaving just one platform makes it much easier to give shared access to more powerful computing platforms, to data sources that require some configuration, and to R and Python packages that wrap around system libraries and can be a pain to configure (looking at you, rJava).\nThis can be a great situation for data scientists! If the platform is well-configured and scoped, you can get instant access through their web browser to more compute resources, and don’t have to worry about maintaining local installations of data science tools like R, Python, RStudio, and Jupyter, and you don’t need to worry about how to connect to important data sources – those things are just available for use.\nBut this can also be a bad experience. Long wait times for hardware or software updates, overly restrictive policies – especially around package management – and misunderstandings of what data scientists are trying to do on the platforms can lead to servers going largely unused.\nSo much of whether the server-based experience is good or not depends on the relationship between the data science and IT/Admin group. In organizations where these groups work together smoothly, this can be a huge win for everyone involved. However, there are some organizations where IT/Admins are so concerned with stability and security that they make it impossible to do data science, and the data scientists spend all their time playing cat-and-mouse games to try to get work done behind IT/Admin’s backs.\nIf you work at such a place, it’s frankly hard to get much done on the server. It’s probably worth investing some time into improving your relationship with your favorite person on the IT/Admin team. Hopefully, this book will help you understand a little of what’s on the minds of people in the IT group, and a sense of how to talk to them better.\nAs data science increasingly moves to the cloud, it’s helpful to have specific cloud recommendations. That’s why this book will go pretty deep into how to configure the things we’re discussing inside AWS. You always have the option of using other clouds, and any conceptual information you learn in this book will 100% apply, but trying to provide detailed directions across more than 1 cloud provider is just too difficult."
  },
  {
    "objectID": "chapters/sec1/cmd-line.html",
    "href": "chapters/sec1/cmd-line.html",
    "title": "1  The Command Line + SSH",
    "section": "",
    "text": "Most SysAdmin work is done on the command line\nUnlike your laptop, servers don’t often have a desktop interface\nThe interactive sections of this book will require you to use the command line\n-> How to get to command line on different systems\n-> Set up docker desktop\n#TODO: Address using vi"
  },
  {
    "objectID": "chapters/sec1/cmd-line.html#what-is-ssh",
    "href": "chapters/sec1/cmd-line.html#what-is-ssh",
    "title": "1  The Command Line + SSH",
    "section": "2.1 What is SSH?",
    "text": "2.1 What is SSH?\n\nStands for secure (socket) shell\nWay to remotely access a different (virtual) machine\nWorkhorse of doing work on another server"
  },
  {
    "objectID": "chapters/sec1/cmd-line.html#how-does-ssh-work",
    "href": "chapters/sec1/cmd-line.html#how-does-ssh-work",
    "title": "1  The Command Line + SSH",
    "section": "2.2 How does SSH work?",
    "text": "2.2 How does SSH work?\n\nVia Public Key Encryption\n\nPublic/Private Key\nKnown Hosts [Diagram: SSH Keys] > Sidebar on public key encryption\n\nBy default, available on port 22"
  },
  {
    "objectID": "chapters/sec1/cmd-line.html#basic-ssh-use",
    "href": "chapters/sec1/cmd-line.html#basic-ssh-use",
    "title": "1  The Command Line + SSH",
    "section": "2.3 Basic SSH Use",
    "text": "2.3 Basic SSH Use\n\nThe terminal\n3 step process\n\ngenerate public/private keys ssh-keygen\nplace keys in appropriate place\nuse ssh to do work\n\nPermissions on key"
  },
  {
    "objectID": "chapters/sec1/cmd-line.html#getting-comfortable-in-your-own-setup",
    "href": "chapters/sec1/cmd-line.html#getting-comfortable-in-your-own-setup",
    "title": "1  The Command Line + SSH",
    "section": "2.4 Getting Comfortable in your own setup",
    "text": "2.4 Getting Comfortable in your own setup\n\nUsing ssh-config"
  },
  {
    "objectID": "chapters/sec1/cmd-line.html#advanced-ssh-tips-tricks",
    "href": "chapters/sec1/cmd-line.html#advanced-ssh-tips-tricks",
    "title": "1  The Command Line + SSH",
    "section": "2.5 Advanced SSH Tips + Tricks",
    "text": "2.5 Advanced SSH Tips + Tricks\n\nSSH Tunneling/Port Forwarding\n-vvvv, -i, -p [Diagram: Port Forwarding]"
  },
  {
    "objectID": "chapters/sec1/cmd-line.html#exercises",
    "href": "chapters/sec1/cmd-line.html#exercises",
    "title": "1  The Command Line + SSH",
    "section": "2.6 Exercises",
    "text": "2.6 Exercises\n\nDraw your own diagram of the sequence of things that occur when you execute ssh user@server.company.com in the terminal.\nStand up a new server on one of the major cloud providers. Try logging in using the provided key file. Create a new user on the server.\nGenerate a new SSH key locally and copy the correct key onto the server (think for a moment about which key is the correct one – consult your diagram from step 1 if necessary).\nSet up an SSH alias so you can SSH into your server using just ssh testserver (hint: look at your SSH config).\nCreate a local text file and SFTP it to the server. SSH in, edit the file, and SFTP it back.\n[Advanced] Stand up a nginx server on your remote instance. Don’t open the port to the world, but SSH port forward the server page to your local browser."
  },
  {
    "objectID": "chapters/sec1/code-promotion.html",
    "href": "chapters/sec1/code-promotion.html",
    "title": "2  Code Promotion Workflows",
    "section": "",
    "text": "As a field, IT/Admin/DevOps/SysAdmin has been around for much longer than data science. And while the topics they’re addressing are different than data science, there are some workflows that translate nicely into the data science world.\nOne of those is code and environment promotion.\nOver time, smart SysAdmins realized that it’s better not to make changes for the first time in production systems people depend on for various reasons. And so they’ve developed patterns for putting things into production.\nGenerally, this workflow involves three or four (nearly) identical environments. Most often the workflow is dev/test/prod, though sometimes there are separate test and user acceptance testing (UAT) environments."
  },
  {
    "objectID": "chapters/sec1/code-promotion.html#devtestprod",
    "href": "chapters/sec1/code-promotion.html#devtestprod",
    "title": "2  Code Promotion Workflows",
    "section": "2.1 Dev/Test/Prod",
    "text": "2.1 Dev/Test/Prod\nConceptually, the use of each environment is quite straightforward. Dev is a development environment – a sandbox where new things can be developed freely with no risk. In a data science context, this generally means that dev and test environments may need to be connected to input data, but that output needs to be somehow mocked or redirected so it doesn’t leak back into prod.\nThe test environment is for (shocker) testing. This likely includes functional testing that things work as intended, but also might include UAT. In the context of data science assets, UAT usually answers questions like whether labels and buttons are clear and whether plots and graphs answer exactly the intended questions.\nThe production environment is for production – meaning use by people and machines outside the development process. Changes are never made in prod, except when they’ve already been validated in the test environment, and then the only thing that happens in prod is a quick update to the newest testing version.\nThis obviously requires that the dev, test, and prod environments be very close mirrors of each other. For example, doing test on a Windows laptop and then going to prod on a Linux server introduces a potential that things that worked in test suddenly don’t when going to prod. For that reason, making all three (or at least test and prod) match as precisely as possible is essential (more in chapter 1.3). The need to match these three environments so precisely is one reason for data science workloads moving onto servers.\nIn a data science context, the dev/test/prod sequence looks a little different from in an IT context. In an IT context, dev/test/prod means three identical environments. This is generally not the case in a data science context.\nFor a data scientist, dev is often a different environment than test or prod. For example, it’s likely that you would do your development of your asset in your favorite R or Python data science IDE – perhaps RStudio, JupyterHub, Spyder, or PyCharm. This environment might be server-based, or it might be on your laptop.\nThe biggest reason for this difference is that pure DevOps involves building a known product according to an identified need. Data science is different. The early parts of a data science project are about discovery and iteration – that means that you can’t start “development” right off the bat. There’s first some research and discovery that happens in an environment well-suited to such things before promotion happens into a “deployment” environment.\nIn contrast, traditional software development rarely starts with discovery. Instead, it starts with producing a minimal set of features that has been identified ahead of time. This difference causes much friction and misunderstanding between data science and IT/Admin/DevOps teams.\nAt that point, you’ll promote your asset into test, which might be a docker container, straight onto a server, or a deployment platform like RStudio Connect. Then, once everything is ready to go, you’d promote it into prod. This is somewhat different than a traditional dev/test/prod setup where all three environments are very precise mirrors of each other. In some cases, like if you’re using a product that hosts your apps for you, test and prod may even be on the same physical server, which would be very unusual in the software development world.\nDepending on how dependable your prod environment has to be, it can be useful to maintain a “two dimensional” dev/test/prod setup as in the graphic below.\n\nIn this setup, you would select the IT configuration that works for your organization and maintain one or two copies of the entire environment. I often call this a staging environment to differentiate it from the dev/test/prod environments for the data science assets.\nSo when you wanted to make a chance to the underlying servers or their architecture, that would be tested in the staging environment and then deployed to production. Data scientists would never work in the staging environment (except as testers), that’s purely for IT/Admin testing. The staging environment would include all of the environments data scientists would use – dev, test, and prod.\nThen, data science code promotion through dev/test/prod would be distinct from how server changes get made."
  },
  {
    "objectID": "chapters/sec1/code-promotion.html#cattle-not-pets",
    "href": "chapters/sec1/code-promotion.html#cattle-not-pets",
    "title": "2  Code Promotion Workflows",
    "section": "2.2 Cattle, Not Pets",
    "text": "2.2 Cattle, Not Pets\nIn the IT world, there’s a phrase that servers should be cattle, not pets. The idea here is that servers should be unremarkable and that each one should be more-or-less interchangeable. This matters, for example, in making sure your test and prod environments look exactly the same.\nA bad pattern then would look like this – I develop an update to an important Shiny or Dash app in my local environment and then move it onto a server. At that point, the app doesn’t quite work and I make a bunch of manual changes to the environment – say adjusting file paths or adding R or Python packages. Those manual changes end up not really being documented anywhere. A week later, when I go to update the app in prod, it breaks on first deploy, because the server state of the test and prod servers drifted out of alignment.\nThe main way to combat this kind of state drift is to religiously use state-maintaining infrastructure as code (IaC) tooling. That means that all changes to the state of your servers ends up in your IaC tooling and no “just login and make it work” shenanigans are allowed in prod.\nIf something breaks, you reproduce the error in staging, muck around until it works, update your IaC tooling to fix the broken thing, test that the thing is fixed, and then (and only then) push the updated infrastructure into prod directly from your IaC tooling.\n\n2.2.1 Infrastructure As Code Tooling\nThere are many, many different varieties of infrastructure as code tooling. There are many books on infrastructure as code tooling and I won’t be covering them in any depth here. Instead, I’ll share a few of the different “categories” (parts of the stack) of infrastructure as code tooling and suggest a few of my favorites.\nTo get from “nothing” to a usable server state, there are (at minimum) two things you need to do – provision the infrastructure you need, and configure that infrastructure to do what you want.\nFor example, let’s say I’m standing up a server to deploy a simple shiny app. In order to get that server up, I’ll need to stand up an actual server, including configuring the security settings and networking that will allow the proper people to access the server. Then I’ll need to install a version of R on the server, the Shiny package, and a piece of hosting software like Shiny Server.\nSo, for example, you might use AWS’s CloudFormation to stand up a virtual private cloud (VPC), put an EC2 server instance inside that VPC, attach an appropriately-sized storage unit, and attach the correct networking rules. Then you might use Chef to install the correct software on the server and get your Shiny app up-and-running.1\nIn infrastructure as code tooling, there generally isn’t a clear dividing line between tools that do provisioning and tools that do configuration…but most tools lean one way or the other.\nBasically any tool does provisioning will directly integrate into the APIs of the major cloud providers to make it easy to provision cloud servers. Each of the cloud providers also has their own IaC tool, but many people prefer to use other tools when given the option (to be delicate).\nThe other important division in IaC tools is declarative vs imperative. In declarative tooling, you simply enumerate the things you want, and the tool makes sure they get done in the right order. In contrast, an imperative tool requires that you provide actual instructions to get to where you want to go.\nIn many cases, it’s easy to be declarative with provisioning servers, but it’s often useful to have a way to fall back to an imperative mode when configuring them because there may be dependencies that aren’t obvious to the provisioning tool, but are easy to put down in code. If the tool does have an imperative mode, it’s also nice if it’s compatible with a language you’d be comfortable with.2\nOne somewhat complicated addition to the IaC lineup is Docker and related orchestration tools. There’s a whole chapter on containerization and docker, so check that out if you want more details. The short answer is that docker can’t really do provisioning, but that you can definitely use docker as a configuration management IaC tool, as long as you’re disciplined about updating your Dockerfiles and redeployment when you want to make changes to the contents.\nBasically none of these tools will save you from your own bad habits, but they can give you alternatives.\nIn short, exactly which tool you’ll need will depend a lot on what you’re trying to do. Probably the most important question in choosing a tool is whether you’ll be able to get help from other people at your organization on it. So if you’re thinking about heading into IaC tooling, I’d suggest doing a quick survey of some folks in DevOps and choosing something they already know and like."
  },
  {
    "objectID": "chapters/sec1/code-promotion.html#cicd-and-the-mechanics-of-code-promotion",
    "href": "chapters/sec1/code-promotion.html#cicd-and-the-mechanics-of-code-promotion",
    "title": "2  Code Promotion Workflows",
    "section": "2.3 CI/CD and The mechanics of code promotion",
    "text": "2.3 CI/CD and The mechanics of code promotion\nThe common term for the mechanics of code promotion are Continuous Integration/Continuous Deployment (CI/CD). The actual practice of CI/CD relates deeply to the philosophy of DevOps and agile software development in ways that are beyond the scope of this book.3\nIn this book I’m mainly going to focus on a few workflows and a few tools that I’ve found particularly useful in a data science context.\nThe most concrete way that CI/CD practices get expressed is through integrations into source control software – usually Git. If you’re not already familiar, I’d suggest spending some time learning git. I will admit that learning git is nontrivial. People who say git is easy are either lying to look smarter or learned so long ago that they have forgotten the early-to-git sense that you’re likely to mess up your entire workflow at any moment.\n\nIf you don’t already know git and want to learn, I’d recommend HappyGitWithR by Jenny Bryan. It’s a great on-ramp to learn git.\nEven if you’re a Python user, the sections on getting started with git, on basic git concepts, and on workflows will be useful since they approach git from a data science perspective.\n\nFor the purposes of this section, I’m going to assume you at least conceptually understand what git branches are and what a merge is – as much of your CI/CD pipeline will be based on what happens when you merge.\nFor production data science assets, I generally recommend long-running dev (or test) and prod branches, with feature branches for developing new things. The way this works is that new features are developed in a feature branch, merged into dev for testing, and then promoted to prod when you’re confident it’s ready.\nFor example, if you had two new plots you were adding to an existing dashboard, your git commit graph might look like this:\n\nCI/CD adds a layer on top of this. CI/CD allows you to integrate functional testing by automatically running those tests whenever you do something in git. These jobs can run when a merge request is made, and are useful for tasks like spellchecking, linting, and running tests.\nFor the purposes of CI/CD, the most interesting jobs are those that do something after there’s a commit or a completed merge, often deploying the relevant asset to its designated location.\nSo a CI/CD integration using the same git graph as above would have released 3 new test versions of the app and 2 new prod versions. Note that in this case, the second test release revealed a bug, which was fixed and tested in the test version of the app before a prod release was completed.\nIn years past, the two most popular CI/CD tools were called Travis and Jenkins. By all accounts, these tools were somewhat unwieldy and difficult to get set up. More recently, GitHub – the foremost git server – released GitHub Actions (GHA), which is CI/CD tooling directly integrated into GitHub that’s free for public repositories and free up to some limits for private ones.\nIt’s safe to say GHA is eating the world of CI/CD.4\nFor example, if you’re reading this book online, it was deployed to the website you’re currently viewing using GHA. I’m not going to get deep into the guts of GHA, but instead talk generally about the pattern for deploying data science assets, and then go through how I set up getting this book to run on GHA.\n\n2.3.1 Using CI/CD to deploy data science assets\nIn general, using a CI/CD tool to deploy a data science asset is pretty straightforward. The mental model to have is that the CI/CD tool stands up a completely empty server for you, and runs some code on it.\nThat means that you’re just doing something simple like spellchecking, you can probably just specify to run spellcheck. If you’re doing something more complicated, like rendering an R Markdown document or Jupyter Notebook and then pushing it to a server, you’ll have to take a few extra steps to be sure the right version of R or Python is on the CI/CD server, that your package environment is properly reproduced, and that you have the right code to render your document.\nFeel free to take a look through the code for the GitHub Action for this book. It’s all YAML, so it’s pretty human-readable.\nHere’s what happens every time I make a push to the main branch of the repository for this book:5\n\nCheckout the current main branch of the book.\nUse the r-lib action to install R.\nUse the r-lib action to setup pandoc (a required system library for R Markdown to work).\nGet the cached renv library for the book.\nRender the book.\nPush the book to GitHub Pages, where this website serves from.\n\nYou’ll see that it uses a mixture of pre-defined actions created for general use, pre-defined actions created by people in the R community, and custom R code I insert to restore an renv library and render the book itself."
  },
  {
    "objectID": "chapters/sec1/code-promotion.html#swapping-configurations",
    "href": "chapters/sec1/code-promotion.html#swapping-configurations",
    "title": "2  Code Promotion Workflows",
    "section": "2.4 Swapping Configurations",
    "text": "2.4 Swapping Configurations\nSometimes you want a little more flexibility – for example the option to switch many the environment variables depending on the environment. In R, the standard way to do this is using the config package. There are many options for managing runtime configuration in Python."
  },
  {
    "objectID": "chapters/sec1/code-promotion.html#exercises",
    "href": "chapters/sec1/code-promotion.html#exercises",
    "title": "2  Code Promotion Workflows",
    "section": "2.5 Exercises",
    "text": "2.5 Exercises\n\nCreate something and add GHA integration \\[TODO: What to use as example?\\]\nStand up some virtual environments using ___ \\[TODO: Which ones to try?\\]"
  },
  {
    "objectID": "chapters/sec1/repro-pkgs.html",
    "href": "chapters/sec1/repro-pkgs.html",
    "title": "3  Environments and Reproducibility",
    "section": "",
    "text": "Reproducibility in data science is hard.\nBut it’s also important.\nIn data science, we’re usually either trying to discover something new about the world, or to make a prediction that will prove useful in the future. Our results being reproducible is an important part of assuring that’s the case.\nThere are lots of code-based strategies – like literate programming – that ensure the reproducibility of our code, and various tools to make sure we can reproduce the dataset we ran the code on. Those tools and techniques are outside the scope of this book. In this book, we’re going to talk about how to make an environment that is reproducible.\nIt’s worth saying that reproducibility isn’t binary – it’s a spectrum. There are things that are fully irreproducible, like an analysis that relies on a random number generator with no seed set. In that case, you will never be able to reproduce your earlier results. Going from that to setting a seed is the very lowest level of reproducibility.\nAnd while it might be tempting to say that you should always maximize reproducibility, there’s generally a tradeoff – making things more reproducible is also generally more work.\nDepending on your organization, you may have reproducibility requirements. In highly-regulated industries, these may actually be legal or regulatory requirements you have to meet and it might be worth it for your organization to undertake a significant hassle or cost to ensure reproducibility.\nIn other cases, you’re just trying to make sure you can be confident in and verify your results later. In that case, there’s probably a lower amount of resources available – and lower requirements – in terms of how reproducible it’s worth making things."
  },
  {
    "objectID": "chapters/sec1/repro-pkgs.html#the-environment",
    "href": "chapters/sec1/repro-pkgs.html#the-environment",
    "title": "3  Environments and Reproducibility",
    "section": "3.1 The Environment",
    "text": "3.1 The Environment\nThe example above talked about reproducibility as a function of what you did in your code – did you bother to set a seed for your random number generator or not?\nBut for the purpose of this chapter, that’s the last we’re going to talk about code reproducibility. Instead, we’re going to discuss re-creating the environment. You might well ask: well, what is the environment?\nFor the purposes of data science, I think of everything that isn’t code you’ve written or the data you’re using. The environment is the mixing bowl where those two things meet to do your work. In a lot of cases, data scientists can get away without thinking about the environment…at least for a while.\n\nOften, you’ve got an environment, probably on your laptop, where things work well enough. The problem comes later. Sometimes this comes up when collaborating on a piece of work – how many times have you told a collaborator, “Well, it works on my machine…” Other times it comes up when you get to think about taking an app or report into production – thinking about how to make sure your app or report will reliably run for an extended period of time. And sometimes it comes up in the context of archiving what you’re doing - if you need to re-run this later to verify results, are you confident that it will run cleanly and return the same results.\nAs I said above, there is no perfect answer to reproducing an environment, but there are some general considerations to keep in mind, and a few tools and strategies I’ve found that are useful.\nIn computational terms, the environment is everything that isn’t the code or data themselves. There are layers of environment, one atop the other, right down to the physical hardware.\n[TODO: redo graphic as layers, add hardware]\n\nAgain, depending on your requirements, there may be different answers to your reproducibility issues. In almost all cases, it’s worthwhile to make sure you can easily reproduce your language and package environments. This can solve a lot of headaches down the road, and I recommend it in almost all cases that aren’t just a one-off quick analysis (and sometimes even then!).\nBut there are also highly-regulated industries where you have to ensure the reproducibility of results down to the level of hardware-specific machine instructions and your version of reproducibility might involve just keeping hardware running for a decade to make sure the environment is truly and completely reproducible."
  },
  {
    "objectID": "chapters/sec1/repro-pkgs.html#two-layers-of-environment-management",
    "href": "chapters/sec1/repro-pkgs.html#two-layers-of-environment-management",
    "title": "3  Environments and Reproducibility",
    "section": "3.2 Two Layers of Environment Management",
    "text": "3.2 Two Layers of Environment Management\nIn most cases, you won’t have to persist your physical hardware – and if you’re in the cloud that’s not really even an option. In that case, I generally recommend two “layers” of tooling to ensure environmental reproducibility.\n\nIf you need to maintain your hardware, that’s beyond the scope of this book. But I am going to suggest two sets of tools for managing the R and Python package environments you might have, as well as the underlying versions of R and Python, the system libraries, and the operating system.\n\n3.2.1 Reproducing Package Environments\nFor data science projects, it’s generally worth using a language-specific tool to manage the set of packages you want in your environment. In R, {renv} is the standard on this front. In Python, there are many different options. In the context of production data science, I recommend {virtualenv}/{venv} and related tools.\n\nA sidebar on Conda\nMany data scientists love Conda for managing their Python environments.\nConda is a great tool for its main purpose – allowing you to create a data science environment on your local laptop, especially when you don’t have root access to your laptop because it’s a work machine that’s locked down by the admins.\nIn the context of a production environment, Conda smashes together the language version, the package management, and (sometimes) the system library management. This has the benefit of being conceptually simple and easy-to-use. But I’ve often seen it go awry in production environments, and I generally recommend people use a tool that’s just for package management, like {virtualenv}, as opposed to an all-in-one tool like Conda.\n\nIn both R and Python, these tools accomplish two main tasks for you:\n\nThey create a standalone package environment that’s independent of anything else you do on your computer. That means that you can have another project with different package requirements that won’t mess up this one.\nThey make it easy to record the exact package requirements for your project.\n\nOn a high level, the workflows for these tasks are similar between R and Python. However, there are some meaningful differences in tooling – especially because virtually every computer arrives with a system version of Python installed, while R is only ever installed by a user trying to do data science tasks. At the end of the day, this actually makes it harder to use Python because you do not want to use your system Python for your data science work…but sometimes it accidentally gets into the mix.\nA general suggestion of workflows for data science package management, whether in R or Python – this should be independently done for every project:\n\nCreate a new directory for the project.\nIf you’re in R, make sure you’ve got renv install.packages(\"renv\")\nCreate a standalone library for the project.\n\nrenv::init()\npython -m venv .venv\n\nInstall packages into your project\n\n(note R done from inside a session, python done from command line)\nRenv does some clever caching so if you use the same package in multiple projects, installs will be fast\nTODO: does venv do the same?\n\ninstall.packages(…)\npython -m pip install …\n\n\nSnapshot your package state\n\nrenv::snapshot()\npip freeze > requirements.txt\n\n\nWhen you come back to your project later, you just reactivate these environment, and you’re back into your isolated project environment.\n\nRun the appropriate activation command.\n\nrenv::activate() (automatically in project-level .Rprofile)\nsource .venv/bin/activate\n\n\nThen, when you want to collaborate or move this project to a different machine, you just move the renv lockfile or the requirements.txt. In general, these tools come with default .gitignores that move just the file that describes the package set and not the packages themselves. This is a good thing!\nIn general, packages are specific to the underlying machine – in particular to the operating system and language version, so the optimal thing is to move just the file recording the required versions and then restore those versions wherever you end up, as opposed to moving live package binaries that may or may not work in a new location.\n\nMove your renv lockfile or requirements.txt into place.\nCreate an renv or virtual env\n\nrenv::init()\npython -m venv .venv\n\nRestore the captured set of packages from the lockfile/requirements\n\nrenv::restore()\npip install -r requirements.txt\n\n\nTODO: add section on inspecting lockfiles to understand\n\n\n3.2.2 Reproducing the rest of the stack\nSometimes, just recording the package environment and moving that around is sufficient. In many cases, old versions of R and Python are retained in the environment, and that’s sufficient.\nThere are times where you need to reproduce elements further down the stack. In some highly-regulated industries, you’ll need to go further down the stack because of requirements for numeric reproducibility. Numeric routines in both R and Python call on system-level libraries, often written in C++ for speed. While it’s unlikely that upgrades to these libraries would cause changes to the numeric results you get, it can happen, and it may be worth maintaining parts of the stack.\nIn other cases, your R or Python library might basically just be a wrapper for system libraries. For example, many popular packages for geospatial analysis are just thin language wrappers that call out to the system libraries. In this case, it might be important to be able to maintain a particular version of the underlying system library to ensure that your code runs at all in the future.\nThere are many tools you can use to record and reproduce the R and Python versions you’re using, the system libraries, and the operating system itself. Many of these fall into the category of Infrastructure-as-Code configuration tools.\nThese days, the clear leader of the pack on this front is Docker. It has become an increasingly popular way to create, maintain, and use standalone environments – and for good reason! In fact, the next chapter is going to be all about the use of Docker in data science. However, it’s worth keeping in mind that if you’re working in the context of a formally-supported IT organization, they may have other tooling they prefer to create and maintain environments, and they can be equally valid."
  },
  {
    "objectID": "chapters/sec1/docker.html",
    "href": "chapters/sec1/docker.html",
    "title": "4  Docker for Data Science",
    "section": "",
    "text": "If you’re in the data science world, especially the world of “production data science”, you’ve probably heard of Docker – but you might not really be sure what it is or whether you need it.\nThis chapter is designed to clarify what Docker is and how it might help you. We’ll start with a general intro to Docker, proceed with some discussion of data science-relevant Docker workflows, and then dive in to a hands-on lab that will be just the thing to get you started if you’re not sure where to go."
  },
  {
    "objectID": "chapters/sec1/docker.html#what-is-docker-anyway",
    "href": "chapters/sec1/docker.html#what-is-docker-anyway",
    "title": "4  Docker for Data Science",
    "section": "4.1 What is Docker anyway?",
    "text": "4.1 What is Docker anyway?\nHere’s an unhelpful definition – Docker is an open-source containerization platform.\nTo get away from unhelpful definitions, let’s talk a little about why you might want to use Docker.\nLet’s say you’ve got a Shiny app you want to share with your colleagues. Before you can share that app, you’ve got to make sure that your colleagues have the right set of R packages, a compatible version of R, and a compatible operating system. In contrast to the difficulty of making sure all those things match, a container allows you to simply share the entire container and go!\nLet’s say you’re developing an app. If you read the chapter on reproducibility, you understand that there are layers of dependencies you’re accumulating, including your code, the R and Python packages your code needs, the versions of the langauges themselves, and right down to the operating system.\n\nIt would be very useful to be able to take all these things, package them up, and ship them off somewhere – maybe you want to share all of this with a collaborator or ship your app to production and be pretty confident the app will run without a lot of work to re-create the environment.\nIn the last 5-10 years, containers have become the primary way people package up their apps with their dependencies to share them like this.\nWhen you pull and load this image, you’ll find it takes a long time. The real power of Docker becomes apparent the second time you start this image. Once you’ve got the image downloaded locally, the container comes alive in a fraction of a second. This is in stark contrast to other sorts of local segregated environments that allow you to have a computer in a computer like VirtualBox – there are real benefits to having a full reproduction of another computer, rather than just a hypervisor-based container – but they also take seconds or minutes to stand up.\nRelative to earlier technologies like virtual machines, containers are much more lightweight. In fact, Docker containers can start up in a fraction of a second and be ready to go. This means that putting your app in a container adds very little overhead in terms of performance, which is great!\nIT/Admins tend to love Docker because apps show up pre-packaged, because Docker makes it really easy to scale up apps in a “microservices architecture”, and because you can tightly control the resources available to a Docker container, making it more secure and easier to ensure system stability.\nTo a first approximation, discussions about containers are discussions about Docker. There are other containers that exist, and some people use them for particular use cases. But for general purpose, if you’re using containers, they’re Docker conatiners.\nThe key word in that sentence is containerization. For most purposes these days, Docker and containers are synonymous since Docker was the first containerization platform to hit the mainstream, and is still by far the most popular.1\n[Graphic: Docker on underlying host]\nLet’s talk through a simple example to make this clearer.\nIn the reproducibility chapter, you learned about the layers of reproducibility for an app. Docker takes care of everything at or above the operating system level. It doesn’t completely negate the need for other sorts of IT tooling, because you still have to provision the physical hardware somehow, but it does make everything much more self-contained.\nYou can think of containers in two different ways – as encapsulations of a development environment, or as a mobile encapsulation of an app that can be deployed. In fact, the same container can be this for different people. Assume for a second, you’re an IT/Admin responsibile for making JupyterHub available to your organization. You can pull and run the JupyterHub Docker image and have an initial installation of JupyterHub up and running in seconds. From the Data Scientist’s perspective, this is now a development environment for them.\ndocker run --rm -p 8787:8787 -e PASSWORD=rs_pass rocker/rstudio\nThe last few years have seen the rise of containerization as a strategy for managing applications in enterprise software deployments. The main benefit of containerization is it allows you to create a container that includes both an application you want to run, as well as all its dependencies.\n\n4.1.1 Container Lifecycle\nOne of the hardest parts of understanding Docker is the language around how you are interacting with Docker and what exactly it is you’ve got.\n\n\n\n\n\n\nNote\n\n\n\nI’m glossing over the many capabilities of Docker and the things you can do to discuss Docker in the way I generally find most useful to Data Scientists.\n\n\n\nA container starts its life as a Dockerfile. A Dockerfile is a set of instructions for how to build a container. For example, here’s the Dockerfile for the example above:\n[TODO: DOCKERFILE + EXPLANATION]\nOnce you’ve got a working Dockerfile, you build it into an image – the image is an immutable point-in-time snapshot of the container. In general, the image is going to be the thing that you share with other people, as it’s the version of the container that’s compiled and ready to go.\nDocker images can be shared directly like any other file, or via pushing and pulling to a Docker image registry. You can create and use private registries or the public Docker Hub. It’s worth noting that relativey few people actually run their own container registry. Many take advantge of cloud offerings for container registries as a service offerings. The big 3’s are Amazon’s Elastic Container Registry (ECR), Azure Container Registry, and Google Container Registry.\nIt is possible to interactively build a container as you go and create an image, but for the purposes of reproducibility, it’s generally preferable to build the image from a Dockerfile, and adjust the Dockerfile if you need to adjust the image."
  },
  {
    "objectID": "chapters/sec1/docker.html#docker-for-the-data-scientist",
    "href": "chapters/sec1/docker.html#docker-for-the-data-scientist",
    "title": "4  Docker for Data Science",
    "section": "4.2 Docker for the Data Scientist",
    "text": "4.2 Docker for the Data Scientist\nIf you’ve read this far into the chapter, you might be about ready to go put every project you’ve got into a docker container, ask your colleagues to do the same, and plan to share Shiny apps by making everyone download Docker Desktop and give them a docker run command.\nSlow down.\nDocker is a great solution for some problems, but it’s not a perfect fit for many of the issues data scientists face, and there are better tools for many of them. In particular, one pattern that feels appealing, but I would describe as an anti-pattern is the temptation to just take a Docker container, start installing packages, and then save the image. Even in some cases, you’ll see a Dockerfile that looks like\ninstall.packages(\"shiny\")\ninstall.packages(\"dplyr\")\n...\nBuilding a Dockerfile like this into an image provides a reasonable level of reproducibility, but it still is quite fragile if you ever find reason to rebuild this Dockerfile. For that reason, the best move is to combine the ability of R and Python-specific libraries for capturing package state – like renv and virtualenv with Docker’s ability to save the state of the underlying operating system and any system packages you’ve got.\nThere are a few particularities of Docker containers that can make operating inside them a little difficult. The first is the tradeoff of Docker’s strength – a container only gets access to the resources it has specifically been allowed to access. This is a great feature for security and process isolation, but it means you may run into some issues with networking and access to system resources, like your files.\nYou’ll have to develop a reasonably good mental model of the relationship of the Docker container to the rest of your machine in order to be able to develop effectively. For many data scientists, this isn’t a primary concern of theirs.\nFor that reason, it’s often preferable to skip Docker altogether and develop on a centralized server environment like RStudio Server or JupyterHub. Then, when it comes time to deploy apps, you can use a platform specifically for deploying data science content, like RStudio Connect, or build your app into a Docker container specifically for deployment."
  },
  {
    "objectID": "chapters/sec1/docker.html#hands-on-with-docker",
    "href": "chapters/sec1/docker.html#hands-on-with-docker",
    "title": "4  Docker for Data Science",
    "section": "4.3 Hands on with Docker",
    "text": "4.3 Hands on with Docker\nIn this section, we’re going to spend a few minutes learning how to get hands-on with Docker. This is not designed to be a full introduction to Docker, but just a quick overview of the commands you’ll probably need to know to get started. There are many great resources out there for really learning Docker. I’d suggest picking up one of them rather than relying on this book to learn everything you need to know about Docker.\nThere are at least three different cases where you might want to run Docker:\n\nYou are running a batch (non-interactive) job in R or Python. By packaging it up with all of its dependencies, you make it really easy to run quickly.\nYou are running a service – like an app or api – out of the container.\nYou are running a development environment like RStudio or JupyterHub out of the container and find the an easy way to control the environment.\n\nWe’ll run through an example of how to do all 3, as they’ll help us build up an understanding of how to do everything we’d need to with Docker.\n\n4.3.1 Getting prepped\nInstall docker desktop.\nKnow how to open a terminal\n\n\n4.3.2 Running an app from Docker\nRunning an interactive app or api is one of the most common use cases for Docker. In this case, we’re going to start there because it’s the simplest from a Docker perspective.\nI’ve already created a container with Shiny Server and a running app in it, so all you’ve got to do is pull it down to run locally.\nTo run a Docker container, you just do docker run <image name>, however you’ll usually specify one or more options to the docker run command to make it work the way you want.\n\n4.3.2.1 Port Publishing\nIn the case of a running app, we’ll want to “publish” the port where Shiny Server runs inside the container to the host so that you can actually get there in your browser. By default Shiny Server runs on port 3838, and we’ll use that same port outside by adding a -p 3838:3838 to our docker run command.\n\n\n\n\n\n\nHow to read port mapping\n\n\n\nI always forget the order, so this is as much a note for me as it is for you!\nIn this command, we’re just mapping 3838 inside the container to 3838 on the host (your laptop). But sometimes, you’ll want to map to a different port, and then you have to remember the order.\nThe syntax is <host port>:<container port>. So if we did 8383:3838, would make whatever is available at 3838 inside the container accessible at 8383 on your laptop at http://localhost:8383.\nFor more on ports, see the section\n\n\nWe’re also going to add a --rm so the container cleans up nicely after itself.\n\n\n\n\n\n\nNote\n\n\n\nThe --rm flag is something you’ll usually want if you’re just messing around – and you’ll probably never want in production. Cleanup after the container closes includes deleting any stored logs.\nYou’re definitely going to want those stored logs if you have a failure when you’re in production!\n\n\nOk, so let’s try it. Open a terminal and type\ndocker run -rm -p 3838:3838 alexkgold/interactive\nIf you’ve never run this before, Docker will pull down the container and launch it. docker pull. Now, go to http://localhost:3838, and you should see the Old Faithful shiny app.You can always explicitly pull a container with\n<TODO - change this app to be something more book-relevant>\nThat’s great! Now let’s close it. Uh oh…unless you’re already pretty terminal-savvy, you’re stuck. You could close the terminal window, but you’re just locked watching logs. If you do need to get out, you can just send an interrupt to the terminal (Ctrl + c).\n\n\n4.3.2.2 Detaching a container\nGenerally, when you’re running a process like this, you’ll want to run it in “detached” mode, so it doesn’t take over a terminal window. That’s quite easy just by adding a -d to your run command.\ndocker run -d --rm -p 3838:3838 alexkgold/interactive\nLike before, we can go to http://localhost:3838 to see the running Shiny app. Now when we go to close it down, we have a different problem…where did the app go?\nWe can find all of our running containers using the docker ps (short for process status) command. For me, that looks something like:\n$ docker ps                                                                       \nCONTAINER ID   IMAGE                   COMMAND   CREATED         STATUS         PORTS                                       NAMES\nd67d19d488b6   alexkgold/interactive   \"/init\"   6 seconds ago   Up 4 seconds   0.0.0.0:3838->3838/tcp, :::3838->3838/tcp   pedantic_williams\nThis gives me all the information I need about the running container, including the image, how long it has existed, and any port mapping that I’m doing.\nWhen it comes time to close the container, I can close it with the docker kill command. I can use either the id or the name. If you use the ID, Docker does automatic matching, so you usually only need the first 3-4 characters. In this case docker kill d67 was sufficient. docker kill my-shiny.You can also provide a name when you start the container with the --name flag. So if I added --name my-shiny when I started, I could kill the container with\n\n\n\n4.3.3 Running a batch job\nFor this first task, let’s say we’ve got a Jupyter notebook that we know we’ll want to re-render in the future. For that purpose, it might be useful to bake it into a Docker container, so we can re-run it at will. This container is using Quarto to render my Jupyter Notebook into an html document I could serve on a webpage or share with others.\n<TODO - change this book to be something more book-relevant, perhaps w/ input data, perhaps PDF>\nBefore we go any further, let’s just try out running this job with\ndocker run alexkgold/batch\nIt may take a minute to download the container, as it’s over 600Mb. But once you’ve got it, it should take just a few seconds to run. Note that because there isn’t an ongoing running service, this container just shuts down when it has done its business.\nBut wait…where’d my document go?\nRemember – containers are completely ephemeral. As they say (they do not) – what happens in the container stays in the container. This means that when my document is rendered inside the container, it gets deleted when the container ends its job.\nBut that’s not what I wanted – I wanted to get the output back out of the container.\n\n4.3.3.1 Mounting a volume\nThe solution – making data outside the container available to the container and vice-versa – is accomplished by mounting a volume into the container. This an essential concept to understand when working with containers. Because containers are so ephemeral, volumes are the way to persist anything that you want to outlast the lifecycle of the container.\nTODO - diagram of a volume\nThe way to think of this is that the file directories inside the container are completely independent from the file directories on the host. By mounting a volume, you can make a directory inside the container correspond to a volume outside the container.\nThis is accomplished with the -v flag on the docker run command. Like with mounting a port, the syntax is -v <directory outside container>:<directory inside container>.\nThis time, mount your current directory as /project-out inside the container.\n$ docker run -v $(pwd):/project-out alexkgold/batch\nThis time, when the container finishes, you should see a file named hello.html in your working directory. If you right click it to open it in your browser, you should see a rendered Jupyter Notebook!\n\n\n\n4.3.4 Using Dockerfiles\nSo far, we’ve just been working on running containers based on images I’ve already prepared for you. Let’s look at how those images got created so you can try building your own.\nA Dockerfile is just a set of instructions that you use to build a Docker image. If you have a pretty good idea how to accomplish something on a running machine, you shouldn’t have too much trouble building a Dockerfile to do the same as long as you remember two things:\n\nDocker containers only have access to exactly the resources you provide to them. That means that they won’t have access to libraries or programs unless you give them access, and you also won’t have access to files from your computer unless you make them available.\nYou have to keep in mind what you want to have happen at build time and how that’s different fro what should happen at run time. So, for example, I want to get all the pieces in place – my Python environment, the notebook, etc at build time – and then at run time I want the notebook to be rendered and output.\n\nThere are many different commands you can use inside a Dockerfile, but with just 4, you’ll be able to build most images you might need.\n\nFROM – every container starts from a base image. In some cases, like in my Jupyter example, you might start with a bare bones container that’s just the operating system (ubuntu:20.04). In other cases, like in my shiny example, you might start with a container that’s almost there, and all you need to do is to copy in a file or two.\nRUN – run any command as if you were sitting at the command line inside the container. Just remember, if you’re starting from a very basic container, you may need to make a command available before you can run it (like wget in my container below).\nCOPY – copy a file from the host filesystem into the container. Note that the working directory for your Dockerfile will be whatever your working directory is when you run your build command.\nCMD - This is the command the Docker container will run when you run it with docker run. This is the thing you actually want to happen when you run your container.\n\nNow that you have that background, hopefully this Dockerfile makes some sense to you. If you don’t know what some of the individual commands are, consider googling them.\n# syntax=docker/dockerfile:1\nFROM ubuntu:20.04\n\n# Copy external files\nRUN mkdir -p /project/out/\n\nCOPY ./requirements.txt /project/\nCOPY ./hello.ipynb /project/\n\n# Install system packages\nRUN apt-get update && apt-get install -y \\\n  wget python3 python3-pip\n\n# Install quarto CLI + clean up\nRUN wget https://github.com/quarto-dev/quarto-cli/releases/download/v0.9.83/quarto-0.9.83-linux-amd64.deb\nRUN dpkg -i ./quarto-0.9.83-linux-amd64.deb\nRUN rm -f ./quarto-0.9.83-linux-amd64.deb\n\n# Install Python requirements\nRUN pip3 install -r /project/requirements.txt\n\n# Render notebook\nCMD cd /project && \\\n  quarto render ./hello.ipynb && \\\n  # Move output to correct directory\n  # Needed because quarto requires relative paths in --output-dir: \n  # https://github.com/quarto-dev/quarto-cli/issues/362\n  rm -rf /project-out/hello_files/ && \\\n  mkdir -p /project-out/hello_files && \\\n  mv ./hello_files/* /project-out/hello_files/ && \\\n  mv ./hello.html /project-out/\nOnce you’ve created your Dockerfile, you build it into an image using docker build -t <image name> and then you can push that to DockerHub or another registry using docker push.\n\n\n4.3.5 Docker Cheatsheet\nWORK IN PROGRESS\n\n4.3.5.1 Docker Commands\ndocker run\n\n-d – run in “detached mode” – this is often a good idea, as it puts the docker container into the background of your terminal, so it doesn’t stay blocking anything else you’d want to do.\n-p – “publish” ports from inside the container to the host\n-v – mount a volume into the container\n-n – give your container a name\n\ndocker ps lists all of your running containers. Because a container can just run quietly in the background, it’s easy to lose track of them, and docker ps will give you the list. It’s also worth remarking that you don’t need to be a command-line hero – you can open the docker desktop dashboard to see these things.\ndocker exec -it <container id>\ndocker kill\n\n\n\n\n\n\nCleaning up old containers\n\n\n\nDocker images can be quite large. If you’re doing a lot of different things on Docker, you may quickly find yourself using A LOT of storage space.\nYou can see all the images you’ve got downloaded with docker ps -a (-a is short for all) and you can delete them with docker container rm <container id>.\nDon’t be precious about your images! You’re always just a docker pull away from getting them back.\nIf you want to delete all the images you’ve ever downloaded, you can do that with docker container rm -f $(docker container ls -aq)."
  },
  {
    "objectID": "chapters/sec1/docker.html#exercises",
    "href": "chapters/sec1/docker.html#exercises",
    "title": "4  Docker for Data Science",
    "section": "4.4 Exercises",
    "text": "4.4 Exercises\n\nPut a shiny app in a container, run it on your desktop.\nPut that container into a container registry.\nUse a local K8S distribution to run several instances of that container.\nRedo the Jupyter example but instead of baking the notebook into the conatiner, provide the notebook at runtime."
  },
  {
    "objectID": "chapters/sec1/data.html",
    "href": "chapters/sec1/data.html",
    "title": "5  Taking Data to Production",
    "section": "",
    "text": "I frequently speak to data scientists who are responsible for taking an insight they’ve discovered or a report they’ve built and moving that insight to a server where the report can run on a schedule, or the dashboard can live on. While most data scientists have spent a lot of time thinking about the contents of their data, they’ve probably spent less time thinking about how their data is stored, or how it moves around.\nWhen you start taking apps to production, these things start to matter. Depending on the security requirements of your app and the underlying data, you may also have to spend a good bit of time thinking about how your app connects to your data, and how you make sure only the right people have access.\nIn this chapter, we’ll walk through the decision-making process you can follow to figure out how to architect the data inside your app, API, or report. Going to use the word app throughout this doc, but applies equally to an API."
  },
  {
    "objectID": "chapters/sec1/data.html#options-for-storing-app-data",
    "href": "chapters/sec1/data.html#options-for-storing-app-data",
    "title": "5  Taking Data to Production",
    "section": "5.1 Options for storing app data",
    "text": "5.1 Options for storing app data\nIn this chapter, we’re going to explore how you decide how to store the data for your app. But first, let’s just talk through what the options are so it makes sense as we talk about why you might choose one over the other in the rest of the chapter.\n\n5.1.1 Storage Format\nThe first question of how to store the data is the storage format. There are really three distinct options for storage format.\nFlat file storage describes writing the data out into a simple file. The canonical example of a flat file is a csv file. However, there are also other formats that may make data storage smaller because of compression, make reads faster, and/or allow you to save arbitrary objects rather than just rectangular data. In R, the rds format is the generic binary format, while pickle is the generic binary format in python.\nFlat files can be moved around just like any other file on your computer. You can put them on your computer, share them through tools like dropbox, google drive, scp, or more.\nThe biggest disadvantage of flat file data storage is twofold – and is related to their indivisibility. In order to use a flat file in R or Python, you’ll need to load it into your R or Python session. For small data files, this isn’t a big deal. But if you’ve got a large file, it can take a long time to read, which you may not want to wait for. Also, if your file has to go over a network, that can be a very slow operation. Or having to load it into an app at startup. Also, there’s generally no way to version data, or just update part, so if you’re saving archival versions, they can take up a lot of space very quickly.\nAt the other extreme end from a flat file format is a database. A database is a standalone server with its own storage, memory, and compute. In general, you’ll recall things from a database using some sort of query language. Most databases you’ll interact with in a data science context are designed around storing rectangular data structures and use Structured Query Language (SQL) to get at the data inside.\nThere are other sorts of databases that store other kinds of objects – you may need these depending on the kind of objects you’re working with. Often the IT/Admin group will have standard databases they work with or use, and you can just piggyback on their decisions. Sometimes you’ll also have choices to make about what database to use, which are beyond the scope of this book.\nThe big advantage of a database is that the data is stored and managed by an independent process. This means that accessing data from your app is often a matter of just connecting to the database, as opposed to having to move files around.\nWorking with databases can also be frought – you usually end up in one of two situations – either the database isn’t really for the data science team, in which case you can probably get read access, but not write. So you’ll be able to use the database as your source of truth, but you won’t be able to write there for intermediate tables and other things you might need. Or you’ll have freedom to set up your own database, in which case you’ll have to own it – and that comes with its own set of headaches.\nThere’s a third option for data storage that is quickly rising in popularity for medium size data. These options are ones that allow you to store data in a flat file, but access it in a smarter way than “just load all the data into memory”. SQLite is a classic on this front that gives you SQL access to what is basically just a flat file. There are also newer entrants into this place that are better from an analytics perspective, like combining Apache Arrow with feather and parquet files and the dask project in Python.\nThese tools can give you the best of both worlds – you get away from the R and Python limitation of having to load all your data into memory, without having to run a separate database server. But you’ll still have to keep track of where the actual files are and make them accessible to your app.\n\n\n5.1.2 Storage Location\nThe second question after what you’re storing is where. If you are using a database, then the answer is easy. The database just lives where it lives, and you’ll need to make sure you have a way to access it – both in terms of network access – as well as making sure you can authenticate into it (more on that below).\nIf you’re not using a database, then you’ll have to decide where to store the data for your app. Most apps that aren’t using a database start off rather naively with the data in the app bundle.\n<TODO: Image of data in app bundle>\nThis works really well during development and is an easy pattern to get started with. The problem is that this pattern generally falls apart when it goes to production. Usually this pattern works fine for a while. Problems start to arise when the data needs updating, and most data needs updating. Usually, you’ll be ready to update the data in the app long before you’re ready to update the app itself.\nAt this point, you’ll be kicking yourself that you now have to update the data inside the app every time you want to want to make a data update. It’s generally a better idea to have the data live outside the app bundle. Then you can update the data without mucking around with the app itself.\nA few options for this include just putting a flat file (or flat with differential read) into a directory near the app bundle. The pins package is also a great option here"
  },
  {
    "objectID": "chapters/sec1/data.html#choosing-your-storage-solution",
    "href": "chapters/sec1/data.html#choosing-your-storage-solution",
    "title": "5  Taking Data to Production",
    "section": "5.2 Choosing your storage solution",
    "text": "5.2 Choosing your storage solution\n\n5.2.1 How frequently are the data updated relative to the code?\nMany data apps have different update requirements for different data in the app.\nFor example, imagine you were the data scientist for a wildlife group that needed a dashboard to track the types of animals that had been spotted by a wilderness wildlife camera. You probably have a table that gives parameters for the animals themselves, perhaps things like endangered status, expected frequency, and more. That table probably needs to be updated very infrequently.\nOn the other hand, the day to day counts of the number of animals spotted probably needs to be updated much more frequently. <TODO: change to hospital example?>\nIf your data is updated only very infrequently, it might make sense to just bundle it up with the app code and update it on a similar cadence to the app itself.\n<TODO: Picture data in app bundle>\nOn the other hand, the more frequently updated data probably doesn’t make sense to update at the same cadence as the app code. You probably want to access that data in some sort of external location, perhaps on a mounted drive outside the app bundle, in a pin or bucket, or in a database.\nIn my experience, you almost never want to actually bundle data into the app. You almost always want to allow for the app data (“state”) to live outside the app and for the app to read it at runtime. Even data that you think will be updated infrequently, is unlikely to be updated as infrequently as your app code. Animals move on and off the endangered list, ingredient substitutions are made, and hospitals open and close and change their names in memoriam of someone.\nIt’s also worth considering whether your app needs a live data connection to do processing, or whether looking up values in a pre-processed table will suffice. The more complex the logic inside your app, the less likely you’ll be able to anticipate what users need, and the more likely you’ll have to do a live lookup.\n\n\n5.2.2 Is your app read-only, or does it have to write?\nMany data apps are read-only. This is nice. If you’re going to allow your app to write, you’ll need to be careful about permissions, protecting from data loss via SQL injection or other things, and have to be careful to check data quality.\nIf you want to save the data, you’ll also need a solution for that. There’s no one-size-fits-all answer here as it really depends on the sort of data you’re using. The main thing to keep in mind is that if you’re using a database, you’ll have to make sure you have write permissions."
  },
  {
    "objectID": "chapters/sec1/data.html#when-does-the-app-fetch-its-data",
    "href": "chapters/sec1/data.html#when-does-the-app-fetch-its-data",
    "title": "5  Taking Data to Production",
    "section": "5.3 When does the app fetch its data?",
    "text": "5.3 When does the app fetch its data?\nAt app open or throughout runtime?\nThe first important question you’ll have to figure out is what the requirements are for the code you’re trying to put into production.\n\n5.3.1 How big are the data in the app?\nWhen I ask this question, people often jump to the size of the raw data they’re using – but that’s often a completely irrelevant metric. You’re starting backwards if you start from the size of the raw data. Instead, you should figure out what’s the size of data you actually need inside the app.\nTo make this a little more concrete, let’s imagine you work for a large retailer and are responsible for creating a dashboard that will allow people to visualize the last week’s worth of sales for a variety of products. With this vague prompt, you could end up needing to load a huge amount of data into your app – or very little at all.\nOne of the most important questions is how much you can cache before someone even opens the app. For example, if you need to\n-> data granularity -> does it even need to be an app, or will a report do?\n\n\n5.3.2 What are the performance requirements for the app?\nOne crucial question for your app is how much wait time is acceptable for people wanting to see the app – and when is that waiting ok? For example, if people need to be able to make selections and see the results in realtime, then you probably need a snappy database, or all the data preloaded into memory when they show up.\nFor some apps, you want the data to be snappy throughout runtime, but it’s ok to have a lengthy startup process (perhaps because it can happen before the user actually arrives) and you want to load a lot of data as the app is starting and do much less throughout the app runtime.=\n\n\n5.3.3 Creating Performant Database Queries\nIf you are using a database, you’ll want to be careful about how you construct your queries to make sure they perform well. The main way to think about this is whether your queries will be eager or lazy.\nIn an eager app, you’ll pull basically all of the data for the app as it starts up, while a lazy app will pull data only as it is need.\n<TODO: Diagram of eager vs lazy data pulling>\nMaking your app eager is usually much simpler – you just read in all the data at the beginning. This is often a good first cut at writing an app, as you’re not sure exactly what requirements your app has. For relatively small datasets, this is often good enough.\nIf it seems like your app is starting up slowly – or your data’s too big to all pull in, you may want to pull data more lazily.\n\n\n\n\n\n\nTip\n\n\n\nBefore you start converting queries to speed up your app, it’s always worthwhile to profile your app and actually check that the data pulling is the slow step. I’ve often been wrong in my intuitions about what the slow step of the app is.\nThere’s nothing more annoying than spending hours refactoring your app to pull data more lazily only to realize that pulling the data was never the slow step to begin with.\n\n\nIt’s also worth considering how to make your queries perform better, regardless of when they occur in your code. Obviously you want to pull the minimum amount of data possible, so making data less granular, pulling in a smaller window of data, or pre-computing summaries is great when possible (though again, it’s worth profiling before you take on a lot of work that might result in minimal performance improvements).\nOnce you’ve decided whether to make your app eager or lazy, you can think about whether to make the query eager or lazy. In most cases, when you’re working with a database, the slowest part of the process is the actual process of pulling the data. That means that it’s generally worth it to be lazy with your query. And if you’re using dplyr from R, being eager vs lazy is simply a matter of where in the chain you put the collect statement.\nSo you’re better off sending a query off to the database, letting the database do a bunch of computations, and pulling a small results set back rather than pulling in a whole data set and doing computations in R.\n\n\n5.3.4 How to connect to databases?\nIn R, there are two answers to how to connect to a database. You can either use a direct connector to connect to the database, this generally will provide a driver to the DBI package. There are other database alternatives, but they’re pretty rare.\n<TODO: image of direct connection vs through driver>\nYou can also use an ODBC/JDBC driver to connect to the database. In this case, you’ll use something inside your R or Python session to use an database driver that has nothing to do with R or Python. Many organizations like these because IT/Admins can configure them on behalf of users and can be agnostic about whether users are using them from R, Python, or something else entirely.\nIf you’re in R, the odbc package gives you a way to interface with ODBC drivers. I’m unaware of a general solution for conencting to odbc drivers in Python.\nA DSN is a particular way to configure an ODBC driver. They are nice because it means that the Admin can fill in the connection details ahead of time, and you don’t need to know any details of the connection, other than your username and password.\n<TODO: image of how DSN works>\nIn R, writing a package that creates database connections for users is also a very popular way to provide database connections to the group."
  },
  {
    "objectID": "chapters/sec1/data.html#how-do-i-do-data-authorization",
    "href": "chapters/sec1/data.html#how-do-i-do-data-authorization",
    "title": "5  Taking Data to Production",
    "section": "5.4 How do I do data authorization?",
    "text": "5.4 How do I do data authorization?\nThis is a question you probably don’t think about much as you’re puttering around inside RStudio or in a Jupyter Notebook. But when you take an app to production, this becomes a crucial question.\nThe best and easiest case here is that everyone who views the app has the same permissions to see the data. In that case, you can just allow the app access to the data, and you can check whether someone is authorized to view the app as a whole, rather than at the data access layer.\nIn some cases, you might need to provide differential data access to different users. Sometimes this can be accomplished in the app itself. For example, if you can identify the user, you can gate access to certain tabs or features of your app. Many popular app hosting options for R and Python data science apps pass the username into the app as an environment variable.\nSometimes you might also have a column in a table that allows you to filter by who’s allowed to view, so you might just be able to filter to allowable rows in your database query.\nSometimes though, you’ll actually have to pass database credentials along to the database, which will do the authorization for you. This is nice, because then all you have to do is pass along the correct credntial, but it’s also a pain because you have to somehow get the credential and send it along with the query.\n<TODO: Image of how a kinit/JWT flow work>\nMost commonly, kerberos tickets or JWTs are used for this task. Usually your options for this depend on the database itself, and the ticket/JWT granting process will likely have to be handled by the database admin.\n\n5.4.1 Securely Managing Credentials\nThe single most important thing you can do to secure your credentials for your outside services is to avoid ever putting credentials in plaintext. The simplest alternative is to do a lookup from environment variables in either R or Python. There are many more secure things you can do, but it’s pretty trivial to put Sys.getenv(\"my_db_password\") into an app rather than actually typing the value. In that case, you’d set the variable in a .Rprofile or .Renviron .\nSimilarly, in Python, you can get and set environment variables using the os module. os.environ['DB_PASSWORD'] = 'my-pass' and os.getenv('DB_PASSWORD'), os.environ.get('DB_PASSWORD') or os.environ('DB_PASSWORD'). If you want to set environment variables from a file, generally people in Python use thedotenv package along with a .env file.\nYou should not commit these files to git, but should manually move them across environments, so they never appear anywhere centrally accessible.\nIn some organizations, this will still not be perceived secure enough, because the credentials are not encrypted at rest. Any of the aforementioned files are just plain text files – so if someone unauthorized were to get access to your machine, they’d be able to grab all of the goodies in your .Rprofile and use them themselves.\nSome hosting software, like RStudio Connect, can take care of this problem, as they store your environment variables inside the software in an encrypted fashion and inject them into the R runtime.\nThere are a number of more secure alternatives – but they generally require a little more work.\nThere are packages in both R and Python called keyring that allow you to use the system keyring to securely store environment variables and recall them at runtime. These can be good in a development environment, but run into trouble in a production environment because they generally rely on a user actually inputting a password for the system keyring.\nOne popular alternative is to use credentials pre-loaded on the system to enable using a ticket or token – often a Kerberos token or a JWT. This is generally quite do-able, but often requires some system-level configuration.\n<TODO: image of kerberos>\nYou may need to enable running as particular Linux users if you don’t want to do all of the authentication interactively in the browser. You usually cannot just recycle login tokens, because they are service authorization tokens, not token-granting tokens.1"
  },
  {
    "objectID": "chapters/sec1/data.html#some-notes-if-youre-administering-the-service",
    "href": "chapters/sec1/data.html#some-notes-if-youre-administering-the-service",
    "title": "5  Taking Data to Production",
    "section": "5.5 Some notes if you’re administering the service",
    "text": "5.5 Some notes if you’re administering the service\nThis chapter has mostly been intended for consumption by app authors who will be creating data science assets. But in some cases, you might also have to administer the service yourself. Here are a few tips and thoughts.\nChoosing which database to use in a given case is very complicated. There are dozens and dozens of different kinds of databases, and many different vendors, all trying to convince you that theirs is superior.\nIf you’re doing certain kinds of bespoke analytics, then it might really matter. In my experience, using postgres is good enough for most things involving rectangular data of moderate size, and a storage bucket is often good enough for things you might be tempted to put in a NoSQL database until the complexity of the data gets very large.\nEither way, a database as a service is a pretty basic cloud service. For example, AWS’s RDS is their simplest database service, and you can get a PostgreSQL, MySQL, MariaDB, or SQL Server database for very reasonable pricing.2 It works well for reasonably sized data loads (up to 64Tb).\nRDS is optimized for individual transactions. In some cases, you might want to consider a full data warehouse. AWS’s is called Redshift and it runs a flavor of PostgreSQL. In general it’s better when you have a lot of data (goes up to several petabytes) or are doing demanding queries a lot more often that you’re querying for individual rows. Redshift is a good bit more expensive, so it’s worth keeping that in mind.\nIf you’re storing data on a share drive, you’ll have to make sure that it’s available in your prod environment. This process is called mounting a drive or volume onto a server. It’s quite a straightforward process, but needing to mount a drive into two different servers places some constraints on where those servers have to be.\n<TODO: picture of mounting a drive>\nWhen you’re working in the cloud, you’ll get compute separate from volumes. This works nicely, because you can get the compute you need and separately choose a volume size that works for you. There are many nice tools around volumes that often include automated backups, and the ability to easily move snapshots from place to place – including just moving the same data onto a larger drive in just a few minutes.\nEBS is AWS’s name for their standard storage volumes. You get one whenever you’ve got an EC2 instance.\nIn some cases, you’ll need to have a drive that’s mounted to multiple machines at once, then you’ll need some sort of network drive.\nThe mounting process works exactly the same, but the underlying technology needs to be a little more complex to accommodate how it works. Depending on whether you’re talking about connecting multiple Linux hosts, you might use NFS (Network File Share), or SMB/CIFS (Windows only), or some combination of the two might use Samba. If you’re getting to this level, it’s probably a good idea to involve a professional IT/Admin.\nhttps://www.varonis.com/blog/cifs-vs-smb\nhttps://www.reddit.com/r/sysadmin/comments/1ei3ht/the_difference_between_nfs_cifs_smb_samba/\nhttps://cloudinfrastructureservices.co.uk/nfs-vs-smb/\nhttps://cloudinfrastructureservices.co.uk/nfs-vs-cifs/"
  },
  {
    "objectID": "chapters/sec1/data.html#exercises",
    "href": "chapters/sec1/data.html#exercises",
    "title": "5  Taking Data to Production",
    "section": "5.6 Exercises",
    "text": "5.6 Exercises\n\nConnect to and use an S3 bucket from the command line and from R/Python.\nStand up a container with Postgres in it, connect using isql, then from R/Python using ODBC/JDBC.\nStand up an EC2 instance w/ EBS volume, change EBS to different instances.\nStand up an NFS instance and mount onto a server."
  },
  {
    "objectID": "chapters/sec1/cloud.html",
    "href": "chapters/sec1/cloud.html",
    "title": "6  The Cloud and Cloud Providers",
    "section": "",
    "text": "Like so many tech buzzwords, it’s pretty hard to get a sense for what the cloud actually is beneath all the hype.\nBack in the day – and still in some heavily-regulated industries – getting a new server involved buying a physical piece of hardware, installing it in a server room, getting it configured and up and running, installing the software you want on the server, and configuring access to the server.\nThe Cloud provides layers of “as a service” on top of this former world where someone at your organization would be responsible for buying and maintaining actual hardware.\nIn this chapter, you’ll learn a little bit about how The Cloud works, and how to demystify cloud offerings."
  },
  {
    "objectID": "chapters/sec1/cloud.html#the-rise-of-services",
    "href": "chapters/sec1/cloud.html#the-rise-of-services",
    "title": "6  The Cloud and Cloud Providers",
    "section": "6.1 The Rise of Services",
    "text": "6.1 The Rise of Services\nLike much of the rest of the economy, server provisioning and use has gone the way of services. Instead of buying, owning, and maintaining a physical object, a huge proportion of the world’s server hardware is rented.\n[TODO: quote in paragraph below: https://www.srgresearch.com/articles/cloud-market-growth-rate-nudges-amazon-and-microsoft-solidify-leadership]\nIn the US, a huge fraction runs on servers rented from one of three organizations (in order of how significant they are) – Amazon Web Services (AWS), Microsoft Azure, and Google Cloud Platform (GCP). These three companies account for a huge proportion of what we think of as “The Cloud”. There are other smaller players, and also companies that are popular for particular tasks, like Netlify for hosting static websites.\nHowever, in many cases, the cloud doesn’t just refer to renting a server itself. There are also layers and layers of services on top of the “rent me a server” business.\nIn general, the “rent me a server” model is called Infrastructure as a Service (IaaS - pronounced eye-az). So when you stood up an EC2 instance from AWS in the first chapter, you were using AWS’s IaaS offering.\nIn general, people split the layers on top of IaaS into two categories – Platform as a Service (PaaS – pronounced pass), and Software as a Service (SaaS pronounced sass).\nPaaS is where you rent an environment in which to do development. On the other hand, SaaS is renting software as an end user.\nOne common way to explain the difference is using a baking metaphor. Consider making a cake. An on-prem install would be where you make the cake completely from scratch. A IaaS experience would be buying cake mix and baking at home. PaaS would be buying a premade blank cake that you decorate yourself, and SaaS would be just buying a store-bought cake.\nI find these categories and this metaphor sorta helpful in the abstract, but when getting down to concrete real-world examples, the distinctions get fuzzier, and you have to be careful which perspective you’re talking about.\nFor example, RStudio Cloud is a service where you can get an environment with RStudio preconfigured and ready to use. From the perspective of a data scientist, this is clearly a PaaS offering. RStudio is providing a platform where you can learn or do work as a data scientist.\nBut from the perspective of an IT/Admin considering how to set up a server-based data science environment inside their company, RStudio Cloud is clearly a SaaS offering – you just getting the software configured and ready to use.\nMaking the issue even more difficult, many companies go out of their way to make their services sound grand and important and don’t just say, “this is ___ as a service”. Moreover, it’s very common (especially in AWS) to have many different services that fulfill similar needs, and it can be really hard to concretely tell the difference.\nFor example, if you go to AWS’s database offerings for a “database as a service”, your options include Redshift, RDS, Aurora, DynamoDB, ElastiCache, MemoryDB for Redis, DocumentDB, Keyspaces, Neptune, Timestream, and more.\nThere’s a reason why there’s a meaningful industry of people whose full time job is to consult on which AWS service your company needs and how to take advantage of the pricing rules to make sure you get a good deal.\nThere are a few reasons why organizations are moving to the cloud. Primary among them is that maintaining physical servers is often not the core competency of IT/Admin organizations. They’d rather manage higher-level abstractions than physical servers – or increasingly even virtual servers.\nOne reason that people cite, but very rarely comes to pass, is cost. In theory, the flexibility of the cloud should allow organizations to stand up servers as needed and spin them down when they’re not needed. This flexibility is real, there are times when it’s super useful to be able to bring up a server for a particular project – it’s often far quicker and easier than buying and installing a server of your own.\nIn reality, the engineering needed to stand up and spin down these servers at the right time is really difficult and costly – enough so that most organizations could probably substantailly save money if they repatriated their cloud workloads.\nFor more established organizations, running workloads in the cloud may, in fact, be substantially more costly than just bringing those workloads on prem."
  },
  {
    "objectID": "chapters/sec1/cloud.html#serverless-computing",
    "href": "chapters/sec1/cloud.html#serverless-computing",
    "title": "6  The Cloud and Cloud Providers",
    "section": "6.2 Serverless Computing",
    "text": "6.2 Serverless Computing\nIn the past few years, there has been a rise in interest in “serverless” computing. This is a buzzword and there’s no one shared definition of what serverless means. It’s worth making clear that there is no such thing as truly serverless computing. Every bit of cloud computation runs on a server - the question is whether you have to deal with the server or if someone else is doing it for you.\nHowever, there are two distinct things happening that can meaningfully be described as serverless…but they’re completely different.\nOne is the rise of containerization. In chapter XXXX, we’ll get deep into the weeds on using docker yourself. Docker is a very cool tool that makes software much more portable, because you can bring the environment – all the way down to the operating system – around with you very easily. This is kinda a superpower, and many organizations are moving towards using docker containers as the atomic units of their IT infrastructure, so the IT organization doesn’t manage any servers directly, and instead just manages docker containers.\nIn some sense, this is meaningfully serverless. You’ve moved the level of abstraction up a layer from servers and virtual machines to docker containers. And managing docker containers is often meaningfully easier than managing virtual machines directly. However, you still face a lot of the same problems like versioning operating systems, dealing with storage and networking yourself, and more.\nThere is another, stronger, use of serverless that is rising and is also pretty cool, but is super different. In these services, you just hand off a function, written in soem programming langauge to a cloud provider, and they run that function as a service. In a trivial example, imagine a service that adds two numbers together. You could write a Python or R function that does this addition and returns it.\nIt is possible to just deploy this function to a Cloud provider’s environment and then only pay for the actual compute time needed to complete your function calls. This is obviously very appealing because you really don’t have to manage anything at the server level. The disadvantage is that this works only for certain types of operations."
  },
  {
    "objectID": "chapters/sec1/cloud.html#common-services-that-will-be-helpful-to-know-offhand",
    "href": "chapters/sec1/cloud.html#common-services-that-will-be-helpful-to-know-offhand",
    "title": "6  The Cloud and Cloud Providers",
    "section": "6.3 Common Services That will be helpful to know offhand",
    "text": "6.3 Common Services That will be helpful to know offhand\nLuckily, if you’re thinking about setting up and running a standard data science platform in one of the major cloud providers, you’re likely to use one of a few reasonably standard tools.\nIt’s helpful to keep in mind that at the very bottom, there are four basic cloud services: renting servers, configuring networking, identity management, and renting storage. All the other services are recombinations and software installed on top of that.1\nAzure and GCP tend to name their offerings pretty literally. AWS, on the other hand, uses names that have little relationship to the actual task at hand, and you’ll just need to learn them.\n\n6.3.1 IaaS\n\nCompute - AWS EC2, Azure VMs, Google Compute Engine\nStorage\n\nfile - EBS, Azure managed disk, Google Persistent Disk\nNetwork drives - EFS, Azure Files, Google Filestore\nblock/blob - S3, Azure Blob Storage, Google Cloud Storage\n\nNetworking:\n\nPrivate Clouds: VPC, Virtual Network, Google Virtual Private Cloud\nDNS - Route53, Azure DNS + Traffic Manager, Google Cloud DNS\n\n\n\n\n6.3.2 Not IaaS\n\nContainer Hosting - ECS, Azure Container Instances + Container Registry\nK8S cluster as a service - EKS, AKS, GKE\nRun a function as a service - Lambda, Azure Functions, Google Cloud Functions\nDatabase - RDS/Redshift, Azure Database, Google Cloud SQL\nSageMaker - ML platform as a service, Azure ML, Google Notebooks\n\nhttps://docs.microsoft.com/en-us/azure/architecture/aws-professional/services#networking https://cloud.google.com/free/docs/aws-azure-gcp-service-comparison"
  },
  {
    "objectID": "chapters/sec1/cloud.html#cloud-tooling",
    "href": "chapters/sec1/cloud.html#cloud-tooling",
    "title": "6  The Cloud and Cloud Providers",
    "section": "6.4 Cloud Tooling",
    "text": "6.4 Cloud Tooling\n\nIdentity MGMT - IAM, Azure AD\nBilling Mgmt\n\nIaaC tooling\n\n6.4.1 AWS Instance Classes for Data Science\nt3 – good b/c of instance credits, limited size\nCs – good b/c fast CPUs\nR - good b/c high amount of RAM\nP - have GPUs, but also v expensive\nInstance scale linearly w/ number of cores (plot?)"
  },
  {
    "objectID": "chapters/sec1/cloud.html#exercises",
    "href": "chapters/sec1/cloud.html#exercises",
    "title": "6  The Cloud and Cloud Providers",
    "section": "6.5 Exercises",
    "text": "6.5 Exercises\n\nExample of something you’d want to build – for each of the 3 providers, which services would you use if you wanted an IaaS solution, a PaaS solution?"
  },
  {
    "objectID": "chapters/sec2/servers.html",
    "href": "chapters/sec2/servers.html",
    "title": "7  Computers, Computing, and Servers",
    "section": "",
    "text": "Data Science is a delightful mashup of statistics and computer science. While you can be a great data scientists without a deep understanding of computational theory, a mental model of how your computer works is helpful, especially when you head to production.\nIn this chapter, we’ll develop a mental model for how computers work, and explore how well that mental model applies to both the familiar computers in your life, but also more remote servers.\nIf you’re into pedantic nitpicking, you’re going to love this chapter apart, as I’ve grossly oversimplified how computers work. On the other hand, this basic mental model has served me well across hundreds of interactions with data scientists and IT/DevOps professionals.\nAnd by the end of the chapter, we’ll get super practical – giving you a how-to on getting a server of your very own to play with."
  },
  {
    "objectID": "chapters/sec2/servers.html#computers-are-addition-factories",
    "href": "chapters/sec2/servers.html#computers-are-addition-factories",
    "title": "7  Computers, Computing, and Servers",
    "section": "7.1 Computers are addition factories",
    "text": "7.1 Computers are addition factories\nAs a data scientist, the amount of computational theory it’s really helpful to understand in your day-to-day can be summarized in three sentences:\n\nComputers can only add.\nModern ones do so very well and very fast.\nEverything a computer “does” is just adding two (usually very large) numbers, reinterpreted.1\n\nI like to think of computers as factories for doing addition problems.\n\nWe see meaning in typing the word umbrella or jumping Mario over a Chomp Chain and we interpret something from the output of some R code or listening to Carly Rae Jepsen’s newest bop, but to your computer it’s all just addition.\nEvery bit of input you provide your computer is homogenized into addition problems. Once those problems are done, the results are reverted back into something we interpret as meaningful. Obviously the details of that conversion are complicated and important – but for the purposes of understanding what your computer’s doing when you clean some data or run a machine learning model, you don’t have to understand much more than that.\n\n7.1.1 Compute\nThe addition assembly line itself – where the work actually gets done – is referred to as compute. It’s where 2+2 gets turned into 4, and where 345619912 + 182347910 gets turned into 527967822. The heart of the factory in your computer is the central processing unit (CPU).\nThere are two elements to the total speed of your compute – the total number of cores, which you can think of as an individual conveyor belt doing a single problem at a time, and the speed at which each belt is running.\nThese days, most consumer-grade laptops have between 4 and 16 cores, and may have additional capabilities that effectively doubles that number. So most laptop CPUs can do between 4 and 32 simultaneous addition problems.\nIn your computer, the basic measure of conveyor belt speed is single-core “clock speed” in hertz (hz) – operations per second. The cores in your laptop probably run between 2-5 gigahertz (GHz): 2-5 billion operations per second.\n\nA few decades ago, there was a steady march in increases in clock speed – increasing by approximately 10x during the 90s, by 2-3x in the 2000s, and somewhere between not at all and 1.5x in the 2010s. Computers have gotten much faster through this entire time, but mostly from sources other than boosts in single-core clock speeds.\nFor example, in 2020, Apple released a series of Macs with its new M1 chip. Real-world performance of the M1 chip was dramatically better than earlier models, despite modest improvements in clock speeds.\n\n\n7.1.1.1 GPU Computing\nWhile compute usually just refers to the CPU, it’s not completely synonymous. Computers can offload some problems to a graphical processing unit (GPU). GPUs are specialized chips used for tasks like editing photo or videos, rendering video game graphics, some kinds of machine learning, and (yes) Bitcoin mining.\nWhere the CPU has a few fast cores, the GPU takes the opposite approach, with many slower cores. Where a consumer-grade CPU has 4-16 cores, mid-range GPUs have 700-4,000, but each one runs between 1% and 10% the speed of a CPU core.\nFor GPU-centric tasks, the overwhelming parallelism of a GPU is more important than the speed of any individual core, and GPU computation can be dramatically faster. For the purposes of data science, many popular machine learning techniques – including neural networks, XGBoost, and other tree-based models – potentially run much much faster on GPUs relative to CPUs.\n\n\n\n7.1.2 Memory (RAM)\nYour computer’s random access memory (RAM) is its short term storage. Your computer uses RAM to store addition problems it’s going to tackle soon, and results it thinks it might need again in the near future.\nThe benefit of RAM is that it’s very fast to access. The downside is that it’s temporary. When your computer turns off, the RAM gets wiped.2\n\nYou probably know this, but memory and storage is measured in bytes. These days, most consumer hard drives are between 1/2 to a few terrabytes (trillion bytes). Some enterprise data stores run on the scales of thousands of terrabytes (pettabytes) or even thousands of pettabytes (yottabytes).\n\nModern consumer-grade laptops come with somewhere between 4 and 16 Gb of memory.\n\n\n7.1.3 Storage (Hard Drive/Disk)\nYour computer’s storage is like a freezer. It keeps things permanently, whether the factory is operating or not. The tradeoff is that things there are cold, and take some time to warm up before they can be used.\nA few years ago, all hard drives were (HDDs), and were basically the same. HDDs have a bunch of magnetic disks that spin very fast (5,400 and 7,200 RPM are common speeds). Magnetized read/write heads move among the disks and save and read your data.\nIn the last few years, solid-state drives (SSDs) have been taking over. SSDs, which are collections of flash memory chips, are up to 15x faster than HDDs. They also can take a wider variety of shapes and sizes, and are more reliable and durable because they have no moving parts. The main drawback is that they’re usually more expensive per byte, but prices are still quite reasonable.\nMany consumer laptops have only an SSD at this point. Some desktops and high-end laptops combine a smaller SSD with a larger HDD."
  },
  {
    "objectID": "chapters/sec2/servers.html#choosing-the-right-data-science-machine",
    "href": "chapters/sec2/servers.html#choosing-the-right-data-science-machine",
    "title": "7  Computers, Computing, and Servers",
    "section": "7.2 Choosing the right data science machine",
    "text": "7.2 Choosing the right data science machine\nIn my experience as a data scientist and talking to IT/DevOps organizations trying to equip data scientists, the same questions about choosing a computer come up over and over again. Here are the guidelines I often share.\n\n7.2.1 Get as much RAM as feasible\nIn most cases, R and Python have to load all of your data into memory. Thus, the size of the data you can use is limited to the size of your machine’s RAM. Most other limits of your machine will just result in things being slower than you’d really want, but trying to load too much data into memory will result in a session crash, and you won’t be able to do your analysis.\n\nYou can get around the in-memory limitation by using a database or libraries that facilitate on-disk operations like Apache Arrow or dask.\n\nIt’s easy to say that you’ll always want more RAM, but a rough rule of thumb for whether you’ve got enough is the following:\n\nAmount of RAM = max amount of data * 3\n\nBecause you’ll often be doing some sort of transformation that results in invisible data copies and your computer can’t devote all of its memory, you’ll want to leave plenty of room over your actual data size. I’ve never run into problems with a scaling factor of 3, but this is definitely only a rough rule-of-thumb.\n\n\n7.2.2 Go for fewer, faster cores in the CPU\nR and Python are single-threaded. Unless you’re using special libraries for parallel processing, you’ll end up red-lining a single CPU core while the other just look on in silence. Therefore, single core clock speed matters more than the number of cores, and fewer, faster cores are usually preferable to many slower\nIf you’re buying a laptop or desktop, there usually aren’t explicit choices between a few fast cores and more slow cores. Most modern CPUs are pretty darn good, and you should just get one that fits your budget. If you’re standing up a server, you often have an explicit choice between more slower cores and fewer faster ones.\n\n\n7.2.3 Get a GPU…maybe…\nIf you’re doing machine learning that can be improved by GPU-backed operations, you might want a GPU. In general, only highly parallel machine learning problems like training a neural network or tree-based models will benefit from GPU computation.\nOn the other hand, GPUs are expensive, non-machine learning tasks like data processing don’t benefit from GPU computation, and many machine learning tasks are amenable to linear models that run well CPU-only.\n\n\n7.2.4 Get a lot of storage, it’s cheap\nAs for storage – get a lot – but don’t think about it too hard, because it’s cheap. Both a 1TB SSD and a 4TB HDD are around $100. Storage is cheap enough these days that it is almost always more cost efficient to buy more storage rather than making a highly-paid professional spend their time trying to figure out how to move things around.\nOne litmus test of an IT organization that is well-equipped to support data science is whether they understand this. Smart organizations know that just getting more storage is easily worth the cost in terms of the time of admins and data scientists."
  },
  {
    "objectID": "chapters/sec2/servers.html#is-a-server-different",
    "href": "chapters/sec2/servers.html#is-a-server-different",
    "title": "7  Computers, Computing, and Servers",
    "section": "7.3 Is a server different?",
    "text": "7.3 Is a server different?\nNo.\nBut also yes.\nAt its core, a server is exactly the same sort of addition factory as your laptop, and the same mental model of what is happening under the hood will serve you well.\n\nThe big difference is in how the input and output is done. While you interact directly with a computer through keyboard and mouse/touchpad, servers generally don’t have built in graphical interfaces – by default all interaction occurs via command line tools.\nOne of the reasons is that the overwhelming majority of the world’s servers run the Linux operating system, as opposed to the Windows or Mac OS your laptop probably runs.3 There are many different distributions (usually called “distros”) of Linux. For day-to-day enterprise server use, the most common of these are Ubuntu, CentOS, Red Hat Enterprise Linux (RHEL), SUSE Enterprise Linux.\nAlong with most of the world’s servers, almost all of the world’s embedded computers – in ATMs, cars and planes, TVs, and most other gadgets and gizmos – run on Linux. If you have an Android phone or a Chromebook – that’s Linux. Basically all of the world’s supercomputers use Linux."
  },
  {
    "objectID": "chapters/sec2/servers.html#exercises",
    "href": "chapters/sec2/servers.html#exercises",
    "title": "7  Computers, Computing, and Servers",
    "section": "7.4 Exercises",
    "text": "7.4 Exercises\n\nThink about the scenarios below – which part of your computer would you want to upgrade to solve the problem?\n\nYou try to load a big csv file into pandas in Python. It churns for a while and then crashes.\nYou go to build a new ML model on your data. You’d like to re-train the model once a day, but it turns out training this model takes 26 hours on your laptop.\nYou design an visualization Matplotlib , and create a whole bunch in a loop/"
  },
  {
    "objectID": "chapters/sec2/aws-walkthrough.html",
    "href": "chapters/sec2/aws-walkthrough.html",
    "title": "8  Lab 1: Get your own server",
    "section": "",
    "text": "Throughout the rest of the book, we’re going to talk about how to set up a real data science workbench in the cloud. In this first lab, we’re just going to go through the simple steps of getting a free server from AWS to use.\nIf you’ve never set up a server before, I’d strongly recommend you play along.\nThroughout the rest of the book, we’re going to be taking this toy server and upgrading it in various ways. If you’re trying to work along, you’ll probably do this a dozen times or more over the course of the book.\nThere are a number of topics that we’re going to gloss over in this chapter including networking and storage configuration, security, and how SSH works. By the time you’re done with this book, you’ll be able to come back to this chapter and fully understand everything you did.\nYou’ll also understand why this first server is ready for production data science work in the same way throwing a tarp over a tree branch in the forest is a house. It might be a nice place to stay for a little, but you’re going to need something more robust longer-term.\nFor now, you can get a server of your own in 10-15 minutes if you just follow along, which is what I recommend."
  },
  {
    "objectID": "chapters/sec2/aws-walkthrough.html#exercises",
    "href": "chapters/sec2/aws-walkthrough.html#exercises",
    "title": "8  Lab 1: Get your own server",
    "section": "8.1 Exercises",
    "text": "8.1 Exercises\nTry standing up a new server and installing R and RStudio Server Open Source, or Python and JupyterHub.\n\nHint 1: Remember that your instance only allows traffic to SSH in on port 22 by default. You access RStudio on port 8787 by default and JupyterHub on port 8000. You control what ports are open via the Security Group.\nHint 2: You’ll need to create a user on the server. The adduser command is your friend.\n\n#TODO: test out JupyterHub"
  },
  {
    "objectID": "chapters/sec2/understanding-traffic.html",
    "href": "chapters/sec2/understanding-traffic.html",
    "title": "9  Computer Networks and the Internet",
    "section": "",
    "text": "In chapter 1, we got into how the computer on your desk, on your lap, or in your hand works. These days, many or even most of the things we want to do involve sending data across computer networks. When you visit a website, wirelessly print a document, or login to your email, you are making use of a computer network.\nThe computer network we’re all most familiar with is the biggest of them all – The Internet. But there are myriad other networks, like the very small private network of the devices (phones, computers, TVs, etc) connected to your home wifi router, to the somewhat larger VPN (it stands for virtual private network, after all) you might connect to for school or work.\nA computer network is a set of computers that can communicate with each other to send data in the form of network traffic back and forth to each other. These networks are basically self-similar – once you understand the wifi network in your house, you’ve also got a reasonably good understanding of how the entire internet works, which is great if you’re an author trying to explain how this all works.\nIn this chapter, you’ll learn the basics of how computer networks work. In particular, we’ll get into some of the layers of protocols that define how computers communicate with each other. This chapter is mostly going to be background for the next few chapters, where we’ll get into the nitty gritty of how to configure both the public and private elements of networking for your data science workshop."
  },
  {
    "objectID": "chapters/sec2/understanding-traffic.html#computer-communication-packets-traversing-networks",
    "href": "chapters/sec2/understanding-traffic.html#computer-communication-packets-traversing-networks",
    "title": "9  Computer Networks and the Internet",
    "section": "9.1 Computer communication = packets traversing networks",
    "text": "9.1 Computer communication = packets traversing networks\nThe virtual version of the processes that get your letter from your house to your penpal’s is called packet switching, and it’s really not a bad analogy. Like the physical mail, your computer dresses up a message with an address and some other details, sends it on its way, and waits for a response.1 The set of rules – called a protocol – that defines a valid address, envelope type, and more is called TCP/IP.\nUnderneath these protocols is a bunch of hardware, which we’re basically going to ignore.\nEach computer network is governed by a router. For the purposes of your mental model, you can basically think of your router as doing two things – maintaining a table of the IP addresses it knows, and following this algorithm over and over again.\n#TODO: Turn into visual tree – also visual of networks and sub-networks\n\nDo I know where this address is?\n\nYes: Send the packet there.\nNo: Send the packet to the default address and cross fingers.\n\n\nIn general, routers only know about the IP addresses of sub-networks and devices, so if you’re printing something from your laptop to the computer in the next room, the packet just goes to your router and then straight to the printer.\n\nIn your home’s local area network (LAN), your router does one additional thing – as devices like your phone, laptop, or printer attach to the network, it assigns them IP addresses based on the addresses available in a process called Dynamic Host Configuration Protocol (DHCP).\n\nOn the other hand, if you’re sending something to a website or server that’s far away, your computer has no idea where that IP address is. Clever people have solved this problem by setting the default address in each router to be an “upstream” router that is a level more general.\nSo immediately upstream of your router is probably a router specific to your ISP for a relatively small geographic area. Upstream of that is probably a router for a broader geographic area. So your packet will get passed upstream to a sufficiently general network and then back downstream to the actual IP address you’re trying to reach.\n#TODO: Image of computer network w/ upstream and downstream networks\nWhen the packets are received and read – something happens. Maybe you get to watch your show on Netflix, or your document gets printed – or maybe you get an error message back. In any event, the return message will be transmitted exactly the same way as your initial message, though it might follow a different path."
  },
  {
    "objectID": "chapters/sec2/understanding-traffic.html#more-details-about-ip-addresses",
    "href": "chapters/sec2/understanding-traffic.html#more-details-about-ip-addresses",
    "title": "9  Computer Networks and the Internet",
    "section": "9.2 More details about IP Addresses",
    "text": "9.2 More details about IP Addresses\nIP addresses are, indeed, addresses. They are how one computer or server finds another on a computer network, and they are unique within that network.\nMost IP addresses you’ve probably seen before are IPv4 addresses. They’re four blocks of 8-bit fields, so they look something like 65.77.154.233, where each of the four numbers is something between 0 and 255.\nSince these addresses are unique, each server and website on the internet needs a unique IP address. If you do the math, you’ll realize there are “only” about 4 billion of these. It turns out that’s not enough for the public internet and we’re running out.\nIn the last few years, adoption of the new standard, IPv6, has started. IPv6 addresses are eight blocks of hexadecimal (0-9 + a-f) digits separated by colons, with certain rules that allow them to be shortened, so 4b01:0db8:85a3:0000:0000:8a2e:0370:7334 or 3da4:66a::1 are both examples of valid IPv6 addresses.\nIPv6 will coexist with IPv4 for a few decades, and we’ll eventually switch entirely to IPv6. There’s no worry about running out of IPv6 addresses any time soon, because the total number of IPv6 addresses is a number 39 digits long.\n\n9.2.0.1 Special IP Addresses\nAs you work more with IP addresses, there are a few you’ll see over and over. Here’s a quick cheatsheet:\n\n\n\n\n\n\n\nCode\nMeaning\n\n\n\n\n127.0.0.1\nlocalhost or loopback – the machine that originated the request\n\n\n192.168.x.x, 172.16.x.x.x,\n10.x.x.x\nProtected address blocks used for private IP addresses. More on public vs private addresses in chapter XX."
  },
  {
    "objectID": "chapters/sec2/understanding-traffic.html#ports",
    "href": "chapters/sec2/understanding-traffic.html#ports",
    "title": "9  Computer Networks and the Internet",
    "section": "9.3 Ports",
    "text": "9.3 Ports\nTODO\nPort forwarding outside:inside\nWhy you’d want to"
  },
  {
    "objectID": "chapters/sec2/understanding-traffic.html#application-layer-protocols-define-valid-messages",
    "href": "chapters/sec2/understanding-traffic.html#application-layer-protocols-define-valid-messages",
    "title": "9  Computer Networks and the Internet",
    "section": "9.4 Application Layer Protocols define valid messages",
    "text": "9.4 Application Layer Protocols define valid messages\nIf we think of the TCP/IP protocol defining valid addresses, package sizes and shapes, and how the mail gets routed, then application layer protocols are one layer down – they define what are valid messages to put inside the envelope.\nThere are numerous application layer protocols. Some you will see in this book include SSH for direct server access, (S)FTP for file transfers, SMTP for email, LDAP(S) for authentication and authorization, and websockets for persistent bi-directional communication – used for interactive webapps created by the Shiny package in R and the Streamlit package in Python.\nWe’ll talk more about some of those other protocols later in the book. For now, let’s focus on the one you’ll spend most of your time thinking about – http.\n\n9.4.1 http is the most common application layer protocol\nHyptertext transfer protocol (http) is the protocol that underlies a huge fraction of internet traffic. http defines how a computer can initiate a session with a server, request the server do something, and receive a response.\nSo whenever you go to a website, http is the protocol that defines how the underlying interactions that happen as your computer requests the website and the server sends back the various assets that make up the web page, which might include the HTML skeleton for the site, the CSS styling, interactive javascript elements, and more.\n\nIt’s worth noting that these days, virtually all http traffic over the internet is in the form of secured https traffic. We’ll get into what the s means and how it’s secured in the next chapter.\n\nThere are a few important elements to http requests and responses:\n\nRequest Method – getting deep into HTTP request methods is beyond the scope of this book, but there are a variety of different methods you might use to interact with things on the internet. The most common are GET to get a webpage, POST or PUT to change something, and DELETE to delete something.\nStatus Code - each HTTP response includes a status code indicating the response category. Some special codes you’ll quickly learn to recognize are below. The one you’ll (hopefully) see the most is 200, which is a successful response.\nResponse and Request Headers – headers are metadata included with the request and response. These include things like the type of the request, the type of machine you’re coming from, cookie-setting requests and more. In some cases, these headers include authentication credentials and tokens, and other things you might want to inspect.\nBody - this is the content of the request or response.\n\nIt’s worth noting that GET requests for fetching something generally don’t include a body. Instead, any specifics on what is to be fetched are specified through query parameters, the part of the URL that shows up after the ?. They’re often something like, ?first_name=alex&last_name=gold\n\n\n\n\n9.4.2 Understand http traffic by inspecting it\nThe best way to understand http traffic is to take a close look at some. Luckily, you’ve got an easy tool – your web browser!\nOpen a new tab in your browser and open your developer tools. How this works will depend on your browser. In Chrome, you’ll go to View > Developer > Developer Tools and then make sure the Network tab is open.\nNow, navigate to a URL in your browser (say google.com).\nAs you do this, you’ll see the traffic pane fill up. These are the requests and responses going back and forth between your computer and the server.\nIf you click on any of them, there are a few useful things you can learn.\n\nAt the top, you can see the timing. This can be helpful in debugging things that take a long time to load. Sometimes it’s helpful to see what stage in the process bogs down.\nIn the pane below, you can inspect the actual content that is going back and forth between your computer and the server you’re accessing including the request methods, status codes, headers, and bodies.\n\n9.4.2.1 Special HTTP Codes\nAs you work more with http traffic, you’ll learn some of the common codes. Here’s a cheatshet for some of the most frequent you’ll see.\n\n\n\n\n\n\n\nCode\nMeaning\n\n\n\n\n200\nEveryone’s favorite, a successful response.\n\n\n3xx\nYour query was redirected somewhere else, usually ok.\n\n\n4xx and 5xx\nErrors with, respectively, the request itself and the server.\n\n\n\nParticular Error Codes\n\n\n400\nBad request. This isn’t a request the server can understand.\n\n\n401 and 403\nUnauthorized or forbidden. Often means required authentication hasn’t been provided.\n\n\n404\nNot found. There isn’t any content at the address you’re trying to access.\n\n\n504\ngateway timeout. This means that a proxy or gateway between you and the server you’re trying to access timed out before it got a response from the server."
  },
  {
    "objectID": "chapters/sec2/understanding-traffic.html#exercises",
    "href": "chapters/sec2/understanding-traffic.html#exercises",
    "title": "9  Computer Networks and the Internet",
    "section": "9.5 Exercises",
    "text": "9.5 Exercises\n#TODO"
  },
  {
    "objectID": "chapters/sec2/using-urls.html",
    "href": "chapters/sec2/using-urls.html",
    "title": "10  Getting a real URL",
    "section": "",
    "text": "```\n1.An IP address just specifies the location while a URL specifies location, protocol, and specific resource\n2.URL requires a DNS server while an IP address doesn’t\n3.URLs are unlimited while IP addresses are limited\n4.IP addresses and URLs have a one to many relationship\n\nRead more: Difference Between URL and IP Address | Difference Between http://www.differencebetween.net/technology/internet/difference-between-url-and-ip-address/#ixzz7GHcaYyk6\n```\nIn the last chapter, we talked about how network traffic knows where to go and what to do when it gets there. That’s all fine and dandy, but you noticed that we spoke almost entirely in terms of IP addresses. You probably almost never work with IP addresses. Instead, we’re used to visiting websites at universal resource locators (URLs), like google.com. What gives?\nA URL is a more complete description of how to get to a website and what do to with the traffic than just an IP address. In this chapter, we’ll discuss what a URL is and how each of the components is determined.\nA URL looks like this:\n\\[\n\\overbrace{\\text{http://}}^\\text{protocol}\\overbrace{\\text{example.com}}^\\text{server location}\\overbrace{\\text{:80}}^\\text{port}\\overbrace{\\text{/}}^\\text{resource}\n\\]\nNow, this probably isn’t what you type into your address bar in your browser. That’s because modern browsers do most of this for you by default. So if you type \\(google.com\\) into your browser’s address bar, your browser will automatically assume the correct defaults for the rest. Try going to https://google.com:443/. What do you get?"
  },
  {
    "objectID": "chapters/sec2/using-urls.html#using-ports-to-get-to-the-right-service",
    "href": "chapters/sec2/using-urls.html#using-ports-to-get-to-the-right-service",
    "title": "10  Getting a real URL",
    "section": "10.1 Using ports to get to the right service",
    "text": "10.1 Using ports to get to the right service\nLet’s say you want to run software on a server. One of the big differences between server software, and software on your laptop is that server software needs to be able to interact with the outside world to be useful.\nFor example, when you want to use Microsoft Word on your computer, you just click on the button and then it’s ready to go. But say I want to use RStudio Server. I don’t have a desktop where I click to open RStudio Server. Instead, I go to a particular URL and I expect that RStudio will be there, ready to listen."
  },
  {
    "objectID": "chapters/sec2/using-urls.html#special-ip-addresses-and-ports",
    "href": "chapters/sec2/using-urls.html#special-ip-addresses-and-ports",
    "title": "10  Getting a real URL",
    "section": "10.2 Special IP Addresses and Ports",
    "text": "10.2 Special IP Addresses and Ports\nAll ports below 1024 reserved.\n80 - HTTP default\n443 - HTTPS default\n22 - SSH default\nNormally, you’ll see a URL written something like this:\n\\[\nexample.com\n\\]\nIt doesn’t seem like this little sni\n\\[\n\\overbrace{https://}^{\\text{Protocol}}\\overbrace{\\underbrace{www}_{\\text{Subdomain}}.\\underbrace{example}_{\\text{Primary Domain}}.\\underbrace{com}_{\\text{Top-Level Domain}}}^{\\text{Domain}}/\\overbrace{engineering}^{\\text{Path}}\n\\]\nEven worse, IP addresses generally aren’t permanent – they can change when individual servers are replaced, or if you were to change the server architecture (say by adding and load-balancing a second instance – see chapter XX).\nAnatomy of a URL\nIn order to have something human-friendly and permanent, we access internet resources at uniform resource locators (URLs), like google.com, rather than an IP address."
  },
  {
    "objectID": "chapters/sec2/using-urls.html#getting-your-own-domain",
    "href": "chapters/sec2/using-urls.html#getting-your-own-domain",
    "title": "10  Getting a real URL",
    "section": "10.3 Getting your own domain",
    "text": "10.3 Getting your own domain\nIn the last chapter, we spent most of the time talking about server locations in terms of IP addresses. And it’s true – the “real” address of any server is its IP address. But we generally don’t access websites or other resources at IP addresses – they’re hard to remember, and they can also change over time.\nInstead, we generally use domains for websites, and hostnames for individual servers. We’ll get into hostnames later on – for now we’re going to focus on domains.\nA domain is simply a convenient alias for an IP address. The domain name system (DNS) is the decentralized internet phonebook that translates back and forth between domains and IP addresses. The details of how DNS resolution works are quite intricate – but the important thing to know is that there are layers of DNS servers that eventually return an IP address to your computer for where to find your website.\nFrom the perspective of someone trying to set up their own website, there’s only one DNS server that matters to you personally – the DNS server for your domain name registrar.\nDomain name registrars are the companies that actually own domains. You can buy or rent one from them in order to have a domain on the internet. So let’s say you take the data science server you set up in lab 1 and decide that you want to host it at a real domain.\nYour first stop would be a domain name registrar where you’d find an available domain you like and pull out your credit card.\nCosts for domain names vary widely. Buying a meaningless domain in a less popular top-level domain, say ladskfmlsdf.me can cost as little as $3 per year. On the other hand, buying a .com domain that’s a real word or phrase can be a few thousand dollars – and there are articles every few years about some major company accidentally allowing their domain name to lapse and ransoming it back for stupid amounts of money.\nSo, conceptually, it’s easy to understand how a domain comes to stand in for an IP address, with DNS being the directory that ties the two together.\n\n10.3.1 Configuring DNS to connect IP addresses and Domains\nThe harder part is the nitty gritty of how you accomplish that mapping yourself, which we’ll get into now.\nConfiguration of DNS is done by way of records, of which there are a menagerie of types you can configure. Luckily, most simple configurations only need CNAME and A records.\nHere’s an imaginary DNS record table for the domain example.com:\n\n\n\nPath/Host\nType\nTarget\n\n\n\n\n@\nA\n143.122.8.32\n\n\nwww\nCNAME\nexample.com\n\n\n*\nA\n143.122.8.33\n\n\n\nLet’s go through how to read this table.\nSince we’re configuring example.com, the paths/hosts in this table are relative to example.com.\nIn the first row we’re configuring an A record to go to the target IP address. A records (or their IPv6 cousin AAAA records) map a domain to an actual IP address. The path @ is a special symbol meaning exact match. So by this configuration, any traffic to example.com will be passed straight through to the specified IP address.\nThe second row deals with traffic to the www subdomain. CNAME records alias sub-domains. They’re most frequently used to map sub-domains to the main domain. Since this is a CNAME record for example.com, this record indicates that traffic to www.example.com should be treated exactly like traffic to example.com. Some domain providers do automatic redirection of www traffic, and so this row may not be necessary in some configurations.\nThe last record uses the wildcard symbol * to send all subdomain traffic that’s not already spoken for – say blog.example.com or info.example.com directly to the IP address specified. In this case, I’m sending all of those subdomains to a different IP address, maybe a 404 (not found) page – or maybe I’m serving all the subdomains off a different server.\nSo what happens is that your query goes through several layers of public DNS servers to get to the DNS entry for your domain name registrar. In many cases, you’ll directly configure your domain name registrar to point to your website or server – but you also can configure the domain name registrar to point at another set of DNS servers you actually control with an NS record.\nIf you’re setting up your own server, this probably isn’t the case, but some large enterprises do run their own private DNS servers.\n\nYou should always configure your domain provider as narrowly as possible – and you should configure your website or server first.\n#TODO: why?\n\n\n\n10.3.2 Learning to Hate DNS\nAs you get deeper into using servers, you will learn to hate DNS with a fiery passion. While it’s necessary so we’re not running around trying to remember incomprehensible IP addresses, it’s also very hard to debug as a server admin.\nLet’s say I’ve got the public domain example.com, and I’m taking down the server and putting up a new one. I’ve got to alter the public DNS record so that everyone going to example.com gets routed to the new IP address, and not the old one.\nThe thing that makes it particularly challenging is that the DNS system is decentralized. There are thousands of public DNS servers that a request could get routed to, and many of them may need updating.\nObviously, this is a difficult problem to solve, and it can take up to 24 hours for DNS changes to propagate across the network. So making changes to DNS records and checking if they’ve worked is kinda a guessing game of whether enough time has passed that you can conclude that your change didn’t work right, or if you should just wait longer.\nTo add an additional layer of complexity, DNS lookups are slow, so your browser caches the results of DNS lookups it has done before. That means that it’s possible you’ll still get an old website even once the public DNS record has been updated. If a website has ever not worked for you and then worked when you tried a private browser, DNS caching is likely the culprit. Using a private browsing window sidesteps your main DNS cache and forces lookups to happen afresh.\n\n\n10.3.3 Trying it out\nGo through hosting this book somewhere."
  },
  {
    "objectID": "chapters/sec2/using-urls.html#exercises",
    "href": "chapters/sec2/using-urls.html#exercises",
    "title": "10  Getting a real URL",
    "section": "10.4 Exercises",
    "text": "10.4 Exercises\n\nFind a cheap domain you like and buy it.\nPut an EC2 server back up with the Nginx hello-world example.\nConfigure your server to be available at your new domain.\n\nHint: In AWS, Route 53 is the service that handles incoming networking. They can serve as a domain name registrar, or you can buy a domain elsewhere and just configure the DNS using Route 53."
  },
  {
    "objectID": "chapters/sec2/using-urls.html#securing-traffic-with-https",
    "href": "chapters/sec2/using-urls.html#securing-traffic-with-https",
    "title": "10  Getting a real URL",
    "section": "10.5 Securing Traffic with https",
    "text": "10.5 Securing Traffic with https\nWhen you go to a website on the internet, you’ll see the URL prefixed by the https (though it’s sometimes hidden by your browser because it’s assumed). https is actually a mashup that is short for http with secure sockets layer (SSL).\nThese days, almost everyone actually uses the successor to SSL, transport layer security (TLS). However, because the experience of configuring TLS is identical to SSL, admins usually just talk about configuring SSL even when they mean TLS.\nThese days, almost every bit of internet traffic is actually https traffic. You will occasionally see http traffic inside private networks where encryption might not be as important – but more and more organizations are requiring end-to-end use of SSL.\nSecuring your website or server using SSL/TLS is one of the most basic things you can do to make sure your website traffic is safe. You should always configure https – full stop.\nSSL/TLS security is accomplished by configuring your site or server to use a SSL certificate (often abbreviated to cert). We’ll go through the details of how to get and configure an SSL certificate in this chapter – but first a little background on how SSL/TLS works.\n\n10.5.1 How SSL/TLS Enhances Security\nSSL accomplishes two things for you – identity validation and traffic encryption.\nWhen you go to a website, SSL/TLS is the technology that verifies that you’re actually reaching the website you think you’re reaching. This prevents something called a man-in-the-middle attack where a malicious actor manages to get in between the server and the client of network traffic. So, for example, you might think you’re putting your bank login information into your normal bank website, but there’s a hacker sitting in the middle, reading all of the traffic back and forth.\n[TODO: Image of man-in-the-middle]\nYou can see this in action in your web browser. When you go to a website protected by https, you’ll see a little lock icon to the left of the URL. That means that this website’s SSL certificate matches the website and therefore your computer can verify you’re actually at the website you mean to be at.\nBut how does your computer know what a valid SSL certificate is? Your computer has a list of trusted Certificate Authorities (CAs) who create, sell, and validate SSL/TLS certificates. So when you navigate to a website, the website sends back a digital signature. Your computer checks the signature against the indicated CA to verify that it was issued to the site in question.\n[TODO: image of SSL validation]\nThe second type of scary scenario SSL prevents is a snooping/sniffing attack. Even if you’re getting to the right place, your traffic travels through many different channels along the way – routers, network switches, and more. This means that someone could theoretically look at all your traffic along the way to its meaningful destination.\nWhen your computer gets back the digital signature to verify the site’s identity, it also prompts an exchange of encryption keys. These keys are used to encrypt traffic back and forth between you and the server so anyone snooping on your message will just see garbled nonsense and not your actual content. You can think of the SSL/TLS encryption as the equivalent of writing a message on a note inside an envelope, rather than on a postcard anyone could read along the way.\n\n\n10.5.2 Getting a cert of your own\nIn order to configure your site or server with SSL, there are three steps you’ll want to take: getting an SSL certificate, putting the certificate on the server, and making sure the server only accepts https traffic.\nYou can either buy an SSL certificate or make one yourself, using what’s called a self-signed cert.\nThere are a variety of places you can buy an SSL/TLS certificate, in many cases, your domain name registrar can issue you one when you buy your domain.\nWhen you create or buy your cert, you’ll have to choose the scope. A basic SSL certificate covers just the domain itself, formally known as a fully qualified domain name (FQDN). So if you get a basic SSL certificate for www.example.com, www.blog.example.com will not be covered. You can get a wildcard certificate that would cover every subdomain of *.example.com.\n\nNote that basic SSL/TLS certification only validates that when you type example.com in your browser, that you’ve gotten the real example.com. It doesn’t in any way validate who owns example.com, whether they’re reputable, or whether you should trust them.\nThere are higher levels of SSL certification that do validate that, for example, the company that owns google.com is actually the company Google.\n\nBut sometimes it’s not feasible to buy certificates. While a basic SSL certificate for a single domain can cost $10 per year or less, wildcard certificates will all the bells and whistles can cost thousands per year. This can get particularly expensive if you’ve got a lot of domains for some reason.\nMoreover, there are times when you can’t buy a certificate. If you’re encrypting traffic inside a private network, you will need certificates for hosts or IP addresses that are only valid inside the private network, so there’s no public CA to validate them.\nThere are two potential avenues to follow. In some cases, like inside a private network, you want SSL/TLS for the encryption, but don’t really care about the identity validation part. In this case, it’s usually possible to skip that identity validation part and automatically trust the certificate for encryption purposes.\nIt’s also possible to create your own private CA, which would verify all your SSL certificates. This is pretty common in large organizations. At some point, every server and laptop needs to have the private CA added to its set of trusted certificate validators.\nA warning: it is deceptively easy to generate and configure a self-signed SSL certificate. It’s usually just a few lines of shell commands to create a certificate, and adding the certificate to your server or website is usually just a copy/paste affair.\nHowever, it’s pretty common to run into problems with self-signed certs or private CAs. Making sure the certificate chain is correct, or running into a piece of software that doesn’t ignore the identity validation piece right is pretty common. This shouldn’t dissuade you from using SSL/TLS. It’s an essential, and basic, component of any security plan – but using a self-signed cert probably isn’t as easy as it seems.\nWhen you configure your site or server, there will likely be an option to redirect all http traffic to https traffic. If your server or site is open to the internet, you should set this option."
  },
  {
    "objectID": "chapters/sec2/private-network-config.html",
    "href": "chapters/sec2/private-network-config.html",
    "title": "11  Private Networking Configuration",
    "section": "",
    "text": "In the previous chapter, we talked about the first level of upgrades for your data science server – getting a real URL and configuring https for security. So now, the internet is knocking on the front gate of your server.\nThis chapter will be all about how to configure networking on the private siIn this chapter, we’ll talk about how to get traffic from the front door to the right apartment.\nIt wasn’t obvious, but when you set up the server in the first chapter, you also actually created a private network for the server to live in called a virtual private cloud (VPC).\nAs your server configuration gets more complicated, you’re going to put more different servers inside the virtual private cloud. So instead of thinking of your traffic as coming to the front door of your little server/cottage, I think it’s a more helpful analogy to think of coming to the guard box at the front gate of a gated community of apartment buildings.\nYou’re at the front gate, but still need to find your way to the right building and then to the right apartment inside that building.\nUp until now, we’ve been pretending like the server you put up in the first chapter was actually just on the public internet – so it makes sense to ask why we wouldn’t actually just do that.\nThe two reasons are security and reducing administrative headaches.\nThe other reason to make extensive use of private networks has to do with ease-of-configuration. Inside a private network, it’s generally ok to say “accept connections from any server inside the network”, and take care of security at the front gate of the network. Otherwise, you’ve got to be much more careful about how you configure each individual server, which is both more work and a potential security vulnerability if you don’t do it right.\nIn the case of a data science environment you may have a number of different servers you’re setting up – a server for doing data science work, a database, a package repository mirror, and a server for deploying your apps to others.\nSimilarly, because there are single points of ingress and egress (sometimes), security is heightened by having only one way in or out, as opposed to opening the entire network or each server individually to the internet.\nUsing public and private subnets is also really nice for server resiliency. Say your network was attacked with a DDOS attack, where someone tries to overwhelm your servers with junk attacks to take down your servers. It’s way better if they manage to succeed at taking down your server, but it’s just a proxy as opposed to your analytics workhorse server or a mission-critical database.\nWhen configuring servers this way, it’s very common to configure an additional server called a bastion host or jumpbox that is used solely to pass SSH traffic from server administrators through to the hosts in the private network."
  },
  {
    "objectID": "chapters/sec2/private-network-config.html#finding-the-right-server-with-hostnames",
    "href": "chapters/sec2/private-network-config.html#finding-the-right-server-with-hostnames",
    "title": "11  Private Networking Configuration",
    "section": "11.1 Finding the right server with hostnames",
    "text": "11.1 Finding the right server with hostnames\nLike we configured a URL to have an abstraction layer between our IP address and our server, we also will usually configure a hostname rather than using IP addresses."
  },
  {
    "objectID": "chapters/sec2/private-network-config.html#the-role-of-forward-and-reverse-proxies",
    "href": "chapters/sec2/private-network-config.html#the-role-of-forward-and-reverse-proxies",
    "title": "11  Private Networking Configuration",
    "section": "11.2 The role of forward and reverse proxies",
    "text": "11.2 The role of forward and reverse proxies"
  },
  {
    "objectID": "chapters/sec2/private-network-config.html#configuring-the-private-network",
    "href": "chapters/sec2/private-network-config.html#configuring-the-private-network",
    "title": "11  Private Networking Configuration",
    "section": "11.3 Configuring the private network",
    "text": "11.3 Configuring the private network\nA private network is defined by a range of IP addresses, called CIDR blocks. A CIDR block defines the sub-addresses available inside an IP address using a /n notation. So the CIDR block 10.0.0.0/16 is the largest possible CIDR block and includes all IP addresses from 10.0.0.0 to 10.255.255.255.\nThe numbers after the / are related to powers of 2 (binary something blah), so you can fit 2 /x+1 CIDR blocks in a single /x block.\nSo, for example, there are 128 IP addresses in a /25 CIDR block, and 256 in a /24 CIDR block, so 10.0.0.0/24 could have two subnets, 10.0.0.0/25 and 10.0.0.128/25.\n\n11.3.1 Restricting Outbound Connections\nIn AWS, you’ll configure egress from private subnets using a NAT Gateway. Egress from the public subnets goes through the Internet Gateway. Egress is actually needed less than you might think - in our current setup, it’s needed for package updates to RSPM (the RSPM service, not RSP to RSPM), to do google OAuth to the google servers (rstudioservices only), get stuff from github, update products, and get content from apt/yum repos.\nIn Colorado, it’s not needed for RSC because the SAML token is in the browser. It is needed for RSP because Onelogin is outside the VPC. This is generally not the case for customers, who are mostly using on-prem AD servers."
  },
  {
    "objectID": "chapters/sec2/private-network-config.html#bastion-hosts",
    "href": "chapters/sec2/private-network-config.html#bastion-hosts",
    "title": "11  Private Networking Configuration",
    "section": "11.4 Bastion Hosts",
    "text": "11.4 Bastion Hosts"
  },
  {
    "objectID": "chapters/sec2/private-network-config.html#exercises",
    "href": "chapters/sec2/private-network-config.html#exercises",
    "title": "11  Private Networking Configuration",
    "section": "11.5 Exercises",
    "text": "11.5 Exercises\nConsider going to the website google.com. Draw a diagram of how the following are incorporated: TCP/IP, DNS, HTTP, HTTPS.\nSet up a free-tier EC2 instance and put an NGINX server up. Figure out how to allow your computer to access the server, but not your phone. Try accessing it on a non-default port.\nTry to HTTP into a fresh EC2 with the default security group. Take a look at the inbound security group rules. Hint: is there an inbound rule on a default HTTP port?\nSSH into your EC2 instance and try to reach out to something on the internet (curl…). See if you can change security group rules to shut down access.\nCan you do it by changing the IP address range it’s accepting connections from?\nCan you do it by changing the listening ports?"
  },
  {
    "objectID": "chapters/sec3/auth.html",
    "href": "chapters/sec3/auth.html",
    "title": "12  Logging in with auth",
    "section": "",
    "text": "Unless you’re a special kind of nerd, you’ve probably never thought hard about auth. But you do it all the time – every time you log into your bank to check your account or open Instagram on your phone or access RStudio via your corporate Okta account, there’s auth happening under the hood.\nAuth is shorthand for two different things – authentication and authorization.\nAuthentication is the process of verifying identites. When I show up at a website that isn’t just open to the internet, how do I prove that I am who I say I am.\nAuthorization is the process of managing and checking permissions. Once you know that it’s me at the front door, am I allowed to come in?\nFor the most part, this chapter is designed to help you talk to the folks who manage auth at your organization. Unless you’ve got a small organization with only a few data scientists, you probably won’t have to manage this yourself. But it can be extremely helpful to understand how auth works when you’re trying to ask for something from the organization’s admins.\nIn order to talk concretely about logging into systems, it’s helpful to clarify some terms. For the most part, these terms are industry standard, but I’m also going to generalize some terms because they’re used for certain types of auth, and they’re really useful.\nFor the purposes of this document, we’re going to be talking about trying to login to a service. A service is something you want to login to – it could be your phone or your email, or a server, a database, or software like RStudio Server or JupyterHub.\nWhen you go to login to a service, there are two things that have to happen. First, you have to assert and verify who you are. This process is called authentication, and the assertion and proof of identity are your credentials or creds. The most common credentials are a username and password, but there are other options including SSH keys, multi-factor authentication, or biometrics.\nOnce you’ve proven who you are, then the system needs to determine what you’re supposed to have access to. This process is called authorization.\nOften, the authentication + authorization process is referred to collectively as auth.\n[#TODO: Image of Auth]\nSo, to summarize when you go to login to a service, the service reaches out to some sort of system to verify your identity. Depending on the method, it may also check your authorization. The name of that system varies by the auth method, but for the purposes of this chapter, we’ll refer to it generally as the identity store. Depending on the auth method, the identity store may store just authentication records, or both authentication and authorization."
  },
  {
    "objectID": "chapters/sec3/auth.html#the-many-flavors-of-auth-or-what-does-sso-mean",
    "href": "chapters/sec3/auth.html#the-many-flavors-of-auth-or-what-does-sso-mean",
    "title": "12  Logging in with auth",
    "section": "12.1 The many flavors of auth (or what does SSO mean?)",
    "text": "12.1 The many flavors of auth (or what does SSO mean?)\nSingle Sign On (SSO) is a slippery term, so it is almost always necessary to clarify what is meant by the term when you hear it. At some organizations, identity management isn’t centralized at all. This means that usernames and passwords are unique to each service, onboarding and offboarding of users has to be handled independently for each service, and users have to login frequently. In short, it’s often not a great system. This is never referred to as SSO.\n\nMost organizations of a meaningful size have centralized identity management. This means that identities, credentials, authorization, onboarding, and offboarding are handled centrally. However, you may still need to independently login to each system. For example, in this system, every service might take the same username and password as your credentials, but if you go to RStudio Server followed by JupyterHub, you’ll need to provide that username and password independently to each service. This system is often facilitated by PAM, and LDAP/AD. Some organizations call this SSO, because there’s only one set of credentials.\nIn true SSO, users login once and are given a token or ticket.1 Then, when they go to the next service, they don’t have to login again because that service can just look at the token or ticket to do auth for that user. For example, in this system, I could go to RStudio Server and login, and then go to JupyterHub and get in without being prompted again for my password. This type of auth is facilitated by Kerberos, SAML, or OAuth."
  },
  {
    "objectID": "chapters/sec3/auth.html#auth-techniques",
    "href": "chapters/sec3/auth.html#auth-techniques",
    "title": "12  Logging in with auth",
    "section": "12.2 Auth Techniques",
    "text": "12.2 Auth Techniques\nIf you have five data scientists in your group, and the only shared resource you have is an RStudio Server instance, you probably don’t need to think terribly hard about auth. It’s pretty straightforward to just make users on a server and give them access to everything.\nBut as organizations get larger with hundreds or thousands of users, there’s constant churn of people joining and leaving. The number of services can creep into the dozens or hundreds and people may have very different authorization levels to different services. Trying to manage auth on the individual services is a nightmare – as is trying to keep that many usernames and passwords straight for users. That is why almost all organizations with more than a few users have centrally managed auth.\n\n12.2.1 You get a permission and you get a permission!\nFor the most part, we think of people being authenticated and authorized into services. However, it’s sometimes useful to consider the broader class of entities that could do auth. There are two common non-human entities that are included in auth systems that are worth considering.\nService Accounts are accounts given to non-human entities when you want it to be able to do something on its own behalf. For example, maybe you’ve got a Shiny app that users use to visualize data that’s in a database. Very often, you don’t want the app to have the same permisions as the app’s author, or to inherit the permissions of the people viewing the app. Instead, you want the app to be able to have permissions to do certain database operations. In that case, you would create a service account to give to the Shiny app that has exactly those permissions.\nThere are also times where it’s useful to go one level up and give permissions to an entire instance or service. In that case, you might assign permissions to an instance. For example, you could make it the case that anyone who is logged into the JupyterHub server is allowed to read from the database.\nInstance permissions are rather broad, and so they are usually only applied when you’ve got multiple resources inside a private network. In that case, authentication and authorization are only done at a single point and authorization is pretty broad.\n\n\n12.2.2 Authorization is kinda hard\nFrom a management perspective, authentication is pretty simple. A person is given a set of credentials, and they have to supply those credentials when prompted to prove they are who they say they are.\nAuthorization is a whole other can of worms. There is a meaningful literature on varieties of authorization and how they work. We’re not going to get too deep into the weeds, other than to define some common terms and how they’re used.\nThe atomic basis for authorization is a permission. Permissions are a binary switch that answers the question is this person allowed to do the thing they are trying to do?2\nThe simplest way of assigning permissions is called an access control list (ACL). In systems that use ACLs, each piece of content has a list of users who are allowed access. Sometimes, ACLs are also assigned to groups, which are simply sets of users – think data-scientists.\n\nOne ACL implementation with which you may be familiar is file permissions on a Linux server. For example, if you have a Mac or are on a Linux server, you can open your terminal, navigate to a directory and do the following:\n$ ls -l\n-rwxr-xr-x   1 alexkgold  staff   2274 May 10 12:09 README.md\nThat first set of characters describes the ACL for the README.md file. The first character - indicates this is a file, as opposed to a directory of files (which would be d). Then there are three sets of 3 characters, rwx, which are short for read, write, and execute, with the first group for the owner, alexkgold, the second group for anyone else in the group staff, and the third set for anyone else.\nSo you can read -rwxr-xr-x as, this is a file that alexkgold can read, write or execute, and anyone else can read or execute, but not edit.\nACLs are pretty intuitive, but it turns out that when you are managing a lot of users across a lot of files, directories, and services, they can get pretty difficult to manage, so many organizations use Role Based Access Control (RBAC).\nRBAC adds a layer of abstraction between users and permissions, which makes it a little harder to understand, but ultimately results in a much more flexible system. In RBAC, permissions are not assigned to individual pieces of content or to users or groups. Instead, permissions are assigned to roles, and roles are given to users or groups.3\n\nThere are also further iterations on the RBAC model, like Attribute Based Access Control (ABAC) or Policy Based Access Control (PBAC) in which there’s a long list of attributes that could be considered for a user to compute their permissions for a given service."
  },
  {
    "objectID": "chapters/sec3/auth.html#auth-technologies",
    "href": "chapters/sec3/auth.html#auth-technologies",
    "title": "12  Logging in with auth",
    "section": "12.3 Auth Technologies",
    "text": "12.3 Auth Technologies\n\n12.3.1 Username + Password\nMany pieces of software come with integrated authentication. When you use those system, the product stores encrypted username and password pairs in a database.\nThese setups are often really easy from an admin perspective – you just set up individual users on the server. However, the flip side is that users have one more username and password to remember, which is annoying for them. Moreover, if you have more than a few users, or the system is one of more than a few, it’s hard to manage users on a lot of different systems. It can be a real pain to create accounts on a ton of different systems when a new person joins the organization, or to remove their permissions one-by-one when they leave.\nFor this reason, most IT/Admin organizations strongly prefer using some sort of centralized identity store.\n\n\n12.3.2 PAM\nPluggable Authentication Modules (PAM) is a Linux system for doing authentication. As of this writing, PAM is the default authentication method for both RStudio Server and JupyterHub.\n\nConceptually PAM is pretty straightforward. You install a service on a Linux machine and configure it to use PAM authentication from the underlying host. By default, PAM just authenticates against the users configured on the Linux server, but it can also be configured to use other sorts of “modules” to authenticate against other systems – most commonly LDAP/AD or Kerberos. PAM can also be used to do things when users login – the most common being initializing tokens or tickets to other systems, like a database.\nPAM is often paired with System Security Services Daemon (SSSD), which is most commonly used to automatically create Linux users on a server based on the identities stored in an LDAP/AD instance.\nThough conceptually simple, reading, writing, and managing PAM modules is kinda painful.\n#TODO: Add PAM example\n\n\n12.3.3 LDAP/AD\nLightweight Directory Access Protocol (LDAP) is a relatively old, open, protocol used for maintaining a set of entities and their attributes. To be precise, LDAP is actually a protocol for maintaining and accessing entities and their attributes in a tree. It happens that this is a really good structure for maintaining permissions and roles of users at an organization, and it’s the main thing LDAP is used for.\nActive Directory (AD) is Microsoft’s implementation of LDAP, and is by-far the most common LDAP “flavor” out there. AD so thoroughly owns the LDAP enterprise market, that LDAP is often referred to as LDAP/AD. There are other implementations you may run across, the most common being OpenLDAP.\n\nAzure, Microsoft’s cloud provider, offers an authentication service called Azure Active Directory (AAD). Confusingly, AAD is usually used in combination with SAML, not LDAP.\n\nIt’s worth distinguishing the use of LDAP as an identity store from its use as an authentication technology. As a tree-based database, LDAP is uniquely well-suited to storing the identities, and other attributes of people at the organization. However, as discussed below, using LDAP to authenticate into actual services has security and convenience drawbacks, and many organizations consider it outdated and insecure.\nA lot of organizations are moving away from LDAP for authentication in favor of token-based technologies like SAML or OAuth, but many are keeping LDAP as their identity “source of truth” that is referenced by the SAML or OAuth Identity Provider.\nLDAP has three main disadvantages relative to other technologies. First, LDAP requires that your credentials (username and password, usually) actually be provided to the service you’re trying to use. This is fundamentally insecure relative to a system where your credentials are provided only to the identity provider, and the service just gets a token verifying who you are. In token-based systems, adding additional requirements like MFA or biometrics are easy, as they’re simply added at the IdP layer. In contrast, doing those things in LDAP would require the service to implement them, which usually is not the case, so you’re usually limited to username and password.\nThe second disadvantage of LDAP is that it does not allow for central administration of permissions. LDAP directly records only objects and their attributes. Say, for example, you want only users of a particular group to have access to a certain resource. In LDAP, you would have to specify in that resource that it should only allow in users of that group. This is in contrast to SAML/OAuth, where the authorization is centrally managed.\nLastly, LDAP authentication is based on each service authenticating. Once you authenticate, the service might give you a cookie so that your login persists, but there is no general-purpose token that will allow you to login to multiple services.\n\n12.3.3.1 How LDAP Works\nWhile the technical downsides of LDAP are real, the technical operations of LDAP are pretty straightforward. In short, you try to login to a service, the service collects your username and password, sends it off to the LDAP server, and checks that your username and password are valid.\n\nNote that LDAP is purely for authentication. When you’re using LDAP, authorization has to be handled separately, which is one of the disadvantages.\n\n\n12.3.3.2 Deeper Than You Need on LDAP\nLDAP is a tree-based entity and value store. This means that LDAP stores things and their attributes, which include a name and one or more values. For example, my entry in a corporate LDAP directory might look like this:\ncn: Alex Gold\nmail: alex.gold@example.com\nmail: alex.gold@example.org\ndepartment: solutions\nmobile: 555-555-5555\nobjectClass = Person\nMost of these attributes should be pretty straightforward. cn is short for common name, and is part of the way you look up an entity in LDAP (more on that below). Each entity in LDAP must have an objectClass, which determines the type of entity it is. In this case, I am a Person , as opposed to a device, domain, organizationalRole, or room – all of which are standard objectClasses.\nLet’s say that your corporate LDAP looks like the tree below:\n\n#TODO: make solutions an OU in final\nThe most common way to look up LDAP entities is with their distinguished name (DN), which is the path of names from the point you’re starting all the way back to the root of the tree. In the tree above, my DN would be cn=alex,ou=solutions,dc=example,dc=com.\nNote that you read the DN from right to left to work your way down the tree. Aside from cn for common name, other common fields include ou for organizational unit, and dc for domain component.\n\n\n12.3.3.3 Trying out LDAP\nNow that we understand in theory how LDAP works, let’s try out an actual example.\nTo start, let’s stand up LDAP in a docker container:\n#TODO: update ldif\ndocker network create ldap-net\ndocker run -p 6389:389 \\\n  --name ldap-service \\\n  --network ldap-net \\\n  --detach alexkgold/auth\nldapsearch is a utility that lets us run queries against the LDAP tree. Let’s try it out against the LDAP container we just stood up.\nLet’s say I want to return everything in the subtree under example.org. In that case, I would run ldapsearch -b dc=example,dc=org, where b indicates my search base, which is a dn. But in order to make this actually work, we’ll need to include a few more arguments, including\n\nthe host where the LDAP server is, indicated by -H\nthe bind DN we’ll be using, flagged with -D\nthe bind password we’ll be using, indicated by -w\n\nSince we’re testing, we’re also going to provide the flag -x to use whatever certificate is present on the server. Putting it altogether, along with the commands to reach the docker container, let’s try:\nldapsearch -x -H ldap://localhost:6389 -b dc=example,dc=org -D \"cn=admin,dc=example,dc=org\" -w admin\n\n# extended LDIF\n#\n# LDAPv3\n# base <dc=example,dc=org> with scope subtree\n# filter: (objectclass=*)\n# requesting: ALL\n#\n\n# example.org\ndn: dc=example,dc=org\nobjectClass: top\nobjectClass: dcObject\nobjectClass: organization\no: Example Inc.\ndc: example\n\n# admin, example.org\ndn: cn=admin,dc=example,dc=org\nobjectClass: simpleSecurityObject\nobjectClass: organizationalRole\ncn: admin\ndescription: LDAP administrator\nuserPassword:: e1NTSEF9d3IyVFp6SlAyKy9xT2RsQ0owTDYzR0RzNFo0NUFrQ00=\n\n# search result\nsearch: 2\nresult: 0 Success\n\n# numResponses: 3\n# numEntries: 2\nYou should be able to read what got returned pretty seamlessly. One thing to notice is that the user password is returned, so it can be compared to a password provided. It is encrypted, so it doesn’t appear in plain text.\nNote that ldap is a protocol – so it takes the place of the http you’re used to in normal web operations. Like there’s https, there is also a protocol called LDAPS, which is ldap + tls for the same reason you’ve got https. LDAP is (almost) always running in the same private network as the service, so many organizations don’t require using LDAPS, but others do require it.\nRunning the ldapadmin\ndocker run -p 6443:443 \\\n        --name ldap-admin \\\n        --env PHPLDAPADMIN_LDAP_HOSTS=ldap-service \\\n        --network ldap-net \\\n        --detach osixia/phpldapadmin\ndn for admin cn=admin,dc=example,dc=org pw: admin\nhttps://localhost:6443\n# Replace with valid license\nexport RSC_LICENSE=XXXX-XXXX-XXXX-XXXX-XXXX-XXXX-XXXX\n\n# Run without persistent data and using default configuration\ndocker run -it --privileged \\\n    --name rsc \\\n    --volume $PWD/rstudio-connect.gcfg:/etc/rstudio-connect/rstudio-connect.gcfg \\\n    -p 3939:3939 \\\n    -e RSC_LICENSE=$RSC_LICENSE \\\n    --network ldap-net \\\n    rstudio/rstudio-connect:latest\n\n\n12.3.3.4 Single vs Double Bind\nThere are two different ways to establish a connection between your server and the LDAP server. The first method is called Single Bind. In a single bind authentication, the user credentials are used both to authenticate to the LDAP server, and to query the server.\nIn a Double Bind configuration, there is a separate administrative service account, used to authenticate to the LDAP server. Once authentication is complete, then the user is queried in the system.\nSingle bind configurations are often more limited than double bind ones. For example, in most cases you’ll only be able to see the single user as well as the groups they’re a part of. This can limit application functionality in some cases. On the other hand, there need be no master key maintained on your server, and some admins may prefer it for security reasons.\nWe can see this really concretely. In the example above, you used a double bind by supplying admin credentials to LDAP. Let’s say instead, you just provide a single user’s credentials. In that case, I don’t get anything back if I just do a general search.\nldapsearch -x -H ldap://localhost:6389 -b dc=example,dc=org -D \"cn=joe,dc=engineering,dc=example,dc=org\" -w joe                                       \n# extended LDIF\n#\n# LDAPv3\n# base <dc=example,dc=org> with scope subtree\n# filter: (objectclass=*)\n# requesting: ALL\n#\n\n# search result\nsearch: 2\nresult: 32 No such object\n\n# numResponses: 1\nBut just searching for information about Joe does return his own information.\nldapsearch -x -H ldap://localhost:6389 -b cn=joe,dc=engineering,dc=example,dc=org -D \"cn=joe,dc=engineering,dc=example,dc=org\" -w joe                    32 ✘\n# extended LDIF\n#\n# LDAPv3\n# base <cn=joe,dc=engineering,dc=example,dc=org> with scope subtree\n# filter: (objectclass=*)\n# requesting: ALL\n#\n\n# joe, engineering.example.org\ndn: cn=joe,dc=engineering,dc=example,dc=org\ncn: joe\ngidNumber: 500\ngivenName: Joe\nhomeDirectory: /home/joe\nloginShell: /bin/sh\nmail: joe@example.org\nobjectClass: inetOrgPerson\nobjectClass: posixAccount\nobjectClass: top\nsn: Golly\nuid: test\\joe\nuidNumber: 1000\nuserPassword:: e01ENX1qL01raWZrdk0wRm1sTDZQM0MxTUlnPT0=\n\n# search result\nsearch: 2\nresult: 0 Success\n\n# numResponses: 2\n# numEntries: 1\n\n\n\n12.3.4 Kerberos Tickets\nKerberos is a relatively old ticket-based auth technology. In Kerberos, encrypted tickets are passed around between servers. Because these tickets live entirely on servers under the control of the organization, they are generally quite secure.\nThough Kerberos is freely available, it was widely adopted along with Active Directory, and it’s used almost exclusively in places that are running a lot of Microsoft products. A frequent use of Kerberos tickets is to establish database connections.\nBecause the tickets are passed around from server to server, Kerberos can be used to create a true SSO experience for users.\n\n12.3.4.1 How Kerberos Works\nAll of Kerberos works by sending information to and from the central Kerberos Domain Controller (KDC). In Kerberos, authentication and authorization are handled independently.\nWhen a Kerberos session is initialized, the service sends the users credentials off to the KDC and requests something called the Ticket Granting Ticket (TGT) from the KDC. TGTs have a set expiration period. When they expire, the client has to request an updated TGT. This is one reason why Kerberos is considered quite secure - even if someone managed to steal a TGT, they’d only be able to use it for a little while before it went stale and could be revoked.\nWhen the user wants to actually do something, they send the TGT back to the KDC again and get a session key (sometimes referred to as a service ticket) that allows access to the service, usually with a specified expiration period.\n\n\n\n12.3.4.2 Try out Kerberos\n#TODO\n\n\n\n12.3.5 SAML\nThese days Security Assertion Markup Language (SAML) is probably the most common system that provides true SSO – including single login and centrally-managed permissions. SAML does this by passing around XML tokens.4\nThe way this generally works is that a user attempts to login to a Service Provider (SP). The SP redirects the user to an Identity Provider (IdP), which checks either for a preexisting token in the users browser, or verifies the users credentials. The IdP checks for the user’s authorization to access the SP in question, and sends an authorization token back to the SP.\n\nRelative to LDAP/AD, which is from the early 1990s, SAML is a new kid on the block. SAML 1.0 was introduced in 2002, and SAML 2.0, which is the current standard, came out in 2005. Many large enterprises are switching their systems over to use SAML or have already done so.\nOne superpower of SAML IdPs is that many of them can federate identity management to other systems. So, it’s pretty common for large enterprises to maintain their user base in one or more LDAP/AD system, but actually use a SAML IdP to do authentication and authorization. In fact, this is what Azure Active Directory (AAD), which is Microsoft Azure’s hosted authentication offering does. It is possible to use LDAP/AD with AAD, but most organizations use it with SAML.\nOne of the nice things about SAML is that credentials are never shared directly with the SP. This is one of the ways in which SAML is fundamentally more secure than LDAP/AD – the users credentials are only ever shared with the IdP.\nThere are two different ways logins can occur – starting from the SP, or starting from the IdP.\nIn SAML, the XML tokens that are passed back and forth are called assertions.\n\n12.3.5.1 Try SAML\nWe’re going to use a simple SAML IdP to try out SAML a bit. This container only supports a single SP. Any IdP that might be used in an enterprise environment is going to support many SPs simultaneously.\nLet’s go through the environment variables we’re providing to this docker run command. We’re providing three different arguments:\n\nThe SP_ENTITY_ID is the URL of the\nSP_ASSERTION_CONSUMER_SERVICE is the URL of the SP that is prepared to receive the authorized responses coming back from the SAML IdP.\nSP_SINGLE_LOGOUT_SERVICE is the URL where the SP will receive a logout command once someone has been logged out at the IdP level. Many SPs do not implement single logout.\n\ndocker run --name=saml_idp \\\n-p 8080:8080 \\\n-p 8443:8443 \\\n-e SIMPLESAMLPHP_SP_ENTITY_ID=http://app.example.com \\\n-e SIMPLESAMLPHP_SP_ASSERTION_CONSUMER_SERVICE=http://localhost/simplesaml/module.php/saml/sp/saml2-acs.php/test-sp \\\n-e SIMPLESAMLPHP_SP_SINGLE_LOGOUT_SERVICE=http://localhost/simplesaml/module.php/saml/sp/saml2-logout.php/test-sp \\\n-d kristophjunge/test-saml-idp:1.15\nhttp://localhost:8080/simplesaml\nadmin/secret\n\n\n\n12.3.6 OIDC/OAuth2.0\nOIDC/OAuth is slightly newer than SAML, created in 2007 by engineers at Google and Twitter. OAuth 2.0 – the current standard was released in 2012. If you’re being pedantic, OAuth is a authorization protocol, and OpenID Connect (OIDC) is an authorization protocol that uses OAuth. In most cases, people will just call it OAuth.\n\n#TODO: this picture is bad\nIn an enterprise context, OAuth/OIDC is conceptually very similar to SAML – but instead of passing around XML tokens, it’s based on JSON Web Tokens (JWT, usually pronounced “jot”).\n\nThe fact that JSON is much more human-readable than XML is one of the big advantages of OIDC/OAuth compared to SAML.\n\n#TODO: try it out\n\n12.3.6.1 OAuth/OIDC vs SAML\nFrom a practical perspective, the biggest difference between OAuth/OIDC and SAML is that SAML is quite strict about what SPs are allowed. Each SP needs to be registered at a specific web address that the IdP knows it’s allowed to receive requests from.\nIn contrast, OAuth/OIDC was designed to be used to delegate authentication and authorization to different kinds of services that might be widely available on the internet. If you’ve ever allowed a website to Login with Apple/Google/Facebook/Github, that has been an application of OAuth/OIDC.\nBecause the set of allowable SPs is fixed under SAML, it’s more common in enterprise settings. Some admins consider SAML more secure for that reason as well.\nIn some situations, SAML is used for authentication and OAuth is used for access to other services. Most commonly in the data science world, this can come up when a user logs into a service like RStudio Server and is then authorized to a database using an OAuth JWT.\nResources: https://www.okta.com/identity-101/saml-vs-oauth/ https://www.okta.com/identity-101/whats-the-difference-between-oauth-openid-connect-and-saml/ https://phoenixnap.com/blog/kerberos-authentication https://www.dnsstuff.com/rbac-vs-abac-access-control"
  },
  {
    "objectID": "chapters/sec3/offline.html",
    "href": "chapters/sec3/offline.html",
    "title": "13  Offline",
    "section": "",
    "text": "This makes things hard.\nUnderstanding whether it’s inbound connections that are disallowed or both outbound + inbound is important.\nDifficulties that tend to arise:\n\nDownloading software updates\nGetting R/Python packages\nSoftware licensing (often reach to license servers)"
  },
  {
    "objectID": "chapters/sec3/scaling.html",
    "href": "chapters/sec3/scaling.html",
    "title": "14  Scaling",
    "section": "",
    "text": "At some point, your data science environment may reach the stage where you need to start thinking about how you’re going to scale your environment out to accommodate more users – or sometimes this will come up in the context of disaster recovery or maintaining service uptime.\nHopefully, if you’re at this level of complexity, you’ve got someone in the IT/Admin organization to help you. This chapter is mostly going to be focused around the conceptual components of how scaling works, and how to talk about them. We’re not going to get deep into the weeds on how to configure them.\nThere are two main types of scaling that get discussed. This is an example where the language definitely impedes understanding rather than furthering it.\nVertical scaling is just a fancy way of saying making a server bigger. So maybe you’re running RStudio Server or JupyterHub on a server of a particular size. Vertically scaling that server just means making the server itself bigger. If you’re running your own server, this is a huge pain. You’ve got to buy and configure a new server, and switch over. The ability to quickly vertically scale hardware is one of the best things about the cloud. Taking a server down, transferring the attached volume to a new server of a different size, and putting it back up takes just a few minutes.\nVertical scaling in the cloud is great – but there are limits. ]For example, the AWS C line of instances instances are their “compute optimized” instances that have fast CPUs and are good for general-purpose data science workloads. Generally, AWS scales their EC2 instances linearly in terms of the number of CPU cores offered on the instance, but across most reasonably-priced instance types, the instance sizes max out at 96-128 cores these days. That’s probably sufficient for many workloads, but if you’ve got an RStudio Server with 50 concurrent users doing reasonably heavy compute loads, that can quickly get eaten up.\nHorizontal scaling means distributing the workload across multiple servers or machines. It is almost always more complicated than it seems like it should be, and more complicated than you want it to be. Horizontal scaling is often referred to as load balancing.\nTODO: Image of vertical + horizontal scaling\nSometimes horizontal scaling is undertaken for pure scaling purposes, but sometimes it’s undertaken for cluster resilience purposes. For example, you might want the cluster to be resilient to a node randomly failing, or being taken down for maintenance. In this context, horizontal scaling is often called high availability. Like many other things, high availability is a squishy term, and different organizations have very different definitions of what it means.\nFor example, in one organization, high availability might just mean that there’s a robust disaster recovery plan so servers can be brought back online with little data loss. In another organization, high availability might mean having duplicate servers that aren’t physically colocated to avoid potential outages due to server issues or natural disasters. In other contexts, it might be a commitment to a particular amount of uptime.1\nSpannning Multiple AZs\nAs the requirements for high availability get steeper, the engineering cost to make sure the service really is that resilient rise exponentially…so be careful how much uptime you’re trying to achieve."
  },
  {
    "objectID": "chapters/sec3/scaling.html#container-deployment-orchestration",
    "href": "chapters/sec3/scaling.html#container-deployment-orchestration",
    "title": "14  Scaling",
    "section": "14.1 Container Deployment + Orchestration",
    "text": "14.1 Container Deployment + Orchestration\nOne tool that comes up increasingly frequently when talking about scaling is Kubernetes (sometimes abbreviated as K8S).4 Kubernetes is the way people orchestrate Docker containers in production settings.5 So basically that it’s the way to put containers into production when you want more than one to interact – say you’ve got an app that separately has a database and a front end in different containers, or, like in this chapter, multiple load-balanced instances of the same containers.\nWhile the operational details of Kubernetes are very different from the horizontal scaling patterns we’ve discussed so far in this chapter, the conceptual requirements are the same.\nTODO: Diagram of K8S\nMany people like Kubernetes because of its declarative nature. If you recall from the section on Infrastructure as Code, declarative code allows you to make a statement about what the thing is you want and just get it, instead of specifying the details of how to get there.\nOf course, in operation this all can get much more complicated, but once you’ve got the right containers, Kubernetes makes it easy to say, “Ok, I want one instance of my load balancer container connected to three instances of my compute container with the same volume connected to all three.”\n\n\n\n\n\n\nKubernetes Tripwire!\n\n\n\nIf you’re reading this and are extremely excited about Kubernetes – that’s great! Kubernetes does make a lot of things easy that used to be hard. Just know, networking configuration is the place you’re likely to get tripped up. You’ve got to deal with networking into the cluster, networking among the containers inside the cluster, and then networking within each container.\nComplicated kubernetes networking configurations are not for the faint of heart.\n\n\nFor individual data scientists, Kubernetes is usually overkill for the type of work you’re doing. If you find yourself in this territory, it’s likely you should try to work with you organization’s IT/Admin group.\nOne of the nice abstraction layers Kubernetes provides is that in Kubernetes, you provide declarative statements of the containers you want to run, and any requirements you have. You separately register actual hardware with the cluster, and Kubernetes takes care of placing the conatiners onto the hardware depending on what you’ve got available.\nIn practice, unless you’re part of a very sophisticated IT organization, you’ll almost certainly use Kubernetes via one of the cloud providers’ Kubernetes clusters as a service. AWS’s is called Elastic Kubernetes Service (EKS).6\nOne really nice thing about using these Kubernetes clusters as a service is that adding more compute power to your cluster is generally as easy as a few button clicks. On the other hand, that also makes it dangerous from a cost perspective.\nIt is possible to define a Kubernetes cluster “on the fly” and deploy things to a cluster in an ad hoc way. I wouldn’t recommend this for any production system. Helm is the standard tool for defining kubernetes deployments in code, and Helmfile is a templating system for Helm.\nSo, for example, if you had a standard “Shiny Server” that was one load balancer containers, two containers each running a Shiny app, and a volume mounted to both, you would define that cluster in Helm. If you wanted to be able to template that Helm code for different clusters, you’d use Helmfile."
  },
  {
    "objectID": "chapters/sec3/scaling.html#exercises",
    "href": "chapters/sec3/scaling.html#exercises",
    "title": "14  Scaling",
    "section": "14.2 Exercises",
    "text": "14.2 Exercises\nTODO"
  },
  {
    "objectID": "chapters/append/env-req.html",
    "href": "chapters/append/env-req.html",
    "title": "Appendix A — Determining What You Need",
    "section": "",
    "text": "Determining Requirements for your platform Platform requirements combination of Work being done on the platform Data Science needs (i.e. horsepower) Existing state of org’s data Type of work data scientists might do Batch Jobs ETL Reports Data Generation Real-Time Interactive Models APIs Apps Data Science + IT Needs Languages (R, Python, SQL) Sharing – internal, external Online or offline? Data Security – row based access? Centralized workbench vs individual workstations Existing Assets BI/Analytics Platforms Data Connections Checklist of above"
  },
  {
    "objectID": "chapters/append/terminal.html",
    "href": "chapters/append/terminal.html",
    "title": "Appendix B — Getting Started with Terminal",
    "section": "",
    "text": "windows WSL Powershell PuTTY – no longer needed, but still an option Do need to enable SSH client\ntmux vim/nano"
  },
  {
    "objectID": "chapters/append/linux-cmd.html",
    "href": "chapters/append/linux-cmd.html",
    "title": "Appendix C — Useful Shell Commands",
    "section": "",
    "text": "Unlike on your Windows or Mac desktop, most work on a server is done via the shell – an interactive text-based prompt. If you master the set of commands below, you’ll always feel like a real hacker.\nIn most cases, you’ll be using the bash shell. If you’re using Linux or Mac, that’ll be the default. Below, I’m intentionally mixing up bash commands and Linux system commands because they’re useful. If you know the difference and are pedantic enough to care, this list isn’t for you anyway."
  },
  {
    "objectID": "chapters/append/linux-cmd.html#miscellaneous-symbols",
    "href": "chapters/append/linux-cmd.html#miscellaneous-symbols",
    "title": "Appendix C — Useful Shell Commands",
    "section": "C.1 Miscellaneous Symbols",
    "text": "C.1 Miscellaneous Symbols\n\n\n\n\n\nSymbol\n\n\n\n\nWhat it is\n\n\n\n\nHelpful options\n\n\n\n\nExample\n\n\n\n\n\n\n\n\n/\n\n\n\n\nsystem root\n\n\n\n\n\n\n\n\n\n\n~\n\n\n\n\nyour home directory\n\n\n\n\n\n\necho ~\n\n\n/ home/alex.gold\n\n\n\n\n\n\n.\n\n\n\n\ncurrent working directory\n\n\n\n\n\n\n\n\n\n\nman\n\n\n\n\nmanual\n\n\n\n\n\n\n\n\n\n\n|\n\n\n\n\nthe pipe\n\n\n\n\n\n\n\n\n\n\necho\n\n\n\n\n\n\n\n\n\n\n\n\n$\n\n\n\n\n\n\n\n\n\n\n\n\nsudo\n\n\n\n\n\n\n\n\n\n\n\n\nsu"
  },
  {
    "objectID": "chapters/append/linux-cmd.html#moving-yourself-and-your-files",
    "href": "chapters/append/linux-cmd.html#moving-yourself-and-your-files",
    "title": "Appendix C — Useful Shell Commands",
    "section": "C.2 Moving yourself and your files",
    "text": "C.2 Moving yourself and your files\n\n\n\n\n\nC ommand\n\n\n\n\nWhat it does\n\n\n\n\nHelpful options\n\n\n\n\nExample\n\n\n\n\n\n\n\n\npwd\n\n\n\n\nprint working directory\n\n\n\n\n\n\n$ pwd\n\n\n/U sers/alex.gold/\n\n\n\n\n\n\ncd\n\n\n\n\nchange directory\n\n\n\n\n\n\n$ cd ~/Documents\n\n\n\n\n\n\nls\n\n\n\n\nlist\n\n\n\n\n-l - format as list\n\n\n-a - all include hidden files\n\n\n\n\n$ ls .\n\n\n$ ls -la\n\n\n\n\n\n\nrm\n\n\n\n\nremove delete permanently!\n\n\n\n\n-r - recursively a directory and included files\n\n\n-f - force - don’t ask for each file\n\n\n\n\n$ rm old_doc\n\n\nr m -rf old_docs/\n\n\nBE VERY CAREFUL WITH -rf\n\n\n\n\n\n\ncp\n\n\n\n\ncopy\n\n\n\n\n\n\n\n\n\n\nmv\n\n\n\n\nmove\n\n\n\n\n\n\n\n\n\n\nchmod"
  },
  {
    "objectID": "chapters/append/linux-cmd.html#checking-out-files",
    "href": "chapters/append/linux-cmd.html#checking-out-files",
    "title": "Appendix C — Useful Shell Commands",
    "section": "C.3 Checking out Files",
    "text": "C.3 Checking out Files\nOften useful in server contexts for reading log files.\n\n\n\nC ommand\nWhat it does\nHelpful options\nExample\n\n\n\n\ncat\n\n\n\n\n\nless\n\n\n\n\n\ntail\n\n-f\n\n\n\ngrep\n\n\n\n\n\ntar"
  },
  {
    "objectID": "chapters/append/linux-cmd.html#checking-out-server-activity",
    "href": "chapters/append/linux-cmd.html#checking-out-server-activity",
    "title": "Appendix C — Useful Shell Commands",
    "section": "C.4 Checking out Server Activity",
    "text": "C.4 Checking out Server Activity\n\n\n\nC ommand\nWhat it does\nHelpful options\nExample\n\n\n\n\ndf\n\n-h\n\n\n\ntop\n\n\n\n\n\nps\n\n\n\n\n\nlsof"
  },
  {
    "objectID": "chapters/append/linux-cmd.html#checking-out-networking",
    "href": "chapters/append/linux-cmd.html#checking-out-networking",
    "title": "Appendix C — Useful Shell Commands",
    "section": "C.5 Checking out Networking",
    "text": "C.5 Checking out Networking\n\n\n\nC ommand\nWhat it does\nHelpful options\nExample\n\n\n\n\nping\n\n\n\n\n\nne tstat\n\n\n\n\n\ncurl"
  },
  {
    "objectID": "chapters/append/linux-cmd.html#user-management",
    "href": "chapters/append/linux-cmd.html#user-management",
    "title": "Appendix C — Useful Shell Commands",
    "section": "C.6 User Management",
    "text": "C.6 User Management\n\n\n\nC ommand\nWhat it does\nHelpful options\nExample\n\n\n\n\nw hoami\n\n\n\n\n\np asswd\n\n\n\n\n\nus eradd"
  }
]