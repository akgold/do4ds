[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DevOps for Data Science",
    "section": "",
    "text": "This is the website for the book DevOps for Data Science, currently in draft form.\n\n\n\n\n\n\nWarning\n\n\n\nThis book is very much still a work in progress.\nContent is likely to move from section to section and chapter to chapter.\nLinks are likely to break.\nThanks for your patience.\n\n\nIn this book, you’ll learn about DevOps conventions, tools, and practices that can be useful to you as a data scientist. You’ll also learn how to work better with the IT/Admin team at your organization, and even how to do a little server administration of your own if you’re pressed into service.\nThis website is (and always will be) free to use, and is licensed under the Creative Commons Attribution-NonCommercial-NoDerivs 3.0 license. If you’d like a physical copy of the book, they will be available once it’s finished!\n\n\nI used the knitr package [@xie2015] and the quarto package [@quarto] to compile my book.\nPackage names are in bold text (e.g., rmarkdown), and inline code and filenames are formatted in a typewriter font (e.g., knitr::knit('foo.Rmd')). Function names are followed by parentheses (e.g., bookdown::render_book()).\n\n\n\nAlex Gold leads the Solutions Engineering team at RStudio.\nHe works with customers of RStudio’s professional software to help them deploy, configure, and use RStudio’s professional software and open source tooling in R and Python.\nIn his free time, he enjoys landscaping, handstands, and Tai Chi.\nHe occasionally blogs about data, management, and leadership at alexkgold.space.\n\n\n\nA lot of people are helping me write this book.\nThis book is published to the web using GitHub Actions from rOpenSci.\n\n\n\nTea Green: CAFFDO Steel Blue: 3E7CB1 Kombu Green: 273c2c Bright Maroon: B33951 Sandy Brown: FCAA67"
  },
  {
    "objectID": "chapters/intro.html",
    "href": "chapters/intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Data science itself is pretty useless.\nIt’s likely you became a data scientist because you love creating beautiful charts, minimizing model prediction error, or writing elegant code in R or Python.\nUltimately – and perhaps frustratingly – these things don’t matter. What matters is whether the output of your work is useful in affecting decisions at your organization or in the broader world.\nThat means you’re going to have to share your work by putting it in production.\nMany data scientists think of in production as some mythical state of super computers running ultra-complex machine learning models running over dozens of shards of data, terrabytes each. It definitely occurs on a misty mountaintop, and does not involve the google sheets, csv files, or half-baked database queries you probably wrangle every day.\nBut that’s wrong. If you’re a data scientist and you’re trying to put your work in front of someone else’s eyes, you’re in production. And if you’re in production, this book is for you.\nYou may sensibly be asking who I am to make such a proclamation.\nAs of this writing, I’ve been on the Solutions Engineering team at RStudio (soon to be Posit) for nearly four years. The Solutions Engineering team at RStudio helps users of our open source and professional tools understand how to deploy, install, configure, and use RStudio’s Professional Products.\nAs such, I’ve spoken with hundreds of organizations managing data science in production about what being in production means for them, and how to make their production systems for developing and sharing data science products more robust – both with RStudio’s Professional Products and using purely open source tooling.\nFor some orgaizations, in production means a report that will get emailed around at the end of each week. For others, it will mean hosting a live app or dashboard that people visit. For others, it means serving live predictions to another service from a machine learning model.\nRegardless of the actual character of the data science products, organizations are universally concerned with making these assets reliable, reproducible, and performant (enough).\nAt RStudio, the Solutions Engineering team is most directly responsible for engaging with the IT/Admin organizations at our customers. So that’s what this book is about – all of the stuff that is not data science that it takes to deploy a data science asset into production."
  },
  {
    "objectID": "chapters/intro.html#a-short-history-of-devops",
    "href": "chapters/intro.html#a-short-history-of-devops",
    "title": "Introduction",
    "section": "A short history of DevOps",
    "text": "A short history of DevOps\nHere’s the one sentence definition: DevOps is a set of cultural norms, practices, and supporting tooling to help make the process of developing and deploying software smoother and lower risk.\nIf you feel like that definition is pretty vague and unhelpful, you’re right. Like Agile software development, to which it is closely related, DevOps is a squishy concept. That’s partially because DevOps isn’t just one thing – it’s the application of some principles and process ideas to whatever context you’re actually working in. That malleability is one of the great strengths of DevOps, but it also makes the concept quite squishy.\nThis squishiness is furthered by the ecosystem of companies enabling DevOps. There are dozens and dozens of companies proselytizing their own particular flavor of DevOps – one that (curiously) reflects the capabilities of whatever product they’re selling.\nBut underneath the industry hype and the marketing jargon, there are some extremely valuable lessons to take from the field.\nTo understand better, let’s go back to the birth of the field.\nThe Manifesto for Agile Software Development was originally published in 2001. Throughout the 1990s, software developers had begun observing that delivering software in small units, quickly collecting feedback, and iterating was an effective model. After that point, many different frameworks of actual working patterns were developed and popularized.\nHowever, many of these frameworks were really focused on software development. What happened once the software was written?\nHistorically, IT Administrators managed the servers, networking, and workstations needed to deploy, release, and operate that software. So, when an application was complete (or perceived as such), it was hurled over the wall from Development to Operations. They’d figure out the hardware and networking requirements, check that it was performant enough, and get it going in the real world.\nThis pattern is very fragile and subject to many errors, and it quickly became apparent that the Agile process – creating and getting feedback on small, iterative changes to working software – needed a complementary process to get that software deployed and into production.\nDevOps arose as this discipline – a way for software developers and the administrators of operational software to better collaborate on making sure the software being written was making it reliably and quickly into production. It took a little while for the field to be formalized, and the term DevOps came into common usage around 2010."
  },
  {
    "objectID": "chapters/intro.html#those-who-do-devops",
    "href": "chapters/intro.html#those-who-do-devops",
    "title": "Introduction",
    "section": "Those who do DevOps",
    "text": "Those who do DevOps\nThroughout this book, I’ll use two different terms – DevOps and IT/Admin – and though they may sound similar, I mean very different things by them.\nDevOps refers to the knowledge, practices, and tools that make it easier, safer, and faster to put work into production. So, if you’re a software developer (and as a data scientist, you are) you need to be thinking about DevOps.\nMost organizations also have a set of people and roles who have the permission and responsibility for managing the servers and computers at your organization. I’m going to refer to this group as IT/Admins. Their names vary widely by organization – they might be named Information Technology (IT), SysAdmin, Site Reliability Engineering (SRE), or DevOps.1\nDepending on what you’re trying to accomplish, the relevant IT/Admins may change. For example, if you’re trying to get access to a particular database, the relevant IT/Admins may be a completely different group of people than if you’re trying to procure a new server.\nFundamentally, DevOps is about creating good patterns for people to collaborate on developing and deploying software. As a data scientist, you’re on the Dev side of the house, and so a huge part of making DevOps work at your organization is about finding some Ops counterparts with whom you can develop a successful collaboration. There are many different organizational structures that support collaboration between data scientists and IT/Admins.\nHowever, I will point out three patterns that are almost always red flags – mostly because they make it hard to develop relationships that can sustain the kind of collaboration DevOps neccesitates. If you find yourself in these situations, you’re not doomed – you can still get things done. But progress is likely to be slow.\n\nAt some very large organizations, IT/Admin functions are split into small atomic units like security, databases, networking, storage, procurement, cloud, and more. This is useful for keeping the scope-of-work manageable for the people in that group – and often results in super deep expertise within the group – but also means that you’ll need to bring people together from disparate teams to actually get anything done. And even when you find the person who can help you with one task, they’re probably not the right person to help you with anything else. They may not even know who is.\nSome organizations have chosen to outsource their IT/Admin functions. This isn’t a problem per-se – the people who work for outsourced IT/Admin companies are often very competent, but it does indicate a lack of commitment at your organization. The main issues in this case tend to be logistical. Outsourced IT/Admin teams are often in India, so it can be hard to find meeting times with American and European teams. Additionally, turnover on projects and systems tends to be very high at outsourced IT/Admin organizations. That means it can be really hard to find anyone who’s an expert on a particular system – or to be able to go back to them once you’ve found them.\nAt some very small organizations, there isn’t yet an IT/Admin function. And at others, the IT/Admins are preoccupied with other tasks and don’t have the capacity to help the data science team. This isn’t a tragedy, but it probably means you’re about to become your own IT/Admin. If your organization is pretty happy to let you do what you want, you’ve picked up this book, so you’re in the right place. If you’re going to have to fight for money to get servers, that’s an uphill battle.\n\nWhether your organization has an IT/Admin setup that facilitates DevOps best practices or not, hopefully this book can help you take the first steps towards making your path to production smoother and simpler.\nTODO: Ways to ameliorate red flags"
  },
  {
    "objectID": "chapters/intro.html#whats-in-this-book",
    "href": "chapters/intro.html#whats-in-this-book",
    "title": "Introduction",
    "section": "What’s in this book?",
    "text": "What’s in this book?\nMy hope for this book is twofold.\nFirst, I’d like to share some patterns.\nDevOps is a well-developed field in its own right. However, a simple 1-1 transposition of DevOps practices from traditional software to data science would be a mistake. Over the course of engaging with so many organizations at RStudio, I’ve observed some particular patterns, borrowed from traditional DevOps, that work particularly well to grease the path to production for data scientists.\nHopefully, by the time you’re done with this book, you’ll have a pretty good mental model of some patterns and principles you can apply in your own work to make deployment more reliable. That’s what’s in the first section of this book.\nSecond, I want to equip you with some technical knowledge.\nIT administration is an older field than DevOps or data science, full of arcane language and technologies. My hope in this book is to equip you with the vocabulary to talk to the IT/Admins at your organization and the (beginning of) skills you’ll need if it turns out that you need to DIY a lot of what you’re doing.\nThe second section is the hands-on section for anyone who’s administering a data science server for themselves. In this section, we’re going to DIY a data science environment. If you have to, this could be an environment you actually work in. If not, this section will help you learn the language and tools to better collaborate with the IT/Admins at your organization.\nThe final section is about taking the DIY environment we’ll set up in section 2 and making it enterprise-grade. Hopefully, if you have enterprise requirements, you also have enterprise IT support, so this section will be more focused on the conceptual knowledge and terminology you’ll need to productively interact with the IT/Admins who are (hopefully) responsible for helping you. And if you don’t have IT/Admins to help you, it will at least give you some terms to google.\n\nChapter List (FOR DEV PURPOSES ONLY, WILL BE REMOVED):\nSection 1: DevOps for DS\n\nCode Promotion\nEnvironments as Code\nProject Components\nLogging and Monitoring\nDocker for Data Science\n\nSection 2: DIY Data Science Workbench\n\n\n\n\n\n\n\n\nNumber\nExplain\nLab\n\n\n\n\n1\nCloud\nAWS Console + Get Instance\n\n\n2\nCommand Line + SSH\nSSH into server\n\n\n3\nLinux SysAdmin\nInstall R, Py, RS, JH\n\n\n4\nNetworking, DNS, SSL\nURL, SSL\n\n\n5\nHow servers work + Choosing the right one\nTake down instance + attach to a bigger one\n\n\n\nSection 3: Steps not Taken\n\nCode Promotion for DevOps, Dev/Test/Prod, Docker\nBetter Networking (Proxies, Bastion)/Offline\nAuth Integrations\nScaling your servers"
  },
  {
    "objectID": "chapters/sec1/1-0-sec-intro.html",
    "href": "chapters/sec1/1-0-sec-intro.html",
    "title": "DevOps Lessons for Data Science",
    "section": "",
    "text": "DevOps Is a set of processes, tools, and cultural norms designed to simplify, speed, and lower the risk of putting software into production.\nThe term DevOps is a portmanteau reflecting the two halves of software delivery it is meant to bring closer together – development and operations. DevOps is a somewhat slippery concept as it’s not a specific dogma or set of tools. Instead, it’s the application of principles and norms – combined with tooling – to the particular situation you face in delivering software.\nAnd for you, the particular situation you face is delivering data science assets. Delivering data science fundamentally is a form of software development. Whether you consciously acknowledge it or not, delivering a data science asset is the same as delivering software in many important ways."
  },
  {
    "objectID": "chapters/sec1/1-0-sec-intro.html#the-problems-devops-solves",
    "href": "chapters/sec1/1-0-sec-intro.html#the-problems-devops-solves",
    "title": "DevOps Lessons for Data Science",
    "section": "The Problems DevOps Solves",
    "text": "The Problems DevOps Solves\nDevOps started as an offshoot of the Agile software movement. In particular, Agile’s focus on quick iteration via frequent delivery of small chunks and immediate feedback proved completely incompatible with a pattern where developers completed software and hurled it over an organizational wall to somehow be put into production by an IT/Admin team.\nThere are a few particular problems that DevOps attempted to solve – problems that will probably feel familiar if you’ve ever tried to put a data science asset into production.\nThe first issue DevOps addresses is the “works on my machine” phenomenon. If you’ve ever collaborated on a piece of data science code, you’ve almost certainly gotten an email, instant message, or quick shout that some code that was working great for you is failing now that your colleague is trying to work on it to collaborate.\nThe processes and tooling of DevOps is designed to link application much more closely to environment in order to prevent the “works on my machine” phenomenon from rearing its head.\nThe second problem DevOps addresses is the “breaks on deployment” issue. Perhaps you wrote some code and tested it lovingly on your machine, but didn’t have the chance to test it against a production configuration. Or perhaps you don’t really have patterns around testing code in your organization. Even if you tested thoroughly, you might not know if something breaks when its deployed. DevOps is designed to reduce the risk of deploying code that won’t function as intended the first time it’s deployed.\nDevOps is designed to incorporate ideas about scaling into the genesis of software, helping avoid software that works fine locally, but can’t be deployed for real."
  },
  {
    "objectID": "chapters/sec1/1-0-sec-intro.html#core-principles-and-best-practices-of-devops",
    "href": "chapters/sec1/1-0-sec-intro.html#core-principles-and-best-practices-of-devops",
    "title": "DevOps Lessons for Data Science",
    "section": "Core principles and best practices of DevOps",
    "text": "Core principles and best practices of DevOps\nAs I’ve mentioned, the term DevOps is squishy. So squishy that there isn’t even agreeement on what the basic tenets of DevOps are that help solve the problems its attempting to solve. Basically every resource on DevOps lists a different set of core principles and frameworks.\nAnd the profusion of xOps like DataOps, MLOps, and more just add confusion about what DevOps itself actually is.\nI’m going to name five core tenets of DevOps. Some lists of DevOps have more components, and some fewer, but this is a good-faith attempt to summarize what I believe the core components are.\n\nCode should be well-tested and tests should be automated.\nCode updates should be frequent and low-risk.\nSecurity concerns should be considered up front as part of architecture.\nProduction systems should have monitoring and logging.\nFrequent opportunities for reviewing, changing, and updating process should be built into the system – both culturally and technically.\n\nThese five tenets are a great philosophical stance, they’re about things that should happen, and they seem pretty inarguably good.1 But they’re also kinda vague and it’s not clear how to go from them to a your actual day-to-day work."
  },
  {
    "objectID": "chapters/sec1/1-0-sec-intro.html#applying-devops-to-data-science",
    "href": "chapters/sec1/1-0-sec-intro.html#applying-devops-to-data-science",
    "title": "DevOps Lessons for Data Science",
    "section": "Applying DevOps to data science",
    "text": "Applying DevOps to data science\nHopefully, you’re convinced that the principles of DevOps are relevant to you as a data scientist and you’re excited to learn more!\nHowever, it would be inappropriate to just take the DevOps principles and practices and apply them to data science.\nAs a data scientist, the huge majority of what you’re doing is taking data generated by a business process, deriving some sort of signal from that data flow, and making it available to other people or other software. Fundamentally, data science apps are consumers of data, almost by definition.\nIn contrast, most traditional pieces of software either don’t involve meaningful data flows, or are producers of business data. An online store, software for managing inventory, and electronic health records – these tools all produce data.\nThere’s a major architectural and process implication from this difference – how much freedom you have. Software engineers get to dream up data structures and data flows from scratch, designing them to work optimally for their systems. In contrast, you are stuck with the way the data flows into your system – most likely designed by someone who wasn’t thinking about the needs of data science at all.\n\n\n\n\n\n\nLanguage-specific tooling\n\n\n\nThere’s one other important difference between data science and general purpose software development. As of the writing of this book, a huge majority of data science work is done in just two programming languages, R and Python (and SQL). For that reason, this book on DevOps for Data Science can get much deeper into the particularities of applying DevOps principles to those specific languages than a general purpose book on DevOps ever would.\n\n\nSo while the problems DevOps attempts to solve will probably resonate with most data scientists, and the core principles seem equally applicable, the technical best practices need some translation.\nSo here are four technical best practices from DevOps and their equivalents in the data science world.2\n\nUse CI/CD\nContinuous Integration/Continuous Delivery/Continuous Deployment (CI/CD) is the notion that there should be a central repository of code where changes are merged. Once these changes are merged, the code should be tested, built, and delivered/deployed in an automated way.\nThe data science analog of CI/CD is code promotion and integration processes. This chapter will help you think about how to structure your app or report so that you can feel secure moving an app into production and updating it later. This chapter will also include an introduction to real CI/CD tools, so that you can get started using them in your own work.\n\n\nInfrastructure as Code\nThe underlying infrastructure for development and deployment should be reproducible using code so it can be updated and replaced with minimal fuss or disruption.\nThe data science analog is thinking about managing environments as code. This chapter will help you think about how to create a reproducible and secure project-level data science environment so you can be confident it can be used, secured, and resurrected later (or somewhere else) as need be.\n\n\nMicroservices\nAny large application should be decomposed into smaller services that are as atomic and lightweight as possible. This makes large projects easier to reason about and makes interfaces between components clearer, so changes and updates are safer.\nI believe this technical best practice has the furthest to go to translate to data science, so this chapter is about how to think of your data science project in terms of its components. This chapter will help you think about what the various components of your projects are and how to split them up for painless and simple updating and atomizing.\n\n\nMonitoring and Logging\nApplication metrics and logs are essential for understanding the usage and performance of production services, and should be leveraged as much as possible to have a holistic picture at all times.\nThe fourth chapter in this section is on monitoring and logging, which is – honestly – in its infancy in the data science world, but deserves more love and attention.\n\n\nOther Things\nMost DevOps frameworks also include communication, collaboration, and review practices as part of their framework, as the technical best practices of DevOps exist to support the work of the people who use them. This is obviously equally important in the data science world – it’s what the entire third section is about.\nAnd in the fifth chapter, we’ll learn about Docker – a tool that has become so common in DevOps practices that it deserves some discussion all on its own. In this section, you’ll get a general intro to what Docker is and how it works – as well as a hands-on intro to using Docker yourself."
  },
  {
    "objectID": "chapters/sec1/1-1-code-promotion.html",
    "href": "chapters/sec1/1-1-code-promotion.html",
    "title": "1  Code promotion and integration",
    "section": "",
    "text": "If you’ve ever taken an app or report into production, you know the moment. You get a call, a text, a slack – “the app is down”. A bead of cold sweat runs down your back. Your world narrows to you and your agonizingly nonfunctional app that the CEO, of course, needs right now.\nBasically all of DevOps is designed around preventing this moment.\nPromotion workflows are the core of DevOps practices.\nIn this chapter in particular, we’ll be thinking about deployments – the moment something goes into production, whether that’s a new app entirely or an update to an existing app. You want this moment to be as smooth as possible. Thus, you want to ensure that deployments don’t happen unless they’re supposed to and that when a deployment happens, it is as seamless as possible, with minimal downtime in the live production system.\nIn this chapter, we’ll explore how to set up multiple environments so you can safely develop and test your app before going to production and how to design and execute a promotion workflow that makes your deployment completely seamless."
  },
  {
    "objectID": "chapters/sec1/1-1-code-promotion.html#the-three-environments",
    "href": "chapters/sec1/1-1-code-promotion.html#the-three-environments",
    "title": "1  Code promotion and integration",
    "section": "1.1 The Three Environments",
    "text": "1.1 The Three Environments\nThe best way to ensure that things only get deployed when you mean them to – and sleep securely knowing that it won’t be disturbed – is to have standalone environments for development, testing, and production. Having separate environments and a plan for promoting content from one to the other is the core of workflows that de-risk pushing to production.\nUsually, these environments are referred to as Dev, Test, and Prod.\nTODO: Picture of asset promotion Dev/Test/Prod\nThe best way to ensure that deployments go smoothly is to make them as minimal and predictable as possible. This requires that the dev, test, and prod environments be very close mirrors of each other. We’ll get into how to accomplish that at the end of this chapter.\n\n1.1.1 Dev for Data Science\nDev is a sandbox where people can install packages and try new things with no risk of breaking anything in production.\nIf you’re a data scientist, you probably have a really good idea what your Dev environment looks like. It’s probably a Jupyter Notebook or an IDE with data science capabilities like RStudio, Spyder, or PyCharm. You use this environment to do exploratory data analysis, try out new types of charts and graphs, and test model performance against new features that you might design. This is really different than what most IT/Admins imagine is happening in Dev.\nFor a pure software engineering project, the Dev environment isn’t about exploration and experimentation – it’s about building. The relationship between data science and software engineering is akin to the difference between archaeology and architecture. Data science is about exploring existing relationships and sharing them with others once they’ve been discovered. The path is often meandering – and it’s usually not clear whether it’s even a possible one when you start. In contrast, pure software engineering is like designing a building. You know from the beginning that you’re designing a building for a particular purpose. You might need a little exploration to ensure you’ve thought through all of the nooks and crannies, or that you’ve chosen the right materials, or that you’re going to stay on budget, but you’ll have a pretty good idea up front whether it’s possible to design the building you want.\nThis means that Dev environments look really different for a data scientist versus a software engineer.\nThe biggest difference is that most IT/Admins are going to think of Dev, Test, and Prod being three identical copies of the same environment. That’s close to what you need as a data scientist, but really it’s more like you need a sandbox, a test environment, and prod. That means that if you’re using a deployment platform like RStudio Connect or Dash Enterprise, you probably don’t need it in your Dev environment, and that you don’t need your development tool in Test or Prod (any changes should go back through the deployment pipeline).\n\n\n1.1.2 Test and Prod\nTest is (unsurprisingly) an environment for testing. Depending on the type of asset you’re developing, the test environment might incorporate testing that you do, testing by outside entities like security, and/or performance testing. Generally, the test environment facilitates User Acceptance Testing (UAT), where you can investigate whether labels and buttons are clear or whether plots and graphs meet the need. Depending on your organization, test might be collapsed with dev, it might be a single environment, or it could be multiple environments for the different types of testing.\nProd is the gold standard environment where things run without any manual human intervention. Usually the only way to get things into prod is through some type of formalized process – sometimes backed by a computer process like a git merge or push from a Continuous Integration/Continuous Deployment (CI/CD) platform (more on that below). One of the most important ways to keep prod stable is that nothing changes in prod other than via a simple promotion from the test environment.\n\n\n1.1.3 Protecting Prod Data\nOne of the biggest risks during the dev and test parts of an assets lifecycle is that you might mess up real data during your work. In a software engineering context, it’s common to use completely fake data or for the app to be the data generation tool.\nIn contrast, data science is all about using and learning from your organization’s actual data. So a dev environment that doesn’t include access to your organization’s real data is going to be completely useless if it doesn’t have real data in it. This is often a difficult thing to convince an IT/Admin of.\nIn many cases, data science assets and reports are read-only. If you’re mostly building visualizations or dashboards that just consume the business data, perhaps clean it for analytics purposes, you can happily accept a read-only connection to your organization’s data.1 In this case, it works just fine to connect to your real data from your Dev and Test environments and create graphs, models, and dashboards based on the real data, testing out new visualizations and model features in a safe sandbox while the existing version of your app or report runs smoothly in prod.\nOn the other hand, if your app or report actually writes data, you’ll have to be a little more clever. In general, you’ll have to figure out how to redirect your apps output into a test data store, or to mock responses from the real services you’re interacting with. The easiest way to do this is by including your output locations as variables inside your code and then setting them at runtime based on an environment variable. See below for an example of how to do this."
  },
  {
    "objectID": "chapters/sec1/1-1-code-promotion.html#taking-data-science-to-production-with-cicd",
    "href": "chapters/sec1/1-1-code-promotion.html#taking-data-science-to-production-with-cicd",
    "title": "1  Code promotion and integration",
    "section": "1.2 Taking data science to production with CI/CD",
    "text": "1.2 Taking data science to production with CI/CD\nThe common term for the mechanics of code promotion is Continuous Integration/Continuous Deployment (CI/CD).\nCI/CD is about moving your data science projects, including code, data, and environment, into successively more formal states of production. While it’s not a requirement to use source control to make this happen, the mechanics of CI/CD is usually tightly linked to source control procedures.\nIf you’re not already familiar, I’d suggest spending some time learning git. If you’re just starting, you’re in for a bit of a ride. People who say git is easy are either lying to look smarter or learned so long ago that they have forgotten how easy it is to mess up your entire workflow at any moment.\n\nIf you don’t already know git and want to learn, I’d recommend HappyGitWithR by Jenny Bryan. It’s a great on-ramp to learn git.\nEven if you’re a Python user, the sections on getting started with git, on basic git concepts, and on workflows will be useful since they approach git from a data science perspective.\n\nFor the purposes of this section, I’m going to assume you at least conceptually understand what git branches are and what a merge is – as much of your CI/CD pipeline will be based on what happens when you merge.\nFor production data science assets, I generally recommend long-running dev (or test) and prod branches, with feature branches for developing new things. The way this works is that new features are developed in a feature branch, merged into dev for testing, and then promoted to prod when you’re confident it’s ready.\nFor example, if you had two new plots you were adding to an existing dashboard, your git commit graph might look like this:\n\nCI/CD adds a layer on top of this. CI/CD allows you to integrate functional testing by automatically running those tests whenever you do something in git. These jobs can run when a merge request is made, and are useful for tasks like spellchecking, linting, and running tests.\nFor the purposes of CI/CD, the most interesting jobs are those that do something after there’s a commit or a completed merge, often deploying the relevant asset to its designated location.\nA CI/CD integration using the same git graph as above would have released 3 new test versions of the app and 2 new prod versions. Note that in this case, the second test release revealed a bug, which was fixed and tested in the test version of the app before a prod release was completed.\nIn years past, the two most popular CI/CD tools were called Travis and Jenkins. By all accounts, these tools were somewhat unwieldy and difficult to get set up. More recently, GitHub – the foremost git server – released GitHub Actions (GHA), which is CI/CD tooling directly integrated into GitHub that’s free for public repositories and free up to some limits for private ones.\nIt’s safe to say GHA is eating the world of CI/CD.2\nFor example, if you’re reading this book online, it was deployed to the website you’re currently viewing using GHA. I’m not going to get deep into the guts of GHA, but instead talk generally about the pattern for deploying data science assets, and then go through how I set up this book on GHA.\n\n1.2.1 Using CI/CD to deploy data science assets\nIn general, using a CI/CD tool to deploy a data science asset is pretty straightforward. The mental model to have is that the CI/CD tool stands up a completely empty server for you, and runs some code on it.\nThat means that if you’re just doing something simple like spellchecking, you can probably just specify to run spellcheck. If you’re doing something more complicated, like rendering an R Markdown document or Jupyter Notebook and then pushing it to a server, you’ll have to take a few extra steps to be sure the right version of R or Python is on the CI/CD server, that your package environment is properly reproduced, and that you have the right code to render your document.\nFeel free to take a look through the code for the GitHub Action for this book. It’s all YAML, so it’s pretty human-readable.\nHere’s what happens every time I make a push to the main branch of the repository for this book:3\n\nCheckout the current main branch of the book.\nUse the r-lib action to install R.\nUse the r-lib action to setup pandoc (a required system library for R Markdown to work).\nGet the cached renv library for the book.\nRender the book.\nPush the book to GitHub Pages, where this website serves from.\n\nYou’ll see that it uses a mixture of pre-defined actions created for general use, pre-defined actions created by people in the R community, and custom R code I insert to restore an renv library and render the book itself."
  },
  {
    "objectID": "chapters/sec1/1-1-code-promotion.html#per-environment-configuration",
    "href": "chapters/sec1/1-1-code-promotion.html#per-environment-configuration",
    "title": "1  Code promotion and integration",
    "section": "1.3 Per-Environment Configuration",
    "text": "1.3 Per-Environment Configuration\nSometimes you want a little more flexibility – for example the option to switch many the environment variables depending on the environment.\nIn R, the standard way to do this is using the config package. There are many options for managing runtime configuration in Python, including a package called config.\nFor example, let’s consider this shiny app. In this app, every time I press the button, the app sends a POST request to an external service indicating that the button has been pressed.\n\nlibrary(shiny)\n\n# UI that's just a button\nui <- fluidPage(\n  actionButton(\"button\", \"Press Me!\")\n)\n\n# Do something on button press\nserver <- function(input, output) {\n  observeEvent(\n    input$button, \n    httr::POST(\n      \"www.my-external-system.com\", \n      body = list(button_pressed = TRUE)\n    )\n  )\n}\n\n# Run the application \nshinyApp(ui = ui, server = server)\n\nWith the URL hardcoded like this, it’s really hard to imagine doing this in a Dev or Test environment.\nHowever, with R’s config package, you can create a config.yml file that looks like this:\n\ndev:\n  url: \"www.test-system.com\"\n  \nprod:\n  url: \"www.my-external-system.com\"\n\nThen you can use an environment variable to the correct config and apply that configuration inside the app.4\n\nlibrary(shiny)\nconfig <- config::get()\n\n# UI that's just a button\nui <- fluidPage(\n  actionButton(\"button\", \"Press Me!\")\n)\n\n# Do something on button press\nserver <- function(input, output) {\n  observeEvent(\n    input$button, \n    httr::POST(\n      config$url, \n      body = list(button_pressed = TRUE)\n    )\n  )\n}\n\n# Run the application \nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "chapters/sec1/1-1-code-promotion.html#creating-and-maintaining-identical-environments",
    "href": "chapters/sec1/1-1-code-promotion.html#creating-and-maintaining-identical-environments",
    "title": "1  Code promotion and integration",
    "section": "1.4 Creating and Maintaining Identical Environments",
    "text": "1.4 Creating and Maintaining Identical Environments\nIn the IT world, there’s a phrase that servers should be cattle, not pets. The idea here is that servers should be unremarkable and that each one should be more-or-less interchangeable. This matters, for example, in making sure your test and prod environments look exactly the same.\nTODO: Notes on using virtual environments + Docker.\nFor example, doing test on a Windows laptop and then going to prod on a Linux server introduces a potential that things that worked in test suddenly don’t when going to prod. For that reason, making all three (or at least test and prod) match as precisely as possible is essential. The need to match these three environments so precisely is one reason for data science workloads moving onto servers.\nA bad pattern then would look like this:\n\nI develop an update to an important Shiny or Dash app in my local environment and then move it onto a server.\nAt that point, the app doesn’t quite work and I make a bunch of manual changes to the environment – say adjusting file paths or adding R or Python packages. Those manual changes end up not really being documented anywhere.\nA week later, when I go to update the app in prod, it breaks on first deploy, because the server state of the test and prod servers drifted out of alignment.\n\nThe main way to combat this kind of state drift is to religiously use state-maintaining infrastructure as code (IaC) tooling. That means that all changes to the state of your servers ends up in your IaC tooling and no “just login and make it work” shenanigans are allowed in prod.\nTODO: Graphic - fixing problems using IaC tooling\nIf something breaks, you reproduce the error in staging, muck around until it works, update your IaC tooling to fix the broken thing, test that the thing is fixed, and then (and only then) push the updated infrastructure into prod directly from your IaC tooling."
  },
  {
    "objectID": "chapters/sec1/1-1-code-promotion.html#exercises",
    "href": "chapters/sec1/1-1-code-promotion.html#exercises",
    "title": "1  Code promotion and integration",
    "section": "1.5 Exercises",
    "text": "1.5 Exercises\n\nCreate something and add GHA integration \\[TODO: What to use as example?\\]\nStand up some virtual environments using ___ \\[TODO: Which ones to try?\\]"
  },
  {
    "objectID": "chapters/sec1/1-2-env-as-code.html",
    "href": "chapters/sec1/1-2-env-as-code.html",
    "title": "2  Environments as Code",
    "section": "",
    "text": "I like to think of doing data science much like producing a delicious meal. In my data, I have the raw ingredients I’ll slice, dice, and recombine to make something great, and my code is the recipe I’ll follow to get there.1\nNovice cooks generally think that their prep is done once they’ve gathered their ingredients and recipes. They’ll grab a tomato and chop/smush it with whatever dull butter knife happens to be at hand. But that’s not how the pros think. A pro probably has a knife just for tomatoes and they frequently – perhaps every time they use it – hone it to razor sharpness so they can slice paper-thin slices off even the mushiest tomato.\nRegardless of your proficiency in the kitchen, you’re a pro (or aiming to be one) at data science. In this chapter, we’re going to talk about the data science equivalent of prepping your knives in your kitchen – actively managing your data science environments using code.\nKitchen metaphors aside, your data science environment is the stack of software and hardware below the level of your code, from the R and Python packages you’re using right down to the physical hardware your code runs on.\nMost data scientists are like novice cooks, and think little – or not at all – about the readiness of their environment and the sharpness of their tools. One of the primary results of this is the oft-spoken, and dreaded phrase, “well, it works on my machine” after attempting to share code with a colleague or deploy an app into a different environment.\nSome people who read this will just throw up their hands and say, “well, it’s impossible to make things completely reproducible, so I’m not going to bother”. And they’re right about the first part. Trying to craft an environment that’s completely reproducible is somewhat of a fool’s errand.\nThere’s a tradeoff. Making things more reproducible generally takes more work – in a way that’s frustratingly asymptotic.\nSome industries are highly regulated and need to be able to guarantee that they can reproduce an analysis exactly – down to the layer of machine instructions – a decade later. In this world, the general reproducibility practice for an analysis is to take a physical piece of hardware that they know runs the analysis, make a backup or two, and just keep that physical piece of hardware running for many years.\nBut you don’t have to go all the way there. Making things 100% reproducible is really, really hard. But making things a little more reproducible is really quite easy.\nThe first step towards making environments more reproducible is to start to create Environments as Code. In the DevOps world, the aim is to create environments that are largely “stateless” – functionally identical copies of any environment can be created and destroyed at whim using code.\nThe glib-but-useful shorthand for the idea of Infrastucture or Environments as code is that they should be “cattle, not pets” – interchangeable one for the other.\nIn this chapter, we’ll get into the why and how of capturing data science environments in code, saving them for later, and easily moving them around from place to place."
  },
  {
    "objectID": "chapters/sec1/1-2-env-as-code.html#environments-have-layers",
    "href": "chapters/sec1/1-2-env-as-code.html#environments-have-layers",
    "title": "2  Environments as Code",
    "section": "2.1 Environments have layers",
    "text": "2.1 Environments have layers\nWhen you first start thinking about environments, it can be hard to wrap your head around them. The environment seems like a monolith, and it can be hard to figure out what the different components are.\nI generally think of three layers in data science environments, and these are in order – each layer of the environment is actually built on the ones below. Once you understand the layers of an environment, you can think more clearly about what your actual reproducibility needs are, and which environmental layers you need to target putting into code.\n\n\n\n\n\n\n\nLayer\nContents\n\n\n\n\nPackages\nR + Python Packages\n\n\nSystem\nR + Python Language Versions\nOther System Libraries\nOperating System\n\n\nHardware\nVirtual Hardware\nPhysical Hardware\n\n\n\nNote that your code and your data are not the environment – they are what the environment is for. As you’re thinking about reproducibility, I’d encourage you to think about how they fit inside the environment and how they might be reproduced.2 But we’re not going to address them in this book.\nFor most data scientists, the biggest bang for your buck is getting the package layer right. In a lot of organizations, another team entirely will be responsible for the system and hardware layers, but the package layer is always your responsibility as the data scientist. Moreover, managing that layer isn’t terribly hard, and if you get it right, you’ll solve a huge fraction of the “runs on my machine” issues you’re likely to encounter.\n\n2.1.1 Package environments as code\nA successful package Environment as Code setup has two key attributes:\n\nYour package environment is isolated and cannot be disrupted by other activities on the same machine.\nYour package environment can easily be captured and transported elsewhere.\n\nBefore we get to what a good Environment as Code setup looks like, let’s dive into what bad setups look like.\nIn a lot of cases, data scientists have the habit of starting a project, and, when they need to install packages, they just run an install.packages command in their console or pip install in their terminal. This works fine for a while. But the problem with this is that the default has you installing things into a cache that’s shared among every project on your system.\nWhat happens if you come back to a project after a year and you’ve been installing things into your machine-wide package cache the whole time. It’s very possible you won’t have the right versions and your code will break.\nThe other problem happens when it comes time to share a project with others. It’s not uncommon to see an intro to an R script that looks something like this:\n\n# Check if dplyr installed\nif (!\"dplyr\" %in% row.names(installed.packages())) {\n  # install if not\n  install.packages(\"dplyr\")\n}\n\nACK! Please don’t do this!\nNumber one, this is very rude. If someone runs your code, you’ll be installing packages willy-nilly into their system. Additionally, because this doesn’t specify a version of the {dplyr} package, it doesn’t even really fix the problem!\n\n\n2.1.2 Step 1: Standalone Package Libraries\nAs a data scientist, you’re probably familiar with installing packages from a repository using the install.packages command in R or pip install or conda install in Python. But do you really understand what’s happening when you type that command?\nLet’s first level-set on what the various states for R or Python packages are. There are three states packages can be in – and we’re going to go back to our data science as cooking analogy.\n\nPackages can be stored in a repository, like CRAN or BioConductor in R or PyPI or Conda in Python. You can think of a package in a repository like food at the grocery store – it’s packaged up and ready to go, but inert. Setting aside groovy bodegas with eat-in areas, you don’t get to eat at the grocery store. You’ve got to buy the food and take it home before you can use it – and you’ve got to install the food before you can use it.\nAnd then your library is your pantry, where you keep a private set of packages, bespoke to you and the food you like to cook – the projects you’re likely to do.\nLoading a package with a library or import command is like taking the food out of the pantry and putting it on the counter so you can actually cook with it.\n\n[Diagram of package states]\nBy default, when you install an R or Python package, that package installs into user space. That means that it installs into a package library that is specific to your user, but is shared among every time that package is used by you on the machine.\nThis isn’t a disastrous situation, but it is a recipe for package incompatibilities down the road.\n[TODO: diagram of user-level vs project-level installs]\nThe most important thing to understand about package libraries is that libraries can only have one version of any given package at a time. So that means that if I have code that relies on version 1.0 of a given package and I install a new version of that package, version 1.0 is gone and I am likely to run into package incompatibility issues.\nIt’s for this reason that you want to have standalone package libraries for each project on your system. Hopefully, you already have good practices around having each project in a standalone directory on your system and making a git repo in that system. Now just make the base directory of that directory a standalone library as well.\n\n\n\n\n\n\nWhat if I have multiple content items?\n\n\n\nIn many data science projects, you’ve got multiple content items within a single project. Maybe you have an ETL script and an API and an app. After a lot of experimenting, my recommendation is to create one git repo for the whole project and have content-level package libraries.\nThis is not a rule. It’s just a suggestion about how I’ve found it works best over time.\n[TODO: Add image]\n\n\n\n2.1.2.1 What’s really happening?\nI happen to think the grocery store metaphor for package management is a useful one, but you might be wondering what the heck is actually happening when you’re using {renv} or {venv}. How does this package magic happen?\nFirst, let’s quickly go over what happens when you install or load a package.\nWhenever you install a package, there are two key settings that R or Python consult: the URL of the repository to install from, and the library to install to. Similarly, when you load an R or Python library, the install checks the library location. In R, the command used is .libPaths(), and in Python it’s sys.path.\nSo you can see that it’s (conceptually) pretty simple to create a standalone package library for any project – when the virtual environment is activated, just make sure that the project-level library is what comes back when checking the library path.\nYou can see it pretty easily in R. If I run .libPaths() before and after activating an {renv} environment, the first entry from the .libPaths() call changes from a user-level library /Users/alexkgold to a project-level library /Users/alexkgold/Documents/do4ds/.\n\n.libPaths()\n[1] \"/Users/alexkgold/Library/R/x86_64/4.2/library\"                 \n[2] \"/Library/Frameworks/R.framework/Versions/4.2/Resources/library\"\nrenv::activate()\n* Project '~/Documents/do4ds/docker/docker/plumber' loaded. [renv 0.15.5]\n.libPaths()\n[1] \"/Users/alexkgold/Documents/do4ds/docker/docker/plumber/renv/library/R-4.2/x86_64-apple-darwin17.0\"\n[2] \"/Library/Frameworks/R.framework/Versions/4.2/Resources/library\"  \n\nSimilarly, in Python it looks like this. Note that the “after” version replaces the last line of the sys.path with a project-level library:\n\n❯ python3 -m site                                       \nsys.path = [\n    '/Users/alexkgold/Documents/python-examples/dash-app',\n    '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python38.zip',\n    '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8',\n    '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/lib-dynload',\n    '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/site-packages',\n]\nUSER_BASE: '/Users/alexkgold/Library/Python/3.8' (doesn't exist)\nUSER_SITE: '/Users/alexkgold/Library/Python/3.8/lib/python/site-packages' (doesn't exist)\nENABLE_USER_SITE: True\n\n❯ source .venv/bin/activate                       \n(.venv)\n\n❯ python3 -m site\nsys.path = [\n    '/Users/alexkgold/Documents/python-examples/dash-app',\n    '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python38.zip',\n    '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8',\n    '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/lib-dynload',\n    '/Users/alexkgold/Documents/python-examples/dash-app/.venv/lib/python3.8/site-packages',\n]\nUSER_BASE: '/Users/alexkgold/Library/Python/3.8' (doesn't exist)\nUSER_SITE: '/Users/alexkgold/Library/Python/3.8/lib/python/site-packages' (doesn't exist)\nENABLE_USER_SITE: False\n(.venv)\n\n\n\n\n2.1.3 Step 2: Document environment state.\nUsing standalone package libraries for each project ensures that your projects remain undisturbed when you come back to them months or years later and keeps your work reproducible.\nBut it doesn’t solve the sharing problem.\nThat is, you still need some help when it comes time to share an environment with someone else. So how does that work?\n[TODO: image – anatomy of a lockfile]\nBoth R and Python have great utilities that make it easy to capture the current state of a library into a lockfile or requirements.txt and to restore those libraries at a later date or somewhere else.\nIn R, {renv} is the standard on this front. In Python, there are many different options. In the context of production data science, I recommend {virtualenv}/{venv} and related tools.\nNow, when you share your project with someone else, your lockfile or requirements.txt goes along for the ride. Sometimes people are dismayed that their library doesn’t go along as well and that people have to install the packages themselves – but this is by design!\nFirst, the actual package libraries can be very large, so putting just a short lockfile or requirements file into git is definitely preferred. The other reason is that the actual package install can differ from system to system. For example, if you’re working on a Windows laptop and your colleague is on a Mac, an install of {dplyr} 1.0 means that different files are installed – but with exactly the same functionality. You want to respect this, so instead of sending the whole library along for the ride, you just send the specification that dplyr 1.0 is needed.\n\n\n\n\n\n\nA sidebar on Conda\n\n\n\nMany data scientists love Conda for managing their Python environments.\nConda is a great tool for its main purpose – allowing you to create a data science environment on your local laptop, especially when you don’t have root access to your laptop because it’s a work machine that’s locked down by the admins.\nIn the context of a production environment, Conda smashes together the language version, the package management, and (sometimes) the system library management. This has the benefit of being conceptually simple and easy-to-use. But I’ve often seen it go awry in production environments, and I generally recommend people use a tool that’s just for package management, like {venv}, as opposed to an all-in-one tool like Conda.\n\n\nOn a high level, the workflows for these tasks are similar between R and Python. However, there are some meaningful differences in tooling – especially because many computers arrive with a system version of Python installed, while R is only ever installed by a user trying to do data science tasks.\nAt the end of the day, this actually makes it harder to use Python because you do not want to use your system Python for your data science work…but sometimes it accidentally gets into the mix.\n\n\n2.1.4 Step 3: Collaborate\nTODO\n\n\n2.1.5 Reproducing the rest of the stack\nSometimes, just recording the package environment and moving that around is sufficient. In many cases, old versions of R and Python are retained in the environment, and that’s sufficient.\nThere are times where you need to reproduce elements further down the stack. In some highly-regulated industries, you’ll need to go further down the stack because of requirements for numeric reproducibility. Numeric routines in both R and Python call on system-level libraries, often written in C++ for speed. While it’s unlikely that upgrades to these libraries would cause changes to the numeric results you get, it can happen, and it may be worth maintaining parts of the stack.\nIn other cases, your R or Python library might basically just be a wrapper for system libraries. For example, many popular packages for geospatial analysis are just thin language wrappers that call out to the system libraries. In this case, it might be important to be able to maintain a particular version of the underlying system library to ensure that your code runs at all in the future.\nThere are many tools you can use to record and reproduce the R and Python versions you’re using, the system libraries, and the operating system itself.\nThese days, the clear leader of the pack on this front is Docker. It has become an increasingly popular way to create, maintain, and use standalone environments – and for good reason! In fact, the next chapter is going to be all about the use of Docker in data science. However, it’s worth keeping in mind that if you’re working in the context of a formally-supported IT organization, they may have other tooling they prefer to use to create and maintain environments, and they can be equally valid."
  },
  {
    "objectID": "chapters/sec1/1-2-env-as-code.html#environments-as-code-cheatsheet",
    "href": "chapters/sec1/1-2-env-as-code.html#environments-as-code-cheatsheet",
    "title": "2  Environments as Code",
    "section": "2.2 Environments as Code Cheatsheet",
    "text": "2.2 Environments as Code Cheatsheet\n\n2.2.1 Checking your library + repository status\nTODO\n\n\n2.2.2 Creating and Using a Standalone Project Library\n\n\n\n\n\n\n\n\nStep\nR Command\nPython Command\n\n\n\n\nMake a standalone project directory.\n-\n-\n\n\nMake sure you’ve got {renv}/{venv}.\ninstall.packages(\"renv\")\nIncluded w/ Python 3.5+\n\n\nCreate a standalone library.\nrenv::init()\npython -m venv <dir>\nRecommend: use .venv for <dir>\n\n\nActivate project library.\nrenv::activate()\nHappens automatically if using projects.\nsource <dir>/bin/activate\n\n\nInstall packages as normal.\ninstall.packages(\"<pkg>\")\npython -m pip install <pkg>\n\n\nSnapshot package state.\nrenv::snapshot()\npip freeze > requirements.txt\n\n\nExit project environment.\nLeave R project.\ndeactivate\n\n\n\n\n\n2.2.3 Collaborating on someone else’s project\n\n\n\n\n\n\n\n\nStep\nR Command\nPython Command\n\n\n\n\nDownload project.\n\n\n\n\nMove into project directory.\nsetwd(\"<project-dir>\")\nOr just open R project in RStudio.\ncd <project-dir>\n\n\nCreate project environment.\nrenv::init()\npython -m venv <dir>\nRecommend: use .venv for <dir>\n\n\nEnter project environment.\nHappens automatically.\nsource <dir> /bin/activate\n\n\nRestore packages.\nMay happen automatically or renv::restore()\npip install -r requirements.txt"
  },
  {
    "objectID": "chapters/sec1/1-3-proj-components.html",
    "href": "chapters/sec1/1-3-proj-components.html",
    "title": "3  Data Science Project Components",
    "section": "",
    "text": "One of the fundamental building blocks of good DevOps practices is microservices. The idea of microservices is that you want to build each application as a collection of smaller, separable components. These components interact via well-defined interfaces so that it’s easy to change the internal workings of each component without messing up the system as whole. It also allows you to make these changes with a much higher degree of confidence that changes to one component can be integrated and deployed without unanticipated downstream consequences.\nYou can think of microservices as the idea of writing functions in your code (you are writing functions, right?), but one level up. Instead of thinking of a single function, you’re thinking at the level of an entire component of your analysis, report, app, or pipeline.\nThe idea of microservices for a general software engineering project is pretty straightforward, you have services that interact with each other over APIs – RESTful ones where possible. Honestly, this isn’t a terrible idea for data scientists, and we’ll get a little bit into what RESTful APIs are in this chapter and how to use them.\nBut a data science app stack is actually much more complicated than a generic three-tier web app architecture.\nIn software engineering, there’s the notion of a three-tier app. It’s helpful to be acquainted with this model as a data scientist, but I find it’s woefully insufficient for data science purposes.\nThe three-tier app architecture is\nIn general, the presentation tier is the front-end, while the other two are the back-end. Usually, the application tier will be one or more APIs and the data tier is some sort of database storage.\nTo think of a simple webapp, say that allows someone to buy a copy of a book online, a user visits the website, picks out a book, puts in their credit card, and clicks a button to purchase.\nIn the application layer, an API receives their order, pushes that order into the “orders to be shipped” queue in the warehouse, and adds the user’s contact info and order history to the database, which is the data tier.\nThe big difference between many software engineering projects and data science is the direction of the data flow. This is what makes it hard to take software engineering workflows and simply adapt them to data science.\nTODO - image of software engineering data flowing front to back, and data science flowing back to front\nIn most software engineering workflows, the user interactions are what create the data. A user interacts with a webpage and creates an order or history that needs to be captured or used somehow. The software engineer, by circumscribing the ways the user interacts with the app actually defines the data flowing through the app.\nIn contrast, most data science apps are designed to give the user a view or a way to interact with an existing bunch of data – and that initial data flow is usually real-world data. It takes work to reshape that data into something useful to the end user – that’s in fact the entire practice of data science.\nSo in data science, the set of microservices you’ll want to consider is pretty broad, and they each look pretty different. I’ve come up with seven reasonably distinct components that make up most data science projects. Some may not have all these components, but most data science processes fit somewhere into these seven services:\nBeyond this taxonomy, I don’t have much to say about each of these layers, except that you should probably have a separate piece of code for each one. Keeping them separate allows you to change and tweak each layer without major implications for the other ones.\nI don’t have anything particularly special to say about data ingestion + refining, except that you should almost certainly keep these processes separate. It’s tempting not to save original data (if you’re respoonsible for ingesting) and to just keep refined data. This is almost always a mistake. I can’t tell you how many times I’ve gone back later and realized that I wanted to update my refining process in a way that relied on still having the source data. Save your source data.\nSimilarly, I have nothing of interest to say about model training, except that this step is by far the most over-hyped part of the data science process. If you’re a young data scientist, you’ll almost surely get more success out of getting really good at any other step of this process rather than focusing on model training. It’s the most crowded and the most automate-able.\nSimilarly, for app operations, there are lots of books on how to write good apps. I think the Shiny framework is great, and Hadley Wickham’s Mastering Shiny book is the go-to reference if you want to…well, you know.\nBUT, I’ve got a lot to say about steps 4 and 5. All too often, I see monolithic Shiny apps of thousands or tens of thousands of lines of code, mixing up business logic and app logic. Or that serve model predictions right inside the app. These apps would almost always be better served by moving the business logic into a standalone API.\nAnd then there’s the question of how to serve your data into the running app. That’s what I’ll spend much of the rest of this chapter on."
  },
  {
    "objectID": "chapters/sec1/1-3-proj-components.html#you-should-use-apis-more",
    "href": "chapters/sec1/1-3-proj-components.html#you-should-use-apis-more",
    "title": "3  Data Science Project Components",
    "section": "3.1 You should use APIs more",
    "text": "3.1 You should use APIs more\nYou are a data scientist. I don’t know you. I’ve almost certainly never looked at your code. But I can tell you, that in very high likelihood, you should be writing more APIs.\nAPIs can seem intimidating if you’ve never written one before. Writing an API seems like crossing the rubicon between data scientist and software engineer.\nI’m here to tell you that if you know how to write a function in R or Python, you can write an API. For all intents and purposes, an API is just a function that is ready for you to run outside your console.\nWhile the concept of APIs can be extremely intimidating, the actual process of writing an API is no more complicated that writing and documenting a function in R or Python. There are a variety of API frameworks in both R and Python. As of this writing, the ones I’d recommend the most are FastAPI in Python and Plumber in R.\nTo demystify an API, let’s start with a use case.\nLet’s say I’ve got a shiny app that visualizes certain data points from the palmer penguins data set.\nTODO – see if I can deploy to shinyapps and iframe in\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(palmerpenguins)\nlibrary(shiny)\n\n# Define UI\nui <- fluidPage(\n  # Sidebar with a slider input for number of bins \n  sidebarLayout(\n    sidebarPanel(\n      selectInput(\n        inputId = \"species\", \n        label = \"Species\", \n        choices = c(\"Adelie\", \"Chinstrap\", \"Gentoo\"), \n        selected = c(\"Adelie\", \"Chinstrap\", \"Gentoo\"),\n        multiple = TRUE\n      )\n    ),\n    # Show a plot of the generated distribution\n    mainPanel(\n      plotOutput(\"penguinPlot\")\n    )\n  )\n)\n\nserver <- function(input, output) {\n  \n  output$penguinPlot <- renderPlot({\n    # Filter data\n    dat <- palmerpenguins::penguins %>%\n      dplyr::filter(\n        species %in% input$species\n      )\n    \n    # Render Plot\n    dat %>%\n      ggplot(\n        aes(\n          x = flipper_length_mm,\n          y = body_mass_g,\n          color = sex\n        )\n      ) +\n      geom_point()\n  })\n  \n}\n\n# Run the application \nshinyApp(ui = ui, server = server)\n\nThe structure of this Shiny app is bad. Now, it’s not a huge deal, because this is a simple shiny app that’s pretty easy to parse, but I’ve seen many much larger apps with this same structure. Why is this bad? Because all of the app’s logic is contained inside a plotRender-er.\nIn this case, I’ve combined business logic and app logic. As your apps get more complicated, you’ll want to keep the app itself strictly for the logic of the app – what selections has the user made and what needs to be displayed to them as a result.\nThe business logic – what those decisions mean, and the resulting calculations should – at minimum – be moved into standalone functions.\nRewriting this app to make better use of functions, it looks something like this:\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(palmerpenguins)\nlibrary(shiny)\n\n# Define UI\nui <- fluidPage(\n  # Sidebar with a slider input for number of bins \n  sidebarLayout(\n    sidebarPanel(\n      selectInput(\n        inputId = \"species\", \n        label = \"Species\", \n        choices = c(\"Adelie\", \"Chinstrap\", \"Gentoo\"), \n        selected = c(\"Adelie\", \"Chinstrap\", \"Gentoo\"),\n        multiple = TRUE\n      )\n    ),\n    # Show a plot of the generated distribution\n    mainPanel(\n      plotOutput(\"penguinPlot\")\n    )\n  )\n)\n\nfilter_data <- function(species) {\n  palmerpenguins::penguins %>%\n    dplyr::filter(\n      species %in% !!species\n    )\n}\n\nserver <- function(input, output) {\n  \n  # Filter data\n  dat <- reactive(filter_data(input$species))\n  \n  # Render Plot\n  output$penguinPlot <- renderPlot({\n    dat() %>%\n    ggplot(\n      aes(\n        x = flipper_length_mm,\n        y = body_mass_g,\n        color = sex\n      )\n    ) +\n    geom_point()\n  })\n  \n}\n\n# Run the application \nshinyApp(ui = ui, server = server)\n\nNow, I’ve separated the user interaction and visualization layer from the business logic, which is pretty trivial in this case.\nHere’s what a plumber API version of my business logic looks like:\n\nlibrary(plumber)\n\n#* @apiTitle Penguin Explorer\n#* @apiDescription An API for exploring palmer penguins.\n\n#* Get data set based on parameters\n#* @param species which penguin species to include\n#* @get /data\nfunction(species = c(\"Adelie\", \"Chinstrap\", \"Gentoo\")) {\n  palmerpenguins::penguins %>%\n    dplyr::filter(\n      species %in% !!species\n    )\n}\n\nI’ll need to change my function in the app somewhat to actually call the API, but it’s pretty easy.\n\nfilter_data <- function(species) {\n  httr::GET(\n    url = \"http://127.0.0.1:9046\", \n    path = \"data\", \n    query = list(species = species)\n  ) \n}\n\nI can now host this plumber API somewhere, and everyone I’ve allowed to have access can access the data just as easily as the app can. This is a really powerful ability for you."
  },
  {
    "objectID": "chapters/sec1/1-3-proj-components.html#separate-updating-data-from-code",
    "href": "chapters/sec1/1-3-proj-components.html#separate-updating-data-from-code",
    "title": "3  Data Science Project Components",
    "section": "3.2 Separate updating data from code",
    "text": "3.2 Separate updating data from code\nIn this chapter, we’re going to explore how you decide how to store the data for your app. But first, let’s just talk through what the options are so it makes sense as we talk about why you might choose one over the other in the rest of the chapter.\n\n3.2.1 Storage Format\nThe first question of how to store the data is the storage format. There are really three distinct options for storage format.\nFlat file storage describes writing the data out into a simple file. The canonical example of a flat file is a csv file. However, there are also other formats that may make data storage smaller because of compression, make reads faster, and/or allow you to save arbitrary objects rather than just rectangular data. In R, the rds format is the generic binary format, while pickle is the generic binary format in python.\nFlat files can be moved around just like any other file on your computer. You can put them on your computer, share them through tools like dropbox, google drive, scp, or more.\nThe biggest disadvantage of flat file data storage is twofold – and is related to their indivisibility. In order to use a flat file in R or Python, you’ll need to load it into your R or Python session. For small data files, this isn’t a big deal. But if you’ve got a large file, it can take a long time to read, which you may not want to wait for. Also, if your file has to go over a network, that can be a very slow operation. Or having to load it into an app at startup. Also, there’s generally no way to version data, or just update part, so if you’re saving archival versions, they can take up a lot of space very quickly.\nAt the other extreme end from a flat file format is a database. A database is a standalone server with its own storage, memory, and compute. In general, you’ll recall things from a database using some sort of query language. Most databases you’ll interact with in a data science context are designed around storing rectangular data structures and use Structured Query Language (SQL) to get at the data inside.\nThere are other sorts of databases that store other kinds of objects – you may need these depending on the kind of objects you’re working with. Often the IT/Admin group will have standard databases they work with or use, and you can just piggyback on their decisions. Sometimes you’ll also have choices to make about what database to use, which are beyond the scope of this book.\nThe big advantage of a database is that the data is stored and managed by an independent process. This means that accessing data from your app is often a matter of just connecting to the database, as opposed to having to move files around.\nWorking with databases can also be frought – you usually end up in one of two situations – either the database isn’t really for the data science team, in which case you can probably get read access, but not write. So you’ll be able to use the database as your source of truth, but you won’t be able to write there for intermediate tables and other things you might need. Or you’ll have freedom to set up your own database, in which case you’ll have to own it – and that comes with its own set of headaches.\nThere’s a third option for data storage that is quickly rising in popularity for medium size data. These options are ones that allow you to store data in a flat file, but access it in a smarter way than “just load all the data into memory”. SQLite is a classic on this front that gives you SQL access to what is basically just a flat file. There are also newer entrants into this place that are better from an analytics perspective, like combining Apache Arrow with feather and parquet files and the dask project in Python.\nThese tools can give you the best of both worlds – you get away from the R and Python limitation of having to load all your data into memory, without having to run a separate database server. But you’ll still have to keep track of where the actual files are and make them accessible to your app.\n\n\n3.2.2 Storage Location\nThe second question after what you’re storing is where. If you are using a database, then the answer is easy. The database just lives where it lives, and you’ll need to make sure you have a way to access it – both in terms of network access – as well as making sure you can authenticate into it (more on that below).\nIf you’re not using a database, then you’ll have to decide where to store the data for your app. Most apps that aren’t using a database start off rather naively with the data in the app bundle.\n<TODO: Image of data in app bundle>\nThis works really well during development and is an easy pattern to get started with. The problem is that this pattern generally falls apart when it goes to production. Usually this pattern works fine for a while. Problems start to arise when the data needs updating, and most data needs updating. Usually, you’ll be ready to update the data in the app long before you’re ready to update the app itself.\nAt this point, you’ll be kicking yourself that you now have to update the data inside the app every time you want to want to make a data update. It’s generally a better idea to have the data live outside the app bundle. Then you can update the data without mucking around with the app itself.\nA few options for this include just putting a flat file (or flat with differential read) into a directory near the app bundle. The pins package is also a great option here"
  },
  {
    "objectID": "chapters/sec1/1-3-proj-components.html#choosing-your-storage-solution",
    "href": "chapters/sec1/1-3-proj-components.html#choosing-your-storage-solution",
    "title": "3  Data Science Project Components",
    "section": "3.3 Choosing your storage solution",
    "text": "3.3 Choosing your storage solution\n\n3.3.1 How frequently are the data updated relative to the code?\nMany data apps have different update requirements for different data in the app.\nFor example, imagine you were the data scientist for a wildlife group that needed a dashboard to track the types of animals that had been spotted by a wilderness wildlife camera. You probably have a table that gives parameters for the animals themselves, perhaps things like endangered status, expected frequency, and more. That table probably needs to be updated very infrequently.\nOn the other hand, the day to day counts of the number of animals spotted probably needs to be updated much more frequently. <TODO: change to hospital example?>\nIf your data is updated only very infrequently, it might make sense to just bundle it up with the app code and update it on a similar cadence to the app itself.\n<TODO: Picture data in app bundle>\nOn the other hand, the more frequently updated data probably doesn’t make sense to update at the same cadence as the app code. You probably want to access that data in some sort of external location, perhaps on a mounted drive outside the app bundle, in a pin or bucket, or in a database.\nIn my experience, you almost never want to actually bundle data into the app. You almost always want to allow for the app data (“state”) to live outside the app and for the app to read it at runtime. Even data that you think will be updated infrequently, is unlikely to be updated as infrequently as your app code. Animals move on and off the endangered list, ingredient substitutions are made, and hospitals open and close and change their names in memoriam of someone.\nIt’s also worth considering whether your app needs a live data connection to do processing, or whether looking up values in a pre-processed table will suffice. The more complex the logic inside your app, the less likely you’ll be able to anticipate what users need, and the more likely you’ll have to do a live lookup.\n\n\n3.3.2 Is your app read-only, or does it have to write?\nMany data apps are read-only. This is nice. If you’re going to allow your app to write, you’ll need to be careful about permissions, protecting from data loss via SQL injection or other things, and have to be careful to check data quality.\nIf you want to save the data, you’ll also need a solution for that. There’s no one-size-fits-all answer here as it really depends on the sort of data you’re using. The main thing to keep in mind is that if you’re using a database, you’ll have to make sure you have write permissions."
  },
  {
    "objectID": "chapters/sec1/1-3-proj-components.html#when-does-the-app-fetch-its-data",
    "href": "chapters/sec1/1-3-proj-components.html#when-does-the-app-fetch-its-data",
    "title": "3  Data Science Project Components",
    "section": "3.4 When does the app fetch its data?",
    "text": "3.4 When does the app fetch its data?\nAt app open or throughout runtime?\nThe first important question you’ll have to figure out is what the requirements are for the code you’re trying to put into production.\n\n3.4.1 How big are the data in the app?\nWhen I ask this question, people often jump to the size of the raw data they’re using – but that’s often a completely irrelevant metric. You’re starting backwards if you start from the size of the raw data. Instead, you should figure out what’s the size of data you actually need inside the app.\nTo make this a little more concrete, let’s imagine you work for a large retailer and are responsible for creating a dashboard that will allow people to visualize the last week’s worth of sales for a variety of products. With this vague prompt, you could end up needing to load a huge amount of data into your app – or very little at all.\nOne of the most important questions is how much you can cache before someone even opens the app. For example, if you need to\n-> data granularity -> does it even need to be an app, or will a report do?\n\n\n3.4.2 What are the performance requirements for the app?\nOne crucial question for your app is how much wait time is acceptable for people wanting to see the app – and when is that waiting ok? For example, if people need to be able to make selections and see the results in realtime, then you probably need a snappy database, or all the data preloaded into memory when they show up.\nFor some apps, you want the data to be snappy throughout runtime, but it’s ok to have a lengthy startup process (perhaps because it can happen before the user actually arrives) and you want to load a lot of data as the app is starting and do much less throughout the app runtime.=\n\n\n3.4.3 Creating Performant Database Queries\nIf you are using a database, you’ll want to be careful about how you construct your queries to make sure they perform well. The main way to think about this is whether your queries will be eager or lazy.\nIn an eager app, you’ll pull basically all of the data for the app as it starts up, while a lazy app will pull data only as it is need.\n<TODO: Diagram of eager vs lazy data pulling>\nMaking your app eager is usually much simpler – you just read in all the data at the beginning. This is often a good first cut at writing an app, as you’re not sure exactly what requirements your app has. For relatively small datasets, this is often good enough.\nIf it seems like your app is starting up slowly – or your data’s too big to all pull in, you may want to pull data more lazily.\n\n\n\n\n\n\nTip\n\n\n\nBefore you start converting queries to speed up your app, it’s always worthwhile to profile your app and actually check that the data pulling is the slow step. I’ve often been wrong in my intuitions about what the slow step of the app is.\nThere’s nothing more annoying than spending hours refactoring your app to pull data more lazily only to realize that pulling the data was never the slow step to begin with.\n\n\nIt’s also worth considering how to make your queries perform better, regardless of when they occur in your code. Obviously you want to pull the minimum amount of data possible, so making data less granular, pulling in a smaller window of data, or pre-computing summaries is great when possible (though again, it’s worth profiling before you take on a lot of work that might result in minimal performance improvements).\nOnce you’ve decided whether to make your app eager or lazy, you can think about whether to make the query eager or lazy. In most cases, when you’re working with a database, the slowest part of the process is the actual process of pulling the data. That means that it’s generally worth it to be lazy with your query. And if you’re using dplyr from R, being eager vs lazy is simply a matter of where in the chain you put the collect statement.\nSo you’re better off sending a query off to the database, letting the database do a bunch of computations, and pulling a small results set back rather than pulling in a whole data set and doing computations in R.\n\n\n3.4.4 How to connect to databases?\nIn R, there are two answers to how to connect to a database. You can either use a direct connector to connect to the database, this generally will provide a driver to the DBI package. There are other database alternatives, but they’re pretty rare.\n<TODO: image of direct connection vs through driver>\nYou can also use an ODBC/JDBC driver to connect to the database. In this case, you’ll use something inside your R or Python session to use an database driver that has nothing to do with R or Python. Many organizations like these because IT/Admins can configure them on behalf of users and can be agnostic about whether users are using them from R, Python, or something else entirely.\nIf you’re in R, the odbc package gives you a way to interface with ODBC drivers. I’m unaware of a general solution for conencting to odbc drivers in Python.\nA DSN is a particular way to configure an ODBC driver. They are nice because it means that the Admin can fill in the connection details ahead of time, and you don’t need to know any details of the connection, other than your username and password.\n<TODO: image of how DSN works>\nIn R, writing a package that creates database connections for users is also a very popular way to provide database connections to the group."
  },
  {
    "objectID": "chapters/sec1/1-3-proj-components.html#how-do-i-do-data-authorization",
    "href": "chapters/sec1/1-3-proj-components.html#how-do-i-do-data-authorization",
    "title": "3  Data Science Project Components",
    "section": "3.5 How do I do data authorization?",
    "text": "3.5 How do I do data authorization?\nThis is a question you probably don’t think about much as you’re puttering around inside RStudio or in a Jupyter Notebook. But when you take an app to production, this becomes a crucial question.\nThe best and easiest case here is that everyone who views the app has the same permissions to see the data. In that case, you can just allow the app access to the data, and you can check whether someone is authorized to view the app as a whole, rather than at the data access layer.\nIn some cases, you might need to provide differential data access to different users. Sometimes this can be accomplished in the app itself. For example, if you can identify the user, you can gate access to certain tabs or features of your app. Many popular app hosting options for R and Python data science apps pass the username into the app as an environment variable.\nSometimes you might also have a column in a table that allows you to filter by who’s allowed to view, so you might just be able to filter to allowable rows in your database query.\nSometimes though, you’ll actually have to pass database credentials along to the database, which will do the authorization for you. This is nice, because then all you have to do is pass along the correct credntial, but it’s also a pain because you have to somehow get the credential and send it along with the query.\n<TODO: Image of how a kinit/JWT flow work>\nMost commonly, kerberos tickets or JWTs are used for this task. Usually your options for this depend on the database itself, and the ticket/JWT granting process will likely have to be handled by the database admin.\n\n3.5.1 Securely Managing Credentials\nThe single most important thing you can do to secure your credentials for your outside services is to avoid ever putting credentials in plaintext. The simplest alternative is to do a lookup from environment variables in either R or Python. There are many more secure things you can do, but it’s pretty trivial to put Sys.getenv(\"my_db_password\") into an app rather than actually typing the value. In that case, you’d set the variable in a .Rprofile or .Renviron .\nSimilarly, in Python, you can get and set environment variables using the os module. os.environ['DB_PASSWORD'] = 'my-pass' and os.getenv('DB_PASSWORD'), os.environ.get('DB_PASSWORD') or os.environ('DB_PASSWORD'). If you want to set environment variables from a file, generally people in Python use thedotenv package along with a .env file.\nYou should not commit these files to git, but should manually move them across environments, so they never appear anywhere centrally accessible.\nIn some organizations, this will still not be perceived secure enough, because the credentials are not encrypted at rest. Any of the aforementioned files are just plain text files – so if someone unauthorized were to get access to your machine, they’d be able to grab all of the goodies in your .Rprofile and use them themselves.\nSome hosting software, like RStudio Connect, can take care of this problem, as they store your environment variables inside the software in an encrypted fashion and inject them into the R runtime.\nThere are a number of more secure alternatives – but they generally require a little more work.\nThere are packages in both R and Python called keyring that allow you to use the system keyring to securely store environment variables and recall them at runtime. These can be good in a development environment, but run into trouble in a production environment because they generally rely on a user actually inputting a password for the system keyring.\nOne popular alternative is to use credentials pre-loaded on the system to enable using a ticket or token – often a Kerberos token or a JWT. This is generally quite do-able, but often requires some system-level configuration.\n<TODO: image of kerberos>\nYou may need to enable running as particular Linux users if you don’t want to do all of the authentication interactively in the browser. You usually cannot just recycle login tokens, because they are service authorization tokens, not token-granting tokens.1"
  },
  {
    "objectID": "chapters/sec1/1-3-proj-components.html#some-notes-if-youre-administering-the-service",
    "href": "chapters/sec1/1-3-proj-components.html#some-notes-if-youre-administering-the-service",
    "title": "3  Data Science Project Components",
    "section": "3.6 Some notes if you’re administering the service",
    "text": "3.6 Some notes if you’re administering the service\nThis chapter has mostly been intended for consumption by app authors who will be creating data science assets. But in some cases, you might also have to administer the service yourself. Here are a few tips and thoughts.\nChoosing which database to use in a given case is very complicated. There are dozens and dozens of different kinds of databases, and many different vendors, all trying to convince you that theirs is superior.\nIf you’re doing certain kinds of bespoke analytics, then it might really matter. In my experience, using postgres is good enough for most things involving rectangular data of moderate size, and a storage bucket is often good enough for things you might be tempted to put in a NoSQL database until the complexity of the data gets very large.\nEither way, a database as a service is a pretty basic cloud service. For example, AWS’s RDS is their simplest database service, and you can get a PostgreSQL, MySQL, MariaDB, or SQL Server database for very reasonable pricing.2 It works well for reasonably sized data loads (up to 64Tb).\nRDS is optimized for individual transactions. In some cases, you might want to consider a full data warehouse. AWS’s is called Redshift and it runs a flavor of PostgreSQL. In general it’s better when you have a lot of data (goes up to several petabytes) or are doing demanding queries a lot more often that you’re querying for individual rows. Redshift is a good bit more expensive, so it’s worth keeping that in mind.\nIf you’re storing data on a share drive, you’ll have to make sure that it’s available in your prod environment. This process is called mounting a drive or volume onto a server. It’s quite a straightforward process, but needing to mount a drive into two different servers places some constraints on where those servers have to be.\n<TODO: picture of mounting a drive>\nWhen you’re working in the cloud, you’ll get compute separate from volumes. This works nicely, because you can get the compute you need and separately choose a volume size that works for you. There are many nice tools around volumes that often include automated backups, and the ability to easily move snapshots from place to place – including just moving the same data onto a larger drive in just a few minutes.\nEBS is AWS’s name for their standard storage volumes. You get one whenever you’ve got an EC2 instance.\nIn some cases, you’ll need to have a drive that’s mounted to multiple machines at once, then you’ll need some sort of network drive.\nThe mounting process works exactly the same, but the underlying technology needs to be a little more complex to accommodate how it works. Depending on whether you’re talking about connecting multiple Linux hosts, you might use NFS (Network File Share), or SMB/CIFS (Windows only), or some combination of the two might use Samba. If you’re getting to this level, it’s probably a good idea to involve a professional IT/Admin.\nhttps://www.varonis.com/blog/cifs-vs-smb\nhttps://www.reddit.com/r/sysadmin/comments/1ei3ht/the_difference_between_nfs_cifs_smb_samba/\nhttps://cloudinfrastructureservices.co.uk/nfs-vs-smb/\nhttps://cloudinfrastructureservices.co.uk/nfs-vs-cifs/"
  },
  {
    "objectID": "chapters/sec1/1-3-proj-components.html#exercises",
    "href": "chapters/sec1/1-3-proj-components.html#exercises",
    "title": "3  Data Science Project Components",
    "section": "3.7 Exercises",
    "text": "3.7 Exercises\n\nConnect to and use an S3 bucket from the command line and from R/Python.\nStand up a container with Postgres in it, connect using isql, then from R/Python using ODBC/JDBC.\nStand up an EC2 instance w/ EBS volume, change EBS to different instances.\nStand up an NFS instance and mount onto a server."
  },
  {
    "objectID": "chapters/sec1/1-4-monitor-log.html",
    "href": "chapters/sec1/1-4-monitor-log.html",
    "title": "4  Logging and Monitoring",
    "section": "",
    "text": "Logging and monitoring are the tools used to make the operations of a computer system visible to the outside world. Logging is the process of keeping track of what’s happening inside an app for the purpose of understanding usage or debugging if something goes wrong. Monitoring is the process of checking on the health and performance of a system at a given point in time.\nBoth logging and monitoring have two halves – emitting and aggregating.\nThink, for example, about building a Shiny app. As you’re building your app, you have to think about what are the kinds of events you want to log – perhaps every time someone switches tabs in the app – and put code to do so in your app. Similarly, you’ve got to think about monitoring the app – are you going to save performance metrics for certain operations so you can monitor them over time.\nOn the flip side, there’s the aggregation part of the coin – let’s say your app does a great job emitting logs of all the right things. How will you be able to access those logs from the outside? And if your app is emitting monitoring data, how will you aggregate the monitoring metrics, detect if anything is awry, and alert the right people.\nTODO: diagram of monitoring + logging emission vs aggregation"
  },
  {
    "objectID": "chapters/sec1/1-4-monitor-log.html#you-should-do-more-logging",
    "href": "chapters/sec1/1-4-monitor-log.html#you-should-do-more-logging",
    "title": "4  Logging and Monitoring",
    "section": "4.1 You should do more logging",
    "text": "4.1 You should do more logging\nLogging is a really essential tool that’s underused in data science apps and assets.\nActually implementing logging is not hard. In fact, as a data scientist, you already do a lot of logging!\nThe language around logging and monitoring seems most relevant in the context of a running app or API. But there are all kinds of other things data scientists do, like create reports and run batch jobs.\nA (mildly) controversial take of mine is that data scientists should never run actual jobs in .R or .py files. Instead, all running analysis code should be in a self-documenting literate programming format like a Jupyter Notebook, R Markdown file, or Quarto document. If you exclusively use literate programming formats and are diligent about saving the rendered output after they run, you’ve already got logs of every time things run!\nTODO: Diagram of functions in .R and .py sourced into notebook.\nSo, what should go into .R and .py files? Function and class definitions. Use these formats to create the objects you’ll manipulate in your actual analysis that you can render and make visible to end users or yourself later.\nAnd if you’re thoughtful about what you note in the notebook, the document itself is the log.\nIf you’re creating an interactive app or API and are adding logging, you should use a formal logging library. For example, python has the standard logging package, and there are a number of great logging packages in R. I’m a particularly big fan of log4r.\n\n4.1.1 What to log\nI’ve generally seen three main purposes for logging in data science apps. The first is to log access operations for when people visit your app. If you aggregate these later, they can be really useful for making a business case about how important your app is. In addition to overall access to the app, logging when people access particular tabs or parts of the app can be really useful.\nIt’s worth noting that some commercial products where you might host your app, like RStudio Connect, create app access logs for you. You’re still responsible for logging anything that happens within your app, but the deployment platform takes care of logging who’s accessing your app and when.\nThe second is for audit trail reasons. In many cases, data science apps are read-only with respect to production data sources. I’d say this is generally a best practice, and if you can avoid writing production data sources from your app, that’s a great thing – create copies of data as needed. But if you must write to production data sources, being able to keep an audit trail of who made changes in the app and what they did is really essential.\nThe last purpose is for debugging reasons. If you’re running a production app, it’s really useful to log what people are doing inside your app. That way if you have an error that occurs, it’ll be much easier to understand what happened immediately before the error. The first step to debugging an error in an app is to be able to reproduce the conditions that caused it, which will be way easier if you implement a good logging system.\nOne important consideration is to make sure that you’re not accidentally logging sensitive information, like API keys to outside data sources in your logs.\n\n4.1.1.1 How to structure logs\nLogging is structured around the criticality of the logged message. For example, if you’re emitting logging from a Shiny app, you probably want to log every time someone changes tabs in the app, and also if something happens that makes the entire app crash – but you want to log them in different ways and be able to only pay attention to the logs you care about.\nFormal logging libraries offer you actual levels of logging, which you can then selectively expose in the logs themselves. While you will have to use whatever levels your logging library exposes, it’s great to have a conceptual model of different logging levels, so even if you’re not using a formal logging framework (for example in a notebook), you have a sense of what you’d want to log and how to do so.\nMost logging libraries have 5-7 levels of logging. The six below are reasonably common. If you understand these, you can condense or expand to match whatever framework you’re actually using. Six levels of logging from least to most granular:\n\nCritical/Fatal - an error so big that the app itself shuts down. For example, if your app cannot run without a connection to an outside service, you might log an inability to connect as a Critical error.\nError – an issue that will make an operation not work, but that won’t bring down your app. In the language of software engineering, you might think of this as a caught exception. An example might be a user submitting invalid input.\nWarning – an unexpected application issue that isn’t fatal. For example, you might include having to retry doing something or noticing that resource usage is high. If something were to go wrong later, these might be helpful breadcrumbs to look at.\nInfo – something normal happened in the app. These record things like starting and stopping, successfully making database and other connections, and configuration options that are used.\nTrace – a record of user interaction. If a user were to run into an issue, a trace should allow you to reconstruct what the user was doing when they ran into the issue and (hopefully) reproduce it.\nDebug – a deep record of what the app was doing. The difference between trace and debug is that the trace would be useful to reconstruct a sequence of events even if you weren’t familiar with the inner workings of the app. On the other hand, the debug log is meant for people who actually understand the app itself. Where a trace log would record “user pressed the button”, the debug might record the actual functions invoked and their arguments.\n\nThese aren’t hard-and-fast rules, but can be useful rules of thumb for how to use logging. Remember, far more important than sticking to any particular logging framework is creating logging that’s useful for whoever will be monitoring the app.\nAlmost all frameworks include Info, Warn, Error, and Critical/Fatal. Some have several levels more acute than Error to alert on what should happen as a result. Others (log4r, for example) only include one level more granular than info, so you’ll have to figure out how you want to adjust for that."
  },
  {
    "objectID": "chapters/sec1/1-4-monitor-log.html#do-you-really-need-to-monitor",
    "href": "chapters/sec1/1-4-monitor-log.html#do-you-really-need-to-monitor",
    "title": "4  Logging and Monitoring",
    "section": "4.2 Do you really need to monitor?",
    "text": "4.2 Do you really need to monitor?\nMany (probably most) data science assets are used to display the output of an analysis to stakeholders. So the running data science processes are either async scripts that run to output a static report or analysis that stakeholders can access, or they’re apps that are largely used to display data. In many cases, these apps are important, but minute-to-minute access is not and so the effort required to implement good monitoring and logging simply isn’t worth it.\nFor most data science use cases, monitoring is not terribly important. You don’t really care if your ETL job takes a few extra minutes, and the hassle of setting up a monitoring platform generally isn’t worth it if the audience for your app is small and they won’t care too much if it’s down for a few hours.\nThe one exception to this is monitoring machine learning models. In that case, you want to model performance over time to make sure that the model performance isn’t changing too much over time. This is a distinct question from whether speed performance of serving predictions is changing or degrading over time.\nRight now, the field of ML Ops is the next hot thing, and it’s not clear what the industry standards will be for monitoring machine learning model performance over time. My (admittedly biased) take is that I’m excited by the vetiver project, which is an open source framework for deploying machine learning models and monitoring their performance."
  },
  {
    "objectID": "chapters/sec1/1-5-docker.html",
    "href": "chapters/sec1/1-5-docker.html",
    "title": "5  Docker for Data Science",
    "section": "",
    "text": "If you’re in the data science world, especially the world of “production data science”, you’ve probably heard of Docker – but you might not really be sure what it is or whether you need it.\nThis chapter is designed to clarify what Docker is and how it might help you. We’ll start with a general intro to Docker, proceed with some discussion of data science-relevant Docker workflows, and then dive in to a hands-on lab that will be just the thing to get you started if you’re not sure where to go.\nIt’s worth noting that there are many entire books on how to use Docker in a software engineering context. This chapter is really just meant to give you a jumping off point. If you find that you want to get deep, I’d recommend you pick up another resource after this."
  },
  {
    "objectID": "chapters/sec1/1-5-docker.html#containers-are-a-packaging-tool",
    "href": "chapters/sec1/1-5-docker.html#containers-are-a-packaging-tool",
    "title": "5  Docker for Data Science",
    "section": "5.1 Containers are a Packaging Tool",
    "text": "5.1 Containers are a Packaging Tool\nLet’s tell a story that might feel familiar. A collaborator sends you a piece of code. You go to run it on your machine and an error pops up Python 3.7 not found. So you spend an hour on Stack Overflow, figuring out how to install version 3.7 of Python.\nThen you try to run the code again, which creates some maps, and this time get an error System library gdal not found. “Augh!” you cry, “Why is there not a way to include all of these dependencies with the code?!?”\nYou have just discovered one of the primary use cases for a container.\nContainers are a way to package up some code with all of its dependencies, making it easy to run the code later, share it with someone else for collaboration, or put it onto a production server – all while being reasonably confident that you won’t ever have to say, “well, it runs on my machine”.\nDocker is by far the most popular open-source containerization platform. So much so that for most purposes container is a synonym for Docker container.1 In this chapter, containers will exclusively refer to Docker containers.\nIn addition to making it easy to get all of the dependencies with an app, Docker also makes it easy to run a bunch of different isolated apps without having them interfere with each other.\nVirtual machines of various sorts have been around since the 1960s, and are still used for many applications. In contrast to a virtual machine, Docker is much more lightweight. Once a container has been downloaded to your machine, it can start up in less than a second.\nThis is why Docker – not the only, or even the first – open source containerization system was the first to hit the mainstream, as much as any esoteric code-development and deployment tool can be said to “hit the mainstream”.\nThis means that – for the most part – anything that can run in a Docker container in one place can be run on another machine with very minimal configuration.\n\n\n\n\n\n\nNote\n\n\n\nThere are exceptions. Until recently, a huge fraction of laptop CPUs were of a particular architecture called x86.\nApple’s recent M1 and M2 chips run on an ARM64 architecture, which had previously been used almost exclusively for phones and tablets. The details aren’t super important, but the upshot is that getting containers working on Apple silicon may not be trivial.\n\n\nDocker doesn’t completely negate the need for other sorts of IT tooling, because you still have to provision the physical hardware somehow, but it does make everything much more self-contained. And if you’ve already got a laptop, you can easily run Docker containers with just a few commands (we’ll get to that below)."
  },
  {
    "objectID": "chapters/sec1/1-5-docker.html#containers-for-data-science",
    "href": "chapters/sec1/1-5-docker.html#containers-for-data-science",
    "title": "5  Docker for Data Science",
    "section": "5.2 Containers for Data Science",
    "text": "5.2 Containers for Data Science\nIn a data science context, there are two main ways you might use containers – as a way to package a development environment for someone else to use, and as a way to package a finished app for archiving, reproducibility, and production.\n\n\n\n\n\n\nThe Data Science Reproducibility Stack\n\n\n\nA reminder from the reproducibility chapter:\nThe data science reproducibility stack generally includes 6 elements:\n\nCode\nData\nR + Python Packages\nR + Python Versions\nOther System Libraries\nOperating System\n\n\n\nIf you’re running RStudio Server or JupyterHub on a centralized server, Docker can be a great way to maintain that server. In my opinion, maintaining a Docker container is one of the easiest ways to start on an infrastructure-as-code journey.\nWe’re not going to get terribly deep into this use case, as creating the overwhelming majority of the work involved is standard IT/Admin tasks for hosting a server - things like managing networking, authentication and authorization, security, and more.\n\n\n\n\n\n\nNote\n\n\n\nIf you’re thinking about hosting a data science workbench in Docker, you should think carefully about whether you want to deal with standalone containers, or whether you’re really looking for container orchestration using Kubernetes.\n\n\nIn this chapter, I’ll suggest trying to stand up RStudio Server in a container on your desktop, but don’t let the ease fool you. The majority of difficulties with administering a server are the same, even if you put your application stack into a Docker container. Section II of this book will have a lot more on those challenges, and I suggest you check it out if you’re interested.\nInstead, we’re going to stick with talking about how actual data scientists would want to use Docker – to archive and share completed data science assets.\n\nIn this pattern, you’ll put your whole reproducibility stack inside the container itself – perhaps minus your data."
  },
  {
    "objectID": "chapters/sec1/1-5-docker.html#container-gotchas",
    "href": "chapters/sec1/1-5-docker.html#container-gotchas",
    "title": "5  Docker for Data Science",
    "section": "5.3 Container Gotchas",
    "text": "5.3 Container Gotchas\nDocker containers are great for certain purposes, but there are also some tradeoffs from working inside a Docker container that it’s worth being aware of.\nThe first is the tradeoff of Docker’s strength – a container only gets access to the resources it has specifically been allowed to access.\nThis is a great feature for security and process isolation, but it means you may run into some issues with networking and access to system resources, like your files. You’ll have to develop a reasonably good mental model of the relationship of the Docker container to the rest of your machine in order to be able to develop effectively.\nIt’s worth noting that in some environments – especially highly-regulated ones – a Docker container may not be a sufficient level of reproducibility. Differences between machines at the physical hardware level could potentially mean that numeric solutions could differ across machines, even with the same container. You probably know if you’re in this kind of environment and you have to maintain physical machines.\nThere are also several antipatterns that using a container could facilitate.\nThe biggest reproducibility headache for most data scientists is managing R and Python package environments. While you can just install a bunch of packages into a container, save the container state, and move on, this really isn’t a good solution.\nIf you do this, you’ve got the last state of your environment saved, but it’s not really reproducible. If you come back next year and need to add a new package, you’ll have no way to do it without potentially breaking the whole environment.\nThe obvious solution is to write down the steps for creating your Docker container – in a file called a Dockerfile. Here, it’s tempting to create a Dockerfile that looks like:\n...\nRUN /opt/R/4.1.0/bin/R install.packages(c(\"shiny\", \"dplyr\"))\n...\nBut this is also completely non-reproducible. Whenever you rebuild your container, you’ll install the newest versions of Shiny and Dplyr afresh, potentially ruining the reproducibility of your code. For that reason, the best move is to combine the ability of R and Python-specific libraries for capturing package state – like renv and rig in R and virtualenv , conda , and pyenv in Python are a much better choice than going all in on Docker. More on those topics in the chapter on environments."
  },
  {
    "objectID": "chapters/sec1/1-5-docker.html#trying-out-docker",
    "href": "chapters/sec1/1-5-docker.html#trying-out-docker",
    "title": "5  Docker for Data Science",
    "section": "5.4 Trying out Docker",
    "text": "5.4 Trying out Docker\nIf you’ve read this far, you have a reasonably good mental model of when you might want to use Docker to encapsulate your data science environment or when you might not. The rest of this chapter will be a hands-on intro to using Docker to run a finished app.\nThere are many great resources out there for really learning Docker. I’d suggest picking up one of them rather than relying on this book to learn everything you need to know about Docker – but hopefully this can get you started.\n\n5.4.1 Prerequisites\nIn order to get started with Docker, you’ll need to know a few things. The first is that you’ll have to actually have Docker installed on an environment you can use. The easiest way to do this is to install Docker Desktop on your laptop, but you can also put Docker on a server environment.\nIn order to follow along, you’ll also need to be able to open and use a terminal, see the section on using the command line if you’re not already comfortable.\n\n\n5.4.2 Getting Started\nLet’s get started with an example that demonstrates the power of Docker right off the bat.\nOnce you’ve got Docker Desktop installed and running, type the following in your terminal:\ndocker run --rm -d \\\n  -p 8000:8000 \\\n  --name palmer-plumber \\\n  alexkgold/plumber\nOnce you type in this command, it’ll take a minute to pull, extract, and start the container.\nOnce the container starts up, you’ll see a long sequence of letters and numbers. Now, navigate to http://localhost:8000/docs/ in your browser (this URL has to be exact!), and you should see the documentation for an R language API that lets you explore the Palmer Penguins data set\nThat was probably pretty uninspiring. It took a long time to download and get started. In order to show the real power of Docker, let’s now kill the container with\ndocker kill palmer-plumber\nYou can check that the container isn’t running by trying to visit that URL again. You’ll get an error.\nLet’s bring the container back up by running the docker run command above again.\nThis time is should be quick – probably less than a second – now that you’ve got the container downloaded. THIS is the power of Docker.\n\nAs you click around, seeing penguin stats and seeing plots, you might notice that nothing is showing up on the command line…but what if I want logs of what people are doing? Or I need to look at the app code?\nYou can get into the container to poke around using the command\ndocker exec -it palmer-plumber /bin/bash\nOnce you’re in, try cat api/plumber.R to look at the code of the running API.\ndocker exec is a general purpose command for executing a command inside a running container. The overwhelming majority of the time I use it, it’s to get a terminal inside a running container so I can poke around.\nYou can spend a lot of time getting deep into why the command works, but just memorizing (or, more likely, repeatedly googling) docker exec -it <container> /bin/bash will get you pretty far.\n\n\n\n\n\n\nNote\n\n\n\nIf you’re used to running things on servers, you might be in the habit of SSH-ing in, poking around, and fixing things that are broken. This isn’t great for a lot of reasons, but it’s a huge anti-pattern in Docker land.\nContainers are stateless and immutable. This means that anything that happens in the container stays in the container – even when the container goes away. If something goes wrong in your running container, you may need to exec in to poke around, but you should fix it by rebuilding and redeploying your image, not by changing the running container.\n\n\nOne nicety of Docker is that it gives you quick access to the most common reason you’d probably exec into the container – looking at logs.\nAfter you’ve clicked around a little in the API, try running:\ndocker logs palmer-plumber\n\nGreat! We’ve played around with this container pretty thoroughly.\nBefore we get into how this all works, let’s try one more example.\nGo back into your terminal and navigate to a directory you can play around in (the cd command is your friend here). Run the following in your terminal:\ndocker run \\\n-v $(pwd):/project-out \\\nalexkgold/batch:0.1\nIt’ll take a minute to download – this container is about 600Mb. You may need to grant the container access to a directory on your machine when it runs. This container will take a few moments to run. If you go to the directory in file browser, you should be able to open hello.html in your web browser – it should be a rendered version of a Jupyter Notebook.\nThis notebook is just a very basic visualization, but you can see how it’s nice to be able to render a Jupyter Notebook locally without having to worry about making sure you had any of the dependencies installed. This is good both for running on demand, and also for archival purposes.\n\nNow that we’ve got Docker working for you, let’s take a step back, explain what we just did, and dive deeper into how this can be helpful.\nHopefully these two examples are exciting – in the first, we got an interactive web API running like a server on our laptop in just a few seconds – and without installing any of the packages or even a version of R locally. In the second, we rendered a Jupyter Notebook using the quarto library – again, without worrying about downloading it locally."
  },
  {
    "objectID": "chapters/sec1/1-5-docker.html#container-lifecycle",
    "href": "chapters/sec1/1-5-docker.html#container-lifecycle",
    "title": "5  Docker for Data Science",
    "section": "5.5 Container Lifecycle",
    "text": "5.5 Container Lifecycle\nBefore we dig into the nitty-gritty of how that all worked – and how you might change it for your own purposes, let’s spend just a minute clarifying the lifecycle of a Docker container.\nThis image explains the different states a Docker container can be in, and the commands you’ll need to move them around.\n\nA container starts its life as a Dockerfile. A Dockerfile is a set of instructions for how to build a container. Dockerfiles are usually stored in a git repository, just like any other code, and it’s common to build them on push via a CI/CD pipeline.\nA working Dockerfile gets built into a Docker image with the build command. Images are immutable snapshots of the state of the container at a given time.\nIt is possible to interactively build a container as you go and snapshot to create an image, but for the purposes of reproducibility, it’s generally preferable to build the image from a Dockerfile, and adjust the Dockerfile if you need to adjust the image.\nUsually, the image is going to be the thing that you share with other people, as it’s the version of the container that’s compiled and ready to go.\nDocker images can be shared directly like any other file, or via sharing on an image registry via the push and pull commands.\nIf you’re familiar with git, the mental model for Docker is quite similar. There is a public Docker Hub you can use, and it’s also possible to run private image registries. Many organizations make use of the image registries as a service offerings from cloud providers. The big 3’s are Amazon’s Elastic Container Registry (ECR), Azure Container Registry, and Google Container Registry.\nOnce you’ve got an image downloaded locally, you can run it with the run command. Note that you generally don’t have to pull before running a container, as it will auto-pull if it’s not available.\nNow that you’re all excited, let’s dig in on how the docker run command works, and the command line flags we used here, which are the ones you’ll use most often."
  },
  {
    "objectID": "chapters/sec1/1-5-docker.html#understanding-docker-run",
    "href": "chapters/sec1/1-5-docker.html#understanding-docker-run",
    "title": "5  Docker for Data Science",
    "section": "5.6 Understanding docker run",
    "text": "5.6 Understanding docker run\nAt it’s most basic, all you need to know is that you can run a Docker image locally using the docker run <name> command. However, Docker commands usually use a lot of command line flags – so many that it’s easy to miss what the command actually is.\n\n\n\n\n\n\nNote\n\n\n\nA command line flag is an argument passed to a command line program. If you’re not familiar, it might be helpful to start by checking out that section of the book.\n\n\nLet’s pull apart the two commands we just used – which use the command line flags you’re most likely to need.\n\n5.6.1 Parsing container names\nTo start with, let’s parse the name of the container. In this example, you used two different container names – alexkgold/plumber and alexkgold/batch:0.1. All containers have an id, and they may also have a tag. If you’re using the public DockerHub registry, like I am, container ids are of the form <user>/<name>. This should look very familiar if you already use a git repository.\nIn addition to an id, containers can also have a tag. For example, for the alexkgold/batch image, we specified a version: 0.1. If you don’t specify a tag when pulling or pushing an image, you’ll automatically create or get latest – the newest version of a container that was pushed to the registry.\nUsers often create tags that are relevant to the container – often versions of the software contained within. For example, the rocker/r-ver container, which is a container pre-built with a version of R in it uses tags for the version of R.\nAll these examples use the public DockerHub. Many organizations use a private image registry, in which case you can prefix the container name with the URL of the registry.\n\n\n5.6.2 docker run flags\nIn this section we’re going to go through the docker run flags we used in quite a bit of detail.\n\n\n\n\n\n\nNote\n\n\n\nIf you just want a quick reference later, there’s a cheatsheet in the appendix.\n\n\nLet’s first look at how we ran the container with the plumber API in it.\nFor this container, we used the --rm flag, the -d flag, the -p flag with the argument 8000:8000, and the --name flag with the argument plumber-palmer.\nThe --rm flag removes the container after it finishes running. This is nice when you’re just playing around with a container locally because then you can use the same container name repeatedly, but it’s a flag you’ll almost never use in production because it removes everything from the container, including logs.\nYou can check this by running docker kill palmer-plumber to make sure the container is down and then try to get to the logs with docker logs palmer-plumber. But they don’t exist because they got cleaned up!\nFeel free to try running to container without the --rm flag, playing around, killing the container, and then looking at the logs. Before you’re able to bring back another container with the same name, you’ll have to remove the container with docker rm palmer-plumber.\nThe -d flag instructs the container to run in detached mode so the container won’t block the terminal session. You can feel free to run the container attached – but you’ll have to quit the container by aborting the command from inside the terminal (Ctrl + c), or opening another terminal to docker kill the container.\nThe -p flag publishes a port from inside the container to the host machine. So by specifying -p 8000:8000, we’re taking whatever’s available on the port 8000 inside the container and making it available at the same port on the localhost of the machine that’s hosting the container.\nTODO: picture of ports\nPort forwarding is always specified as <host port>:<container port>. Try playing around with changing the values to make the API available on a different port, perhaps 9876. For a more in-depth treatment of ports, see the section later in the book.\nTODO: link to ports section\nThe --name flag gives our container a name. This is really just a convenience so that you could do commands like docker kill in terms of the container name, rather than the container ID, which will be different for each person who runs the command.\nIn a lot of cases, you won’t bother with a name for the container.\nYou can find container ID using the docker ps command to get the process status. In the case below, I could control the container with the name palmer-plumber, or with the container ID. You can abbreviate container IDs as long as they’re unique – I tend to use the first three characters.\n❯ docker ps                                                         [12:23:13]\n\nCONTAINER ID   IMAGE               COMMAND                  CREATED          STATUS          PORTS                    NAMES\n\n35bd54e44015   alexkgold/plumber   \"R -e 'pr <- plumber…\"   29 seconds ago   Up 28 seconds   0.0.0.0:8000->8000/tcp   palmer-plumber\n\nNow let’s head over to the batch document rendering, where we only used one command line flag -v $(pwd):/project-out, short for volume. To demonstrate what this argument does, navigate to a new directory on your command line and re-run the container without the argument.\nWait…where’d my document go?\nRemember – containers are completely ephemeral. What happens in the container stays in the container. This means that when my document is rendered inside the container, it gets deleted when the container ends its job.\nBut that’s not what I wanted – I wanted to get the output back out of the container.\nThe solution – making data outside the container available to the container and vice-versa – is accomplished by mounting a volume into the container using the -v flag. Like with mounting a port, the syntax is -v <directory outside container>:<directory inside container>.\n\nThis is an essential concept to understand when working with containers. Because containers are so ephemeral, volumes are the way to get anything from your host machine in, and to persist anything that you want to outlast the lifecycle of the container.\nIn this case, we actually used a variable $(pwd), which will be evaluated to the current working directory to be the directory project-out inside the container, so the rendered document can be persisted after the container goes away."
  },
  {
    "objectID": "chapters/sec1/1-5-docker.html#build-your-own-with-dockerfiles",
    "href": "chapters/sec1/1-5-docker.html#build-your-own-with-dockerfiles",
    "title": "5  Docker for Data Science",
    "section": "5.7 Build your own with Dockerfiles",
    "text": "5.7 Build your own with Dockerfiles\nSo far, we’ve just been working on running containers based on images I’ve already prepared for you. Let’s look at how those images got created so you can try building your own.\nA Dockerfile is just a set of instructions that you use to build a Docker image. If you have a pretty good idea how to accomplish something on a running machine, you shouldn’t have too much trouble building a Dockerfile to do the same as long as you remember two things:\nTODO: Image of build vs run time\n\nThe difference between build time and run time. There are things that should happen at build time – like setting up the versions of R and Python, copying in the code you’ll run, and installing the system requirements. That’s very different from the thing I want to have happen at run time – rendering the notebook or running the API.\nDocker containers only have access to exactly the resources you provide to them at both build and runtime. That means that they won’t have access to libraries or programs unless you give them access, and you also won’t have access to files from your computer unless you make them available.\n\nThere are many different commands you can use inside a Dockerfile, but with just a handful, you’ll be able to build most images you might need.\nHere are the important commands you’ll need for getting everything you need into your images.\n\nFROM – every container starts from a base image. In some cases, like in my Jupyter example, you might start with a bare bones container that’s just the operating system (ubuntu:20.04). In other cases, like in my shiny example, you might start with a container that’s almost there, and all you need to do is to copy in a file or two.\nRUN – run any command as if you were sitting at the command line inside the container. Just remember, if you’re starting from a very basic container, you may need to make a command available before you can run it (like wget in my container below).\nCOPY – copy a file from the host filesystem into the container. Note that the working directory for your Dockerfile will be whatever your working directory is when you run your build command.\n\nOne really nice thing about Docker containers is that they’re build in layers. Each command in the Dockerfile defines a new layer. If you make changes below a given layer in your Dockerfile, rebuilding will be easy, because Docker will only start rebuilding at the layer with changes.\nIf you’re mainly building containers for finished data science assets to be re-run on demand, there’s only one command you need:\n\nCMD - Specifies what command to run inside the container’s shell at runtime. This would be the same command you’d use to run your project from the command line.\n\nIf you do much digging, you’ll probably run into the ENTRYPOINT command, which can take a while to tell apart from CMD. If you’re building containers to run finished data science assets, you shouldn’t need ENTRYPOINT. If you’re building containers to – for example – accept a different asset to run or allow for particular arguments, you’ll need to use ENTRYPOINT to specify the command that will always run and CMD to specify the default arguments to ENTRYPOINT, which can be overridden on the command line.2\nSo here’s the Dockerfile I used to build the container for the Jupyter Notebook rendering. Look through it, can you understand what it’s doing?\n# syntax=docker/dockerfile:1\nFROM ubuntu:20.04\n\n# Copy external files\nRUN mkdir -p /project/out/\n\nCOPY ./requirements.txt /project/\nCOPY ./hello.ipynb /project/\n\n# Install system packages\nRUN apt-get update && apt-get install -y \\\n  wget python3 python3-pip\n\n# Install quarto CLI + clean up\nRUN wget https://github.com/quarto-dev/quarto-cli/releases/download/v0.9.83/quarto-0.9.83-linux-amd64.deb\nRUN dpkg -i ./quarto-0.9.83-linux-amd64.deb\nRUN rm -f ./quarto-0.9.83-linux-amd64.deb\n\n# Install Python requirements\nRUN pip3 install -r /project/requirements.txt\n\n# Render notebook\nCMD cd /project && \\\n  quarto render ./hello.ipynb && \\\n  # Move output to correct directory\n  # Needed because quarto requires relative paths in --output-dir: \n  # https://github.com/quarto-dev/quarto-cli/issues/362\n  rm -rf /project-out/hello_files/ && \\\n  mkdir -p /project-out/hello_files && \\\n  mv ./hello_files/* /project-out/hello_files/ && \\\n  mv ./hello.html /project-out/\nOnce you’ve created your Dockerfile, you build it into an image using docker build -t <image name> and then you can push that to DockerHub or another registry using docker push."
  },
  {
    "objectID": "chapters/sec1/1-5-docker.html#exercises",
    "href": "chapters/sec1/1-5-docker.html#exercises",
    "title": "5  Docker for Data Science",
    "section": "5.8 Exercises",
    "text": "5.8 Exercises\n\nSee if you can get a copy of RStudio Server running in a container on your desktop using the rocker/rstudio container. Can you access your home directory from RStudio Server?\n\nHint 1: You’ll probably need all of the docker run flags you’ve learned and one more – the -e KEY=value flag provides an environment variable to the running container.\nHint 2: The default port for RStudio Server is 8787.\nHint 3: If you’re running a laptop with Apple Silicon (M1 or M2), you may need to try a different container. amoselb/rstudio-m1 worked for me.3\n\nCan you build your own container to house an interactive app built in Shiny, Dash, Streamlit, or another framework of your choice?"
  },
  {
    "objectID": "chapters/sec2/2-0-sec-intro.html",
    "href": "chapters/sec2/2-0-sec-intro.html",
    "title": "DIY DevOps for Data Science",
    "section": "",
    "text": "But if you’ve got the need, it is possible to manage your own server.\nI’m not going to say it’s easy, but it is straightforward.\nIn this section, we’re going to explore two main topics – how to set up a data science workbench on a server, and how to host an app of your own.\nThis section is going to be a hands-on walkthrough of setting up a data science workbench of your own that includes both RStudio Server and JupyterHub. Throughout, we’re going to combine hands-on labs with explanations of why we’re doing what we’re doing so that you can feel confident you’ve got it right – or can adjust if the context you’re working in is somewhat different than the one in the book.\nWe’re going to start by learning about what the cloud is and the services you can use there. Then we’re going to set up a cloud server on AWS.\nNext, we’re going to learn about the command line and SSH. We’re going to use them to access the server we just created and poke around a little bit.\nNext, we’re going to learn just a little about Linux system administration including how installing software, managing processes, and managing users works. We’re going to actually install R, Python, RStudio Server, and JupyterHub on the server.\nNext, we’re going to learn a little bit about server networking, DNS, and how to keep web traffic secure. We’re going to configure our server with a real URL and an SSL certificate, so you can access your server securely and at a memorable web address.\nIn the next section, we’re going to stand up a database and connect to it.\nIn the last chapter, we’re going to talk about all the things we didn’t do. By the time you’re done, this server will be totally usable for basic data science workloads and probably will be fine if your company has a relatively permissive security policy. In the last section we’re going to talk about the things we didn’t do – including using networking to further protect our server, design a more sophisticated data science environment, and integrate with centralized authentication providers.\nOther things we could do (TBD):\n\nSet up a Shiny/Dash/Streamlit app to be hosted on the same server\nDo the same thing from a container"
  },
  {
    "objectID": "chapters/sec2/2-1-cloud.html",
    "href": "chapters/sec2/2-1-cloud.html",
    "title": "6  Getting Started with the Cloud",
    "section": "",
    "text": "Like so many tech buzzwords, it’s pretty hard to get a sense for what the cloud actually is beneath all the hype.\nThe Cloud is a description for servers you rent instead of buy.\nBack in the day – and still in some heavily-regulated industries – getting a new server involved buying a physical piece of hardware, installing it in a server room, getting it configured and up and running, installing the software you want on the server, and configuring access to the server.\nThe Cloud provides layers of “as a service” on top of this former world where someone at your organization would be responsible for buying and maintaining actual hardware.\nIn this chapter, you’ll learn a little bit about how The Cloud works, and how to demystify cloud offerings."
  },
  {
    "objectID": "chapters/sec2/2-1-cloud.html#the-rise-of-services",
    "href": "chapters/sec2/2-1-cloud.html#the-rise-of-services",
    "title": "6  Getting Started with the Cloud",
    "section": "6.1 The Rise of Services",
    "text": "6.1 The Rise of Services\nLike much of the rest of the economy, server provisioning and use has gone the way of services. Instead of buying, owning, and maintaining a physical object, a huge proportion of the world’s server hardware is rented.\n[TODO: quote in paragraph below: https://www.srgresearch.com/articles/cloud-market-growth-rate-nudges-amazon-and-microsoft-solidify-leadership]\nIn the US, a huge fraction runs on servers rented from one of three organizations (in order of how significant they are) – Amazon Web Services (AWS), Microsoft Azure, and Google Cloud Platform (GCP). These three companies account for a huge proportion of what we think of as “The Cloud”. There are other smaller players, and also companies that are popular for particular tasks, like Netlify for hosting static websites.\nHowever, in many cases, the cloud doesn’t just refer to renting a server itself. There are also layers and layers of services on top of the “rent me a server” business.\nIn general, the “rent me a server” model is called Infrastructure as a Service (IaaS - pronounced eye-az). So when you stood up an EC2 instance from AWS in the first chapter, you were using AWS’s IaaS offering.\nIn general, people split the layers on top of IaaS into two categories – Platform as a Service (PaaS – pronounced pass), and Software as a Service (SaaS pronounced sass).\nPaaS is where you rent an environment in which to do development. On the other hand, SaaS is renting software as an end user.\nOne common way to explain the difference is using a baking metaphor. Consider making a cake. An on-prem install would be where you make the cake completely from scratch. A IaaS experience would be buying cake mix and baking at home. PaaS would be buying a premade blank cake that you decorate yourself, and SaaS would be just buying a store-bought cake.\nI find these categories and this metaphor sorta helpful in the abstract, but when getting down to concrete real-world examples, the distinctions get fuzzier, and you have to be careful which perspective you’re talking about.\nFor example, RStudio Cloud is a service where you can get an environment with RStudio preconfigured and ready to use. From the perspective of a data scientist, this is clearly a PaaS offering. RStudio is providing a platform where you can learn or do work as a data scientist.\nBut from the perspective of an IT/Admin considering how to set up a server-based data science environment inside their company, RStudio Cloud is clearly a SaaS offering – you just getting the software configured and ready to use.\nMaking the issue even more difficult, many companies go out of their way to make their services sound grand and important and don’t just say, “this is ___ as a service”. Moreover, it’s very common (especially in AWS) to have many different services that fulfill similar needs, and it can be really hard to concretely tell the difference.\nFor example, if you go to AWS’s database offerings for a “database as a service”, your options include Redshift, RDS, Aurora, DynamoDB, ElastiCache, MemoryDB for Redis, DocumentDB, Keyspaces, Neptune, Timestream, and more.\nThere’s a reason why there’s a meaningful industry of people whose full time job is to consult on which AWS service your company needs and how to take advantage of the pricing rules to make sure you get a good deal.\nThere are a few reasons why organizations are moving to the cloud. Primary among them is that maintaining physical servers is often not the core competency of IT/Admin organizations. They’d rather manage higher-level abstractions than physical servers – or increasingly even virtual servers.\nOne reason that people cite, but very rarely comes to pass, is cost. In theory, the flexibility of the cloud should allow organizations to stand up servers as needed and spin them down when they’re not needed. This flexibility is real, there are times when it’s super useful to be able to bring up a server for a particular project – it’s often far quicker and easier than buying and installing a server of your own.\nIn reality, the engineering needed to stand up and spin down these servers at the right time is really difficult and costly – enough so that most organizations could probably substantailly save money if they repatriated their cloud workloads.\nFor more established organizations, running workloads in the cloud may, in fact, be substantially more costly than just bringing those workloads on prem."
  },
  {
    "objectID": "chapters/sec2/2-1-cloud.html#serverless-computing",
    "href": "chapters/sec2/2-1-cloud.html#serverless-computing",
    "title": "6  Getting Started with the Cloud",
    "section": "6.2 Serverless Computing",
    "text": "6.2 Serverless Computing\nIn the past few years, there has been a rise in interest in “serverless” computing. This is a buzzword and there’s no one shared definition of what serverless means. It’s worth making clear that there is no such thing as truly serverless computing. Every bit of cloud computation runs on a server - the question is whether you have to deal with the server or if someone else is doing it for you.\nHowever, there are two distinct things happening that can meaningfully be described as serverless…but they’re completely different.\nOne is the rise of containerization. In chapter XXXX, we’ll get deep into the weeds on using docker yourself. Docker is a very cool tool that makes software much more portable, because you can bring the environment – all the way down to the operating system – around with you very easily. This is kinda a superpower, and many organizations are moving towards using docker containers as the atomic units of their IT infrastructure, so the IT organization doesn’t manage any servers directly, and instead just manages docker containers.\nIn some sense, this is meaningfully serverless. You’ve moved the level of abstraction up a layer from servers and virtual machines to docker containers. And managing docker containers is often meaningfully easier than managing virtual machines directly. However, you still face a lot of the same problems like versioning operating systems, dealing with storage and networking yourself, and more.\nThere is another, stronger, use of serverless that is rising and is also pretty cool, but is super different. In these services, you just hand off a function, written in soem programming langauge to a cloud provider, and they run that function as a service. In a trivial example, imagine a service that adds two numbers together. You could write a Python or R function that does this addition and returns it.\nIt is possible to just deploy this function to a Cloud provider’s environment and then only pay for the actual compute time needed to complete your function calls. This is obviously very appealing because you really don’t have to manage anything at the server level. The disadvantage is that this works only for certain types of operations."
  },
  {
    "objectID": "chapters/sec2/2-1-cloud.html#common-services-that-will-be-helpful-to-know-offhand",
    "href": "chapters/sec2/2-1-cloud.html#common-services-that-will-be-helpful-to-know-offhand",
    "title": "6  Getting Started with the Cloud",
    "section": "6.3 Common Services That will be helpful to know offhand",
    "text": "6.3 Common Services That will be helpful to know offhand\nLuckily, if you’re thinking about setting up and running a standard data science platform in one of the major cloud providers, you’re likely to use one of a few reasonably standard tools.\nIt’s helpful to keep in mind that at the very bottom, there are four basic cloud services: renting servers, configuring networking, identity management, and renting storage. All the other services are recombinations and software installed on top of that.1\nAzure and GCP tend to name their offerings pretty literally. AWS, on the other hand, uses names that have little relationship to the actual task at hand, and you’ll just need to learn them.\n\n6.3.1 IaaS\n\nCompute - AWS EC2, Azure VMs, Google Compute Engine\nStorage\n\nfile - EBS, Azure managed disk, Google Persistent Disk\nNetwork drives - EFS, Azure Files, Google Filestore\nblock/blob - S3, Azure Blob Storage, Google Cloud Storage\n\nNetworking:\n\nPrivate Clouds: VPC, Virtual Network, Google Virtual Private Cloud\nDNS - Route53, Azure DNS + Traffic Manager, Google Cloud DNS\n\n\n\n\n6.3.2 Not IaaS\n\nContainer Hosting - ECS, Azure Container Instances + Container Registry\nK8S cluster as a service - EKS, AKS, GKE\nRun a function as a service - Lambda, Azure Functions, Google Cloud Functions\nDatabase - RDS/Redshift, Azure Database, Google Cloud SQL\nSageMaker - ML platform as a service, Azure ML, Google Notebooks\n\nhttps://docs.microsoft.com/en-us/azure/architecture/aws-professional/services#networking https://cloud.google.com/free/docs/aws-azure-gcp-service-comparison"
  },
  {
    "objectID": "chapters/sec2/2-1-cloud.html#cloud-tooling",
    "href": "chapters/sec2/2-1-cloud.html#cloud-tooling",
    "title": "6  Getting Started with the Cloud",
    "section": "6.4 Cloud Tooling",
    "text": "6.4 Cloud Tooling\n\nIdentity MGMT - IAM, Azure AD\nBilling Mgmt\n\nIaaC tooling\n\n6.4.1 AWS Instance Classes for Data Science\nt3 – good b/c of instance credits, limited size\nCs – good b/c fast CPUs\nR - good b/c high amount of RAM\nP - have GPUs, but also v expensive\nInstance scale linearly w/ number of cores (plot?)"
  },
  {
    "objectID": "chapters/sec2/2-1-cloud.html#lab",
    "href": "chapters/sec2/2-1-cloud.html#lab",
    "title": "6  Getting Started with the Cloud",
    "section": "6.5 Lab",
    "text": "6.5 Lab\n\n6.5.1 Login to the AWS Console\nWe’re going to be standing up a server on Amazon Web Services (AWS). If you feel comfortable, feel free to do this in any cloud provider. All cloud providers have their own equivalents of all the things we’re about to do in AWS.\nWe will be standing up a server in AWS’s free tier – so there will be no cost involved as long as you haven’t used up all your AWS free tier credits before now.\nWe’re going to start by logging into AWS. If you’ve done this before, just go ahead and log in.\nIf not, go to https://aws.amazon.com and click Sign In to the Console .\nIf you’ve never set up an AWS account before, click Create a New AWS account and follow the instructions to create an account. Note that even if you’ve got an Amazon account for ordering stuff online and watching movies, an AWS account is separate.\nOnce you’ve logged in, you’ll be confronted by the AWS console. There are a ton of things here, and it’s rather overwhelming. If you feel like you have to understand what all these things are before going ahead, feel free to check out the chapter on The Cloud.\n\n\n6.5.2 Stand up an instance\nClick on the EC2 service (it’s under Launch a virtual machine or Compute depending on where you landed). The table has all the different quick start Amazon Machine Images (AMIs). Find and click Ubuntu Server 20.04 LTS – it’ll be one of the first handful.\nNow you’ll be seeing the instance size chooser. It should have auto-selected a server with the label Free tier eligible. Just stick with this for now.\nScroll down and click Review and Launch, and Launch on the next page.\nWhen you click Launch, you’ll be asked to use a key pair. Assuming you don’t have an existing keypair, select Create a new key pair, name it my_test_key, and click Download.\nKeep track of the my_test_key.pem file your computer downloads, you’ll need it again in a minute.\nClick Launch Instances. AWS is now creating a virtual server just for you. If you click View Instances in the lower right, you’ll see your instance. When the instance state switches to Running, it’s up and running!\n\n\n6.5.3 Configure Server Access\nTODO: is this necessary just for SSH? I thought the default sg includes SSH.\nBefore we leave the AWS console, let’s make sure that we’ll be able to get to it from the internet.\nScroll down to the Security tab. Click the blue link under Security Groups, which will start with sg- and include launch-wizard- in parentheses.\nClick Edit inbound rules , then Add rule. Under Type, scroll down and select HTTP, and under Source Type, select Anywhere-IPv4. Scroll down and click Save rules.\n\n\n6.5.4 Grab the address of your server\nBefore we leave the AWS console, let’s grab the IP address of the server so we can use it to get back later.\nGo back to the AWS console and click on the Instance ID link for your server, and copy the Public IPv4 DNS, which will start with ec2- and end with amazonaws.com.\nThroughout the next chapters, I’ll refer to this as your <server-address>. Save it somewhere you can easily get back to later.\n\n\n6.5.5 Stopping or burning it down\nIf you’re stopping for the day at this point, it’s handy to be able to stop your server where it is so you don’t pay for it overnight.\nGo back to the EC2 page for your server. Under the Instance State drop down in the upper right, choose Stop Instance.\nAfter a couple minutes the instance will stop and you won’t get charged for it. Before you come back to the next lab, you’ll need to start the instance back up so it’s ready to go.\nIf you want to completely delete the instance at any point, you can choose to Terminate Instance from that same Instance State dropdown."
  },
  {
    "objectID": "chapters/sec2/2-2-cmd-line.html",
    "href": "chapters/sec2/2-2-cmd-line.html",
    "title": "7  The Command Line + SSH",
    "section": "",
    "text": "The biggest difference between working on your desktop and working on a server is that almost all work on a server is done via the command line – an all-text interface where you’ll interact via commands rather than a point-and-click graphical user interface (GUI).\nOnce nice thing is that once you feel comfortable using the command line on a server, you’ll probably find that there are many things that will get easier for you locally as well! Plus you get to feel like a real hacker.\nIn this chapter we’ll walk through getting access to the command line on your computer and talk about how to connect to a remote server using a protocol called SSH."
  },
  {
    "objectID": "chapters/sec2/2-2-cmd-line.html#setting-up-the-command-line",
    "href": "chapters/sec2/2-2-cmd-line.html#setting-up-the-command-line",
    "title": "7  The Command Line + SSH",
    "section": "7.1 Setting up the command line",
    "text": "7.1 Setting up the command line\nThe first step to using the command line is to figure out how to use it on your machine. The way you access the command line differs depending on your operating system.\nBefore you start interacting with the command line on your machine, it’s helpful to have a mental model of what you’re interacting with.\nThere are three layers to interacting with the command line – the terminal, the shell, and the operating system commands.\nThe terminal is the visual program where you’ll type in commands. Depending on which terminal you’re using, the colors and themes available for the window (yay dark mode!), as will the options for having multiple tabs and panes, and the keyboard shortcuts you’ll use to manage them.\nThe shell is the program you’re interacting with as you’re typing in commands. It’s what matches the words you type to actual commands or programs on your system. Depending on which shell you choose, you’ll get different options for autocompletion, handy plugins, and coloring and theming of the actual text in your terminal.\nLastly, the operating system is what actually runs the commands you’re typing in. So the set of commands available to you will differ by whether you’re using Windows or Mac or Linux.\n[TODO] - Image of terminal hosting shell w/ OS commands\nIt is possible to spend A LOT of time customizing your terminal to be exactly what you like. While it might not be the best use of your time, it is a lot of fun, and having a terminal that’s super customized to what you like feels pretty cool.\nWe’re not going to spend much time in this chapter actually playing on the command line – that will come in the next chapter. Instead, we’re going to mostly get everything set up and ready to go.\n\n7.1.1 If you use Linux\nWhy are you even reading this section? You’re probably already an expert terminal user. Skip down to getting SSH set up.\n\n\n7.1.2 If you use MacOS\nThere are a bunch of things you can do with your terminal. I’m not going to exhaust them all. Instead, my goal is to let you know about the various levels at which you can interact, and suggest my favorites.\nMacOS comes with a built-in terminal app (conveniently called Terminal). It’s fine.\nIf you’re going to be using your terminal more than occasionally, I’d recommend using the free iTerm2, which adds a bunch of niceties like better theming and multiple tabs.\nThe default shell for MacOS (and Linux) is called bash. Bash is a program that’s been around for a long time, and it’s super reliable. Many people (including me) like to replace bash with another shell that is basically bash+.\nMost of these other shells include all the things that bash can do and also allow more. I’ve been using zsh for years now and strongly recommend it. In addition to some basic niceties like better autocompletion than bash, zsh has a huge ecosystem of plugins that can do everything from making it easier to interact with git to controlling your Spotify music from the command line. zsh also has some really wonderful theming options that can do things like display your git status right on the command line.\nBecause there’s such a wild array of plugins for zsh, there are also options of plugin managers for zsh. I recommend prezto.\nI’m not going to go through the steps of installing and configuring these tools – there are numerous online walkthroughs and guides.\n\n\n\n\n\n\nWhat do I do?\n\n\n\nIf you want to follow my recommendations, install iTerm2, zsh, and prezto.\nThen customize the look of the window and tab behavior in the iTerm2 preferences and customize the text theme and plugins vis prezto.\n\n\n\n\n7.1.3 If you use a Windows machine\n\n\n\n\n\n\nNote\n\n\n\nI haven’t used a Windows machine in many years. I’ve collected some recommendations here, but I can’t personally vouch for them the way I can my Mac recommendations.\n\n\nWindows comes with a terminal built in. There are many other terminal programs you can use, but many Windows users think that the built in terminal is actually the best option. That makes things easy!\nWindows comes with two shells built in, the Command shell (cmd) and the PowerShell. The command shell is older and has been superseded by PowerShell. If you’re just getting started, you absolutely should just work with PowerShell. If you’ve been using Command shell on a Windows machine for a long time, most Command shell command work in PowerShell, so it may be worth switching over."
  },
  {
    "objectID": "chapters/sec2/2-2-cmd-line.html#the-structure-of-a-bash-command",
    "href": "chapters/sec2/2-2-cmd-line.html#the-structure-of-a-bash-command",
    "title": "7  The Command Line + SSH",
    "section": "7.2 The structure of a bash command",
    "text": "7.2 The structure of a bash command\nOnce you log into your server, you’ll be using bash, whether you’re coming from Windows or Mac.\nSo let’s take a moment and understand how to read and use a bash command.\nEach command in bash is its own small program and so you can interact with them on the command line in a standard way. There are some commands you just run by themselves, but most commands run with options and arguments.\nArguments tell the command what to run on. For example, the ls command lists files in the indicated directory. If you leave the argument blank, it ls just lists files in the current directory, but you can also provide an argument of the directory you’d like to see inside.\nSo ls will list the contents of the current directory, while ls /home/alex will list the contents of the /home/alex directory.\nOptions or flags are sub-commands that modify how the command operates. Flags are denoted by having one or more dashes before them. For example, the ls command, which lists files, has the flag -l, which indicates that the files should be displayed as a list.\nFlags always come in between the command and any arguments to the command. So, for example, if I type ls -l /home/alex in my terminal, I get back the files in /home/alex formatted as a list.\nSome flags themselves have arguments, which appear after the flag and a space. So, for example, if you’re using the -l flag on ls, you can use the -D flag to format the datetime when the file was last updated.\nSo, for example, running ls -l -D %Y-%m-%dT%H:%M:%S /home/alex will list all the files in /home/alex with the date-time of the last update formatted in ISO 8601 format (which is always the correct format for dates. Note that this structure <command> <flags + flag args> <command args> is always the structure of a bash command, and can make it hard to read, since you have to figure out how to read all the flags.\nAll of the flags and arguments for commands can be found in the program’s man page (short for manual). You can access the man page for any command with man <command>. You can scroll the man page with arrow keys and exit with q.\n\n\n\n\n\n\n\n\nSymbol\nWhat it is\nHelpful options + notes\n\n\n\n\nman\nmanual\n\n\n\nq\nExit man pages (and many other situations)"
  },
  {
    "objectID": "chapters/sec2/2-2-cmd-line.html#lab-accessing-your-server",
    "href": "chapters/sec2/2-2-cmd-line.html#lab-accessing-your-server",
    "title": "7  The Command Line + SSH",
    "section": "8.1 Lab: Accessing your server",
    "text": "8.1 Lab: Accessing your server\n\n8.1.1 SSH into the server\nThe .pem key you downloaded is the skeleton key to your server. You should be extremely careful with the .pem key. In the next chapter, we’ll set up an actual user on the server and configure SSH, which is more secure.\nBut we’ve got to get started on the server somehow, and using the .pem key is the way to do it.\nBefore we can use it to open the server, we’ll need to make a quick change to the permissions on the key. We’ll get a lot into users, groups, and permissions in the next chapter. For now, you can just copy paste these commands.\nThe easiest way to use my_test_key.pem key is to open a Terminal right in that folder. Find my_test_key.pem key in the Finder (probably in your Downloads folder), you can right-click and hit New iTerm Window Here to open a terminal session in the directory where your .pem key is.\nNow, you can go to your terminal app and run\n$ chmod 600 my_test_key.pem\n#TODO: Windows?\nIn your terminal type the following\n$ ssh -i my_test_key.pem ubuntu@<Public IPv4 DNS you copied>\nType yes when prompted, and you’re now logged in to your server!\nIn the next chapter, we’ll learn how to do some basic Linux Administration tasks. Let’s get going!"
  },
  {
    "objectID": "chapters/sec2/2-2-cmd-line.html#exercises",
    "href": "chapters/sec2/2-2-cmd-line.html#exercises",
    "title": "7  The Command Line + SSH",
    "section": "8.2 Exercises",
    "text": "8.2 Exercises\n\nDraw your own diagram of the sequence of things that occur when you execute ssh user@server.company.com in the terminal.\nCreate a local text file and SFTP it to the server. SSH in, edit the file, and SFTP it back.\nSomething with prezto themes/plugins.\ntmux"
  },
  {
    "objectID": "chapters/sec2/2-3-linux-admin.html",
    "href": "chapters/sec2/2-3-linux-admin.html",
    "title": "8  Administering a Linux Server",
    "section": "",
    "text": "Now that you’ve got a Linux server and you’re able to log in, it’s time to get acquainted with your server, learn how to move around, and start getting some things done.\nThere are two big differences between your laptop and a Linux server that you’ll have to get used to. The first is that servers generally do not have graphical user interfaces (GUIs) for doing administrative tasks. If you want to adjust the system settings on your laptop or navigate from directory to directory, you can click through a file tree or open up your preferences pane. For the most case, all interaction you’re going to have with your server is going to be via the command line. It’s easier than you might think if you’ve never done it before, but it’ll take a little learning.\nThe second difference is that the server we set up runs Linux – as do the overwhelming majority of the world’s servers. If you’re interacting with the command line, the differences between Linux and other operating systems (especially MacOS) aren’t huge, but there’s a little learning involved.\nIn order to get started, this section is going to be around navigating in Linux and learning how to do some basic administrative tasks. There are entire books written about Linux System Administration. Pick up one of those if you’re curious."
  },
  {
    "objectID": "chapters/sec2/2-3-linux-admin.html#a-little-about-linux",
    "href": "chapters/sec2/2-3-linux-admin.html#a-little-about-linux",
    "title": "8  Administering a Linux Server",
    "section": "8.1 A little about Linux",
    "text": "8.1 A little about Linux\nEvery computer in the world runs on an operating system. The operating system defines the way that applications – like Microsoft Word, RStudio, and Minecraft – can interact with the underlying hardware. They define how files are stored and accessed, how applications are installed and can connect to networks, and more.\nTODO: Image of hardware, operating system, applications\nBack in the early days of computing, basically every computer manufacturer created its own operating system that was super-tightly linked to the hardware. These days, there are only a few operating systems that most systems use.\nFor desktop and laptop computers, there’s Windows, MacOS, and Linux; Windows and Linux for servers, and Android (actually a flavor of Linux) and iOS for phones and tablets.1\nThe 1960s were a wild time for operating systems. Basically every computer company invented their own operating system for their machines. In the early 1970s, AT&T labs released a proprietary operating system called Unix.\nUnix espoused a philosophy of small system-level programs that could be chained together to do more complex things. It turned out that this philosophy made a lot of sense, and starting in the early 1980s, a variety of Unix-like operating systems were released. Unix-like operating systems were clones – they behaved like Unix, but didn’t actually include any code from Unix (because it was proprietary).\n\n\n\n\n\n\nNote\n\n\n\nThis philosophy, called piping, should feel extremely familiar to you if you’re an R user. The tidyverse pipe %>% and the base R pipe introduced in R 4.1 |> are both directly inspired by the Unix/Linux pipe |.\n\n\nLinux is the most successful of those clones, an open source Unix-like operating system released in 1991 by software engineer Linus Torvalds.2 Another of those clones was the predecessor to what is now MacOS.\nA difference you’ve probably experienced before between Unix-like systems and Windows, which is not Unix-like is the type of slashes used in file paths. Unix-like systems use forward slashes /, while Windows uses back slashes \\.\nA huge majority of the world’s servers run on Linux. There are meaningful Windows server deployments in some enterprises, but it’s relatively small compared to the install base of Linux servers. Along with most of the world’s servers, almost all of the world’s embedded computers – in ATMs, cars and planes, TVs, and most other gadgets and gizmos – run on Linux. If you have an Android phone or a Chromebook – that’s Linux. Basically all of the world’s supercomputers use Linux.\nAs you might imagine, the profusion of Linux in all different kinds of systems similarly necessitates different kinds of Linux. The Linux you’re going to run on a server that’s designed to be a data science workbench is going to be very different from the version of Linux running in your car or on your phone.\nThere are many different distributions (usually called “distros”) of Linux, for desktop, server, and other applications.\nThere are a few main distros you’re likely to run across on servers in your organization – Ubuntu, Red Hat Enterprise Linux (RHEL), Amazon Linux 2, and SUSE (pronunced soo-suh).3"
  },
  {
    "objectID": "chapters/sec2/2-3-linux-admin.html#a-tiny-intro-to-linux-administration",
    "href": "chapters/sec2/2-3-linux-admin.html#a-tiny-intro-to-linux-administration",
    "title": "8  Administering a Linux Server",
    "section": "8.2 A tiny intro to Linux administration",
    "text": "8.2 A tiny intro to Linux administration\nBeing a competent Linux admin is a career unto itself. So we’re not going to try to get you there in this chapter. Instead, the goal of this chapter is going to be to get you familiar with the basic tasks of interacting with a Linux server and the tools you need to at least get started working on one.\nWhen you log into a Linux server, you’ll be interacting exclusively via the command line, so all of the commands in this chapter are going to be terminal commands. If you haven’t yet figured out how to open the terminal on your laptop and got it themed and customized so it’s perfect, I’d advise going back to Chapter 2-2 on the command line.\nIt’s also worth mentioning that if you’re using a Mac, many of these same tools and techniques will work out of the box and may be useful on your laptop. If you’re running Windows, you may have to look up the exact commands and syntax – but the general idea will hold.\nIn just a second, we’ll get into how to administer a Linux server, but let’s first talk about what are the main tasks of Linux administration:\n\nMoving around and file operations A lot of the things you’ll do administering a server are just moving around, looking at different files, and interacting with them. We’ll spend some time on how to move around on the command line and how to interact with files.\nManaging who can do what In general, if you’re running a server, you’re going to be managing a number of different users on the server. Creating users and groups and managing them – specifically the things they’re allowed to do is a huge part of server administration, and we’ll go over what you’ll need to do and how.\nManaging resources As a server admin, especially in the cloud, you’ve got the ability to manage the resources – CPU, RAM, and memory – available to you. Keeping track of how much you’ve got of these things, how they’re being used, and making sure everyone is playing nice together with the shared resources is an important task.\nNetworking Because your server is only valuable if you and others can connect to it, managing how your server can connect to the environment around it is an important part of Linux administration.\n\nBelow, I’m intentionally mixing up bash commands and Linux system commands because they’re useful. If you know the difference and are pedantic enough to care, this list isn’t for you anyway.\nThe first thing that’s important to understand is that once you’ve SSH-ed into another server, your terminal is like a little window into that server. So everything that runs in the terminal is actually running on that other server and just bringing the results back to your eyes. But if you want to – for example – actually move a file from place to place, you’ll need to do something else.\nNow we’ll get into some of these topics. Each section will introduce several concepts and the commands you can use to accomplish those things. I’ll include a table of the commands mentioned. At the end of the book, there’s a cheatsheet section that combines all of these commands."
  },
  {
    "objectID": "chapters/sec2/2-3-linux-admin.html#the-filesystem-files-and-editing-text",
    "href": "chapters/sec2/2-3-linux-admin.html#the-filesystem-files-and-editing-text",
    "title": "8  Administering a Linux Server",
    "section": "8.3 The filesystem, files, and editing text",
    "text": "8.3 The filesystem, files, and editing text\nIn the last chapter, you SSH-ed into your server using the pem key that was granted to you when you created the server. When you got there, you got dumped into the command line. So the first step is understanding where you are and how to go elsewhere.\nThe first thing to understand in Linux is that commands always happen in a particular place – called the working directory – and as a particular user. Depending on who you are and where you are, the commands you’re allowed to run and what happens when you do so might be different. It’s worth noting that this is also true on your laptop, but the experience of clicking on and using apps obfuscates the fact that this is happening under the hood.\nWhen you land using the pem key, you’ve logged in as the root user and you’re at the file path root.\nAt any time, you can get the path where you’re sitting with the pwd command, which is an abbreviation for print working directory.\nOn a Linux server, you can think of the entire file system as a tree (for me an upside-down tree resonates more, since we generally talk about going “down” the tree to get to branches). The root of this tree is at /, and every file, folder, directory, and app is somewhere down the tree from /.\n\n\n\n\n\n\nNote\n\n\n\nIf you’re a Windows person, you might think this is analogous to C: – it is, but only sorta.\nYou’re right that the root of your drive is at C: and other things are descended from there.\nUnlike in Linux, in Windows you can have multiple roots, one for each physical or logical disk you’ve got. That’s why your machine may have a D: drive, or if you have network shares, those will often be on M: or N: or P:, each with its own sub-tree.\nIn Linux, everything is a subtree below /, and it has nothing to do with the drives that house each of them. If you do have extra drives mounted to your Linux server, /mnt (short for mount) is a popular place to put them.\n\n\nIn addition to being the root of the file tree, / is used to separate directories and files, so for example /opt/R is the directory (or file) called R inside the opt directory, which is inside /.\nWhenever you’re locating a directory or file, you can do so using either absolute or relative file paths. An absolute file path is the location of a file relative to the file root, /, while a relative file path is the path relative to the spot where you are right now.\nWhen you’re writing out a file path, you can also explicitly access the working directory using .. In the last chapter, we talked about the ls command, which lists what’s in the working directory. Now, you understand that ls just has a default argument of ., so ls and ls . do exactly the same thing.\nSo an absolute (sometime called fully-qualified) path always starts with / and might look something like /home/alex/. So regardless of what your working directory is, ls /home/alex will always show the contents of /home/alex.\nA relative path starts from your working directory. So I can run ls alex, and that will look for a directory named alex on the next rung down the tree from where I am and list its contents, if it exists. So if I’m in /home, ls alex will return the same thing as ls /home/alex, but otherwise it will return something else.\nIf you ever want to explicitly indicate your working directory in a file path, you can do so with ., so ls ./alex is the same as ls alex.\nSometimes absolute file paths make more sense, and sometimes relative paths do. In general, absolute file paths make more sense when you want to access the exact same resource from multiple places, and relative file paths make more sense when you want to access the same resource that might exist in different places.\nOnce you’ve looked around, you’ll have to move somewhere – you can change your working directory with the cd command, short for change directory.\nThere are a few directories with special names, aside from the root /. Your current working directory is always at ., and the parent of your current directory is at .., so you can move to the parent of your current directory using cd .. and up two levels with cd ../...\nWe’ll get more into users below, but if your user has a home directory, that home directory is at ~.\n\n\n\n\n\n\n\n\n\n\n\n\nCommand\n\n\n\n\nWhat it does/is\n\n\n\n\nHelpful options\n\n\n\n\nExample\n\n\n\n\n\n\n\n\n/\n\n\n\n\nsystem root\n\n\n\n\n\n\n\n\n\n\n.\n\n\n\n\ncurrent working directory\n\n\n\n\n\n\n\n\n\n\nls\n\n\n\n\nlist objects in a directory\n\n\n\n\n-l - format as list\n\n\n-a - all (include hidden files)\n\n\n\n\n$ ls .\n\n\n$ ls -la\n\n\n\n\n\n\npwd\n\n\n\n\nprints working directory\n\n\n\n\n\n\n$ pwd\n\n\n\n\n\n\ncd\n\n\n\n\nchange directory\n\n\n\n\n\n\n$ cd ~/Documents\n\n\n\n\n\n\n~\n\n\n\n\nhome directory of the current user\n\n\n\n\n\n\n$ ls ~\n\n\n\n\n\n\n\n8.3.1 Reading text files\nOn a Linux system, almost everything is either a text file or an executable. That means that configuration files and logs are just text files, and so you can interact with them all the same way.\nA very common pattern in Linux administration is to read a log file to look for errors or clues, adjust a configuration setting as a result, and then restart a process.\nYou’ll find that your skills in understanding the Linux file tree, moving around, and seeing what’s in directories will be very helpful in getting to the files. Once you’re there, it’ll be useful to know how to actually interact with files.\nProbably the commands you’ll use most often will be cat and tail. cat is the command to print a file, starting at the beginning. It’s often helpful to read through text files. Sometimes you’ve got a really big file and you want to see the first few rows (especially useful if it’s a csv). In that case, less can be handy because it opens large files much faster.\ntail skips right to the end of a file. This is most useful when you’re reading log files where the newest information is at the end. Log files usually are written so the newest part is last. So much so that tailing a log file is a synonym for looking at it.\nIf you want to get a live view of a log file that will update as more is written, use the -f flag (for follow).\nIf you’ve used regex, you’ll be familiar with the power of grep – a tool for using regex search. grep searches for and returns results that match the pattern you specify. Using grep well requires being quite proficient in regex, so I usually just use it for simple searches.\nThe true power of grep is unlocked in combination with the pipe. In Linux, the pipe operator – | – takes the output of the previous command and sends it into the next one. This kind of work will be very familiar to anyone who’s used the tidyverse in R, which was directly inspired by the Linux pipe.\nSo, for example, a combination I do all the time is to pipe the output of ls into grep when searching for a file inside a directory. So if I was searching for a file that contained the word data somewhere in the filename inside a specific project directory, that might look something like ls ~/projects/my-project | grep data.\n\n\n\n\n\n\n\n\n\n\n\nCommand\n\n\n\n\nWhat it does\n\n\n\n\nNotes + Helpful options\n\n\n\n\n\n\n\n\ncat\n\n\n\n\nPrints a file.\n\n\n\n\n\n\n\n\nless\n\n\n\n\nPrints a file, but just a little.\n\n\n\n\nCan be very helpful to look at a few rows of csv.\n\n\nOnly reads what youre looking at, so can be much faster than cat for big files.\n\n\n\n\n\n\ntail\n\n\n\n\nLook at the end of a file.\n\n\n\n\nLog files usually are written so the newest part is last. So much so that tailing a log file is a synonym for looking at it.\n\n\nIf you want to get a live view of a log file that will update as more is written, use the -f flag (for follow).\n\n\n\n\n\n\ngrep\n\n\n\n\nSearch a file using regex.\n\n\n\n\nUseful to search inside a file, but youve gotta write regex. I suggest testing expressions on regex101.com.\n\n\nOften useful in combination with the pipe.\n\n\n\n\n\n\n|\n\n\n\n\nthe pipe\n\n\n\n\n\n\n\n\n\n\n8.3.2 Deleting and Moving Files\nThere will be times when you have to copy, move, or remove files – each of these things can be accomplished with commands that are similarly abbreviated forms of the relevant words – cp, mv, and rm.\n\n\n\n\n\n\nWarning\n\n\n\nBe very careful with the rm command.\nUnlike on your desktop there’s no recycle bin! Things that are deleted are instantly deleted forever.\n\n\nIf you want to copy, move, or remove files, the -r flag for recursive is a handy one – if you try to copy, move or remove a directory, you often mean to act on the entire subtree below that directory, which the -r flag indicates.\nSimilarly, sometimes you want to list everything in a directory. For example, you might want to copy the entire contents of a directory. In that case, the wildcard, *, returns everything in a directory. So cp alex/* alex2 copies the full contents of alex into alex2.\nThere are times when you want to make files or directories with nothing in them – the touch command makes a blank file at the specified file path, and mkdir makes a directory at the specified filepath. mkdir can be a little finicky about when paths exist and it will only make one level. So mkdir my_dir works, but mkdir my_dir/my_sub_dir does not. Using mkdir -p, it will use existing paths and make whichever parts of the path don’t yet exist.\n\n\n\n\n\n\n\n\n\n\n\n\nCommand\n\n\n\n\nWhat it does/is\n\n\n\n\nHelpful options + Notes\n\n\n\n\nExample\n\n\n\n\n\n\n\n\nrm\n\n\n\n\nremove delete permanently!\n\n\n\n\n-r - recursively a directory and included files\n\n\n-f - force - dont ask for each file\n\n\n\n\n$ rm old_doc\n\n\nr m -rf old_docs/\n\n\nBE VERY CAREFUL WITH -rf\n\n\n\n\n\n\ncp\n\n\n\n\ncopy\n\n\n\n\n\n\n\n\n\n\nmv\n\n\n\n\nmove\n\n\n\n\n\n\n\n\n\n\n*\n\n\n\n\nwildcard\n\n\n\n\n\n\n\n\n\n\nmkdir\n\n\n\n\nmake directory\n\n\n\n\n\n\n\n\n\n\ntouch\n\n\n\n\nupdate files timestamp to current time\n\n\n\n\nCreates file if doesnt already exist.\n\n\n\n\n\n\n\n\n\n\n8.3.3 Moving things to and from the server\nOne thing that’s likely to come up almost immediately when you’re working on your server is how to move files to and from the server. There are two main tools you’ll use for this task. The first is the tar command, which allows you to turn a set of files or whole directory into an archive. This is really handy because then moving a whole set of files turns into just moving one archive file. It also does some amount of file compression when it creates the archive file.\nAnnoyingly, the tar command does both archive creation and extraction, and is almost always used with several other flags. I never remember them – this is a command I google 100% of the time I use it.\nOnce you’ve created an archive file, you’ve got to move it. The scp command is the way to do this. scp – short for secure copy – is basically a combo of SSH and copy. So you will sometimes use ssh flags like -i to specify a particular SSH key, and you’ll also have to specify file paths.\nOne thing to remember about scp is that it makes an SSH connection at your request. This means that the other side of the connection needs to be available to receive an inbound request to connect over SSH. This is probably true of your server, but is almost never true of your laptop. So that means that when you’re getting things to or from your server you’ll almost always run the scp command from your laptop’s terminal, not from a terminal that’s already SSH-ed into the server.\n\n\n\n\n\n\n\n\n\n\n\nCommand\n\n\n\n\nWhat it does\n\n\n\n\nNotes + Helpful options\n\n\n\n\n\n\n\n\ntar\n\n\n\n\ncompress/decompress file/directory\n\n\n\n\nAlmost always used with flags that youll have to google.\n\n\nCreate is usually\n\n\ntar -czf <archive name> <file(s)>\n\n\nExtract is usually\n\n\ntar -xfv <archive name>\n\n\n\n\n\n\nscp\n\n\n\n\n\n\n\n\n\n\n\n\n8.3.4 Writing files - a rough intro to vim\nThere will be many situations where writing into a text file will be handy while administering your server – for example, when changing config files. When you’re on the command line, you’ll use a command line tool for writing into those files – meaning you’ll navigate inside the file and do file operations completely from the keyboard. No mouse or touchpad!\nThere are two main tools you’ll probably encounter, nano and vi/vim.4\nYou can open a file in either by typing nano <filename> or vi <filename>. Once there you’ll be looking at a text file.\nBoth nano and vim offer extremely powerful text editing tools. It might be worth it for you to spend some time really getting comfortable in one! In this book, we’re just going to talk about the absolute minimum you’ll need to do to avoid getting stuck. Getting stuck in nano or vim is an extremely common situation for a newbie Linux admin. Hopefully once you’ve read this, you’ll at least avoid getting stuck in an editor with no way out.\nIn nano there will be helpful prompts along the bottom to tell you how to interact with the file, so you’ll see once you’re ready to go, you can exit with ^x. But what is ^? Pressing that key doesn’t seem to have any effect. The ^ caret is short for your control or command key – depending on whether you’re using a Mac or Windows keyboard. Phew!\nWhere nano gives you helpful hints, vim leaves you all on your own. It doesn’t even tell you you’re inside vim! This is where many people get stuck and end up having to just exit and start a new terminal session. It’s not the end of the world if you do, but a few simple vim commands can help you avoid that fate.\nOne of the most confusing things about vim is that you can’t edit the file when you first enter. That’s because vim keybindings were (1) developed in a time before all keyboards had arrow keys and (2) were designed to never make you take your hands off the center of the keyboard. When you enter, you’re in “normal” mode in which you can’t actually type anything!\nBy pressing i, you can enter insert mode – or “the mode where you can actually type stuff”. These days, almost all keyboard have arrow keys and you can navigate using the arrow keys in insert mode.\nOnce you’re done writing stuff, you can exit to normal mode by pressing the escape key. Once you’re in normal mode, you can do file operations by prefixing commands with a colon :. The two most common commands you’ll use are save (write) and quit. You can combine these together, so you can save and quit in one command using :wq.\nSometimes you may find yourself inside a file having made changes you want to discard. If you try to exit with :q, you’ll again find yourself trapped in and endless loop of warnings that your changes won’t be saved unable to exit. You can tell vim you mean it with the exclamation mark and exit using :q!.\n\n\n\n\n\n\n\n\nCommand\nWhat it does\nNotes + Helpful options\n\n\n\n\n^\nPrefix for file command in nano editor.\nUse the command or control key.\n\n\ni\nEnter insert mode in vim\n\n\n\nescape\nEnter normal mode in vim.\n\n\n\n:w\nWrite the current file in vim (in normal mode)\nCan be combined to save and quit in one, :wq\n\n\n:q\nQuit vim (in normal mode)\n:q! quit without saving"
  },
  {
    "objectID": "chapters/sec2/2-3-linux-admin.html#managing-who-can-do-what",
    "href": "chapters/sec2/2-3-linux-admin.html#managing-who-can-do-what",
    "title": "8  Administering a Linux Server",
    "section": "8.4 Managing who can do what",
    "text": "8.4 Managing who can do what\nWhenever you’re doing something in Linux, you’re doing that thing as a particular user. An important distinction about Linux users is that they may or may not correspond to an actual human.\nIn a minute, we’ll create users on your workbench server. These will correspond to actual humans who will use the servers, and they’ll have usernames, passwords, and home directories. But there are many more users than that on a Linux server. On most servers, there will be many service accounts, accounts that represent a particular service but don’t have a password or a home directory. They basically just exist to be holders for permissions.\nFor example, if you install RStudio Server on your server, there will be a user created called rstudio-server. So, for example, if you go to login to the server, it’s the rstudio-server user who needs permissions to do the relevant mechanics to get you in, like check that you’re a valid user on the system.\nA group is a collection of users for the purpose of managing permissions group-wide. Each user has exactly one primary group, and can be a member of zero or more secondary groups.5 By default, each user has their own primary group of the same name as their username.\nThere’s also an administrative, root, or super user. When you logged in to your server using the pem key, you were logged in as the root user. This is a very dangerous practice, and you should basically never do it, except when you’ve just stood up a fresh server.\nInstead, you’ll want to create a user on the system using the adduser command, log in as that actual user, and adopt super user privileges to run particular commands by prefixing them with sudo.\nIf you think back a little, this is one of the most common reasons for being in a file, having made edits, and being unable to exit. Your user very well may have read privileges, but not write. So you could get in and muck around, but when you went to save, you can’t! Exiting with :q! and reopening with sudo vim is your best bet.\nYou can change your user’s password at any time with passwd and you can check the user you are with whoami or id.\n\n\n\n\n\n\n\n\n\nCommand\nWhat it does\nHelpful options\nExample\n\n\n\n\nsudo\nAdopt super user permissions.\n\n\n\n\nsu <username>\nChange to be a different user.\n\n\n\n\nwhoami / id\nCheck the current user.\nid gives more information, but is less catchy.\n\n\n\npasswd\nChange password.\n\n\n\n\nuseradd\nAdd a new user.\n\n\n\n\n\n\n8.4.1 File Permissions\nEvery object in Linux is just a file. Every log – file. Every picture – file. Every program – file. So with a pretty simple set of permissions, you can assign what everyone on the system is allowed to do.\n\n\n\n\n\n\nNote\n\n\n\nThe question of who’s allowed to do what – authorization – is an extremely deep one. There’s a chapter all about authorization, how it differs from authentication, and the different ways your IT/Admins might want to manage it later in the book.\nThis is just going to be a high-level overview of basic Linux authorization.\n\n\nThere are three permissions in Linux: read, write, and execute. For some files execute doesn’t really make sense - what would it mean to execute a csv file? But Linux doesn’t care – you can assign any combination of these three permissions for any file.\nNow, how are these permissions assigned?\nEach file in Linux belongs to a user and a group. So for each file, read, write, and execute permissions can be set for the user who owns it, the group it belongs to, and everyone else.\nTo understand better, let’s look at the actual permissions on a file.\nIf you run ls -l on a directory, you get the list of the files – and the first few columns give you all the information you need to know about the file’s permissions.\nSo, for example, here’s a few lines of the output of running ls -l on a python project I’ve got.\n❯ ls -l                                                           \n-rw-r--r--  1 alexkgold  staff     28 Oct 30 11:05 config.py\n-rw-r--r--  1 alexkgold  staff   2330 May  8  2017 credentials.json\n-rw-r--r--  1 alexkgold  staff   1083 May  8  2017 main.py\ndrwxr-xr-x 33 alexkgold  staff   1056 May 24 13:08 tests\nThe first character indicates the type of file – - for normal and d for a directory.\nThe next 9 characters are indicators for the three permissions – r for read, a w for write, and a x for execute or - for not – first for the user, then the group, then any other user on the system.\nSo, for example, my config.py file with permissions of rw-r-r-- indicates the user (alexkgold) can read and write the file, and everyone else – including in the file’s group staff – has read only.\nIn some cases, you may need to change a file’s permissions. You can do so using the chmod command. For chmod, you indicate permissions with the sum of numbers – 4 for read, 2 for write, and 1 for execute – one number for the user, group, and everyone else. So chmod 765 <filename> would give the user full permissions, read and write to the group, and read and execute to everyone else. This would be a strange set of permissions to give a file, but it’s a perfectly valid chmod command.\n\n\n\n\n\n\nNote\n\n\n\nIf you spend any time administering a Linux server, you almost certainly will at some point finding yourself frustratedly applying chmod 777 to a file to give full permissions to everyone.\nI can’t in good faith tell you not to do this – we’ve all been there. But if it’s something important, be sure you change it back once you’re finished figuring out what’s going on.\n\n\n\n\n\n\n\n\n\n\nCommand\nWhat it does\nHelpful options + notes\n\n\n\n\nchmod <permissions> <file>\nModifies permissions on a file.\nNumber indicates permissions for user, group, others: add 4 for read, 2 for write, 1 for execute, 0 for nothing."
  },
  {
    "objectID": "chapters/sec2/2-3-linux-admin.html#managing-server-resources",
    "href": "chapters/sec2/2-3-linux-admin.html#managing-server-resources",
    "title": "8  Administering a Linux Server",
    "section": "8.5 Managing server resources",
    "text": "8.5 Managing server resources\nManaging server resources is the third main activity you’ll need to do as a server admin. There are three resources you’ll need to manage – CPU, RAM, and storage space. More on all three of these and how to make sure you’ve got enough later in this section.\nFor now, we’re just going to go over how to check how much you’ve got, how much you’re using, and getting rid of stuff that’s misbehaving.\nFor many of these commands, the amount returned can be overwhelming, so you’ll usually use some sort of filtering mechanism. For many of these commands, that means you’ll run it, and send it into a pipe. On the other side of the pipe you might have grep to look for specific files or processes, or head or tail to get the first or last of them. If you want to specify how many, head -n <n> gives you the top n results.\n\n\n\n\n\n\n\n\nCommand\nWhat it does\nHelpful options\n\n\n\n\nhead\nReturns the first results from a command.\n-n <n> to return the first n results.\n\n\n\n\n8.5.1 Managing storage\nIf you’re running low on storage space, or think you might be, there are two things you might try to do – delete some stuff or add a bigger disk. There are two commands – du gives you the size of individual files inside a directory. This can be helpful for finding your largest files or directories if you think you might need to clean up things.\ndf is the more IT/Admin way of thinking about storage usage – given a file or directory, what device is it mounted on and how full is that device? This can be really helpful if you’re thinking about swapping out for a bigger storage volume.\nYou’ll almost always use du and df with the -h flag, which puts the numbers in human-readable format.\n\n\n\n\n\n\n\n\n\n\n\nCommand\n\n\n\n\nWhat it does\n\n\n\n\nHelpful options\n\n\n\n\n\n\n\n\ndu\n\n\n\n\nCheck size of files.\n\n\n\n\nMost likely to be used du -h <dir> | head -n 10\n\n\ndu head -n 10\n\n\n\n\n\n\ndf\n\n\n\n\nCheck storage space on device.\n\n\n\n\n-h\n\n\n\n\n\n\n\n\n8.5.2 Running processes\nEverything running on a computer is a process. So, for example, running a R session is (usually) one process. Some processes are more complex than others. For example, just running R or Python in your terminal and using the console is just a single process.\nBut more complex interactions, like running R inside RStudio or Python in Jupyter involves a number of different processes and subprocesses.\nEach process has a numeric process id or pid that can be useful for referring to them.\nSometimes these processes take up more than their share of RAM and CPU – the most relevant resources for running processes.\nAs an admin, you’ll occasionally have to track down rogue processes and shut them down. Shutting down a rogue process is pretty simple – you’ll use the kill command to kill processes once you’ve identified them. The trick is identifying the problematic ones.\nGenerally, if the system is doing something weird, top is a good first stop. top shows the processes consuming the most system resources in real time. It can help you find the processes that you might need to kill.\nIf you have a better idea of where troublesome processes might be, ps aux lists processes for all users.6 It’s common to pipe the output into grep to identify processes by names.\n\n\n\n\n\n\n\n\nCommand\nWhat it does\nHelpful options\n\n\n\n\ntop\nSee what’s running on the system.\n\n\n\nps aux\nSee all system processes.\nThe second column is the pid if you want to kill them.\n\n\nkill\nKill a system process.\n-9 to force kill"
  },
  {
    "objectID": "chapters/sec2/2-3-linux-admin.html#managing-networking",
    "href": "chapters/sec2/2-3-linux-admin.html#managing-networking",
    "title": "8  Administering a Linux Server",
    "section": "8.6 Managing networking",
    "text": "8.6 Managing networking\nThe last thing you’ll have to manage on your Linux server is networking. After all, servers are only valuable to the degree they can serve people something! Very often, you’ll experience configuring something on your server, observing it working, and then not being able to get to it…without really understanding why.\nIn these cases, your first assumption should probably be that there’s an issue with the networking. In another section, we’ll get into the many places networking can be misconfigured, but the first thing to check is whether networking is the issue.\nping and curl are useful tools for checking whether traffic can get into or out of your server. For example, if you’re on your server and struggling to install packages from CRAN or PyPi, a ping to the relevant URL can check whether your request is getting through to those servers at all.\nOn the flip side, if you can’t log into your server, a ping command from your laptop to your server is a good check of whether you’re correctly configured inbound networking.\nLastly, netstat is a useful command for checking which ports are being used on your machine. If you’ve got a service running, you need to make sure it’s available on a port – and that it’s the right port! netstat can help you check. For this purpose, netstat is most useful with the -tlp flags to show programs that are listening and the programs associated.\n\n\n\n\n\n\n\n\n\n\n\nCommand\n\n\n\n\nWhat it does\n\n\n\n\nHelpful Options + Notes\n\n\n\n\n\n\n\n\nping\n\n\n\n\nChecks whether an URL/IP address is reachable.\n\n\n\n\n\n\n\n\ncurl\n\n\n\n\nTries to access a specific HTTP endpoint.\n\n\n\n\n\n\n\n\nnetstat\n\n\n\n\nGives overview of network statistics and ports used on your server.\n\n\n\n\nMost useful with -tlp flag.\n\n\n-n may also be useful if you want numeric IP addresses\n\n\n\n\n\n\n\n8.6.1 Practical SSH\nNote that you don’t need to use -i assuming you’re using default SSH key name. If you wanted to use a different name, you can use -i to specify it, or you can set up an SSH config so your terminal knows which SSH key to use with which host.\nBy default, SSH always uses port 22. If for some reason you want to use a different port, you can use the -p flag with your SSH command.\nSSH has one of the neatest debugging modes of any command. If you can’t connect via SSH for some reason, just add a -v to your command for verbose mode. If that’s not enough information, add another v for -vv, and even another! Every v you add (up to 3) will make the output more verbose.\nThere’s one more SSH trick that can be useful – port forwarding, also called tunneling. SSH port forwarding allows you to take the output of a port on a remote server, route it through SSH, and display it as if it were on a local port. What this means, for example, is that you can connect to a server via SSH, and once you’ve set up port forwarding, you can access, for example, port 3939 on the remote server at localhost:3939 from your laptop’s browser.\nThis can be helpful if you want to access a particular port on the remote server, but you haven’t yet set up public networking to it.\nPort forwarding is, unfortunately, difficult to read and you’ll almost certainly have to google every time you use it unless you’re doing so on a daily basis.\nFor the kind of port forwarding you’ll use in debugging, you’ll use the -L flag. The syntax looks like this:\nssh <local port>:<remote ip>:<remote port> <ssh hostname>"
  },
  {
    "objectID": "chapters/sec2/2-3-linux-admin.html#lab-setting-up-a-user-configuring-ssh-installing-r-python-and-more",
    "href": "chapters/sec2/2-3-linux-admin.html#lab-setting-up-a-user-configuring-ssh-installing-r-python-and-more",
    "title": "8  Administering a Linux Server",
    "section": "8.7 Lab: Setting up a user, configuring SSH, Installing R, Python, and More",
    "text": "8.7 Lab: Setting up a user, configuring SSH, Installing R, Python, and More\nNow that you’ve SSH-ed into your server using the pem key, let’s make things more secure.\nThe first thing we’re going to do is create a user so that you can login without running as root all the time.\nLet’s get started by just running useradd. This will walk us through a set of prompts to create a new user with a home directory and a password. Make sure to give this user sudo privileges. Great!\nNow, let’s create an SSH key on your machine. Feel free to accept all the defaults.\nYou’ll need to scp the key into place on the server – in ~/.ssh/authorized_keys.\nNow, login using your user, so it’ll be ssh <username>@<server IPv4 address>.\nNow that we’re all set up, you should store the pem key somewhere safe and never use it to log in again.\nNow, let’s get RStudio Server, Jupyter Lab, and the associated programming languages up and running.\n\nR\nPython\nRStudio\nJupyter\n\nTry standing up a new server and installing R and RStudio Server Open Source, or Python and JupyterHub.\n\nHint 1: Remember that your instance only allows traffic to SSH in on port 22 by default. You access RStudio on port 8787 by default and JupyterHub on port 8000. You control what ports are open via the Security Group.\nHint 2: You’ll need to create a user on the server. The adduser command is your friend."
  },
  {
    "objectID": "chapters/sec2/2-4-understanding-traffic.html",
    "href": "chapters/sec2/2-4-understanding-traffic.html",
    "title": "9  Computer Networks and the Internet",
    "section": "",
    "text": "In chapter 1, we got into how the computer on your desk, on your lap, or in your hand works. These days, many or even most of the things we want to do involve sending data across computer networks. When you visit a website, wirelessly print a document, or login to your email, you are making use of a computer network.\nThe computer network we’re all most familiar with is the biggest of them all – The Internet. But there are myriad other networks, like the very small private network of the devices (phones, computers, TVs, etc) connected to your home wifi router, to the somewhat larger VPN (it stands for virtual private network, after all) you might connect to for school or work.\nA computer network is a set of computers that can communicate with each other to send data in the form of network traffic back and forth to each other. These networks are basically self-similar – once you understand the wifi network in your house, you’ve also got a reasonably good understanding of how the entire internet works, which is great if you’re an author trying to explain how this all works.\nIn this chapter, you’ll learn the basics of how computer networks work. In particular, we’ll get into some of the layers of protocols that define how computers communicate with each other. This chapter is mostly going to be background for the next few chapters, where we’ll get into the nitty gritty of how to configure both the public and private elements of networking for your data science workshop."
  },
  {
    "objectID": "chapters/sec2/2-4-understanding-traffic.html#computer-communication-packets-traversing-networks",
    "href": "chapters/sec2/2-4-understanding-traffic.html#computer-communication-packets-traversing-networks",
    "title": "9  Computer Networks and the Internet",
    "section": "9.1 Computer communication = packets traversing networks",
    "text": "9.1 Computer communication = packets traversing networks\nThe virtual version of the processes that get your letter from your house to your penpal’s is called packet switching, and it’s really not a bad analogy. Like the physical mail, your computer dresses up a message with an address and some other details, sends it on its way, and waits for a response.1 The set of rules – called a protocol – that defines a valid address, envelope type, and more is called TCP/IP.\nUnderneath these protocols is a bunch of hardware, which we’re basically going to ignore.\nEach computer network is governed by a router. For the purposes of your mental model, you can basically think of your router as doing two things – maintaining a table of the IP addresses it knows, and following this algorithm over and over again.\n#TODO: Turn into visual tree – also visual of networks and sub-networks\n\nDo I know where this address is?\n\nYes: Send the packet there.\nNo: Send the packet to the default address and cross fingers.\n\n\nIn general, routers only know about the IP addresses of sub-networks and devices, so if you’re printing something from your laptop to the computer in the next room, the packet just goes to your router and then straight to the printer.\n\nIn your home’s local area network (LAN), your router does one additional thing – as devices like your phone, laptop, or printer attach to the network, it assigns them IP addresses based on the addresses available in a process called Dynamic Host Configuration Protocol (DHCP).\n\nOn the other hand, if you’re sending something to a website or server that’s far away, your computer has no idea where that IP address is. Clever people have solved this problem by setting the default address in each router to be an “upstream” router that is a level more general.\nSo immediately upstream of your router is probably a router specific to your ISP for a relatively small geographic area. Upstream of that is probably a router for a broader geographic area. So your packet will get passed upstream to a sufficiently general network and then back downstream to the actual IP address you’re trying to reach.\n#TODO: Image of computer network w/ upstream and downstream networks\nWhen the packets are received and read – something happens. Maybe you get to watch your show on Netflix, or your document gets printed – or maybe you get an error message back. In any event, the return message will be transmitted exactly the same way as your initial message, though it might follow a different path."
  },
  {
    "objectID": "chapters/sec2/2-4-understanding-traffic.html#more-details-about-ip-addresses",
    "href": "chapters/sec2/2-4-understanding-traffic.html#more-details-about-ip-addresses",
    "title": "9  Computer Networks and the Internet",
    "section": "9.2 More details about IP Addresses",
    "text": "9.2 More details about IP Addresses\nIP addresses are, indeed, addresses. They are how one computer or server finds another on a computer network, and they are unique within that network.\nMost IP addresses you’ve probably seen before are IPv4 addresses. They’re four blocks of 8-bit fields, so they look something like 65.77.154.233, where each of the four numbers is something between 0 and 255.\nSince these addresses are unique, each server and website on the internet needs a unique IP address. If you do the math, you’ll realize there are “only” about 4 billion of these. It turns out that’s not enough for the public internet and we’re running out.\nIn the last few years, adoption of the new standard, IPv6, has started. IPv6 addresses are eight blocks of hexadecimal (0-9 + a-f) digits separated by colons, with certain rules that allow them to be shortened, so 4b01:0db8:85a3:0000:0000:8a2e:0370:7334 or 3da4:66a::1 are both examples of valid IPv6 addresses.\nIPv6 will coexist with IPv4 for a few decades, and we’ll eventually switch entirely to IPv6. There’s no worry about running out of IPv6 addresses any time soon, because the total number of IPv6 addresses is a number 39 digits long.\n\n9.2.0.1 Special IP Addresses\nAs you work more with IP addresses, there are a few you’ll see over and over. Here’s a quick cheatsheet:\n\n\n\n\n\n\n\nCode\nMeaning\n\n\n\n\n127.0.0.1\nlocalhost or loopback – the machine that originated the request\n\n\n192.168.x.x, 172.16.x.x.x,\n10.x.x.x\nProtected address blocks used for private IP addresses. More on public vs private addresses in chapter XX."
  },
  {
    "objectID": "chapters/sec2/2-4-understanding-traffic.html#ports",
    "href": "chapters/sec2/2-4-understanding-traffic.html#ports",
    "title": "9  Computer Networks and the Internet",
    "section": "9.3 Ports",
    "text": "9.3 Ports\nTODO\nPort forwarding outside:inside\nWhy you’d want to"
  },
  {
    "objectID": "chapters/sec2/2-4-understanding-traffic.html#application-layer-protocols-define-valid-messages",
    "href": "chapters/sec2/2-4-understanding-traffic.html#application-layer-protocols-define-valid-messages",
    "title": "9  Computer Networks and the Internet",
    "section": "9.4 Application Layer Protocols define valid messages",
    "text": "9.4 Application Layer Protocols define valid messages\nIf we think of the TCP/IP protocol defining valid addresses, package sizes and shapes, and how the mail gets routed, then application layer protocols are one layer down – they define what are valid messages to put inside the envelope.\nThere are numerous application layer protocols. Some you will see in this book include SSH for direct server access, (S)FTP for file transfers, SMTP for email, LDAP(S) for authentication and authorization, and websockets for persistent bi-directional communication – used for interactive webapps created by the Shiny package in R and the Streamlit package in Python.\nWe’ll talk more about some of those other protocols later in the book. For now, let’s focus on the one you’ll spend most of your time thinking about – http.\n\n9.4.1 http is the most common application layer protocol\nHyptertext transfer protocol (http) is the protocol that underlies a huge fraction of internet traffic. http defines how a computer can initiate a session with a server, request the server do something, and receive a response.\nSo whenever you go to a website, http is the protocol that defines how the underlying interactions that happen as your computer requests the website and the server sends back the various assets that make up the web page, which might include the HTML skeleton for the site, the CSS styling, interactive javascript elements, and more.\n\nIt’s worth noting that these days, virtually all http traffic over the internet is in the form of secured https traffic. We’ll get into what the s means and how it’s secured in the next chapter.\n\nThere are a few important elements to http requests and responses:\n\nRequest Method – getting deep into HTTP request methods is beyond the scope of this book, but there are a variety of different methods you might use to interact with things on the internet. The most common are GET to get a webpage, POST or PUT to change something, and DELETE to delete something.\nStatus Code - each HTTP response includes a status code indicating the response category. Some special codes you’ll quickly learn to recognize are below. The one you’ll (hopefully) see the most is 200, which is a successful response.\nResponse and Request Headers – headers are metadata included with the request and response. These include things like the type of the request, the type of machine you’re coming from, cookie-setting requests and more. In some cases, these headers include authentication credentials and tokens, and other things you might want to inspect.\nBody - this is the content of the request or response.\n\nIt’s worth noting that GET requests for fetching something generally don’t include a body. Instead, any specifics on what is to be fetched are specified through query parameters, the part of the URL that shows up after the ?. They’re often something like, ?first_name=alex&last_name=gold\n\n\n\n\n9.4.2 Understand http traffic by inspecting it\nThe best way to understand http traffic is to take a close look at some. Luckily, you’ve got an easy tool – your web browser!\nOpen a new tab in your browser and open your developer tools. How this works will depend on your browser. In Chrome, you’ll go to View > Developer > Developer Tools and then make sure the Network tab is open.\nNow, navigate to a URL in your browser (say google.com).\nAs you do this, you’ll see the traffic pane fill up. These are the requests and responses going back and forth between your computer and the server.\nIf you click on any of them, there are a few useful things you can learn.\n\nAt the top, you can see the timing. This can be helpful in debugging things that take a long time to load. Sometimes it’s helpful to see what stage in the process bogs down.\nIn the pane below, you can inspect the actual content that is going back and forth between your computer and the server you’re accessing including the request methods, status codes, headers, and bodies.\n\n9.4.2.1 Special HTTP Codes\nAs you work more with http traffic, you’ll learn some of the common codes. Here’s a cheatshet for some of the most frequent you’ll see.\n\n\n\n\n\n\n\nCode\nMeaning\n\n\n\n\n200\nEveryone’s favorite, a successful response.\n\n\n3xx\nYour query was redirected somewhere else, usually ok.\n\n\n4xx and 5xx\nErrors with, respectively, the request itself and the server.\n\n\n\nParticular Error Codes\n\n\n400\nBad request. This isn’t a request the server can understand.\n\n\n401 and 403\nUnauthorized or forbidden. Often means required authentication hasn’t been provided.\n\n\n404\nNot found. There isn’t any content at the address you’re trying to access.\n\n\n504\ngateway timeout. This means that a proxy or gateway between you and the server you’re trying to access timed out before it got a response from the server."
  },
  {
    "objectID": "chapters/sec2/2-4-understanding-traffic.html#getting-a-real-url",
    "href": "chapters/sec2/2-4-understanding-traffic.html#getting-a-real-url",
    "title": "9  Computer Networks and the Internet",
    "section": "9.5 Getting a real URL",
    "text": "9.5 Getting a real URL\n```\n1.An IP address just specifies the location while a URL specifies location, protocol, and specific resource\n2.URL requires a DNS server while an IP address doesn’t\n3.URLs are unlimited while IP addresses are limited\n4.IP addresses and URLs have a one to many relationship\n\nRead more: Difference Between URL and IP Address | Difference Between http://www.differencebetween.net/technology/internet/difference-between-url-and-ip-address/#ixzz7GHcaYyk6\n```\nIn the last chapter, we talked about how network traffic knows where to go and what to do when it gets there. That’s all fine and dandy, but you noticed that we spoke almost entirely in terms of IP addresses. You probably almost never work with IP addresses. Instead, we’re used to visiting websites at universal resource locators (URLs), like google.com. What gives?\nA URL is a more complete description of how to get to a website and what do to with the traffic than just an IP address. In this chapter, we’ll discuss what a URL is and how each of the components is determined.\nA URL looks like this:\n\\[\n\\overbrace{\\text{http://}}^\\text{protocol}\\overbrace{\\text{example.com}}^\\text{server location}\\overbrace{\\text{:80}}^\\text{port}\\overbrace{\\text{/}}^\\text{resource}\n\\]\nNow, this probably isn’t what you type into your address bar in your browser. That’s because modern browsers do most of this for you by default. So if you type \\(google.com\\) into your browser’s address bar, your browser will automatically assume the correct defaults for the rest. Try going to https://google.com:443/. What do you get?"
  },
  {
    "objectID": "chapters/sec2/2-4-understanding-traffic.html#using-ports-to-get-to-the-right-service",
    "href": "chapters/sec2/2-4-understanding-traffic.html#using-ports-to-get-to-the-right-service",
    "title": "9  Computer Networks and the Internet",
    "section": "9.6 Using ports to get to the right service",
    "text": "9.6 Using ports to get to the right service\nLet’s say you want to run software on a server. One of the big differences between server software, and software on your laptop is that server software needs to be able to interact with the outside world to be useful.\nFor example, when you want to use Microsoft Word on your computer, you just click on the button and then it’s ready to go. But say I want to use RStudio Server. I don’t have a desktop where I click to open RStudio Server. Instead, I go to a particular URL and I expect that RStudio will be there, ready to listen."
  },
  {
    "objectID": "chapters/sec2/2-4-understanding-traffic.html#special-ip-addresses-and-ports",
    "href": "chapters/sec2/2-4-understanding-traffic.html#special-ip-addresses-and-ports",
    "title": "9  Computer Networks and the Internet",
    "section": "9.7 Special IP Addresses and Ports",
    "text": "9.7 Special IP Addresses and Ports\nAll ports below 1024 reserved.\n80 - HTTP default\n443 - HTTPS default\n22 - SSH default\nNormally, you’ll see a URL written something like this:\n\\[\nexample.com\n\\]\nIt doesn’t seem like this little sni\n\\[\n\\overbrace{https://}^{\\text{Protocol}}\\overbrace{\\underbrace{www}_{\\text{Subdomain}}.\\underbrace{example}_{\\text{Primary Domain}}.\\underbrace{com}_{\\text{Top-Level Domain}}}^{\\text{Domain}}/\\overbrace{engineering}^{\\text{Path}}\n\\]\nEven worse, IP addresses generally aren’t permanent – they can change when individual servers are replaced, or if you were to change the server architecture (say by adding and load-balancing a second instance – see chapter XX).\nAnatomy of a URL\nIn order to have something human-friendly and permanent, we access internet resources at uniform resource locators (URLs), like google.com, rather than an IP address."
  },
  {
    "objectID": "chapters/sec2/2-4-understanding-traffic.html#getting-your-own-domain",
    "href": "chapters/sec2/2-4-understanding-traffic.html#getting-your-own-domain",
    "title": "9  Computer Networks and the Internet",
    "section": "9.8 Getting your own domain",
    "text": "9.8 Getting your own domain\nIn the last chapter, we spent most of the time talking about server locations in terms of IP addresses. And it’s true – the “real” address of any server is its IP address. But we generally don’t access websites or other resources at IP addresses – they’re hard to remember, and they can also change over time.\nInstead, we generally use domains for websites, and hostnames for individual servers. We’ll get into hostnames later on – for now we’re going to focus on domains.\nA domain is simply a convenient alias for an IP address. The domain name system (DNS) is the decentralized internet phonebook that translates back and forth between domains and IP addresses. The details of how DNS resolution works are quite intricate – but the important thing to know is that there are layers of DNS servers that eventually return an IP address to your computer for where to find your website.\nFrom the perspective of someone trying to set up their own website, there’s only one DNS server that matters to you personally – the DNS server for your domain name registrar.\nDomain name registrars are the companies that actually own domains. You can buy or rent one from them in order to have a domain on the internet. So let’s say you take the data science server you set up in lab 1 and decide that you want to host it at a real domain.\nYour first stop would be a domain name registrar where you’d find an available domain you like and pull out your credit card.\nCosts for domain names vary widely. Buying a meaningless domain in a less popular top-level domain, say ladskfmlsdf.me can cost as little as $3 per year. On the other hand, buying a .com domain that’s a real word or phrase can be a few thousand dollars – and there are articles every few years about some major company accidentally allowing their domain name to lapse and ransoming it back for stupid amounts of money.\nSo, conceptually, it’s easy to understand how a domain comes to stand in for an IP address, with DNS being the directory that ties the two together.\n\n9.8.1 Configuring DNS to connect IP addresses and Domains\nThe harder part is the nitty gritty of how you accomplish that mapping yourself, which we’ll get into now.\nConfiguration of DNS is done by way of records, of which there are a menagerie of types you can configure. Luckily, most simple configurations only need CNAME and A records.\nHere’s an imaginary DNS record table for the domain example.com:\n\n\n\nPath/Host\nType\nTarget\n\n\n\n\n@\nA\n143.122.8.32\n\n\nwww\nCNAME\nexample.com\n\n\n*\nA\n143.122.8.33\n\n\n\nLet’s go through how to read this table.\nSince we’re configuring example.com, the paths/hosts in this table are relative to example.com.\nIn the first row we’re configuring an A record to go to the target IP address. A records (or their IPv6 cousin AAAA records) map a domain to an actual IP address. The path @ is a special symbol meaning exact match. So by this configuration, any traffic to example.com will be passed straight through to the specified IP address.\nThe second row deals with traffic to the www subdomain. CNAME records alias sub-domains. They’re most frequently used to map sub-domains to the main domain. Since this is a CNAME record for example.com, this record indicates that traffic to www.example.com should be treated exactly like traffic to example.com. Some domain providers do automatic redirection of www traffic, and so this row may not be necessary in some configurations.\nThe last record uses the wildcard symbol * to send all subdomain traffic that’s not already spoken for – say blog.example.com or info.example.com directly to the IP address specified. In this case, I’m sending all of those subdomains to a different IP address, maybe a 404 (not found) page – or maybe I’m serving all the subdomains off a different server.\nSo what happens is that your query goes through several layers of public DNS servers to get to the DNS entry for your domain name registrar. In many cases, you’ll directly configure your domain name registrar to point to your website or server – but you also can configure the domain name registrar to point at another set of DNS servers you actually control with an NS record.\nIf you’re setting up your own server, this probably isn’t the case, but some large enterprises do run their own private DNS servers.\n\nYou should always configure your domain provider as narrowly as possible – and you should configure your website or server first.\n#TODO: why?\n\n\n\n9.8.2 Learning to Hate DNS\nAs you get deeper into using servers, you will learn to hate DNS with a fiery passion. While it’s necessary so we’re not running around trying to remember incomprehensible IP addresses, it’s also very hard to debug as a server admin.\nLet’s say I’ve got the public domain example.com, and I’m taking down the server and putting up a new one. I’ve got to alter the public DNS record so that everyone going to example.com gets routed to the new IP address, and not the old one.\nThe thing that makes it particularly challenging is that the DNS system is decentralized. There are thousands of public DNS servers that a request could get routed to, and many of them may need updating.\nObviously, this is a difficult problem to solve, and it can take up to 24 hours for DNS changes to propagate across the network. So making changes to DNS records and checking if they’ve worked is kinda a guessing game of whether enough time has passed that you can conclude that your change didn’t work right, or if you should just wait longer.\nTo add an additional layer of complexity, DNS lookups are slow, so your browser caches the results of DNS lookups it has done before. That means that it’s possible you’ll still get an old website even once the public DNS record has been updated. If a website has ever not worked for you and then worked when you tried a private browser, DNS caching is likely the culprit. Using a private browsing window sidesteps your main DNS cache and forces lookups to happen afresh.\n\n\n9.8.3 Trying it out\nGo through hosting this book somewhere."
  },
  {
    "objectID": "chapters/sec2/2-4-understanding-traffic.html#exercises",
    "href": "chapters/sec2/2-4-understanding-traffic.html#exercises",
    "title": "9  Computer Networks and the Internet",
    "section": "9.9 Exercises",
    "text": "9.9 Exercises\n\nFind a cheap domain you like and buy it.\nPut an EC2 server back up with the Nginx hello-world example.\nConfigure your server to be available at your new domain.\n\nHint: In AWS, Route 53 is the service that handles incoming networking. They can serve as a domain name registrar, or you can buy a domain elsewhere and just configure the DNS using Route 53."
  },
  {
    "objectID": "chapters/sec2/2-4-understanding-traffic.html#securing-traffic-with-https",
    "href": "chapters/sec2/2-4-understanding-traffic.html#securing-traffic-with-https",
    "title": "9  Computer Networks and the Internet",
    "section": "9.10 Securing Traffic with https",
    "text": "9.10 Securing Traffic with https\nWhen you go to a website on the internet, you’ll see the URL prefixed by the https (though it’s sometimes hidden by your browser because it’s assumed). https is actually a mashup that is short for http with secure sockets layer (SSL).\nThese days, almost everyone actually uses the successor to SSL, transport layer security (TLS). However, because the experience of configuring TLS is identical to SSL, admins usually just talk about configuring SSL even when they mean TLS.\nThese days, almost every bit of internet traffic is actually https traffic. You will occasionally see http traffic inside private networks where encryption might not be as important – but more and more organizations are requiring end-to-end use of SSL.\nSecuring your website or server using SSL/TLS is one of the most basic things you can do to make sure your website traffic is safe. You should always configure https – full stop.\nSSL/TLS security is accomplished by configuring your site or server to use a SSL certificate (often abbreviated to cert). We’ll go through the details of how to get and configure an SSL certificate in this chapter – but first a little background on how SSL/TLS works.\n\n9.10.1 How SSL/TLS Enhances Security\nSSL accomplishes two things for you – identity validation and traffic encryption.\nWhen you go to a website, SSL/TLS is the technology that verifies that you’re actually reaching the website you think you’re reaching. This prevents something called a man-in-the-middle attack where a malicious actor manages to get in between the server and the client of network traffic. So, for example, you might think you’re putting your bank login information into your normal bank website, but there’s a hacker sitting in the middle, reading all of the traffic back and forth.\n[TODO: Image of man-in-the-middle]\nYou can see this in action in your web browser. When you go to a website protected by https, you’ll see a little lock icon to the left of the URL. That means that this website’s SSL certificate matches the website and therefore your computer can verify you’re actually at the website you mean to be at.\nBut how does your computer know what a valid SSL certificate is? Your computer has a list of trusted Certificate Authorities (CAs) who create, sell, and validate SSL/TLS certificates. So when you navigate to a website, the website sends back a digital signature. Your computer checks the signature against the indicated CA to verify that it was issued to the site in question.\n[TODO: image of SSL validation]\nThe second type of scary scenario SSL prevents is a snooping/sniffing attack. Even if you’re getting to the right place, your traffic travels through many different channels along the way – routers, network switches, and more. This means that someone could theoretically look at all your traffic along the way to its meaningful destination.\nWhen your computer gets back the digital signature to verify the site’s identity, it also prompts an exchange of encryption keys. These keys are used to encrypt traffic back and forth between you and the server so anyone snooping on your message will just see garbled nonsense and not your actual content. You can think of the SSL/TLS encryption as the equivalent of writing a message on a note inside an envelope, rather than on a postcard anyone could read along the way.\n\n\n9.10.2 Getting a cert of your own\nIn order to configure your site or server with SSL, there are three steps you’ll want to take: getting an SSL certificate, putting the certificate on the server, and making sure the server only accepts https traffic.\nYou can either buy an SSL certificate or make one yourself, using what’s called a self-signed cert.\nThere are a variety of places you can buy an SSL/TLS certificate, in many cases, your domain name registrar can issue you one when you buy your domain.\nWhen you create or buy your cert, you’ll have to choose the scope. A basic SSL certificate covers just the domain itself, formally known as a fully qualified domain name (FQDN). So if you get a basic SSL certificate for www.example.com, www.blog.example.com will not be covered. You can get a wildcard certificate that would cover every subdomain of *.example.com.\n\nNote that basic SSL/TLS certification only validates that when you type example.com in your browser, that you’ve gotten the real example.com. It doesn’t in any way validate who owns example.com, whether they’re reputable, or whether you should trust them.\nThere are higher levels of SSL certification that do validate that, for example, the company that owns google.com is actually the company Google.\n\nBut sometimes it’s not feasible to buy certificates. While a basic SSL certificate for a single domain can cost $10 per year or less, wildcard certificates will all the bells and whistles can cost thousands per year. This can get particularly expensive if you’ve got a lot of domains for some reason.\nMoreover, there are times when you can’t buy a certificate. If you’re encrypting traffic inside a private network, you will need certificates for hosts or IP addresses that are only valid inside the private network, so there’s no public CA to validate them.\nThere are two potential avenues to follow. In some cases, like inside a private network, you want SSL/TLS for the encryption, but don’t really care about the identity validation part. In this case, it’s usually possible to skip that identity validation part and automatically trust the certificate for encryption purposes.\nIt’s also possible to create your own private CA, which would verify all your SSL certificates. This is pretty common in large organizations. At some point, every server and laptop needs to have the private CA added to its set of trusted certificate validators.\nA warning: it is deceptively easy to generate and configure a self-signed SSL certificate. It’s usually just a few lines of shell commands to create a certificate, and adding the certificate to your server or website is usually just a copy/paste affair.\nHowever, it’s pretty common to run into problems with self-signed certs or private CAs. Making sure the certificate chain is correct, or running into a piece of software that doesn’t ignore the identity validation piece right is pretty common. This shouldn’t dissuade you from using SSL/TLS. It’s an essential, and basic, component of any security plan – but using a self-signed cert probably isn’t as easy as it seems.\nWhen you configure your site or server, there will likely be an option to redirect all http traffic to https traffic. If your server or site is open to the internet, you should set this option."
  },
  {
    "objectID": "chapters/sec2/2-5-servers.html",
    "href": "chapters/sec2/2-5-servers.html",
    "title": "10  Choosing the right server for you",
    "section": "",
    "text": "Data Science is a delightful mashup of statistics and computer science. While you can be a great data scientists without a deep understanding of computational theory, a mental model of how your computer works is helpful, especially when you head to production.\nIn this chapter, we’ll develop a mental model for how computers work, and explore how well that mental model applies to both the familiar computers in your life, but also more remote servers.\nIf you’re into pedantic nitpicking, you’re going to love this chapter apart, as I’ve grossly oversimplified how computers work. On the other hand, this basic mental model has served me well across hundreds of interactions with data scientists and IT/DevOps professionals.\nAnd by the end of the chapter, we’ll get super practical – giving you a how-to on getting a server of your very own to play with."
  },
  {
    "objectID": "chapters/sec2/2-5-servers.html#computers-are-addition-factories",
    "href": "chapters/sec2/2-5-servers.html#computers-are-addition-factories",
    "title": "10  Choosing the right server for you",
    "section": "10.1 Computers are addition factories",
    "text": "10.1 Computers are addition factories\nAs a data scientist, the amount of computational theory it’s really helpful to understand in your day-to-day can be summarized in three sentences:\n\nComputers can only add.\nModern ones do so very well and very fast.\nEverything a computer “does” is just adding two (usually very large) numbers, reinterpreted.1\n\nI like to think of computers as factories for doing addition problems.\n\nWe see meaning in typing the word umbrella or jumping Mario over a Chomp Chain and we interpret something from the output of some R code or listening to Carly Rae Jepsen’s newest bop, but to your computer it’s all just addition.\nEvery bit of input you provide your computer is homogenized into addition problems. Once those problems are done, the results are reverted back into something we interpret as meaningful. Obviously the details of that conversion are complicated and important – but for the purposes of understanding what your computer’s doing when you clean some data or run a machine learning model, you don’t have to understand much more than that.\n\n10.1.1 Compute\nThe addition assembly line itself – where the work actually gets done – is referred to as compute. It’s where 2+2 gets turned into 4, and where 345619912 + 182347910 gets turned into 527967822. The heart of the factory in your computer is the central processing unit (CPU).\nThere are two elements to the total speed of your compute – the total number of cores, which you can think of as an individual conveyor belt doing a single problem at a time, and the speed at which each belt is running.\nThese days, most consumer-grade laptops have between 4 and 16 cores, and may have additional capabilities that effectively doubles that number. So most laptop CPUs can do between 4 and 32 simultaneous addition problems.\nIn your computer, the basic measure of conveyor belt speed is single-core “clock speed” in hertz (hz) – operations per second. The cores in your laptop probably run between 2-5 gigahertz (GHz): 2-5 billion operations per second.\n\nA few decades ago, there was a steady march in increases in clock speed – increasing by approximately 10x during the 90s, by 2-3x in the 2000s, and somewhere between not at all and 1.5x in the 2010s. Computers have gotten much faster through this entire time, but mostly from sources other than boosts in single-core clock speeds.\nFor example, in 2020, Apple released a series of Macs with its new M1 chip. Real-world performance of the M1 chip was dramatically better than earlier models, despite modest improvements in clock speeds.\n\n\n10.1.1.1 GPU Computing\nWhile compute usually just refers to the CPU, it’s not completely synonymous. Computers can offload some problems to a graphical processing unit (GPU). GPUs are specialized chips used for tasks like editing photo or videos, rendering video game graphics, some kinds of machine learning, and (yes) Bitcoin mining.\nWhere the CPU has a few fast cores, the GPU takes the opposite approach, with many slower cores. Where a consumer-grade CPU has 4-16 cores, mid-range GPUs have 700-4,000, but each one runs between 1% and 10% the speed of a CPU core.\nFor GPU-centric tasks, the overwhelming parallelism of a GPU is more important than the speed of any individual core, and GPU computation can be dramatically faster. For the purposes of data science, many popular machine learning techniques – including neural networks, XGBoost, and other tree-based models – potentially run much much faster on GPUs relative to CPUs.\n\n\n\n10.1.2 Memory (RAM)\nYour computer’s random access memory (RAM) is its short term storage. Your computer uses RAM to store addition problems it’s going to tackle soon, and results it thinks it might need again in the near future.\nThe benefit of RAM is that it’s very fast to access. The downside is that it’s temporary. When your computer turns off, the RAM gets wiped.2\n\nYou probably know this, but memory and storage is measured in bytes. These days, most consumer hard drives are between 1/2 to a few terrabytes (trillion bytes). Some enterprise data stores run on the scales of thousands of terrabytes (pettabytes) or even thousands of pettabytes (yottabytes).\n\nModern consumer-grade laptops come with somewhere between 4 and 16 Gb of memory.\n\n\n10.1.3 Storage (Hard Drive/Disk)\nYour computer’s storage is like a freezer. It keeps things permanently, whether the factory is operating or not. The tradeoff is that things there are cold, and take some time to warm up before they can be used.\nA few years ago, all hard drives were (HDDs), and were basically the same. HDDs have a bunch of magnetic disks that spin very fast (5,400 and 7,200 RPM are common speeds). Magnetized read/write heads move among the disks and save and read your data.\nIn the last few years, solid-state drives (SSDs) have been taking over. SSDs, which are collections of flash memory chips, are up to 15x faster than HDDs. They also can take a wider variety of shapes and sizes, and are more reliable and durable because they have no moving parts. The main drawback is that they’re usually more expensive per byte, but prices are still quite reasonable.\nMany consumer laptops have only an SSD at this point. Some desktops and high-end laptops combine a smaller SSD with a larger HDD."
  },
  {
    "objectID": "chapters/sec2/2-5-servers.html#choosing-the-right-data-science-machine",
    "href": "chapters/sec2/2-5-servers.html#choosing-the-right-data-science-machine",
    "title": "10  Choosing the right server for you",
    "section": "10.2 Choosing the right data science machine",
    "text": "10.2 Choosing the right data science machine\nIn my experience as a data scientist and talking to IT/DevOps organizations trying to equip data scientists, the same questions about choosing a computer come up over and over again. Here are the guidelines I often share.\n\n10.2.1 Get as much RAM as feasible\nIn most cases, R and Python have to load all of your data into memory. Thus, the size of the data you can use is limited to the size of your machine’s RAM. Most other limits of your machine will just result in things being slower than you’d really want, but trying to load too much data into memory will result in a session crash, and you won’t be able to do your analysis.\n\nYou can get around the in-memory limitation by using a database or libraries that facilitate on-disk operations like Apache Arrow or dask.\n\nIt’s easy to say that you’ll always want more RAM, but a rough rule of thumb for whether you’ve got enough is the following:\n\nAmount of RAM = max amount of data * 3\n\nBecause you’ll often be doing some sort of transformation that results in invisible data copies and your computer can’t devote all of its memory, you’ll want to leave plenty of room over your actual data size. I’ve never run into problems with a scaling factor of 3, but this is definitely only a rough rule-of-thumb.\n\n\n10.2.2 Go for fewer, faster cores in the CPU\nR and Python are single-threaded. Unless you’re using special libraries for parallel processing, you’ll end up red-lining a single CPU core while the other just look on in silence. Therefore, single core clock speed matters more than the number of cores, and fewer, faster cores are usually preferable to many slower\nIf you’re buying a laptop or desktop, there usually aren’t explicit choices between a few fast cores and more slow cores. Most modern CPUs are pretty darn good, and you should just get one that fits your budget. If you’re standing up a server, you often have an explicit choice between more slower cores and fewer faster ones.\n\n\n10.2.3 Get a GPU…maybe…\nIf you’re doing machine learning that can be improved by GPU-backed operations, you might want a GPU. In general, only highly parallel machine learning problems like training a neural network or tree-based models will benefit from GPU computation.\nOn the other hand, GPUs are expensive, non-machine learning tasks like data processing don’t benefit from GPU computation, and many machine learning tasks are amenable to linear models that run well CPU-only.\n\n\n10.2.4 Get a lot of storage, it’s cheap\nAs for storage – get a lot – but don’t think about it too hard, because it’s cheap. Both a 1TB SSD and a 4TB HDD are around $100. Storage is cheap enough these days that it is almost always more cost efficient to buy more storage rather than making a highly-paid professional spend their time trying to figure out how to move things around.\nOne litmus test of an IT organization that is well-equipped to support data science is whether they understand this. Smart organizations know that just getting more storage is easily worth the cost in terms of the time of admins and data scientists.\n\n\n10.2.5 AWS Instance Classes for Data Science\nTODO: base storage vs mounted\nt3 – good b/c of instance credits, limited size\nCs – good b/c fast CPUs\nR - good b/c high amount of RAM\nP - have GPUs, but also v expensive\nInstance scale linearly w/ number of cores (plot?)"
  },
  {
    "objectID": "chapters/sec2/2-5-servers.html#exercises",
    "href": "chapters/sec2/2-5-servers.html#exercises",
    "title": "10  Choosing the right server for you",
    "section": "10.3 Exercises",
    "text": "10.3 Exercises\n\nThink about the scenarios below – which part of your computer would you want to upgrade to solve the problem?\n\nYou try to load a big csv file into pandas in Python. It churns for a while and then crashes.\nYou go to build a new ML model on your data. You’d like to re-train the model once a day, but it turns out training this model takes 26 hours on your laptop.\nYou design an visualization Matplotlib , and create a whole bunch in a loop/"
  },
  {
    "objectID": "chapters/sec3/3-0-sec-intro.html",
    "href": "chapters/sec3/3-0-sec-intro.html",
    "title": "Making it Enterprise-Grade",
    "section": "",
    "text": "In the last section, we walked through getting set up with a server, accessing and managing the server via SSH, understanding DNS and getting a real URL, securing the server with SSL/HTTPS, and right-sizing the server for your needs.\nAnd if you walked through the process with the labs, you’ve got a data science workbench running RStudio Server and JupyterHub all of your own. That’s awesome!\nThat server is great if you’re working alone on some data science projects you want to host in the cloud or if you’ve got a small team of data scientists in a small organization.\nBut if you work at a large organization or an organization with higher security needs, or if you’re having to manage a server for a larger team of data scientists, you’re going to need to start considering some more complex ways of managing your data science environment for security and stability.\nAs a data scientist, you’re primarily concerned with getting access to data to discover new exciting things and develop systems that can inform decisions at your organization. As you start trying to make systems more production-grade, security becomes more of an issue.\nIt’s easy to think that security isn’t really relevant for you because you don’t think you’re much of a target, but security encompasses more than just bad actors trying to kick in the virtual front door.\nThere are really two big concerns that IT/Admins are thinking about constantly – access and availability.\nAccess is about making sure that the right people can interact with the systems they’re supposed to and that unauthorized people aren’t. It’s easy to think about the grandest version of this – you don’t want people outside your organization getting to your data science workbench. But there are also versions of this inside your organization – how do you assign and manage who’s able to access different data sets? Who is able to make server-wide changes on your workbench?\nAvailability is about making sure that your enterprise-grade systems are around when people need them, and that they stand up under whatever load they face during the course of operating.\nThere are two main ways to ensure that systems remain accessible and available – mainly they need to be secured from bad actors or accidental breakage, and they need proper resourcing to stay online.\nThis section is about some of the most important ways IT/Admin professionals think about keeping their systems available and accessible.\nHopefully you won’t have to implement much of what’s in this section yourself. Instead, the hope is that reading and understanding the content in this chapter will help make you a better partner to the teams at your organization who are responsible for these things. You’ll be equipped with the language and mental models to ask good questions and give informative answers to the questions the IT/Admins have about your team’s requirements."
  },
  {
    "objectID": "chapters/sec3/3-0-sec-intro.html#the-deal-with-security",
    "href": "chapters/sec3/3-0-sec-intro.html#the-deal-with-security",
    "title": "Making it Enterprise-Grade",
    "section": "The deal with security",
    "text": "The deal with security\nSome organizations have standalone security teams – I tend to think this is an anti-pattern. For example, your server could be unavailable because a bad actor broke in and took it offline, someone used up all the resources and it crashed, or because the networking broke and it can’t be reached.\nAs a data scientist, you probably have a getting-things-done kind of mindset. And that’s great!\nBut if you’re having to interact with security professionals at your organization, you’re probably finding that they have a whole different set of concerns – perhaps concerns that you’re struggling to communicate with them about.\nAs a data scientist, you’re probably thinking that you’ve secured your server. After all, we protected the traffic to and from the server with SSL/HTTPS so no one can snoop on it or jump in the middle and we configured authentication in the server, so only your authorized users can get in.\nYou’re not wrong, but that’s not how security professionals think about security. IT security professionals think about security in layers. And while you’ve done a good job setting your server up to comply with basic security best practices, there are no layers. That server front door is open to the internet. Literally anyone in the world can come to that authentication page for your RStudio Server or JupyterHub and start trying out passwords. That means you’re just one person choosing the password password away from a bad actor getting access to your server.\nLest you think you’re immune because you’re not an interesting target, there are plenty of bots out there randomly trying to break in to every existing IP address, not because they care about what’s inside, but because they want to co-opt your resources for their own purposes like crypto mining or virtual attacks on Turkish banks.1\nMoreover, security and IT professionals aren’t just concerned with bad actors from outside (called outsider threat) or even someone internal who decides to steal data or resources (insider threat). They are (or at least should be) also concerned with accidents and mistakes – these are just as big a threat to accessibility and availability.\nFor example, many basic data science workbenches give root access to everyone – it’s easy and simple! That obviously exposes the possibility that someone with access to your server could decide to steal data for their own ends (insider threat). But it also opens up the possibility that someone makes a mistake! What if they mean to just delete a project directory, but forget to add the whole path in and wipe your whole server. Yikes!\nThe first two chapters are going to be about two important topics in making sure things are accessible – secure networking and auth.\nSecure networking is one of the primary ways to enforce security in layers. We’ll get into how the data science workbench we set up right on the internet might be ok for a basic workbench, but probably won’t be secure enough if it has to interact with other systems or when they system gets more complex.\nAuth is how you verify that the people trying to access your resources are who they say they are and manage the set of things they’re allowed to do.\n\nProper Resourcing\nIf you’re a small data science team, you might not be too concerned if someone accidentally knocks your data science workbench offline for 30 minutes because they tried to run a job that was too big. You’re probably all sitting in the same room and you can learn something from the experience.\nThat’s not the case when you get to enterprise-grade tooling. An enterprise-grade data science workbench probably supports dozens or hundreds of professionals across multiple teams. The server being down isn’t a sorta funny occurrence you can all fix together – it’s a problem that must be fixed immediately – or even better avoided altogether.\nThat’s why the third chapter in this section is going to get deep into how IT/Admins think about scaling a server to avoid hitting resource constraints. We’ll also get into some of the tools, like load balancers and Kubernetes that they use to scale the servers so you can have an intelligent conversation with them about what you need.\n\n\nTooling to fit it all together\nBack in section one, we learned about environments as code – using code to make sure that our data science environments are reproducible and can be re-created as needed.\nThis idea isn’t original – in fact, DevOps has it’s own set of practices and tooling around using code to manage DevOps tasks, broadly called Infrastructure As Code (IaC). This chapter will get broadly into some of the things you can do with IaC tooling, and will introduce some key concepts and some of the popular tooling options for doing IaC you can try out.\nTODO: developing relationships with IT/Admins"
  },
  {
    "objectID": "chapters/sec3/3-1-env-promote.html",
    "href": "chapters/sec3/3-1-env-promote.html",
    "title": "11  Managing DevOps Environments",
    "section": "",
    "text": "If you’re reading this section and trying it out, you’ve moved past DevOps for Data Science. This is just plain ’ol DevOps. In this chapter, you’ll learn about some standard DevOps tooling you might want to use, and some ways to think about how to divide your work as a data scientist from your (forced) moonlighting as a DevOps engineer."
  },
  {
    "objectID": "chapters/sec3/3-1-env-promote.html#infrastructure-as-code-tooling",
    "href": "chapters/sec3/3-1-env-promote.html#infrastructure-as-code-tooling",
    "title": "11  Managing DevOps Environments",
    "section": "11.1 Infrastructure As Code Tooling",
    "text": "11.1 Infrastructure As Code Tooling\nThere are many, many varieties of infrastructure as code tooling. There are many books on infrastructure as code tooling and I won’t be covering them in any depth here. Instead, I’ll share a few of the different “categories” (parts of the stack) of infrastructure as code tooling and suggest a few of my favorites.\nTo get from “nothing” to a usable server state, there are (at minimum) two things you need to do – provision the infrastructure you need, and configure that infrastructure to do what you want.\nFor example, let’s say I’m standing up a server to deploy a simple shiny app. In order to get that server up, I’ll need to stand up an actual server, including configuring the security settings and networking that will allow the proper people to access the server. Then I’ll need to install a version of R on the server, the Shiny package, and a piece of hosting software like Shiny Server.\nSo, for example, you might use AWS’s CloudFormation to stand up a virtual private cloud (VPC), put an EC2 server instance inside that VPC, attach an appropriately-sized storage unit, and attach the correct networking rules. Then you might use Chef to install the correct software on the server and get your Shiny app up-and-running.\nIn infrastructure as code tooling, there generally isn’t a clear dividing line between tools that do provisioning and tools that do configuration…but most tools lean one way or the other.\nBasically any tool does provisioning will directly integrate into the APIs of the major cloud providers to make it easy to provision cloud servers. Each of the cloud providers also has their own IaC tool, but many people prefer to use other tools when given the option (to be delicate).\nThe other important division in IaC tools is declarative vs imperative. In declarative tooling, you simply enumerate the things you want, and the tool makes sure they get done in the right order. In contrast, an imperative tool requires that you provide actual instructions to get to where you want to go.\nIn many cases, it’s easy to be declarative with provisioning servers, but it’s often useful to have a way to fall back to an imperative mode when configuring them because there may be dependencies that aren’t obvious to the provisioning tool, but are easy to put down in code. If the tool does have an imperative mode, it’s also nice if it’s compatible with a language you’d be comfortable with.\nOne somewhat complicated addition to the IaC lineup is Docker and related orchestration tools. There’s a whole chapter on containerization and docker, so check that out if you want more details. The short answer is that docker can’t really do provisioning, but that you can definitely use docker as a configuration management IaC tool, as long as you’re disciplined about updating your Dockerfiles and redeployment when you want to make changes to the contents.\nBasically none of these tools will save you from your own bad habits, but they can give you alternatives.\nIn short, exactly which tool you’ll need will depend a lot on what you’re trying to do. Probably the most important question in choosing a tool is whether you’ll be able to get help from other people at your organization on it. So if you’re thinking about heading into IaC tooling, I’d suggest doing a quick survey of some folks in DevOps and choosing something they already know and like."
  },
  {
    "objectID": "chapters/sec3/3-1-env-promote.html#devtestprod-for-itadmin",
    "href": "chapters/sec3/3-1-env-promote.html#devtestprod-for-itadmin",
    "title": "11  Managing DevOps Environments",
    "section": "11.2 Dev/Test/Prod for IT/Admin",
    "text": "11.2 Dev/Test/Prod for IT/Admin\nIn many organizations, the entire data science stack is supported by the IT/Admin group. In this case, you probably want a two-dimensional Dev/Test/Prod setup. The IT/Admin group maintains their own Dev/Test/Prod configuration.\nDev and Test are where they try out and test new hardware configurations – the data scientists doing their work only have access to the Prod environment. They have their own Dev/Test/Prod setup within the IT/Admin prod environment.\nFor simplicity of terminology, I often refer to the IT/Admin Dev + Test environments as staging to differentiate from the Data Science Dev/Test/Prod environments.\nIn this setup, you would select the IT configuration that works for your organization and maintain one or two copies of the entire environment. I often call this a staging environment to differentiate it from the dev/test/prod environments for the data science assets.\nSo when you wanted to make a chance to the underlying servers or their architecture, that would be tested in the staging environment and then deployed to production. Data scientists would never work in the staging environment (except as testers), that’s purely for IT/Admin testing. The staging environment would include all of the environments data scientists would use – dev, test, and prod.\nThen, data science code promotion through dev/test/prod would be distinct from how server changes get made."
  },
  {
    "objectID": "chapters/sec3/3-1-env-promote.html#lab-dockerized-deployment",
    "href": "chapters/sec3/3-1-env-promote.html#lab-dockerized-deployment",
    "title": "11  Managing DevOps Environments",
    "section": "11.3 Lab: Dockerized Deployment",
    "text": "11.3 Lab: Dockerized Deployment\n\n11.3.1 Running a service in a container\nConsider reviewing the containers section if you’re not generally familiar with how to run a container.\nIf you want to run a service, like RStudio Server, out of a container, the pattern is very similar to running an interactive app. You’ll find an appropriate container, bring it up, and do some port mapping to make it available to the outside world.\nThe rocker organization makes available a number of containers related to R and RStudio. So if you want a container running RStudio Server on your laptop, it’s as easy as running\ndocker run \\\n  --rm -d \\\n  -p 8787:8787 \\\n  --name rstudio \\\n  rocker/rstudio\nNow, when you go to http://localhost:8787 on your laptop, you should see the the RStudio login screen…but what’s the password? Luckily, the wonderful folks who built the rocker/rstudio container made it easy to supply a password for the default rstudio user.\nWe do this by supplying an environment variable to the container named PASSWORD using the -e flag.\nSo, docker kill rstudio and try again by adding a password when you start:\ndocker run \\\n  --rm -d \\\n  -p 8787:8787 \\\n  --name rstudio \\\n  -e PASSWORD=my-rstudio-pass\n  rocker/rstudio\nNow you should be looking at the RStudio IDE! Hurray!\nThis is great for standing up a quick sandbox…but before you go standing up an RStudio Server on your laptop and spreading it around the world, there are a few things you’ll want to think about.\nIt’s totally possible to run a service like RStudio Server in a docker container, but you’ll need to take all the same steps to make it available to the outside world in terms of hosting it on a server, routing traffic properly, and making sure that you’re using HTTPS to secure your traffic. See chapters XX-XX for more on all that.\nNow, let’s say I wanted to create another user on the server (docker exec)\nBut there’s also one more concern that’s particular to Docker.\nOne of the best things about a Docker container is how ephemeral it is. Things come up in moments, and when they’re gone you don’t have to worry about them. But that’s also very dangerous if you want things to persist.\nThe best way to fix this is to mount in external volumes that will maintain the state should the container die or should you want to replace it. We went over how to do that in the last section.\nAt a minimum, you’ll want to mount in the user home directories that store all the data and code you can see in RStudio Server. You may also want to mount in other bits of state, like wherever you’ve installed your version of R, and the config file you’re using to maintain the server.\nNote that unlike on a server, where you’ll restart the process of the server, the pattern with a container generally is to kill and restart the container, and let changes come up with the new container."
  },
  {
    "objectID": "chapters/sec3/3-2-secure-networking.html",
    "href": "chapters/sec3/3-2-secure-networking.html",
    "title": "12  Upgraded Networking",
    "section": "",
    "text": "In the last section, we went over a general introduction to networking, including an introduction to DNS, how to get and use a real URL, and making sure your servers are secure with SSL/HTTPS.\nWhen you’re managing a relatively simple server – especially one that’s relatively low-threat, this is perfectly fine. However, if you’re managing a cluster of several servers, you have other security concerns, or a high-threat environment, you may need more complex networking solutions to make sure that your environment stays secure and manageable.\nIn this chapter, we’ll learn a little about how an environment can be further secured from internet traffic."
  },
  {
    "objectID": "chapters/sec3/3-2-secure-networking.html#the-role-of-proxies",
    "href": "chapters/sec3/3-2-secure-networking.html#the-role-of-proxies",
    "title": "12  Upgraded Networking",
    "section": "12.1 The role of proxies",
    "text": "12.1 The role of proxies\nA proxy is a server that exists solely as an intermediary to manage the traffic coming to it. Proxies can serve a variety of different roles including acting as a firewall, routing traffic to different places, doing authentication into the servers behind, or caching data for better performance on frequent queries.\nFor an IT/Admin, managing proxies is an everyday activity.\nHopefully for you as a data scientist, you almost never have to know much about the proxies themselves, but being able to ask intelligent questions – and know how they might get in the way of what you’re trying to go – can relieve some headaches before they occur.\n\n\n\n\n\n\nNetwork Debugging Tip\n\n\n\nIf you’re experiencing weird behavior in your data science environment – files failing to upload or download, sessions getting cutoff strangely, or data not transferring right – issues with a proxy are probably the first thing you should check.\n\n\nIf you’re talking to your IT/Admin about the network where your data science environment sits, one of the most important questions is whether that network has a proxy. There are two different kinds of proxies that are important to know about – forward and reverse. Proxies are discussed from the perspective of being inside the network, so a forward proxy is one that intercepts and does something to outbound traffic, while a reverse proxy is one that does something to inbound traffic. Personally, I much prefer the terms inbound and outbound to forward and reverse, because it’s much easier to remember which is which.\nIn most cases, proxies sit right on the edge of your network – in the DMZ (public subnet) between your private servers and the open internet. In some cases, there are also proxies inside different segments of the private network. These are generally for the purposes of isolating components as a security measure – like having watertight bulkheads between sections of a ship. If you’ve got a proxy – for example – between your app development environment and where it’s deployed, that’s a reasonably common cause of issues along the way.\nDeveloping a good mental model of where network connections originate is really important in terms of understanding why proxies might be causing you trouble. One of the most helpful things you can do when talking to your IT/Admin about networking issues is help them understand when your data science environment requires an inbound connection and when it requires an outbound connection.\nTODO: image inbound vs outbound connection\nAs we went over in chapter 2-4, network traffic always operates on a call and response model. So whether your traffic is inbound or outbound is dependent on who makes the call. Inbound means that the call is coming from a computer outside the private network directed to a server inside the private network, and outbound is the opposite.\nSo basically, anything that originates on your laptop – including the actual session into the server is an inbound connection, while anything that originates on the server – including everything in code that runs on the server is an outbound connection.\n\n12.1.1 What proxies do\nThere are a number of different functions proxies can do – here are a few of the most common.\nProxies – especially reverse/inbound – often do redirection. This means that the proxy is generally the what’s actually available at the public URL of your server. It then redirects people along to the actual hostname for your server. Inside your private network, you can name your server whatever you want, so if you’ve got a server that’s just named rstudio inside your network, your proxy would know that anyone going to my-org.com/rstudio ends up at the host named rstudio.\nOne other nice thing proxies can do along with redirection is managing ports more securely. Many services run on a high-numbered port by default to avoid conflicts with other services. For example, RStudio Server runs on port 8787 by default. But remembering what weird port has to be open for every service can be kinda a pain. Thus it can be much easier to just keep standard ports (80 for HTTP, 443 for HTTPS, and 22 for SSH) open on your proxy and have the proxy just redirect the traffic coming into it on 80 to 8787 on the server with RStudio Server.\nThere’s a special kind of reverse proxy called a load-balancer. This is a kind of proxy that redirects traffic coming to a single URL to not just one – but one of a pool of servers. In the scaling chapter, we’ll get more into how to think about pools of servers and load-balancing work, but the networking part is handled by a load-balancer.\n[TODO: image of path rewriting + load-balancing]\nSometimes proxies also terminate SSL. Because the proxy is the last server that is accessible from the public network, many organizations don’t bother to implement SSL/HTTPS inside the private network so they don’t have to worry about managing SSL certificates inside their private network. This is getting rarer as tooling for managing SSL certificates gets better, but it’s common enough that you might start seeing HTTP addresses if you’re doing server-to-server things inside the private network.\nTODO: image of SSL termination\nOccasionally proxies also do authentication. In most cases, proxies pass along any traffic that comes in to where it’s supposed to go. If there’s authentication, it’s often at the server itself. Sometimes the proxy is actually where authentication happens, so you have to provide the credentials at the edge of the network. Once those credentials have been supplied, the proxy will let you through. Depending on the configuration, the proxy may also add some sort of token or header to your incoming traffic to let the servers inside know that your authentication is good and to pass along identification for authorization purposes.\nTODO: image of auth at proxy\nThe last thing that proxies can do is just block traffic that isn’t explicitly allowed. If you’ve got a forward proxy in your environment, you may have to work with your IT/Admins to make sure that you’re able to access the resources you need to get your work done. If that’s not an option, you may have to think about how to operate offline, which we’ll address towards the end of the chapter.\n\n\n12.1.2 Inbound vs outbound proxies\nThe things you’ll have to think about are very different depending on whether your VPC has a outbound/forward or an inbound/reverse proxy. If you’ve got a reverse proxy (and most enterprise networks do), the main thing you probably have to consider is path-rewriting – generally the proxy is what’s actually hosted at the public URL of the server, and then it passes people along to the internal hostname of the actual server.\nThe other big concern is what kinds of connections your proxy supports. For example, many data science app frameworks (including Shiny and Streamlit) use a technology called Websockets for maintaining the connection between the user and the app session. Most modern proxies support Websockets – but some don’t and you’ll have to figure out workarounds if you can’t get Websockets enabled.\nAdditionally, some inbound proxies have limitations that can make things weird for data science use cases – the most common are limiting file size for uploads and downloads and implementing timeouts on file uploads, downloads, and sessions. It’s reasonably common for organizations to have standard file size limits or timeouts that don’t work well in a data science contexts. In data science contexts, files tend to be big and session lengths long. If you’re trying to work in a data science context and weird things are happening with file uploads or downloads or sessions ending unexpectedly, checking on inbound proxy settings is a good first hunch.\nInbound proxies are what you’re thinking about when you’re wondering if anything is going wrong for your connection to your server. If you can’t upload or download the files you need directly to the server or your RStudio session keeps timing out, a reverse proxy is a likely candidate.\nOn the other hand, forward/outbound proxies are the likely culprit if your code is having trouble running. In general, outbound proxies are simpler than inbound. The most common thing they do is simply block traffic from leaving the private network. Many organizations have these proxies to reduce the risk of someone getting in and then being able to exfiltrate valuable resource.\nThe most common reasons you’d hit a forward/outbound proxy in your code is when you’re installing packages from a public repository or when you’re trying to make use of an API or other web service that’s outside your private network.\nIn some cases, ameliorating these issues is as easy as talking to your IT/Admin and asking them to open the outbound proxy to the right server. Especially if it’s a URL protected by HTTPS and that’s for only one thing – for example CRAN, PyPI, or public RStudio Package Manager, it’s generally pretty safe and many organizations are happy to allow-list a limited number of outbound addresses.\nIf not, the next section is for you."
  },
  {
    "objectID": "chapters/sec3/3-2-secure-networking.html#fully-offlineairgapped-operations",
    "href": "chapters/sec3/3-2-secure-networking.html#fully-offlineairgapped-operations",
    "title": "12  Upgraded Networking",
    "section": "12.2 Fully Offline/Airgapped operations",
    "text": "12.2 Fully Offline/Airgapped operations\nOffline or airgapped environments are quite common in highly regulated industries with strong requirements around data security and governance.\nThe term airgapped comes from the notion that there is a physical gap – air – between the internet and the environment. In these instances, the servers where your data science environment exists is physically disconnected from the outside world, and if you need to move something into the environment, you’ll have to load it onto a physical drive outside the environment and then walk into where the environment is.\nHowever, environments that are this thoroughly airgapped are quite rare. Most airgapped environments are airgapped by proxies.\nThis means that they sharply limit where inbound connections are allowed to come from – for example perhaps only from machines in the physical building where your company sits.1 The way you get into your airgapped environment usually involves either being in a certain physical location to get on the network or logging into a VPN. If your organization requires offline operations, they almost certainly have ways to give you access to the network. Talk to your IT/Admin.\nTODO: image of VPC inside VPN\nIf your server is offline, it’s likely that they strictly limit or completely disallow outbound connections from the servers inside the private network. This is where your organization probably doesn’t have standard practices, and you getting clear on what you need with your IT/Admins will really help.\nHere are the four most common reasons you’ll need to make outbound connections from inside your data science environment.\n\nDownloading Packages Downloading a package requires a network connection to the repository – usually CRAN, BioConductor, public RStudio Package Manager, Conda, PyPI, or GitHub. If you can get narrow exceptions for these, that’s great! If not, you’ll need to figure out how to run a package repository inside your data science environment like we discussed in the last chapter.\nData Science Tasks In many organizations, you don’t need internet access at all for the work you’re doing. You’re just working on data from databases or files inside your private network and don’t really need access to data or resources outside. On the other hand, if you’re consuming data from public APIs or scraping data from the web, that may require external connections.\nSystem Libraries In addition to the R and Python packages, there are also system libraries you’ll need installed, like the versions of R and Python themselves, and other packages used by the system. Generally it’ll be the IT/Admin managing and installing these, so they probably have a strategy for doing it. This may come up specifically in the context of data science if you’re using R or Python packages that are basically just wrappers around system libraries, like the sf package in R, or the GDAL python package which wraps the GDAL system library for geospatial work.\nSoftware Licensing If you’re using all open source software, this probably won’t be an issue. But if you’re buying licenses to a professional product, you’ll have to figure out how to activate the software licensing, which generally operates by reaching out to servers owned by the software vendor. They should have a method for activating servers that can’t reach the internet, but your IT/Admins will appreciate if you’ve done your homework on this before asking them to activate some new software.\n\nBefore you go ahead treating your environment as truly offline/airgapped, it’s almost always worth asking if narrow exceptions can be made to a network that is offline/airgapped. The answer may surprise you.\nIf you are truly offline, you probably won’t be able to move things on or off your private servers. Instead, when you need things, the IT/Admin will either connect to a server in the DMZ that has permission to access both the public internet and the private network to load things, or they’ll actually have to download things to their laptop from the internet, connect to the server in the offline environment, and upload them.\nTODO: drawing of offline operations"
  },
  {
    "objectID": "chapters/sec3/3-2-secure-networking.html#review-questions",
    "href": "chapters/sec3/3-2-secure-networking.html#review-questions",
    "title": "12  Upgraded Networking",
    "section": "12.3 Review Questions",
    "text": "12.3 Review Questions\n\nLet’s say you’ve got a private VPC that hosts an instance of RStudio Server, an instance of JupyterHub, and a Shiny Server that has an app deployed. Here are a few examples of traffic – are they outbound, inbound, or within the network?\n\nSomeone connecting to and starting a session on RStudio Server.\nSomeone SFTP-ing an app and packages from RStudio Server to Shiny Server.\nSomeone installing a package to the Shiny Server.\nSomeone uploading a file to JupyterHub.\nA call in a Shiny app using httr2 or requests to a public API that hosts data.\nAccessing a private corporate database from a Shiny for Python app using sqlalchemy."
  },
  {
    "objectID": "chapters/sec3/3-3-auth.html",
    "href": "chapters/sec3/3-3-auth.html",
    "title": "13  Logging in with auth",
    "section": "",
    "text": "Unless you’re a special kind of nerd, you’ve probably never thought hard about auth. But you do it all the time – every time you log into your bank to check your account or open Instagram on your phone or access RStudio via your corporate Okta account, there’s auth happening under the hood.\nAuth is shorthand for two different things – authentication and authorization.\nAuthentication is the process of verifying identites. When I show up at a website that isn’t just open to the internet, how do I prove that I am who I say I am.\nAuthorization is the process of managing and checking permissions. Once you know that it’s me at the front door, am I allowed to come in?\nFor the most part, this chapter is designed to help you talk to the folks who manage auth at your organization. Unless you’ve got a small organization with only a few data scientists, you probably won’t have to manage this yourself. But it can be extremely helpful to understand how auth works when you’re trying to ask for something from the organization’s admins.\nIn order to talk concretely about logging into systems, it’s helpful to clarify some terms. For the most part, these terms are industry standard, but I’m also going to generalize some terms because they’re used for certain types of auth, and they’re really useful.\nFor the purposes of this document, we’re going to be talking about trying to login to a service. A service is something you want to login to – it could be your phone or your email, or a server, a database, or software like RStudio Server or JupyterHub.\nWhen you go to login to a service, there are two things that have to happen. First, you have to assert and verify who you are. This process is called authentication, and the assertion and proof of identity are your credentials or creds. The most common credentials are a username and password, but there are other options including SSH keys, multi-factor authentication, or biometrics.\nOnce you’ve proven who you are, then the system needs to determine what you’re supposed to have access to. This process is called authorization.\nOften, the authentication + authorization process is referred to collectively as auth.\n[#TODO: Image of Auth]\nSo, to summarize when you go to login to a service, the service reaches out to some sort of system to verify your identity. Depending on the method, it may also check your authorization. The name of that system varies by the auth method, but for the purposes of this chapter, we’ll refer to it generally as the identity store. Depending on the auth method, the identity store may store just authentication records, or both authentication and authorization."
  },
  {
    "objectID": "chapters/sec3/3-3-auth.html#the-many-flavors-of-auth-or-what-does-sso-mean",
    "href": "chapters/sec3/3-3-auth.html#the-many-flavors-of-auth-or-what-does-sso-mean",
    "title": "13  Logging in with auth",
    "section": "13.1 The many flavors of auth (or what does SSO mean?)",
    "text": "13.1 The many flavors of auth (or what does SSO mean?)\nSingle Sign On (SSO) is a slippery term, so it is almost always necessary to clarify what is meant by the term when you hear it. At some organizations, identity management isn’t centralized at all. This means that usernames and passwords are unique to each service, onboarding and offboarding of users has to be handled independently for each service, and users have to login frequently. In short, it’s often not a great system. This is never referred to as SSO.\n\nMost organizations of a meaningful size have centralized identity management. This means that identities, credentials, authorization, onboarding, and offboarding are handled centrally. However, you may still need to independently login to each system. For example, in this system, every service might take the same username and password as your credentials, but if you go to RStudio Server followed by JupyterHub, you’ll need to provide that username and password independently to each service. This system is often facilitated by PAM, and LDAP/AD. Some organizations call this SSO, because there’s only one set of credentials.\nIn true SSO, users login once and are given a token or ticket.1 Then, when they go to the next service, they don’t have to login again because that service can just look at the token or ticket to do auth for that user. For example, in this system, I could go to RStudio Server and login, and then go to JupyterHub and get in without being prompted again for my password. This type of auth is facilitated by Kerberos, SAML, or OAuth."
  },
  {
    "objectID": "chapters/sec3/3-3-auth.html#auth-techniques",
    "href": "chapters/sec3/3-3-auth.html#auth-techniques",
    "title": "13  Logging in with auth",
    "section": "13.2 Auth Techniques",
    "text": "13.2 Auth Techniques\nIf you have five data scientists in your group, and the only shared resource you have is an RStudio Server instance, you probably don’t need to think terribly hard about auth. It’s pretty straightforward to just make users on a server and give them access to everything.\nBut as organizations get larger with hundreds or thousands of users, there’s constant churn of people joining and leaving. The number of services can creep into the dozens or hundreds and people may have very different authorization levels to different services. Trying to manage auth on the individual services is a nightmare – as is trying to keep that many usernames and passwords straight for users. That is why almost all organizations with more than a few users have centrally managed auth.\n\n13.2.1 You get a permission and you get a permission!\nFor the most part, we think of people being authenticated and authorized into services. However, it’s sometimes useful to consider the broader class of entities that could do auth. There are two common non-human entities that are included in auth systems that are worth considering.\nService Accounts are accounts given to non-human entities when you want it to be able to do something on its own behalf. For example, maybe you’ve got a Shiny app that users use to visualize data that’s in a database. Very often, you don’t want the app to have the same permisions as the app’s author, or to inherit the permissions of the people viewing the app. Instead, you want the app to be able to have permissions to do certain database operations. In that case, you would create a service account to give to the Shiny app that has exactly those permissions.\nThere are also times where it’s useful to go one level up and give permissions to an entire instance or service. In that case, you might assign permissions to an instance. For example, you could make it the case that anyone who is logged into the JupyterHub server is allowed to read from the database.\nInstance permissions are rather broad, and so they are usually only applied when you’ve got multiple resources inside a private network. In that case, authentication and authorization are only done at a single point and authorization is pretty broad.\n\n\n13.2.2 Authorization is kinda hard\nFrom a management perspective, authentication is pretty simple. A person is given a set of credentials, and they have to supply those credentials when prompted to prove they are who they say they are.\nAuthorization is a whole other can of worms. There is a meaningful literature on varieties of authorization and how they work. We’re not going to get too deep into the weeds, other than to define some common terms and how they’re used.\nThe atomic basis for authorization is a permission. Permissions are a binary switch that answers the question is this person allowed to do the thing they are trying to do?2\nThe simplest way of assigning permissions is called an access control list (ACL). In systems that use ACLs, each piece of content has a list of users who are allowed access. Sometimes, ACLs are also assigned to groups, which are simply sets of users – think data-scientists.\n\nOne ACL implementation with which you may be familiar is file permissions on a Linux server. For example, if you have a Mac or are on a Linux server, you can open your terminal, navigate to a directory and do the following:\n$ ls -l\n-rwxr-xr-x   1 alexkgold  staff   2274 May 10 12:09 README.md\nThat first set of characters describes the ACL for the README.md file. The first character - indicates this is a file, as opposed to a directory of files (which would be d). Then there are three sets of 3 characters, rwx, which are short for read, write, and execute, with the first group for the owner, alexkgold, the second group for anyone else in the group staff, and the third set for anyone else.\nSo you can read -rwxr-xr-x as, this is a file that alexkgold can read, write or execute, and anyone else can read or execute, but not edit.\nACLs are pretty intuitive, but it turns out that when you are managing a lot of users across a lot of files, directories, and services, they can get pretty difficult to manage, so many organizations use Role Based Access Control (RBAC).\nRBAC adds a layer of abstraction between users and permissions, which makes it a little harder to understand, but ultimately results in a much more flexible system. In RBAC, permissions are not assigned to individual pieces of content or to users or groups. Instead, permissions are assigned to roles, and roles are given to users or groups.3\n\nThere are also further iterations on the RBAC model, like Attribute Based Access Control (ABAC) or Policy Based Access Control (PBAC) in which there’s a long list of attributes that could be considered for a user to compute their permissions for a given service."
  },
  {
    "objectID": "chapters/sec3/3-3-auth.html#auth-technologies",
    "href": "chapters/sec3/3-3-auth.html#auth-technologies",
    "title": "13  Logging in with auth",
    "section": "13.3 Auth Technologies",
    "text": "13.3 Auth Technologies\n\n13.3.1 Username + Password\nMany pieces of software come with integrated authentication. When you use those system, the product stores encrypted username and password pairs in a database.\nThese setups are often really easy from an admin perspective – you just set up individual users on the server. However, the flip side is that users have one more username and password to remember, which is annoying for them. Moreover, if you have more than a few users, or the system is one of more than a few, it’s hard to manage users on a lot of different systems. It can be a real pain to create accounts on a ton of different systems when a new person joins the organization, or to remove their permissions one-by-one when they leave.\nFor this reason, most IT/Admin organizations strongly prefer using some sort of centralized identity store.\n\n\n13.3.2 PAM\nPluggable Authentication Modules (PAM) is a Linux system for doing authentication. As of this writing, PAM is the default authentication method for both RStudio Server and JupyterHub.\n\nConceptually PAM is pretty straightforward. You install a service on a Linux machine and configure it to use PAM authentication from the underlying host. By default, PAM just authenticates against the users configured on the Linux server, but it can also be configured to use other sorts of “modules” to authenticate against other systems – most commonly LDAP/AD or Kerberos. PAM can also be used to do things when users login – the most common being initializing tokens or tickets to other systems, like a database.\nPAM is often paired with System Security Services Daemon (SSSD), which is most commonly used to automatically create Linux users on a server based on the identities stored in an LDAP/AD instance.\nThough conceptually simple, reading, writing, and managing PAM modules is kinda painful.\n#TODO: Add PAM example\n\n\n13.3.3 LDAP/AD\nLightweight Directory Access Protocol (LDAP) is a relatively old, open, protocol used for maintaining a set of entities and their attributes. To be precise, LDAP is actually a protocol for maintaining and accessing entities and their attributes in a tree. It happens that this is a really good structure for maintaining permissions and roles of users at an organization, and it’s the main thing LDAP is used for.\nActive Directory (AD) is Microsoft’s implementation of LDAP, and is by-far the most common LDAP “flavor” out there. AD so thoroughly owns the LDAP enterprise market, that LDAP is often referred to as LDAP/AD. There are other implementations you may run across, the most common being OpenLDAP.\n\nAzure, Microsoft’s cloud provider, offers an authentication service called Azure Active Directory (AAD). Confusingly, AAD is usually used in combination with SAML, not LDAP.\n\nIt’s worth distinguishing the use of LDAP as an identity store from its use as an authentication technology. As a tree-based database, LDAP is uniquely well-suited to storing the identities, and other attributes of people at the organization. However, as discussed below, using LDAP to authenticate into actual services has security and convenience drawbacks, and many organizations consider it outdated and insecure.\nA lot of organizations are moving away from LDAP for authentication in favor of token-based technologies like SAML or OAuth, but many are keeping LDAP as their identity “source of truth” that is referenced by the SAML or OAuth Identity Provider.\nLDAP has three main disadvantages relative to other technologies. First, LDAP requires that your credentials (username and password, usually) actually be provided to the service you’re trying to use. This is fundamentally insecure relative to a system where your credentials are provided only to the identity provider, and the service just gets a token verifying who you are. In token-based systems, adding additional requirements like MFA or biometrics are easy, as they’re simply added at the IdP layer. In contrast, doing those things in LDAP would require the service to implement them, which usually is not the case, so you’re usually limited to username and password.\nThe second disadvantage of LDAP is that it does not allow for central administration of permissions. LDAP directly records only objects and their attributes. Say, for example, you want only users of a particular group to have access to a certain resource. In LDAP, you would have to specify in that resource that it should only allow in users of that group. This is in contrast to SAML/OAuth, where the authorization is centrally managed.\nLastly, LDAP authentication is based on each service authenticating. Once you authenticate, the service might give you a cookie so that your login persists, but there is no general-purpose token that will allow you to login to multiple services.\n\n13.3.3.1 How LDAP Works\nWhile the technical downsides of LDAP are real, the technical operations of LDAP are pretty straightforward. In short, you try to login to a service, the service collects your username and password, sends it off to the LDAP server, and checks that your username and password are valid.\n\nNote that LDAP is purely for authentication. When you’re using LDAP, authorization has to be handled separately, which is one of the disadvantages.\n\n\n13.3.3.2 Deeper Than You Need on LDAP\nLDAP is a tree-based entity and value store. This means that LDAP stores things and their attributes, which include a name and one or more values. For example, my entry in a corporate LDAP directory might look like this:\ncn: Alex Gold\nmail: alex.gold@example.com\nmail: alex.gold@example.org\ndepartment: solutions\nmobile: 555-555-5555\nobjectClass = Person\nMost of these attributes should be pretty straightforward. cn is short for common name, and is part of the way you look up an entity in LDAP (more on that below). Each entity in LDAP must have an objectClass, which determines the type of entity it is. In this case, I am a Person , as opposed to a device, domain, organizationalRole, or room – all of which are standard objectClasses.\nLet’s say that your corporate LDAP looks like the tree below:\n\n#TODO: make solutions an OU in final\nThe most common way to look up LDAP entities is with their distinguished name (DN), which is the path of names from the point you’re starting all the way back to the root of the tree. In the tree above, my DN would be cn=alex,ou=solutions,dc=example,dc=com.\nNote that you read the DN from right to left to work your way down the tree. Aside from cn for common name, other common fields include ou for organizational unit, and dc for domain component.\n\n\n13.3.3.3 Trying out LDAP\nNow that we understand in theory how LDAP works, let’s try out an actual example.\nTo start, let’s stand up LDAP in a docker container:\n#TODO: update ldif\ndocker network create ldap-net\ndocker run -p 6389:389 \\\n  --name ldap-service \\\n  --network ldap-net \\\n  --detach alexkgold/auth\nldapsearch is a utility that lets us run queries against the LDAP tree. Let’s try it out against the LDAP container we just stood up.\nLet’s say I want to return everything in the subtree under example.org. In that case, I would run ldapsearch -b dc=example,dc=org, where b indicates my search base, which is a dn. But in order to make this actually work, we’ll need to include a few more arguments, including\n\nthe host where the LDAP server is, indicated by -H\nthe bind DN we’ll be using, flagged with -D\nthe bind password we’ll be using, indicated by -w\n\nSince we’re testing, we’re also going to provide the flag -x to use whatever certificate is present on the server. Putting it altogether, along with the commands to reach the docker container, let’s try:\nldapsearch -x -H ldap://localhost:6389 -b dc=example,dc=org -D \"cn=admin,dc=example,dc=org\" -w admin\n\n# extended LDIF\n#\n# LDAPv3\n# base <dc=example,dc=org> with scope subtree\n# filter: (objectclass=*)\n# requesting: ALL\n#\n\n# example.org\ndn: dc=example,dc=org\nobjectClass: top\nobjectClass: dcObject\nobjectClass: organization\no: Example Inc.\ndc: example\n\n# admin, example.org\ndn: cn=admin,dc=example,dc=org\nobjectClass: simpleSecurityObject\nobjectClass: organizationalRole\ncn: admin\ndescription: LDAP administrator\nuserPassword:: e1NTSEF9d3IyVFp6SlAyKy9xT2RsQ0owTDYzR0RzNFo0NUFrQ00=\n\n# search result\nsearch: 2\nresult: 0 Success\n\n# numResponses: 3\n# numEntries: 2\nYou should be able to read what got returned pretty seamlessly. One thing to notice is that the user password is returned, so it can be compared to a password provided. It is encrypted, so it doesn’t appear in plain text.\nNote that ldap is a protocol – so it takes the place of the http you’re used to in normal web operations. Like there’s https, there is also a protocol called LDAPS, which is ldap + tls for the same reason you’ve got https. LDAP is (almost) always running in the same private network as the service, so many organizations don’t require using LDAPS, but others do require it.\nRunning the ldapadmin\ndocker run -p 6443:443 \\\n        --name ldap-admin \\\n        --env PHPLDAPADMIN_LDAP_HOSTS=ldap-service \\\n        --network ldap-net \\\n        --detach osixia/phpldapadmin\ndn for admin cn=admin,dc=example,dc=org pw: admin\nhttps://localhost:6443\n# Replace with valid license\nexport RSC_LICENSE=XXXX-XXXX-XXXX-XXXX-XXXX-XXXX-XXXX\n\n# Run without persistent data and using default configuration\ndocker run -it --privileged \\\n    --name rsc \\\n    --volume $PWD/rstudio-connect.gcfg:/etc/rstudio-connect/rstudio-connect.gcfg \\\n    -p 3939:3939 \\\n    -e RSC_LICENSE=$RSC_LICENSE \\\n    --network ldap-net \\\n    rstudio/rstudio-connect:latest\n\n\n13.3.3.4 Single vs Double Bind\nThere are two different ways to establish a connection between your server and the LDAP server. The first method is called Single Bind. In a single bind authentication, the user credentials are used both to authenticate to the LDAP server, and to query the server.\nIn a Double Bind configuration, there is a separate administrative service account, used to authenticate to the LDAP server. Once authentication is complete, then the user is queried in the system.\nSingle bind configurations are often more limited than double bind ones. For example, in most cases you’ll only be able to see the single user as well as the groups they’re a part of. This can limit application functionality in some cases. On the other hand, there need be no master key maintained on your server, and some admins may prefer it for security reasons.\nWe can see this really concretely. In the example above, you used a double bind by supplying admin credentials to LDAP. Let’s say instead, you just provide a single user’s credentials. In that case, I don’t get anything back if I just do a general search.\nldapsearch -x -H ldap://localhost:6389 -b dc=example,dc=org -D \"cn=joe,dc=engineering,dc=example,dc=org\" -w joe                                       \n# extended LDIF\n#\n# LDAPv3\n# base <dc=example,dc=org> with scope subtree\n# filter: (objectclass=*)\n# requesting: ALL\n#\n\n# search result\nsearch: 2\nresult: 32 No such object\n\n# numResponses: 1\nBut just searching for information about Joe does return his own information.\nldapsearch -x -H ldap://localhost:6389 -b cn=joe,dc=engineering,dc=example,dc=org -D \"cn=joe,dc=engineering,dc=example,dc=org\" -w joe                    32 ✘\n# extended LDIF\n#\n# LDAPv3\n# base <cn=joe,dc=engineering,dc=example,dc=org> with scope subtree\n# filter: (objectclass=*)\n# requesting: ALL\n#\n\n# joe, engineering.example.org\ndn: cn=joe,dc=engineering,dc=example,dc=org\ncn: joe\ngidNumber: 500\ngivenName: Joe\nhomeDirectory: /home/joe\nloginShell: /bin/sh\nmail: joe@example.org\nobjectClass: inetOrgPerson\nobjectClass: posixAccount\nobjectClass: top\nsn: Golly\nuid: test\\joe\nuidNumber: 1000\nuserPassword:: e01ENX1qL01raWZrdk0wRm1sTDZQM0MxTUlnPT0=\n\n# search result\nsearch: 2\nresult: 0 Success\n\n# numResponses: 2\n# numEntries: 1\n\n\n\n13.3.4 Kerberos Tickets\nKerberos is a relatively old ticket-based auth technology. In Kerberos, encrypted tickets are passed around between servers. Because these tickets live entirely on servers under the control of the organization, they are generally quite secure.\nThough Kerberos is freely available, it was widely adopted along with Active Directory, and it’s used almost exclusively in places that are running a lot of Microsoft products. A frequent use of Kerberos tickets is to establish database connections.\nBecause the tickets are passed around from server to server, Kerberos can be used to create a true SSO experience for users.\n\n13.3.4.1 How Kerberos Works\nAll of Kerberos works by sending information to and from the central Kerberos Domain Controller (KDC). In Kerberos, authentication and authorization are handled independently.\nWhen a Kerberos session is initialized, the service sends the users credentials off to the KDC and requests something called the Ticket Granting Ticket (TGT) from the KDC. TGTs have a set expiration period. When they expire, the client has to request an updated TGT. This is one reason why Kerberos is considered quite secure - even if someone managed to steal a TGT, they’d only be able to use it for a little while before it went stale and could be revoked.\nWhen the user wants to actually do something, they send the TGT back to the KDC again and get a session key (sometimes referred to as a service ticket) that allows access to the service, usually with a specified expiration period.\n\n\n\n13.3.4.2 Try out Kerberos\n#TODO\n\n\n\n13.3.5 SAML\nThese days Security Assertion Markup Language (SAML) is probably the most common system that provides true SSO – including single login and centrally-managed permissions. SAML does this by passing around XML tokens.4\nThe way this generally works is that a user attempts to login to a Service Provider (SP). The SP redirects the user to an Identity Provider (IdP), which checks either for a preexisting token in the users browser, or verifies the users credentials. The IdP checks for the user’s authorization to access the SP in question, and sends an authorization token back to the SP.\n\nRelative to LDAP/AD, which is from the early 1990s, SAML is a new kid on the block. SAML 1.0 was introduced in 2002, and SAML 2.0, which is the current standard, came out in 2005. Many large enterprises are switching their systems over to use SAML or have already done so.\nOne superpower of SAML IdPs is that many of them can federate identity management to other systems. So, it’s pretty common for large enterprises to maintain their user base in one or more LDAP/AD system, but actually use a SAML IdP to do authentication and authorization. In fact, this is what Azure Active Directory (AAD), which is Microsoft Azure’s hosted authentication offering does. It is possible to use LDAP/AD with AAD, but most organizations use it with SAML.\nOne of the nice things about SAML is that credentials are never shared directly with the SP. This is one of the ways in which SAML is fundamentally more secure than LDAP/AD – the users credentials are only ever shared with the IdP.\nThere are two different ways logins can occur – starting from the SP, or starting from the IdP.\nIn SAML, the XML tokens that are passed back and forth are called assertions.\n\n13.3.5.1 Try SAML\nWe’re going to use a simple SAML IdP to try out SAML a bit. This container only supports a single SP. Any IdP that might be used in an enterprise environment is going to support many SPs simultaneously.\nLet’s go through the environment variables we’re providing to this docker run command. We’re providing three different arguments:\n\nThe SP_ENTITY_ID is the URL of the\nSP_ASSERTION_CONSUMER_SERVICE is the URL of the SP that is prepared to receive the authorized responses coming back from the SAML IdP.\nSP_SINGLE_LOGOUT_SERVICE is the URL where the SP will receive a logout command once someone has been logged out at the IdP level. Many SPs do not implement single logout.\n\ndocker run --name=saml_idp \\\n-p 8080:8080 \\\n-p 8443:8443 \\\n-e SIMPLESAMLPHP_SP_ENTITY_ID=http://app.example.com \\\n-e SIMPLESAMLPHP_SP_ASSERTION_CONSUMER_SERVICE=http://localhost/simplesaml/module.php/saml/sp/saml2-acs.php/test-sp \\\n-e SIMPLESAMLPHP_SP_SINGLE_LOGOUT_SERVICE=http://localhost/simplesaml/module.php/saml/sp/saml2-logout.php/test-sp \\\n-d kristophjunge/test-saml-idp:1.15\nhttp://localhost:8080/simplesaml\nadmin/secret\n\n\n\n13.3.6 OIDC/OAuth2.0\nOIDC/OAuth is slightly newer than SAML, created in 2007 by engineers at Google and Twitter. OAuth 2.0 – the current standard was released in 2012. If you’re being pedantic, OAuth is a authorization protocol, and OpenID Connect (OIDC) is an authorization protocol that uses OAuth. In most cases, people will just call it OAuth.\n\n#TODO: this picture is bad\nIn an enterprise context, OAuth/OIDC is conceptually very similar to SAML – but instead of passing around XML tokens, it’s based on JSON Web Tokens (JWT, usually pronounced “jot”).\n\nThe fact that JSON is much more human-readable than XML is one of the big advantages of OIDC/OAuth compared to SAML.\n\n#TODO: try it out\n\n13.3.6.1 OAuth/OIDC vs SAML\nFrom a practical perspective, the biggest difference between OAuth/OIDC and SAML is that SAML is quite strict about what SPs are allowed. Each SP needs to be registered at a specific web address that the IdP knows it’s allowed to receive requests from.\nIn contrast, OAuth/OIDC was designed to be used to delegate authentication and authorization to different kinds of services that might be widely available on the internet. If you’ve ever allowed a website to Login with Apple/Google/Facebook/Github, that has been an application of OAuth/OIDC.\nBecause the set of allowable SPs is fixed under SAML, it’s more common in enterprise settings. Some admins consider SAML more secure for that reason as well.\nIn some situations, SAML is used for authentication and OAuth is used for access to other services. Most commonly in the data science world, this can come up when a user logs into a service like RStudio Server and is then authorized to a database using an OAuth JWT.\nResources: https://www.okta.com/identity-101/saml-vs-oauth/ https://www.okta.com/identity-101/whats-the-difference-between-oauth-openid-connect-and-saml/ https://phoenixnap.com/blog/kerberos-authentication https://www.dnsstuff.com/rbac-vs-abac-access-control"
  },
  {
    "objectID": "chapters/sec3/3-4-scaling.html",
    "href": "chapters/sec3/3-4-scaling.html",
    "title": "14  Scaling",
    "section": "",
    "text": "Learning Objectives\n\n\n\n\nWhat is scaling in a data science context?\nWhen is scaling necessary?\nWhat are options for scaling, and what are some common pain points?\nAt some point, your data science environment may reach the stage where you need to start thinking about how you’re going to scale your environment out to accommodate more users – or sometimes this will come up in the context of disaster recovery or maintaining service uptime.\nHopefully, if you’re at this level of complexity, you’ve got someone in the IT/Admin organization to help you. This chapter is mostly going to be focused around the conceptual components of how scaling works, and how to talk about them. We’re not going to get deep into the weeds on how to configure them.\nThere are two main types of scaling that get discussed. This is an example where the language definitely impedes understanding rather than furthering it.\nVertical scaling is just a fancy way of saying making a server bigger. So maybe you’re running RStudio Server or JupyterHub on a server of a particular size. Vertically scaling that server just means making the server itself bigger. If you’re running your own server, this is a huge pain. You’ve got to buy and configure a new server, and switch over. The ability to quickly vertically scale hardware is one of the best things about the cloud. Taking a server down, transferring the attached volume to a new server of a different size, and putting it back up takes just a few minutes.\nVertical scaling in the cloud is great – but there are limits. ]For example, the AWS C line of instances instances are their “compute optimized” instances that have fast CPUs and are good for general-purpose data science workloads. Generally, AWS scales their EC2 instances linearly in terms of the number of CPU cores offered on the instance, but across most reasonably-priced instance types, the instance sizes max out at 96-128 cores these days. That’s probably sufficient for many workloads, but if you’ve got an RStudio Server with 50 concurrent users doing reasonably heavy compute loads, that can quickly get eaten up.\nHorizontal scaling means distributing the workload across multiple servers or machines. It is almost always more complicated than it seems like it should be, and more complicated than you want it to be. Horizontal scaling is often referred to as load balancing.\nTODO: Image of vertical + horizontal scaling\nSometimes horizontal scaling is undertaken for pure scaling purposes, but sometimes it’s undertaken for cluster resilience purposes. For example, you might want the cluster to be resilient to a node randomly failing, or being taken down for maintenance. In this context, horizontal scaling is often called high availability. Like many other things, high availability is a squishy term, and different organizations have very different definitions of what it means.\nFor example, in one organization, high availability might just mean that there’s a robust disaster recovery plan so servers can be brought back online with little data loss. In another organization, high availability might mean having duplicate servers that aren’t physically colocated to avoid potential outages due to server issues or natural disasters. In other contexts, it might be a commitment to a particular amount of uptime.1\nSpannning Multiple AZs\nAs the requirements for high availability get steeper, the engineering cost to make sure the service really is that resilient rise exponentially…so be careful how much uptime you’re trying to achieve."
  },
  {
    "objectID": "chapters/sec3/3-4-scaling.html#k8s",
    "href": "chapters/sec3/3-4-scaling.html#k8s",
    "title": "14  Scaling",
    "section": "14.1 Container Deployment + Orchestration",
    "text": "14.1 Container Deployment + Orchestration\nOne tool that comes up increasingly frequently when talking about scaling is Kubernetes (sometimes abbreviated as K8S).4 Kubernetes is the way people orchestrate Docker containers in production settings.5 So basically that it’s the way to put containers into production when you want more than one to interact – say you’ve got an app that separately has a database and a front end in different containers, or, like in this chapter, multiple load-balanced instances of the same containers.\nWhile the operational details of Kubernetes are very different from the horizontal scaling patterns we’ve discussed so far in this chapter, the conceptual requirements are the same.\nTODO: Diagram of K8S\nMany people like Kubernetes because of its declarative nature. If you recall from the section on Infrastructure as Code, declarative code allows you to make a statement about what the thing is you want and just get it, instead of specifying the details of how to get there.\nOf course, in operation this all can get much more complicated, but once you’ve got the right containers, Kubernetes makes it easy to say, “Ok, I want one instance of my load balancer container connected to three instances of my compute container with the same volume connected to all three.”\n\n\n\n\n\n\nKubernetes Tripwire!\n\n\n\nIf you’re reading this and are extremely excited about Kubernetes – that’s great! Kubernetes does make a lot of things easy that used to be hard. Just know, networking configuration is the place you’re likely to get tripped up. You’ve got to deal with networking into the cluster, networking among the containers inside the cluster, and then networking within each container.\nComplicated kubernetes networking configurations are not for the faint of heart.\n\n\nFor individual data scientists, Kubernetes is usually overkill for the type of work you’re doing. If you find yourself in this territory, it’s likely you should try to work with you organization’s IT/Admin group.\nOne of the nice abstraction layers Kubernetes provides is that in Kubernetes, you provide declarative statements of the containers you want to run, and any requirements you have. You separately register actual hardware with the cluster, and Kubernetes takes care of placing the conatiners onto the hardware depending on what you’ve got available.\nIn practice, unless you’re part of a very sophisticated IT organization, you’ll almost certainly use Kubernetes via one of the cloud providers’ Kubernetes clusters as a service. AWS’s is called Elastic Kubernetes Service (EKS).6\nOne really nice thing about using these Kubernetes clusters as a service is that adding more compute power to your cluster is generally as easy as a few button clicks. On the other hand, that also makes it dangerous from a cost perspective.\nIt is possible to define a Kubernetes cluster “on the fly” and deploy things to a cluster in an ad hoc way. I wouldn’t recommend this for any production system. Helm is the standard tool for defining kubernetes deployments in code, and Helmfile is a templating system for Helm.\nSo, for example, if you had a standard “Shiny Server” that was one load balancer containers, two containers each running a Shiny app, and a volume mounted to both, you would define that cluster in Helm. If you wanted to be able to template that Helm code for different clusters, you’d use Helmfile."
  },
  {
    "objectID": "chapters/sec3/3-4-scaling.html#exercises",
    "href": "chapters/sec3/3-4-scaling.html#exercises",
    "title": "14  Scaling",
    "section": "14.2 Exercises",
    "text": "14.2 Exercises\nTODO"
  },
  {
    "objectID": "chapters/append/docker-cheatsheet.html",
    "href": "chapters/append/docker-cheatsheet.html",
    "title": "Appendix A — Docker Cheatsheet",
    "section": "",
    "text": "Command\nPurpose\nExample\n\n\n\n\ndocker run\nRun an image as a container\ndocker run me/my-image\n\n\ndocker ps\nList all containers\ndocker ps\n\n\ndocker kill\nKill a container\ndocker kill my-container\n\n\ndocker exec\nRun a command inside a running container\ndocker exec -it /bin/bash\n\n\ndocker build\nBuild a Dockerfile\ndocker built -t me/my-image .\n\n\ndocker logs\nGet logs from a container\ndocker logs my-container\n\n\ndocker pull\nPull a container from a registry\ndocker pull me/my-image\n\n\ndocker push\nPush a container to a registry\ndocker push me/my-image"
  },
  {
    "objectID": "chapters/append/docker-cheatsheet.html#docker-run-command-flags",
    "href": "chapters/append/docker-cheatsheet.html#docker-run-command-flags",
    "title": "Appendix A — Docker Cheatsheet",
    "section": "A.2 docker run command flags",
    "text": "A.2 docker run command flags\n\n\n\n\n\n\n\n\nFlag\nPurpose\nExample\n\n\n\n\n-d\nRun in “detached” mode that doesn’t block your terminal\ndocker run -d ...\n\n\n--rm\nRemove the container on stop\nReminder: don’t use in prod\ndocker run --rm …\n\n\n-p\nPublish ports from container to host\ndocker run -p 8000:8000 …\n\n\n-v\nMount a volume into the container\ndocker run -v $(pwd):/data\n\n\n--name\nGive container a human-friendly name\ndocker run --name my-container\n\n\n\nReminder - -p and -v order is <host>:<container>"
  },
  {
    "objectID": "chapters/append/docker-cheatsheet.html#dockerfile-commands",
    "href": "chapters/append/docker-cheatsheet.html#dockerfile-commands",
    "title": "Appendix A — Docker Cheatsheet",
    "section": "A.3 Dockerfile Commands",
    "text": "A.3 Dockerfile Commands\nThese are the commands that go in a Dockerfile when you’re building it.\n\n\n\n\n\n\n\n\nCommand\nPurpose\nExample\n\n\n\n\nFROM\nIndicate base container\nFROM rocker/r-ver:4.1.0\n\n\nRUN\nRun a command when building\nRUN apt-get update\n\n\nCOPY\nCopy from the working directory into the container\nCOPY . /app/\n\n\nCMD\nSpecify the command to run when the container starts\nCMD quarto render ."
  }
]