[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DevOps for Data Science",
    "section": "",
    "text": "Welcome!\nThis is the website for the book DevOps for Data Science, currently in draft form.\nIn this book, you’ll learn about DevOps conventions, tools, and practices that can be useful to you as a data scientist. You’ll also learn how to work better with the IT/Admin team at your organization, and even how to do a little server administration of your own if you’re pressed into service.\nThis website is (and always will be) free to use, and is licensed under the Creative Commons Attribution-NonCommercial-NoDerivs 3.0 license. If you’d like a physical copy of the book, they will be available once it’s finished!"
  },
  {
    "objectID": "index.html#software-information-and-conventions",
    "href": "index.html#software-information-and-conventions",
    "title": "DevOps for Data Science",
    "section": "Software information and conventions",
    "text": "Software information and conventions\nI used the knitr package [@xie2015] and the quarto package [@quarto] to compile my book.\nPackage names are in bold text (e.g., rmarkdown), and inline code and filenames are formatted in a typewriter font (e.g., knitr::knit('foo.Rmd')). Function names are followed by parentheses (e.g., bookdown::render_book())."
  },
  {
    "objectID": "index.html#about-the-author",
    "href": "index.html#about-the-author",
    "title": "DevOps for Data Science",
    "section": "About the Author",
    "text": "About the Author\nAlex Gold leads the Solutions Engineering team at RStudio.\nHe works with customers of RStudio’s professional software to help them deploy, configure, and use RStudio’s professional software and open source tooling in R and Python.\nIn his free time, he enjoys landscaping, handstands, and Tai Chi.\nHe occasionally blogs about data, management, and leadership at alexkgold.space."
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "DevOps for Data Science",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nA lot of people are helping me write this book.\nThis book is published to the web using GitHub Actions from rOpenSci."
  },
  {
    "objectID": "index.html#color-palette",
    "href": "index.html#color-palette",
    "title": "DevOps for Data Science",
    "section": "Color palette",
    "text": "Color palette\nTea Green: CAFFDO Steel Blue: 3E7CB1 Kombu Green: 273c2c Bright Maroon: B33951 Sandy Brown: FCAA67"
  },
  {
    "objectID": "chapters/intro.html#a-short-history-of-devops",
    "href": "chapters/intro.html#a-short-history-of-devops",
    "title": "Introduction",
    "section": "A short history of DevOps",
    "text": "A short history of DevOps\nHere’s the one sentence definition: DevOps is a set of cultural norms, practices, and supporting tooling to help make the process of developing and deploying software smoother and lower risk.\nIf you feel like that definition is pretty vague and unhelpful, you’re right. Like Agile software development, to which it is closely related, DevOps is a squishy concept. That’s partially because DevOps isn’t just one thing – it’s the application of some principles and process ideas to whatever context you’re actually working in. That malleability is one of the great strengths of DevOps, but it also makes the concept quite squishy.\nThis squishiness is furthered by the ecosystem of companies enabling DevOps. There are dozens and dozens of companies proselytizing their own particular flavor of DevOps – one that (curiously) reflects the capabilities of whatever product they’re selling.\nBut underneath the industry hype and the marketing jargon, there are some extremely valuable lessons to take from the field.\nTo understand better, let’s go back to the birth of the field.\nThe Manifesto for Agile Software Development was originally published in 2001. Throughout the 1990s, software developers had begun observing that delivering software in small units, quickly collecting feedback, and iterating was an effective model. After that point, many different frameworks of actual working patterns were developed and popularized.\nHowever, many of these frameworks were really focused on software development. What happened once the software was written?\nHistorically, IT Administrators managed the servers, networking, and workstations needed to deploy, release, and operate that software. So, when an application was complete (or perceived as such), it was hurled over the wall from Development to Operations. They’d figure out the hardware and networking requirements, check that it was performant enough, and get it going in the real world.\nThis pattern is very fragile and subject to many errors, and it quickly became apparent that the Agile process – creating and getting feedback on small, iterative changes to working software – needed a complementary process to get that software deployed and into production.\nDevOps arose as this discipline – a way for software developers and the administrators of operational software to better collaborate on making sure the software being written was making it reliably and quickly into production. It took a little while for the field to be formalized, and the term DevOps came into common usage around 2010."
  },
  {
    "objectID": "chapters/intro.html#those-who-do-devops",
    "href": "chapters/intro.html#those-who-do-devops",
    "title": "Introduction",
    "section": "Those who do DevOps",
    "text": "Those who do DevOps\nThroughout this book, I’ll use two different terms – DevOps and IT/Admin – and though they may sound similar, I mean very different things by them.\nDevOps refers to the knowledge, practices, and tools that make it easier, safer, and faster to put work into production. So, if you’re a software developer (and as a data scientist, you are) you need to be thinking about DevOps.\nMost organizations also have a set of people and roles who have the permission and responsibility for managing the servers and computers at your organization. I’m going to refer to this group as IT/Admins. Their names vary widely by organization – they might be named Information Technology (IT), SysAdmin, Site Reliability Engineering (SRE), or DevOps.1\nDepending on what you’re trying to accomplish, the relevant IT/Admins may change. For example, if you’re trying to get access to a particular database, the relevant IT/Admins may be a completely different group of people than if you’re trying to procure a new server.\nFundamentally, DevOps is about creating good patterns for people to collaborate on developing and deploying software. As a data scientist, you’re on the Dev side of the house, and so a huge part of making DevOps work at your organization is about finding some Ops counterparts with whom you can develop a successful collaboration. There are many different organizational structures that support collaboration between data scientists and IT/Admins.\nHowever, I will point out three patterns that are almost always red flags – mostly because they make it hard to develop relationships that can sustain the kind of collaboration DevOps neccesitates. If you find yourself in these situations, you’re not doomed – you can still get things done. But progress is likely to be slow.\n\nAt some very large organizations, IT/Admin functions are split into small atomic units like security, databases, networking, storage, procurement, cloud, and more. This is useful for keeping the scope-of-work manageable for the people in that group – and often results in super deep expertise within the group – but also means that you’ll need to bring people together from disparate teams to actually get anything done. And even when you find the person who can help you with one task, they’re probably not the right person to help you with anything else. They may not even know who is.\nSome organizations have chosen to outsource their IT/Admin functions. This isn’t a problem per-se – the people who work for outsourced IT/Admin companies are often very competent, but it does indicate a lack of commitment at your organization. The main issues in this case tend to be logistical. Outsourced IT/Admin teams are often in India, so it can be hard to find meeting times with American and European teams. Additionally, turnover on projects and systems tends to be very high at outsourced IT/Admin organizations. That means it can be really hard to find anyone who’s an expert on a particular system – or to be able to go back to them once you’ve found them.\nAt some very small organizations, there isn’t yet an IT/Admin function. And at others, the IT/Admins are preoccupied with other tasks and don’t have the capacity to help the data science team. This isn’t a tragedy, but it probably means you’re about to become your own IT/Admin. If your organization is pretty happy to let you do what you want, you’ve picked up this book, so you’re in the right place. If you’re going to have to fight for money to get servers, that’s an uphill battle.\n\nWhether your organization has an IT/Admin setup that facilitates DevOps best practices or not, hopefully this book can help you take the first steps towards making your path to production smoother and simpler.\nTODO: Ways to ameliorate red flags"
  },
  {
    "objectID": "chapters/intro.html#whats-in-this-book",
    "href": "chapters/intro.html#whats-in-this-book",
    "title": "Introduction",
    "section": "What’s in this book?",
    "text": "What’s in this book?\nMy hope for this book is twofold.\nFirst, I’d like to share some patterns.\nDevOps is a well-developed field in its own right. However, a simple 1-1 transposition of DevOps practices from traditional software to data science would be a mistake. Over the course of engaging with so many organizations at RStudio, I’ve observed some particular patterns, borrowed from traditional DevOps, that work particularly well to grease the path to production for data scientists.\nHopefully, by the time you’re done with this book, you’ll have a pretty good mental model of some patterns and principles you can apply in your own work to make deployment more reliable. That’s what’s in the first section of this book.\nSecond, I want to equip you with some technical knowledge.\nIT administration is an older field than DevOps or data science, full of arcane language and technologies. My hope in this book is to equip you with the vocabulary to talk to the IT/Admins at your organization and the (beginning of) skills you’ll need if it turns out that you need to DIY a lot of what you’re doing.\nThe second section is the hands-on section for anyone who’s administering a data science server for themselves. In this section, we’re going to DIY a data science environment. If you have to, this could be an environment you actually work in. If not, this section will help you learn the language and tools to better collaborate with the IT/Admins at your organization.\nThe final section is about taking the DIY environment we’ll set up in section 2 and making it enterprise-grade. Hopefully, if you have enterprise requirements, you also have enterprise IT support, so this section will be more focused on the conceptual knowledge and terminology you’ll need to productively interact with the IT/Admins who are (hopefully) responsible for helping you. And if you don’t have IT/Admins to help you, it will at least give you some terms to google.\n\nQuestions, Portfolio Exercises, and Labs\nFor each chapter, I’ll provide a chance for you to check if you’ve understood the content. Sections 1 and 3 are mostly informational, so in those sections, I’ll provide comprehension questions at the end of each chapter.\n\n\n\n\n\n\nMental Models + Mental Maps\n\n\n\nThroughout the book, I’ll talk a lot about building a mental model. What a mental model is is reasonably intuitive, but just to make clear – a mental model is a model in your head of various entities and how they relate. Many of the chapters in this book will be about helping you build mental models of various DevOps concepts and how they relate to Data Science.\nA mental map is a particularly helpful way to represent mental models. Because I believe generating mental maps is a great test of mental models, I’ll suggest them as exercises in a number of places.\nIn a mental map, you’ll draw out each of the entities I suggest and connect them with arrows. Each node will be a noun, and you’ll write the relationship between entities in the arrows.\nHere’s an example about this book:\n\n\n\n\ngraph LR\n    A[I] -->|Wrote| B[DO4DS]\n    C[You] --> |Are Reading| B\n    B --> |Includes| D[Exercises]\n    D --> |Some Are| E[Mind Maps]\n\n\n\n\n\n\n\n\nNote how every node is a noun, and the edges (labels on the arrows) are verbs. It’s pretty simple! But writing down the relationships between entities like this is a great check on understanding.\n\n\nIn many of the chapters, I’ll include a Portfolio Exercise. These are exercises that you can complete to demonstrate that you’ve really understood the content of the chapter. If you’re looking for a job and you complete these portfolio exercises, you’ll have a great bit of work that shows that you know how to get data science into production.\nSection 2 is all about creating your own data science workbench, so each of the chapters will include a lab – a walkthrough of setting up your data science workbench. By the end of the chapter, you’ll have a place where you can do actual data science work in AWS.\n\nChapter List (FOR DEV PURPOSES ONLY, WILL BE REMOVED):\nSection 1: DevOps for DS\n\nCode Promotion\nEnvironments as Code\nProject Components\nLogging and Monitoring\nDocker for Data Science\n\nSection 2: DIY Data Science Workbench\n\n\n\n\n\n\n\n\nNumber\nExplain\nLab\n\n\n\n\n1\nCloud\nAWS Console + Get Instance\n\n\n2\nCommand Line + SSH\nSSH into server\n\n\n3\nLinux SysAdmin\nInstall R, Py, RS, JH\n\n\n4\nNetworking, DNS, SSL\nURL, SSL\n\n\n5\nHow servers work + Choosing the right one\nTake down instance + attach to a bigger one\n\n\n\nSection 3: Steps not Taken\n\nCode Promotion for DevOps, Dev/Test/Prod, Docker\nBetter Networking (Proxies, Bastion)/Offline\nAuth Integrations\nScaling your servers"
  },
  {
    "objectID": "chapters/sec1/1-0-sec-intro.html#the-problems-devops-solves",
    "href": "chapters/sec1/1-0-sec-intro.html#the-problems-devops-solves",
    "title": "DevOps Lessons for Data Science",
    "section": "The Problems DevOps Solves",
    "text": "The Problems DevOps Solves\nDevOps started as an offshoot of the Agile software movement. In particular, Agile’s focus on quick iteration via frequent delivery of small chunks and immediate feedback proved completely incompatible with a pattern where developers completed software and hurled it over an organizational wall to somehow be put into production by an IT/Admin team.\nThere are a few particular problems that DevOps attempted to solve – problems that will probably feel familiar if you’ve ever tried to put a data science asset into production.\nThe first issue DevOps addresses is the “works on my machine” phenomenon. If you’ve ever collaborated on a piece of data science code, you’ve almost certainly gotten an email, instant message, or quick shout that some code that was working great for you is failing now that your colleague is trying to work on it to collaborate.\nThe processes and tooling of DevOps is designed to link application much more closely to environment in order to prevent the “works on my machine” phenomenon from rearing its head.\nThe second problem DevOps addresses is the “breaks on deployment” issue. Perhaps you wrote some code and tested it lovingly on your machine, but didn’t have the chance to test it against a production configuration. Or perhaps you don’t really have patterns around testing code in your organization. Even if you tested thoroughly, you might not know if something breaks when its deployed. DevOps is designed to reduce the risk of deploying code that won’t function as intended the first time it’s deployed.\nDevOps is designed to incorporate ideas about scaling into the genesis of software, helping avoid software that works fine locally, but can’t be deployed for real."
  },
  {
    "objectID": "chapters/sec1/1-0-sec-intro.html#core-principles-and-best-practices-of-devops",
    "href": "chapters/sec1/1-0-sec-intro.html#core-principles-and-best-practices-of-devops",
    "title": "DevOps Lessons for Data Science",
    "section": "Core principles and best practices of DevOps",
    "text": "Core principles and best practices of DevOps\nAs I’ve mentioned, the term DevOps is squishy. So squishy that there isn’t even agreeement on what the basic tenets of DevOps are that help solve the problems its attempting to solve. Basically every resource on DevOps lists a different set of core principles and frameworks.\nAnd the profusion of xOps like DataOps, MLOps, and more just add confusion about what DevOps itself actually is.\nI’m going to name five core tenets of DevOps. Some lists of DevOps have more components, and some fewer, but this is a good-faith attempt to summarize what I believe the core components are.\n\nCode should be well-tested and tests should be automated.\nCode updates should be frequent and low-risk.\nSecurity concerns should be considered up front as part of architecture.\nProduction systems should have monitoring and logging.\nFrequent opportunities for reviewing, changing, and updating process should be built into the system – both culturally and technically.\n\nThese five tenets are a great philosophical stance, they’re about things that should happen, and they seem pretty inarguably good.1 But they’re also kinda vague and it’s not clear how to go from them to a your actual day-to-day work."
  },
  {
    "objectID": "chapters/sec1/1-0-sec-intro.html#applying-devops-to-data-science",
    "href": "chapters/sec1/1-0-sec-intro.html#applying-devops-to-data-science",
    "title": "DevOps Lessons for Data Science",
    "section": "Applying DevOps to data science",
    "text": "Applying DevOps to data science\nHopefully, you’re convinced that the principles of DevOps are relevant to you as a data scientist and you’re excited to learn more!\nHowever, it would be inappropriate to just take the DevOps principles and practices and apply them to data science.\nAs a data scientist, the huge majority of what you’re doing is taking data generated by a business process, deriving some sort of signal from that data flow, and making it available to other people or other software. Fundamentally, data science apps are consumers of data, almost by definition.\nIn contrast, most traditional pieces of software either don’t involve meaningful data flows, or are producers of business data. An online store, software for managing inventory, and electronic health records – these tools all produce data.\nThere’s a major architectural and process implication from this difference – how much freedom you have. Software engineers get to dream up data structures and data flows from scratch, designing them to work optimally for their systems. In contrast, you are stuck with the way the data flows into your system – most likely designed by someone who wasn’t thinking about the needs of data science at all.\n\n\n\n\n\n\nLanguage-specific tooling\n\n\n\nThere’s one other important difference between data science and general purpose software development. As of the writing of this book, a huge majority of data science work is done in just two programming languages, R and Python (and SQL). For that reason, this book on DevOps for Data Science can get much deeper into the particularities of applying DevOps principles to those specific languages than a general purpose book on DevOps ever would.\n\n\nSo while the problems DevOps attempts to solve will probably resonate with most data scientists, and the core principles seem equally applicable, the technical best practices need some translation.\nSo here are four technical best practices from DevOps and their equivalents in the data science world.2\n\nUse CI/CD\nContinuous Integration/Continuous Delivery/Continuous Deployment (CI/CD) is the notion that there should be a central repository of code where changes are merged. Once these changes are merged, the code should be tested, built, and delivered/deployed in an automated way.\nThe data science analog of CI/CD is code promotion and integration processes. This chapter will help you think about how to structure your app or report so that you can feel secure moving an app into production and updating it later. This chapter will also include an introduction to real CI/CD tools, so that you can get started using them in your own work.\n\n\nInfrastructure as Code\nThe underlying infrastructure for development and deployment should be reproducible using code so it can be updated and replaced with minimal fuss or disruption.\nThe data science analog is thinking about managing environments as code. This chapter will help you think about how to create a reproducible and secure project-level data science environment so you can be confident it can be used, secured, and resurrected later (or somewhere else) as need be.\n\n\nMicroservices\nAny large application should be decomposed into smaller services that are as atomic and lightweight as possible. This makes large projects easier to reason about and makes interfaces between components clearer, so changes and updates are safer.\nI believe this technical best practice has the furthest to go to translate to data science, so this chapter is about how to think of your data science project in terms of its components. This chapter will help you think about what the various components of your projects are and how to split them up for painless and simple updating and atomizing.\n\n\nMonitoring and Logging\nApplication metrics and logs are essential for understanding the usage and performance of production services, and should be leveraged as much as possible to have a holistic picture at all times.\nThe fourth chapter in this section is on monitoring and logging, which is – honestly – in its infancy in the data science world, but deserves more love and attention.\n\n\nOther Things\nMost DevOps frameworks also include communication, collaboration, and review practices as part of their framework, as the technical best practices of DevOps exist to support the work of the people who use them. This is obviously equally important in the data science world – it’s what the entire third section is about.\nAnd in the fifth chapter, we’ll learn about Docker – a tool that has become so common in DevOps practices that it deserves some discussion all on its own. In this section, you’ll get a general intro to what Docker is and how it works – as well as a hands-on intro to using Docker yourself."
  },
  {
    "objectID": "chapters/sec1/1-1-code-promotion.html#the-three-environments",
    "href": "chapters/sec1/1-1-code-promotion.html#the-three-environments",
    "title": "1  Code promotion and integration",
    "section": "1.1 The Three Environments",
    "text": "1.1 The Three Environments\nThe best way to ensure that things only get deployed when you mean them to – and sleep securely knowing that it won’t be disturbed – is to have standalone environments for development, testing, and production. Having separate environments and a plan for promoting content from one to the other is the core of workflows that de-risk pushing to production.\nUsually, these environments are referred to as Dev, Test, and Prod.\n\nThe best way to ensure that deployments go smoothly is to make them as minimal and predictable as possible. This requires that the dev, test, and prod environments be very close mirrors of each other. We’ll get into how to accomplish that at the end of this chapter.\n\n1.1.1 Dev for Data Science\nDev is a sandbox where people can install packages and try new things with no risk of breaking anything in production.\nIf you’re a data scientist, you probably have a really good idea what your Dev environment looks like. It’s probably a Jupyter Notebook or an IDE with data science capabilities like RStudio, Spyder, or PyCharm. You use this environment to do exploratory data analysis, try out new types of charts and graphs, and test model performance against new features that you might design. This is really different than what most IT/Admins imagine is happening in Dev.\nFor a pure software engineering project, the Dev environment isn’t about exploration and experimentation – it’s about building. The relationship between data science and software engineering is akin to the difference between archaeology and architecture. Data science is about exploring existing relationships and sharing them with others once they’ve been discovered. The path is often meandering – and it’s usually not clear whether it’s even a possible one when you start. In contrast, pure software engineering is like designing a building. You know from the beginning that you’re designing a building for a particular purpose. You might need a little exploration to ensure you’ve thought through all of the nooks and crannies, or that you’ve chosen the right materials, or that you’re going to stay on budget, but you’ll have a pretty good idea up front whether it’s possible to design the building you want.\nThis means that Dev environments look really different for a data scientist versus a software engineer.\nThe biggest difference is that most IT/Admins are going to think of Dev, Test, and Prod being three identical copies of the same environment. That’s close to what you need as a data scientist, but really it’s more like you need a sandbox, a test environment, and prod. That means that if you’re using a deployment platform like RStudio Connect or Dash Enterprise, you probably don’t need it in your Dev environment, and that you don’t need your development tool in Test or Prod (any changes should go back through the deployment pipeline).\n\n\n1.1.2 Test and Prod\nTest is (unsurprisingly) an environment for testing. Depending on the type of asset you’re developing, the test environment might incorporate testing that you do, testing by outside entities like security, and/or performance testing. Generally, the test environment facilitates User Acceptance Testing (UAT), where you can investigate whether labels and buttons are clear or whether plots and graphs meet the need. Depending on your organization, test might be collapsed with dev, it might be a single environment, or it could be multiple environments for the different types of testing.\nProd is the gold standard environment where things run without any manual human intervention. Usually the only way to get things into prod is through some type of formalized process – sometimes backed by a computer process like a git merge or push from a Continuous Integration/Continuous Deployment (CI/CD) platform (more on that below). One of the most important ways to keep prod stable is that nothing changes in prod other than via a simple promotion from the test environment.\n\n\n1.1.3 Protecting Prod Data\nOne of the biggest risks during the dev and test parts of an assets lifecycle is that you might mess up real data during your work. In a software engineering context, it’s common to use completely fake data or for the app to be the data generation tool.\nIn contrast, data science is all about using and learning from your organization’s actual data. So a dev environment that doesn’t include access to your organization’s real data is going to be completely useless if it doesn’t have real data in it. This is often a difficult thing to convince an IT/Admin of.\nIn many cases, data science assets and reports are read-only. If you’re mostly building visualizations or dashboards that just consume the business data, perhaps clean it for analytics purposes, you can happily accept a read-only connection to your organization’s data.1 In this case, it works just fine to connect to your real data from your Dev and Test environments and create graphs, models, and dashboards based on the real data, testing out new visualizations and model features in a safe sandbox while the existing version of your app or report runs smoothly in prod.\nOn the other hand, if your app or report actually writes data, you’ll have to be a little more clever. In general, you’ll have to figure out how to redirect your apps output into a test data store, or to mock responses from the real services you’re interacting with. The easiest way to do this is by including your output locations as variables inside your code and then setting them at runtime based on an environment variable. See below for an example of how to do this."
  },
  {
    "objectID": "chapters/sec1/1-1-code-promotion.html#taking-data-science-to-production-with-cicd",
    "href": "chapters/sec1/1-1-code-promotion.html#taking-data-science-to-production-with-cicd",
    "title": "1  Code promotion and integration",
    "section": "1.2 Taking data science to production with CI/CD",
    "text": "1.2 Taking data science to production with CI/CD\nThe common term for the mechanics of code promotion is Continuous Integration/Continuous Deployment (CI/CD).\nCI/CD is about moving your data science projects, including code, data, and environment, into successively more formal states of production. While it’s not a requirement to use source control to make this happen, the mechanics of CI/CD is usually tightly linked to source control procedures.\n\n1.2.1 A Rough Intro to Git\nIf you’re not already familiar, I’d suggest spending some time learning git. If you’re just starting, you’re in for a bit of a ride.\nPeople who say git is easy are either lying to look smarter or learned so long ago that they have forgotten how easy it is to mess up your entire workflow at any moment.\nI’m not going to get into the mechanics of git in this book– what it means to add, commit, push, pull, merge, and more. There are lots of great resources out there that I’m not aiming to reproduce.\n\nIf you don’t already know git and want to learn, I’d recommend HappyGitWithR by Jenny Bryan. It’s a great on-ramp to learn git.\nEven if you’re a Python user, the sections on getting started with git, on basic git concepts, and on workflows will be useful since they approach git from a data science perspective.\n\nI will provide a quick git cheatsheet for memory purposes, and I will talk about some git strategies that match well with using git to execute a data science code promotion strategy.\n\n1.2.1.1 Git Cheatsheet\n\n\n\n\n\n\n\nCommand\nWhat it Does\n\n\n\n\ngit clone <remote>\nClone a remote repo – make sure you’re using SSH URL.\n\n\ngit add <files/dir>\nAdd files/dir to staging area.\n\n\ngit commit -m <message>\nCommit your staging area.\n\n\ngit push origin <branch>\nPush to a remote.\n\n\ngit pull origin <branch>\nPull from a remote.\n\n\ngit checkout <branch name>\nCheckout a branch.\n\n\ngit checkout -b <branch name>\nCreate and checkout a branch.\n\n\ngit branch -d <branch name>\nDelete a branch.\n\n\n\nFor production data science assets, I generally recommend long-running dev (or test) and prod branches, with feature branches for developing new things. The way this works is that new features are developed in a feature branch, merged into dev for testing, and then promoted to prod when you’re confident it’s ready.\nFor example, if you had two new plots you were adding to an existing dashboard, your git commit graph might look like this:\n\nCI/CD adds a layer on top of this. CI/CD allows you to integrate functional testing by automatically running those tests whenever you do something in git. These jobs can run when a merge request is made, and are useful for tasks like spellchecking, linting, and running tests.\nFor the purposes of CI/CD, the most interesting jobs are those that do something after there’s a commit or a completed merge, often deploying the relevant asset to its designated location.\nA CI/CD integration using the same git graph as above would have released 3 new test versions of the app and 2 new prod versions. Note that in this case, the second test release revealed a bug, which was fixed and tested in the test version of the app before a prod release was completed.\nIn years past, the two most popular CI/CD tools were called Travis and Jenkins. By all accounts, these tools were somewhat unwieldy and difficult to get set up. More recently, GitHub – the foremost git server – released GitHub Actions (GHA), which is CI/CD tooling directly integrated into GitHub that’s free for public repositories and free up to some limits for private ones.\nIt’s safe to say GHA is eating the world of CI/CD.2\nFor example, if you’re reading this book online, it was deployed to the website you’re currently viewing using GHA. I’m not going to get deep into the guts of GHA, but instead talk generally about the pattern for deploying data science assets, and then go through how I set up this book on GHA.\n\n\n\n1.2.2 Using CI/CD to deploy data science assets\nIn general, using a CI/CD tool to deploy a data science asset is pretty straightforward. The mental model to have is that the CI/CD tool stands up a completely empty server for you, and runs some code on it.\nThat means that if you’re just doing something simple like spellchecking, you can probably just specify to run spellcheck. If you’re doing something more complicated, like rendering an R Markdown document or Jupyter Notebook and then pushing it to a server, you’ll have to take a few extra steps to be sure the right version of R or Python is on the CI/CD server, that your package environment is properly reproduced, and that you have the right code to render your document.\nFeel free to take a look through the code for the GitHub Action for this book. It’s all YAML, so it’s pretty human-readable.\nHere’s what happens every time I make a push to the main branch of the repository for this book:3\n\nCheckout the current main branch of the book.\nUse the r-lib action to install R.\nUse the r-lib action to setup pandoc (a required system library for R Markdown to work).\nGet the cached renv library for the book.\nRender the book.\nPush the book to GitHub Pages, where this website serves from.\n\nYou’ll see that it uses a mixture of pre-defined actions created for general use, pre-defined actions created by people in the R community, and custom R code I insert to restore an renv library and render the book itself."
  },
  {
    "objectID": "chapters/sec1/1-1-code-promotion.html#per-environment-configuration",
    "href": "chapters/sec1/1-1-code-promotion.html#per-environment-configuration",
    "title": "1  Code promotion and integration",
    "section": "1.3 Per-Environment Configuration",
    "text": "1.3 Per-Environment Configuration\nSometimes you want a little more flexibility – for example the option to switch many the environment variables depending on the environment.\nIn R, the standard way to do this is using the config package. There are many options for managing runtime configuration in Python, including a package called config.\nFor example, let’s consider this shiny app. In this app, every time I press the button, the app sends a POST request to an external service indicating that the button has been pressed.\n\nlibrary(shiny)\n\n# UI that's just a button\nui <- fluidPage(\n  actionButton(\"button\", \"Press Me!\")\n)\n\n# Do something on button press\nserver <- function(input, output) {\n  observeEvent(\n    input$button, \n    httr::POST(\n      \"www.my-external-system.com\", \n      body = list(button_pressed = TRUE)\n    )\n  )\n}\n\n# Run the application \nshinyApp(ui = ui, server = server)\n\nWith the URL hardcoded like this, it’s really hard to imagine doing this in a Dev or Test environment.\nHowever, with R’s config package, you can create a config.yml file that looks like this:\n\ndev:\n  url: \"www.test-system.com\"\n  \nprod:\n  url: \"www.my-external-system.com\"\n\nThen you can use an environment variable to the correct config and apply that configuration inside the app.4\n\nlibrary(shiny)\nconfig <- config::get()\n\n# UI that's just a button\nui <- fluidPage(\n  actionButton(\"button\", \"Press Me!\")\n)\n\n# Do something on button press\nserver <- function(input, output) {\n  observeEvent(\n    input$button, \n    httr::POST(\n      config$url, \n      body = list(button_pressed = TRUE)\n    )\n  )\n}\n\n# Run the application \nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "chapters/sec1/1-1-code-promotion.html#creating-and-maintaining-identical-environments",
    "href": "chapters/sec1/1-1-code-promotion.html#creating-and-maintaining-identical-environments",
    "title": "1  Code promotion and integration",
    "section": "1.4 Creating and Maintaining Identical Environments",
    "text": "1.4 Creating and Maintaining Identical Environments\nIn the IT world, there’s a phrase that servers should be cattle, not pets. The idea here is that servers should be unremarkable and that each one should be more-or-less interchangeable. This matters, for example, in making sure your test and prod environments look exactly the same.\nTODO: Notes on using virtual environments + Docker.\nFor example, doing test on a Windows laptop and then going to prod on a Linux server introduces a potential that things that worked in test suddenly don’t when going to prod. For that reason, making all three (or at least test and prod) match as precisely as possible is essential. The need to match these three environments so precisely is one reason for data science workloads moving onto servers.\nA bad pattern then would look like this:\n\nI develop an update to an important Shiny or Dash app in my local environment and then move it onto a server.\nAt that point, the app doesn’t quite work and I make a bunch of manual changes to the environment – say adjusting file paths or adding R or Python packages. Those manual changes end up not really being documented anywhere.\nA week later, when I go to update the app in prod, it breaks on first deploy, because the server state of the test and prod servers drifted out of alignment.\n\nThe main way to combat this kind of state drift is to religiously use state-maintaining infrastructure as code (IaC) tooling. That means that all changes to the state of your servers ends up in your IaC tooling and no “just login and make it work” shenanigans are allowed in prod.\nTODO: Graphic - fixing problems using IaC tooling\nIf something breaks, you reproduce the error in staging, muck around until it works, update your IaC tooling to fix the broken thing, test that the thing is fixed, and then (and only then) push the updated infrastructure into prod directly from your IaC tooling."
  },
  {
    "objectID": "chapters/sec1/1-1-code-promotion.html#comprehension-questions",
    "href": "chapters/sec1/1-1-code-promotion.html#comprehension-questions",
    "title": "1  Code promotion and integration",
    "section": "1.5 Comprehension Questions",
    "text": "1.5 Comprehension Questions\n\nWrite down a mental map of the relationship between the three environments for data science.\nWhat are the options for protecting production data in a dev or test environment?\nWhy is git so important to a good code promotion strategy? Can you have a code promotion strategy without git?\nWhat is the relationship between git and CI/CD? What’s the benefit of using git and CI/CD together?"
  },
  {
    "objectID": "chapters/sec1/1-1-code-promotion.html#portfolio-exercise-blog-automation",
    "href": "chapters/sec1/1-1-code-promotion.html#portfolio-exercise-blog-automation",
    "title": "1  Code promotion and integration",
    "section": "1.6 Portfolio Exercise: Blog Automation",
    "text": "1.6 Portfolio Exercise: Blog Automation\nMany people in the data science community have personal websites or blogs. They’re a great way to show off your portfolio of work! You may even have one. But how is it built?\nMany people build their blog locally and then just push it up to a website.\nFor this challenge, create a personal website if you don’t have one and configure it so that you can push changes to git and have them render to your website.\nHere are a few suggestions on components you might use:\n\nBuild the website itself using quarto. Quarto is a library for scientific and technical publishing that makes it easy to include R or Python code if you want. It’s the tool this book was written with.\nDeploy the website onto GitHub Pages. At least as of this writing, you can deploy a free website for your GitHub account or for any individual project in your account.\n\nBy default, your website will have a github.io URL for now. If you want to change it, I’d suggest checking out Chapter 11 on networking for more information.\n\nUse GitHub Actions to automate what happens when you push to your website. The GitHub repo for this book (akgold/do4ds) may prove useful as a reference."
  },
  {
    "objectID": "chapters/sec1/1-2-env-as-code.html#environments-have-layers",
    "href": "chapters/sec1/1-2-env-as-code.html#environments-have-layers",
    "title": "2  Environments as Code",
    "section": "2.1 Environments have layers",
    "text": "2.1 Environments have layers\nWhen you first start thinking about environments, it can be hard to wrap your head around them. The environment seems like a monolith, and it can be hard to figure out what the different components are.\nI generally think of three layers in data science environments, and these are in order – each layer of the environment is actually built on the ones below. Once you understand the layers of an environment, you can think more clearly about what your actual reproducibility needs are, and which environmental layers you need to target putting into code.\n\n\n\n\n\n\n\nLayer\nContents\n\n\n\n\nPackages\nR + Python Packages\n\n\nSystem\nR + Python Language Versions\nOther System Libraries\nOperating System\n\n\nHardware\nVirtual Hardware\nPhysical Hardware\n\n\n\nNote that your code and your data are not the environment – they are what the environment is for. As you’re thinking about reproducibility, I’d encourage you to think about how they fit inside the environment and how they might be reproduced.2 But we’re not going to address them in this book.\nFor most data scientists, the biggest bang for your buck is getting the package layer right. In a lot of organizations, another team entirely will be responsible for the system and hardware layers, but the package layer is always your responsibility as the data scientist. Moreover, managing that layer isn’t terribly hard, and if you get it right, you’ll solve a huge fraction of the “runs on my machine” issues you’re likely to encounter.\n\n2.1.1 Package environments as code\nA successful package Environment as Code setup has two key attributes:\n\nYour package environment is isolated and cannot be disrupted by other activities on the same machine.\nYour package environment can easily be captured and transported elsewhere.\n\nBefore we get to what a good Environment as Code setup looks like, let’s dive into what bad setups look like.\nIn a lot of cases, data scientists have the habit of starting a project, and, when they need to install packages, they just run an install.packages command in their console or pip install in their terminal. This works fine for a while. But the problem with this is that the default has you installing things into a cache that’s shared among every project on your system.\nWhat happens if you come back to a project after a year and you’ve been installing things into your machine-wide package cache the whole time. It’s very possible you won’t have the right versions and your code will break.\nThe other problem happens when it comes time to share a project with others. It’s not uncommon to see an intro to an R script that looks something like this:\n\n# Check if dplyr installed\nif (!\"dplyr\" %in% row.names(installed.packages())) {\n  # install if not\n  install.packages(\"dplyr\")\n}\nACK! Please don’t do this!\nNumber one, this is very rude. If someone runs your code, you’ll be installing packages willy-nilly into their system. Additionally, because this doesn’t specify a version of the {dplyr} package, it doesn’t even really fix the problem!\n\n\n2.1.2 Step 1: Standalone Package Libraries\nAs a data scientist, you’re probably familiar with installing packages from a repository using the install.packages command in R or pip install or conda install in Python. But do you really understand what’s happening when you type that command?\nLet’s first level-set on what the various states for R or Python packages are. There are three states packages can be in – and we’re going to go back to our data science as cooking analogy.\n\nPackages can be stored in a repository, like CRAN or BioConductor in R or PyPI or Conda in Python. You can think of a package in a repository like food at the grocery store – it’s packaged up and ready to go, but inert. Setting aside groovy bodegas with eat-in areas, you don’t get to eat at the grocery store. You’ve got to buy the food and take it home before you can use it – and you’ve got to install the food before you can use it.\nAnd then your library is your pantry, where you keep a private set of packages, bespoke to you and the food you like to cook – the projects you’re likely to do.\nLoading a package with a library or import command is like taking the food out of the pantry and putting it on the counter so you can actually cook with it.\n\n[Diagram of package states]\nBy default, when you install an R or Python package, that package installs into user space. That means that it installs into a package library that is specific to your user, but is shared among every time that package is used by you on the machine.\nThis isn’t a disastrous situation, but it is a recipe for package incompatibilities down the road.\n[TODO: diagram of user-level vs project-level installs]\nThe most important thing to understand about package libraries is that libraries can only have one version of any given package at a time. So that means that if I have code that relies on version 1.0 of a given package and I install a new version of that package, version 1.0 is gone and I am likely to run into package incompatibility issues.\nIt’s for this reason that you want to have standalone package libraries for each project on your system. Hopefully, you already have good practices around having each project in a standalone directory on your system and making a git repo in that system. Now just make the base directory of that directory a standalone library as well.\n\n\n\n\n\n\nWhat if I have multiple content items?\n\n\n\nIn many data science projects, you’ve got multiple content items within a single project. Maybe you have an ETL script and an API and an app. After a lot of experimenting, my recommendation is to create one git repo for the whole project and have content-level package libraries.\nThis is not a rule. It’s just a suggestion about how I’ve found it works best over time.\n[TODO: Add image]\n\n\n\n2.1.2.1 What’s really happening?\nI happen to think the grocery store metaphor for package management is a useful one, but you might be wondering what the heck is actually happening when you’re using {renv} or {venv}. How does this package magic happen?\nFirst, let’s quickly go over what happens when you install or load a package.\nWhenever you install a package, there are two key settings that R or Python consult: the URL of the repository to install from, and the library to install to. Similarly, when you load an R or Python library, the install checks the library location. In R, the command used is .libPaths(), and in Python it’s sys.path.\nSo you can see that it’s (conceptually) pretty simple to create a standalone package library for any project – when the virtual environment is activated, just make sure that the project-level library is what comes back when checking the library path.\nYou can see it pretty easily in R. If I run .libPaths() before and after activating an {renv} environment, the first entry from the .libPaths() call changes from a user-level library /Users/alexkgold to a project-level library /Users/alexkgold/Documents/do4ds/.\n.libPaths()\n[1] \"/Users/alexkgold/Library/R/x86_64/4.2/library\"                 \n[2] \"/Library/Frameworks/R.framework/Versions/4.2/Resources/library\"\nrenv::activate()\n* Project '~/Documents/do4ds/docker/docker/plumber' loaded. [renv 0.15.5]\n.libPaths()\n[1] \"/Users/alexkgold/Documents/do4ds/docker/docker/plumber/renv/library/R-4.2/x86_64-apple-darwin17.0\"\n[2] \"/Library/Frameworks/R.framework/Versions/4.2/Resources/library\"  \nSimilarly, in Python it looks like this. Note that the “after” version replaces the last line of the sys.path with a project-level library:\n❯ python3 -m site                                       \nsys.path = [\n    '/Users/alexkgold/Documents/python-examples/dash-app',\n    '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python38.zip',\n    '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8',\n    '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/lib-dynload',\n    '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/site-packages',\n]\nUSER_BASE: '/Users/alexkgold/Library/Python/3.8' (doesn't exist)\nUSER_SITE: '/Users/alexkgold/Library/Python/3.8/lib/python/site-packages' (doesn't exist)\nENABLE_USER_SITE: True\n\n❯ source .venv/bin/activate                       \n(.venv)\n\n❯ python3 -m site\nsys.path = [\n    '/Users/alexkgold/Documents/python-examples/dash-app',\n    '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python38.zip',\n    '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8',\n    '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/lib-dynload',\n    '/Users/alexkgold/Documents/python-examples/dash-app/.venv/lib/python3.8/site-packages',\n]\nUSER_BASE: '/Users/alexkgold/Library/Python/3.8' (doesn't exist)\nUSER_SITE: '/Users/alexkgold/Library/Python/3.8/lib/python/site-packages' (doesn't exist)\nENABLE_USER_SITE: False\n(.venv)\n\n\n\n2.1.3 Step 2: Document environment state.\nUsing standalone package libraries for each project ensures that your projects remain undisturbed when you come back to them months or years later and keeps your work reproducible.\nBut it doesn’t solve the sharing problem.\nThat is, you still need some help when it comes time to share an environment with someone else. So how does that work?\n[TODO: image – anatomy of a lockfile]\nBoth R and Python have great utilities that make it easy to capture the current state of a library into a lockfile or requirements.txt and to restore those libraries at a later date or somewhere else.\nIn R, {renv} is the standard on this front. In Python, there are many different options. In the context of production data science, I recommend {virtualenv}/{venv} and related tools.\nNow, when you share your project with someone else, your lockfile or requirements.txt goes along for the ride. Sometimes people are dismayed that their library doesn’t go along as well and that people have to install the packages themselves – but this is by design!\nFirst, the actual package libraries can be very large, so putting just a short lockfile or requirements file into git is definitely preferred. The other reason is that the actual package install can differ from system to system. For example, if you’re working on a Windows laptop and your colleague is on a Mac, an install of {dplyr} 1.0 means that different files are installed – but with exactly the same functionality. You want to respect this, so instead of sending the whole library along for the ride, you just send the specification that dplyr 1.0 is needed.\n\n\n\n\n\n\nA sidebar on Conda\n\n\n\nMany data scientists love Conda for managing their Python environments.\nConda is a great tool for its main purpose – allowing you to create a data science environment on your local laptop, especially when you don’t have root access to your laptop because it’s a work machine that’s locked down by the admins.\nIn the context of a production environment, Conda smashes together the language version, the package management, and (sometimes) the system library management. This has the benefit of being conceptually simple and easy-to-use. But I’ve often seen it go awry in production environments, and I generally recommend people use a tool that’s just for package management, like {venv}, as opposed to an all-in-one tool like Conda.\n\n\nOn a high level, the workflows for these tasks are similar between R and Python. However, there are some meaningful differences in tooling – especially because many computers arrive with a system version of Python installed, while R is only ever installed by a user trying to do data science tasks.\nAt the end of the day, this actually makes it harder to use Python because you do not want to use your system Python for your data science work…but sometimes it accidentally gets into the mix.\n\n\n2.1.4 Step 3: Collaborate or Deploy\nThe nice thing about these tools is that they make collaboration easy. One big problem people run into is that they plan to share their package library with collaborators. Don’t do this!\nThere are two reasons – first, sharing a library is a bad idea because it can change under people with no warning, leaving them confused about why things are breaking. If you’re potentially working on a different operating system from your collaborators, this won’t even work in the first place. And if you’re trying to deploy - probably on a Linux server – and you’re developing on your Windows or Mac laptop, it’s not going to work at all.\nThe other reason is that this collaboration works really nicely over git. Putting large files – like an entire package library – into git is generally a bad idea. In most cases, you’ll want to gitignore the library itself and instead just share the lockfile.\n\n\n\n\n\n\nSharing package caches\n\n\n\nIf you’re on a shared server, you may want to share a package cache across users. This generally isn’t necessary, but can save some space on the server. Both {renv} and venv include settings to allow you to relocate the package cache to a shared location on the server. You’ll need to make sure that all the relevant users have read and write privileges to this location.\n\n\nIf you are collaborating, both Python and R have one-line commands for restoring a library from a lockfile. In R, you should make sure to set your working directory to the project directory – or open the project in RStudio – and run renv::restore().\nIn Python, you’ll want to make sure you activate your project-level virtual environment using venv source ./venv/bin/activate and then install from a requirements file using pip install -r requirements.txt.\n\n\n2.1.5 Reproducing the rest of the stack\nSometimes, just recording the package environment and moving that around is sufficient. In many cases, old versions of R and Python are retained in the environment, and that’s sufficient.\nThere are times where you need to reproduce elements further down the stack. In some highly-regulated industries, you’ll need to go further down the stack because of requirements for numeric reproducibility. Numeric routines in both R and Python call on system-level libraries, often written in C++ for speed. While it’s unlikely that upgrades to these libraries would cause changes to the numeric results you get, it can happen, and it may be worth maintaining parts of the stack.\nIn other cases, your R or Python library might basically just be a wrapper for system libraries. For example, many popular packages for geospatial analysis are just thin language wrappers that call out to the system libraries. In this case, it might be important to be able to maintain a particular version of the underlying system library to ensure that your code runs at all in the future.\nThere are many tools you can use to record and reproduce the R and Python versions you’re using, the system libraries, and the operating system itself.\nThese days, the clear leader of the pack on this front is Docker. It has become an increasingly popular way to create, maintain, and use standalone environments – and for good reason! In Chapter 5 we’ll get deep into the use of Docker in data science. However, it’s worth keeping in mind that if you’re working in the context of a formally-supported IT organization, they may have other tooling they prefer to use to create and maintain environments, and they can be equally valid."
  },
  {
    "objectID": "chapters/sec1/1-2-env-as-code.html#environments-as-code-cheatsheet",
    "href": "chapters/sec1/1-2-env-as-code.html#environments-as-code-cheatsheet",
    "title": "2  Environments as Code",
    "section": "2.2 Environments as Code Cheatsheet",
    "text": "2.2 Environments as Code Cheatsheet\n\n2.2.1 Checking your library + repository status\nTODO\n\n\n2.2.2 Creating and Using a Standalone Project Library\n\n\n\n\n\n\n\n\nStep\nR Command\nPython Command\n\n\n\n\nMake a standalone project directory.\n-\n-\n\n\nMake sure you’ve got {renv}/{venv}.\ninstall.packages(\"renv\")\nIncluded w/ Python 3.5+\n\n\nCreate a standalone library.\nrenv::init()\npython -m venv <dir>\nRecommend: use .venv for <dir>\n\n\nActivate project library.\nrenv::activate()\nHappens automatically if using projects.\nsource <dir>/bin/activate\n\n\nInstall packages as normal.\ninstall.packages(\"<pkg>\")\npython -m pip install <pkg>\n\n\nSnapshot package state.\nrenv::snapshot()\npip freeze > requirements.txt\n\n\nExit project environment.\nLeave R project.\ndeactivate\n\n\n\n\n\n2.2.3 Collaborating on someone else’s project\n\n\n\n\n\n\n\n\nStep\nR Command\nPython Command\n\n\n\n\nDownload project.\n\n\n\n\nMove into project directory.\nsetwd(\"<project-dir>\")\nOr just open R project in RStudio.\ncd <project-dir>\n\n\nCreate project environment.\nrenv::init()\npython -m venv <dir>\nRecommend: use .venv for <dir>\n\n\nEnter project environment.\nHappens automatically.\nsource <dir>/bin/activate\n\n\nRestore packages.\nMay happen automatically or renv::restore()\npip install -r requirements.txt"
  },
  {
    "objectID": "chapters/sec1/1-2-env-as-code.html#comprehension-questions",
    "href": "chapters/sec1/1-2-env-as-code.html#comprehension-questions",
    "title": "2  Environments as Code",
    "section": "2.3 Comprehension Questions",
    "text": "2.3 Comprehension Questions\n\nWhy does difficulty increase as the level of required reproducibility increase for a data science project. In your day-to-day work, what’s the hardest reproducibility challenge?\nDraw a mental map of the relationships between the 7 levels of the reproducibility stack. Pay particular attention to why the higher layers depend on the lower ones.\nWhat are the two key attributes of environments as code? Why do you need both of them? Are there cases where you might only care about one?\nDraw a mental map of the relationships between the following: package repository, package library, package, project-level-library, .libPaths() (R) or sys.path(python), lockfile\nWhy is it a bad idea to share package libraries? What’s the best way to collaborate with a colleague using an environment as code? What are the commands you’ll run in R or Python to save a package environment and restore it later?"
  },
  {
    "objectID": "chapters/sec1/1-2-env-as-code.html#portfolio-exercise-use-renv-or-venv",
    "href": "chapters/sec1/1-2-env-as-code.html#portfolio-exercise-use-renv-or-venv",
    "title": "2  Environments as Code",
    "section": "2.4 Portfolio Exercise: Use {renv} or venv",
    "text": "2.4 Portfolio Exercise: Use {renv} or venv\nCreate a post for your (new?) personal website that includes a plot or other bit of R or Python code.\nCreate a project-specific library for your blog and integrate it into your existing GitHub Actions pipeline so you can build the website as well as the R or Python Content."
  },
  {
    "objectID": "chapters/sec1/1-3-proj-components.html",
    "href": "chapters/sec1/1-3-proj-components.html",
    "title": "3  Data Science Project Architecture",
    "section": "",
    "text": "4 TODO: Separate API/HTTP into separate chapter.\nTODO: add more on resources, query parameters, and anchors"
  },
  {
    "objectID": "chapters/sec1/1-3-proj-components.html#the-three-layer-app",
    "href": "chapters/sec1/1-3-proj-components.html#the-three-layer-app",
    "title": "3  Data Science Project Architecture",
    "section": "3.1 The three-layer app",
    "text": "3.1 The three-layer app\nThe three-layer app architecture is the most common software architecture that exists in the world today.\nA three-layer app consists, unsurprisingly, of three parts:\n\nPresentation/Interaction layer – the layer where users actually interact.\nApplication layer – processes data and does the actual work of the app.\nData layer – the layer that stores data for the app.\n\nIf you’ve heard the terms front-end and back-end, front-end roughly corresponds to the presentation layer, and back-end to application and data layers.\n\n\n\n\n\n\nIs three-layer dead?\n\n\n\nIf you’re looking around on the internet, you may see assertions that the three-layer app is dead and architectures are moving to microservices or other architectures. In my observation, this is overhyped, and many data science projects would do well to move to a three-layer architecture.\n\n\nThe hardest part of building a three-layer app is understanding what goes in each tier and how to draw the boundaries. In this chapter, I’ll introduce two rules of production app creation:\nSeparate your business logic from app logic\nSeparate your data from the app\nThese roughly correspond to the boundaries between the presentation and application layer, and between the application and data layers."
  },
  {
    "objectID": "chapters/sec1/1-3-proj-components.html#separate-business-and-interaction-logic",
    "href": "chapters/sec1/1-3-proj-components.html#separate-business-and-interaction-logic",
    "title": "3  Data Science Project Architecture",
    "section": "3.2 Separate business and interaction logic",
    "text": "3.2 Separate business and interaction logic\nThere are great frameworks for writing apps in both R and Python – there’s Streamlit, Dash, and Shiny in Python and Shiny in R. This advice also applies if you’re creating a report or some kind of static document using a Jupyter Notebook, R Markdown, or Quarto.\nRegardless of the framework you’re using to create your project, it’s important to separate the business and the interaction logic.\nWhat does this mean?\nAll too often, I see monolithic Shiny apps of thousands or tens of thousands of lines of code, with button definitions, UI bits, and user interaction definitions mixed in among the actual work of the app. There are two reasons this doesn’t work particularly well.\nIt’s much easier to read through your app or report code and understand what it’s doing when the app itself is only concerned with displaying UI elements to the user, passing choices and actions to the backend, and then displaying the result back to the user.\nThe application tier is where your business logic should live. The business logic is that actual work that the application does. For a data science app, this is often slicing and dicing data, computing statistics on that data, generating model predictions, and constructing plots.\nSeparating presentation from business layers means that you want to encapsulate the business logic somehow so that you can work on how the business logic works independently from changing the way users might interact with the app. For example, this might mean creating standalone functions to write plots or create statistics.\nIf you’re using Shiny, R Markdown, or Quarto, this will mean passing values to those functions that are no longer reactive and that have been generated from input parameters.\nLet’s start with a counterexample. Here’s a simple app in Shiny for R that visualizes certain data points from the Palmer Penguins data set. This is an example of bad app architecture.\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(palmerpenguins)\nlibrary(shiny)\n\nall_penguins <- c(\"Adelie\", \"Chinstrap\", \"Gentoo\")\n\n# Define UI\nui <- fluidPage(\n  sidebarLayout(\n    sidebarPanel(\n      # Select which species to include\n      selectInput(\n        inputId = \"species\", \n        label = \"Species\", \n        choices = all_penguins, \n        selected = all_penguins,\n        multiple = TRUE\n      )\n    ),\n    # Show a plot of penguin data\n    mainPanel(\n      plotOutput(\"penguinPlot\")\n    )\n  )\n)\n\nserver <- function(input, output) {\n  \n  output$penguinPlot <- renderPlot({\n    # Filter data\n    dat <- palmerpenguins::penguins %>%\n      dplyr::filter(\n        species %in% input$species\n      )\n    \n    # Render Plot\n    dat %>%\n      ggplot(\n        aes(\n          x = flipper_length_mm,\n          y = body_mass_g,\n          color = sex\n        )\n      ) +\n      geom_point()\n  })\n  \n}\n\n# Run the application \nshinyApp(ui = ui, server = server)\n\nThe structure of this Shiny app is bad. Now, it’s not a huge deal, because this is a simple Shiny app that’s pretty easy to parse if you are reasonably comfortable with R and Shiny.\nWhy is this bad? Look at the app’s server block. Because all of the app’s logic is contained inside a single plotRender statement.\nplotRender is a presentation function – it renders plots. But I’ve got logic in there that generates the data set I need and generates the plot. Again, because this app is simple, it’s not a huge deal here. But imagine if this app had several tabs, multiple input dropdowns, a dozen or more plots, and complicated logic dictating how to process the dropdown choices into the plots. It would be a mess!\nInstead, we should separate the presentation logic from the business logic. That is, let’s separate the code for generating the UI, taking the user’s choice of penguin\nThe business logic – what those decisions mean, and the resulting calculations – should, at minimum, be moved into standalone functions.\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(palmerpenguins)\nlibrary(shiny)\n\nall_penguins <- c(\"Adelie\", \"Chinstrap\", \"Gentoo\")\n\n# Define UI\nui <- fluidPage(\n  sidebarLayout(\n    sidebarPanel(\n      # Select which species to include\n      selectInput(\n        inputId = \"species\", \n        label = \"Species\", \n        choices = all_penguins, \n        selected = all_penguins,\n        multiple = TRUE\n      )\n    ),\n    # Show a plot of penguin data\n    mainPanel(\n      plotOutput(\"penguinPlot\")\n    )\n  )\n)\n\nserver <- function(input, output) {\n  \n  # Filter data\n  dat <- reactive(\n    filter_data(input$species)\n    )\n  \n  # Render Plot\n  output$penguinPlot <- renderPlot(\n    make_penguin_plot(dat())\n    )\n}\n\n# Run the application \nshinyApp(ui = ui, server = server)\n\nNow you can see that the app itself had gotten much simpler. The UI hasn’t changed at all, but the server block is now just two lines! And since I used descriptive function names, it’s really easy to understand what happens in each of the places where my app has reactive behavior.\nEither in the same file, or in another file I can source in, I can now include the two functions that include my business logic:\n\n#' Get the penguin data\n#'\n#' @param species character, which penguin species\n#' @return data frame\n#'\n#' @examples\n#' filter_data(\"Adelie\")\nfilter_data <- function(species = c(\"Adelie\", \"Chinstrap\", \"Gentoo\")) {\n  palmerpenguins::penguins %>%\n    dplyr::filter(\n      species %in% !!species\n    )\n}\n\n#' Create a plot of the penguin data\n#'\n#' @param data data frame\n#'\n#' @return ggplot object\n#'\n#' @examples\n#' filter_data(\"Adelie\") |> plot_gen()\nplot_gen <- function(data) {\n  data %>%\n    ggplot(\n      aes(\n        x = flipper_length_mm,\n        y = body_mass_g,\n        color = sex\n      )\n    ) +\n    geom_point()\n}\n\nNote that somewhere along the way, I also added function definitions and comments using ROxygen. This isn’t an accident! Writing standalone functions is a great way to force yourself to be clear about what should happen, and writing examples is the first step towards writing tests for your code.\n\n3.2.1 Consider using an API for long-running processes\nIn the case of a true three-layer app, it is almost always the case that the middle tier will be an application programming interface (API). In a data science app, separating business logic into functions is often sufficient. But if you’ve got a long-running bit of business logic, it’s often helpful to separate it into an API.\nYou can basically think of an API as a “function as a service”. That is, an API is just one or more functions, but instead of being called within the same process that your app is running or your report is processing, it will run in a completely separate process.\nFor example, let’s say you’ve got an app that allows users to feed in input data and then generate a model based on that data. If you generate the model inside the app, the user will have the experience of pressing the button to generate the model and having the app seize up on them while they’re waiting. Moreover, other users of the app will find themselves affected by this behavior.\nIf, instead, the button in the app ships the long-running process to a separate API, it gives you the ability to think about scaling out the presentation layer separate from the business layer.\nLuckily, if you’ve written functions for your app, turning them into an API is trivial.\nLet’s take that first function for getting the appropriate data set and turn it into an API using the plumber library in R. The FastAPI library is a popular Python library for writing APIs.\n\nlibrary(plumber)\n\n#* @apiTitle Penguin Explorer\n#* @apiDescription An API for exploring palmer penguins.\n\n#* Get data set based on parameters\n#* @param species character, which penguin species\n#* @get /data\nfunction(species = c(\"Adelie\", \"Chinstrap\", \"Gentoo\")) {\n  palmerpenguins::penguins %>%\n    dplyr::filter(\n      species %in% !!species\n    )\n}\n\nYou’ll notice that there are no changes to the actual code of the function. The commented lines that provide the function name and arguments are now prefixed by #* rather than #', and there are a few more arguments, including the type of query this function accepts and the path.\nFor more on querying APIs and working with paths, see Chapter 11.\nI’ll also need to change my function in the app somewhat to actually call the API, but it’s pretty easy using a package like httr2 in R or requests in Python.\nThe easiest way to host an R or Python API is using Docker. See Chapter 5 for how you would host this example inside a Docker container, or the documentation of the relevant package for other options."
  },
  {
    "objectID": "chapters/sec1/1-3-proj-components.html#separate-data-from-app",
    "href": "chapters/sec1/1-3-proj-components.html#separate-data-from-app",
    "title": "3  Data Science Project Architecture",
    "section": "3.3 Separate data from app",
    "text": "3.3 Separate data from app\nSimilarly to separating the presentation layer from the application layer, we’ll also want to separate out the data layer. In a data science app, there are two things you’re likely to have in the data layer.\nThe first is one or more rectangular data frames. A lot of data science projects are dashboards based on existing data. You may need to ingest and clean that data before it goes into the app. You should create standalone jobs to do that before it gets to the actual finished business logic.\nThe other thing you’re likely to store is a machine learning model. You’ll probably have a separate process to train the machine learning model and have it ready to go.\nTODO: Image of two strands of ds project – ML model + rect df\n\n3.3.1 Storage Format\nThe first question of how to store the data is the storage format. There are really three distinct options for storage format.\nFlat file storage describes writing the data out into a simple file. The canonical example of a flat file is a csv file. However, there are also other formats that may make data storage smaller because of compression, make reads faster, and/or allow you to save arbitrary objects rather than just rectangular data. In R, the rds format is the generic binary format, while pickle is the generic binary format in python.\nFlat files can be moved around just like any other file on your computer. You can put them on your computer, and share them through tools like dropbox, google drive, scp, or more.\nThe biggest disadvantage of flat file data storage is twofold – and is related to their indivisibility. In order to use a flat file in R or Python, you’ll need to load it into your R or Python session. For small data files, this isn’t a big deal. But if you’ve got a large file, it can take a long time to read, which you may not want to wait for. Also, if your file has to go over a network, that can be a very slow operation. Or you might have to load it into an app at startup. Also, there’s generally no way to version data, or just update part, so, if you’re saving archival versions, they can take up a lot of space very quickly.\nAt the other extreme end from a flat file format is a database. A database is a standalone server with its own storage, memory, and compute. In general, you’ll recall things from a database using some sort of query language. Most databases you’ll interact with in a data science context are designed around storing rectangular data structures and use Structured Query Language (SQL) to get at the data inside.\nThere are other sorts of databases that store other kinds of objects – you may need these depending on the kind of objects you’re working with. Often the IT/Admin group will have standard databases they work with or use, and you can just piggyback on their decisions. Sometimes you’ll also have choices to make about what database to use, which are beyond the scope of this book.\nThe big advantage of a database is that the data is stored and managed by an independent process. This means that accessing data from your app is often a matter of just connecting to the database, as opposed to having to move files around.\nWorking with databases can also be frought. You usually end up in one of two situations. In the first situation, the database isn’t really for the data science team. You can probably get read access, but not write – so you’ll be able to use the database as your source of truth, but you won’t be able to write there for intermediate tables and other things you might need. In the second situation, you have freedom to set up your own database, in which case you’ll have to own it – and that comes with its own set of headaches.\nThere’s a third family of options for data storage that is quickly rising in popularity for medium-sized data. These options allow you to store data in a flat file, but access it in a smarter way than “just load all the data into memory”. SQLite is a classic example on this front that gives you SQL access to what is basically just a flat file. There are also newer entrants into this place that are better from an analytics perspective, like combining Apache Arrow with feather and parquet files and the dask project in Python.\nThese tools can give you the best of both worlds: you get away from the R and Python limitation of having to load all your data into memory, without having to run a separate database server. But you’ll still have to keep track of where the actual files are and make them accessible to your app.\nOne last option is a shared spreadsheet like Google Drive. This can be a good solution because you don’t have to host it anywhere yourself, access is easy to control, and there are simple tools for interacting.\n\n\n3.3.2 Storage Location\nThe second question after what you’re storing is where. If you are using a database, then the answer is easy. The database just lives where it lives, and you’ll need to make sure you have a way to access it – both in terms of network access, as well as making sure you can authenticate into it (more on that below).\nIf you’re not using a database, then you’ll have to decide where to store the data for your app. Most apps that aren’t using a database start off rather naively with the data in the app bundle.\n<TODO: Image of data in app bundle>\nThis works really well during development and is an easy pattern to get started with. Usually this pattern works fine for a while. The problem is that this pattern generally falls apart when it goes to production. Problems start to arise when the data needs updating – and most data needs updating. Usually, you’ll be ready to update the data in the app long before you’re ready to update the app itself.\nAt this point, you’ll be kicking yourself that you now have to update the data inside the app every time you want to make a data update. It’s generally a better idea to have the data live outside the app bundle. Then you can update the data without mucking around with the app itself.\nA few options for this include just putting a flat file (or flat with differential read) into a directory near the app bundle. The pins package is also a great option here."
  },
  {
    "objectID": "chapters/sec1/1-3-proj-components.html#choosing-your-storage-solution",
    "href": "chapters/sec1/1-3-proj-components.html#choosing-your-storage-solution",
    "title": "3  Data Science Project Architecture",
    "section": "3.4 Choosing your storage solution",
    "text": "3.4 Choosing your storage solution\n\n3.4.1 How frequently are the data updated relative to the code?\nMany data apps have different update requirements for different data in the app.\nFor example, imagine you were the data scientist for a wildlife group that needed a dashboard to track the types of animals that had been spotted by a wilderness wildlife camera. You probably have a table that gives parameters for the animals themselves – perhaps things like endangered status, expected frequency, and more. That table probably needs to be updated very infrequently.\nOn the other hand, the day to day counts of the number of animals spotted probably needs to be updated much more frequently.\nIf your data is updated only very infrequently, it might make sense to just bundle it up with the app code and update it on a similar cadence to the app itself.\n<TODO: Picture data in app bundle>\nOn the other hand, the more frequently updated data probably doesn’t make sense to update at the same cadence as the app code. You probably want to access that data in some sort of external location, perhaps on a mounted drive outside the app bundle, in a pin or bucket, or in a database.\nIn my experience, you almost never want to actually bundle data into the app. You almost always want to allow for the app data (“state”) to live outside the app and for the app to read it at runtime. Even data that you think will be updated infrequently, is unlikely to be updated as infrequently as your app code. Animals move on and off the endangered list, ingredient substitutions are made, and hospitals open and close and change their names in memoriam of someone.\nIt’s also worth considering whether your app needs a live data connection to do processing, or whether looking up values in a pre-processed table will suffice. The more complex the logic inside your app, the less likely you’ll be able to anticipate what users need, and the more likely you’ll have to do a live lookup.\n\n\n3.4.2 Is your app read-only, or does it have to write?\nMany data apps are read-only. This is nice. If you’re going to allow your app to write, you’ll need to be careful about permissions, protecting from data loss via SQL injection or other things, and you have to be careful to check data quality.\nIf you want to save the data, you’ll also need a solution for that. There’s no one-size-fits-all answer here as it really depends on the sort of data you’re using. The main thing to keep in mind is that if you’re using a database, you’ll have to make sure you have write permissions."
  },
  {
    "objectID": "chapters/sec1/1-3-proj-components.html#when-does-the-app-fetch-its-data",
    "href": "chapters/sec1/1-3-proj-components.html#when-does-the-app-fetch-its-data",
    "title": "3  Data Science Project Architecture",
    "section": "3.5 When does the app fetch its data?",
    "text": "3.5 When does the app fetch its data?\nDoes the app fetch its data at app open or throughout runtime?\nThe first important question you’ll have to figure out is what the requirements are for the code you’re trying to put into production.\n\n3.5.1 How big are the data in the app?\nWhen I ask this question, people often jump to the size of the raw data they’re using – but that’s often a completely irrelevant metric. You’re starting backwards if you start from the size of the raw data. Instead, you should figure out what’s the size of data you actually need inside the app.\nTo make this a little more concrete, let’s imagine you work for a large retailer and are responsible for creating a dashboard that will allow people to visualize the last week’s worth of sales for a variety of products. With this vague prompt, you could end up needing to load a huge amount of data into your app – or very little at all.\nOne of the most important questions is how much you can cache before someone even opens the app. For example, if you need to provide total weekly sales at the department level, that’s probably just a few data points. And even if you need to go back a long ways, it’s just a few hundred data points.\nBut if you start needing to slice and dice the data in a lot of directions, then the data size starts multiplying, and you may have to include the entire raw data set in the report. For example, if you need to include weekly sales at the department level, then the size of your data is the \\(number of weeks * number of departments\\). If you need to include more dimensions – say you need to add geographies, then your data size multiplies by the number of geographies.\n\n\n3.5.2 What are the performance requirements for the app?\nOne crucial question for your app is how much wait time is acceptable for people wanting to see the app – and when is that waiting ok? For example, if people need to be able to make selections and see the results in realtime, then you probably need a snappy database, or all the data preloaded into memory when they show up.\nFor some apps, you want the data to be snappy throughout runtime, but it’s ok to have a lengthy startup process (perhaps because it can happen before the user actually arrives) and you want to load a lot of data as the app is starting and do much less throughout the app runtime.\n\n\n3.5.3 Creating Performant Database Queries\nIf you are using a database, you’ll want to be careful about how you construct your queries to make sure they perform well. The main way to think about this is whether your queries will be eager or lazy.\nIn an eager app, you’ll pull basically all of the data for the app as it starts up, while a lazy app will pull data only as it is need.\n<TODO: Diagram of eager vs lazy data pulling>\nMaking your app eager is usually much simpler – you just read in all the data at the beginning. This is often a good first cut at writing an app, as you’re not sure exactly what requirements your app has. For relatively small datasets, this is often good enough.\nIf it seems like your app is starting up slowly – or your data’s too big to all pull in, you may want to pull data more lazily.\n\n\n\n\n\n\nTip\n\n\n\nBefore you start converting queries to speed up your app, it’s always worthwhile to profile your app and actually check that the data pulling is the slow step. I’ve often been wrong in my intuitions about what the slow step of the app is.\nThere’s nothing more annoying than spending hours refactoring your app to pull data more lazily only to realize that pulling the data was never the slow step to begin with.\n\n\nIt’s also worth considering how to make your queries perform better, regardless of when they occur in your code. You want to pull the minimum amount of data possible, so making data less granular, pulling in a smaller window of data, or pre-computing summaries is great when possible (though again, it’s worth profiling before you take on a lot of work that might result in minimal performance improvements).\nOnce you’ve decided whether to make your app eager or lazy, you can think about whether to make the query eager or lazy. In most cases, when you’re working with a database, the slowest part of the process is actually pulling the data. That means that it’s generally worth it to be lazy with your query. And if you’re using dplyr from R, being eager vs lazy is simply a matter of where in the chain you put the collect statement.\nSo you’re better off sending a query to the database, letting the database do a bunch of computations, and pulling a small results set back, rather than pulling in a whole data set and doing computations in R.\n\n\n3.5.4 How to connect to databases?\nIn R, there are two answers to how to connect to a database.\nThe first option is to use a direct connector to connect to the database. This connector generally will provide a driver to the DBI package. There are other database alternatives, but they’re pretty rare.\n<TODO: image of direct connection vs through driver>\nAlternatively, you can use an ODBC/JDBC driver to connect to the database. In this case, you’ll use something inside your R or Python session to use a database driver that has nothing to do with R or Python. Many organizations like these because IT/Admins can configure them on behalf of users and can be agnostic about whether users are using them from R, Python, or something else entirely.\nIf you’re in R, the odbc package gives you a way to interface with ODBC drivers. I’m unaware of a general solution for conencting to odbc drivers in Python.\nA DSN is a particular way to configure an ODBC driver. They are nice because it means that the Admin can fill in the connection details ahead of time, and you don’t need to know any details of the connection, other than your username and password.\n<TODO: image of how DSN works>\nIn R, writing a package that creates database connections for users is also a very popular way to provide database connections to the group."
  },
  {
    "objectID": "chapters/sec1/1-3-proj-components.html#how-do-i-do-data-authorization",
    "href": "chapters/sec1/1-3-proj-components.html#how-do-i-do-data-authorization",
    "title": "3  Data Science Project Architecture",
    "section": "3.6 How do I do data authorization?",
    "text": "3.6 How do I do data authorization?\nThis is a question you probably don’t think about much as you’re puttering around inside RStudio or in a Jupyter Notebook. But when you take an app to production, this becomes a crucial question.\nThe best and easiest case here is that everyone who views the app has the same permissions to see the data. In that case, you can just allow the app access to the data, and you can check whether someone is authorized to view the app as a whole, rather than at the data access layer.\nIn some cases, you might need to provide differential data access to different users. Sometimes this can be accomplished in the app itself. For example, if you can identify the user, you can gate access to certain tabs or features of your app. Many popular app hosting options for R and Python data science apps pass the username into the app as an environment variable.\nSometimes you might also have a column in a table that allows you to filter by who’s allowed to view, so you might just be able to filter to allowable rows in your database query.\nSometimes though, you’ll actually have to pass database credentials along to the database, which will do the authorization for you. This is nice, because then all you have to do is pass along the correct credential, but it’s also a pain because you have to somehow get the credential and send it along with the query.\n<TODO: Image of how a kinit/JWT flow work>\nMost commonly, Kerberos tickets or JSON web tokens (JWTs) are used for this task. Usually your options for this depend on the database itself, and the ticket/JWT granting process will likely have to be handled by the database admin.\n\n3.6.1 Securely Managing Credentials\nThe single most important thing you can do to secure your credentials for your outside services is to avoid ever putting credentials in plaintext. The simplest alternative is to do a lookup from environment variables in either R or Python. There are many more secure things you can do, but it’s pretty trivial to put Sys.getenv(\"my_db_password\") into an app rather than actually typing the value. In that case, you’d set the variable in an .Rprofile or .Renviron file.\nSimilarly, in Python, you can get and set environment variables using the os module. os.environ['DB_PASSWORD'] = 'my-pass' and os.getenv('DB_PASSWORD'), os.environ.get('DB_PASSWORD') or os.environ('DB_PASSWORD'). If you want to set environment variables from a file, generally people in Python use thedotenv package along with a .env file.\nYou should not commit these files to git, but should manually move them across environments, so they never appear anywhere centrally accessible.\nIn some organizations, this will still not be perceived as secure enough, because the credentials are not encrypted at rest. Any of the aforementioned files are just plain text files – so if someone unauthorized were to get access to your machine, they’d be able to grab all of the goodies in your .Rprofile and use them themselves.\nSome hosting software, like RStudio Connect, can take care of this problem, as they store your environment variables inside the software in an encrypted fashion and inject them into the R runtime.\nThere are a number of more secure alternatives – but they generally require a little more work.\nThere are packages in both R and Python called keyring that allow you to use the system keyring to securely store environment variables and recall them at runtime. These can be good in a development environment, but run into trouble in a production environment because they generally rely on a user actually inputting a password for the system keyring.\nOne popular alternative is to use credentials pre-loaded on the system to enable using a ticket or token – often a Kerberos token or a JWT. This is generally quite do-able, but often requires some system-level configuration.\n<TODO: image of kerberos>\nYou may need to enable running as particular Linux users if you don’t want to do all of the authentication interactively in the browser. You usually cannot just recycle login tokens, because they are service authorization tokens, not token-granting tokens.1"
  },
  {
    "objectID": "chapters/sec1/1-3-proj-components.html#comprehension-questions",
    "href": "chapters/sec1/1-3-proj-components.html#comprehension-questions",
    "title": "3  Data Science Project Architecture",
    "section": "3.7 Comprehension Questions",
    "text": "3.7 Comprehension Questions\n\nWhat are the layers of a three-layer application architecture? What libraries could you use to implement a three-layer architecture in R or Python?\nWhat is the relationship between an R or Python function and an API?\nWhat are the different options for data storage formats for apps? What are the advantages and disadvantages of each? How does update frequency relate to choosing a data storage format?\nWhen should an app fetch all of the data up front? When is it better for the app to do live data fetches?\nWhat is a good way to create a non-performant database query?"
  },
  {
    "objectID": "chapters/sec1/1-3-proj-components.html#portfolio-exercise-standing-up-a-loosely-coupled-app",
    "href": "chapters/sec1/1-3-proj-components.html#portfolio-exercise-standing-up-a-loosely-coupled-app",
    "title": "3  Data Science Project Architecture",
    "section": "3.8 Portfolio Exercise: Standing Up a Loosely Coupled App",
    "text": "3.8 Portfolio Exercise: Standing Up a Loosely Coupled App\nFind a public data source you like that’s updated on a regular candence. Some popular ones include weather, public transit, and air travel.\nCreate a job that uses R or Python to access the data and create a job to put it into a database (make sure this is compliant with the data use agreement).\nCreate an API in Plumber (R) or Flask or FastAPI (Python) that allows you to query the data – or maybe create a machine learning model that you can run against new data coming in.\nCreate an app that visualizes what’s coming out of the API using Dash or Streamlit (Python) or Shiny (either R or Python)."
  },
  {
    "objectID": "chapters/sec1/1-3-proj-components.html#comprehension-questions-1",
    "href": "chapters/sec1/1-3-proj-components.html#comprehension-questions-1",
    "title": "3  Data Science Project Architecture",
    "section": "5.1 Comprehension Questions",
    "text": "5.1 Comprehension Questions\n\nFor each of the following, are they a concern of TCP/IP or an application layer protocol: routing packets, defining a server API, defining valid IP addresses, specifying maximum request size, defining server error codes.\nCan a server ever initiate an http request?"
  },
  {
    "objectID": "chapters/sec1/1-4-monitor-log.html#observing-data-science",
    "href": "chapters/sec1/1-4-monitor-log.html#observing-data-science",
    "title": "4  Logging and Monitoring",
    "section": "4.1 Observing data science",
    "text": "4.1 Observing data science\nWhen software engineers think about observability, they’re generally thinking about observing the operational qualities of their app. They’re trying to answer questions like are people using it? Is the app operational or are users experiencing errors? Is performance good enough?\nAs data scientists, we likely have another level of logging to think about, which is the level of the actual data science operations, especially when machine learning models are involved. How are our models performing? Is the level of model performance stable, or is it changing over time? Are our data queries still working or has the data collection process found some new exciting way to mess up our ETL pipeline?\nBut there’s good news – you’re probably already pretty used to data science logging.\nA lot of data science work is done in a literate programming format – Jupyter Notebooks, R Markdown Documents, and Quarto Documents. If used right, the beauty of these documents is that they are self-logging on the data science side!\nAnd in my opinion, you should do all your jobs in a literate programming format. Some people really like running jobs as .R or .py files – that’s fine, but then you also need to make sure your job is emitting the right logs and metrics. On the other hand, if you write your code in a literate programming format and keep an archive of a few runs, you’ve got a self-logging job.\nNow it’s worth spending a few minutes thinking about the things you’ll want to make sure you note in a job to make sure its useful as a log. The number one thing I log is the quality of data joins. When these go awry, your downstream data can get really confusing. It’s also often worthwhile to do cross tabulations of recodes, to make sure your recode logic is working like you think it is. And obviously, if you’re doing modeling, you’ll want to log some metrics around model quality.\nBut we don’t get to ignore the software engineering concerns of a good service experience for our users. This makes observability even more important in a data science context than in a pure software engineering one.\nIn the data science context, you’ll need to do more than literate programming. You’ll want to get comfortable with how and what to log from your jobs, apps, and APIs, and how to emit metrics in a way that can be consumed by the kinds of services you want."
  },
  {
    "objectID": "chapters/sec1/1-4-monitor-log.html#working-with-logs-and-metrics",
    "href": "chapters/sec1/1-4-monitor-log.html#working-with-logs-and-metrics",
    "title": "4  Logging and Monitoring",
    "section": "4.2 Working with logs and metrics",
    "text": "4.2 Working with logs and metrics\nEmitting logs can be straightforward. It can be as simple as just putting print statements throughout your code. Now, there are much better ways to do logging, but print statements are a start.\nWhether you need to emit metrics is a more subtle question. In many data science contexts, you aren’t going to do monitoring on a real-time basis, so emitting metrics that can be used by a real-time monitoring system is less important than for other types of software.\nAnd you’re a data scientist – if you’re putting information into your logs, it’s probably not hard to scrape that data into structured metrics data should the need ever arise. That said, the barrier to entry for actually emitting metrics is way lower than you might realize, and most data scientists should probably pay more attention to emitting metrics than they do.\n\n4.2.1 How to write logs\nAs I mentioned, logging can be as simple as a bunch of print statements in your code. However, there are tools that make it way easier and more pleasant to create and use logs than just a bunch of manual print statements.\nPython has a standard logging package that is excellent and you should use that. R does not have a standard logging package, but there are a number of excellent CRAN packages for logging. My personal favorite is log4r.\nLet’s develop a quick mental model of how logging works inside software.\nLet’s say you’ve decided that you should log every time the user presses a particular button. In your code, you’re going to do the following. First you’re going to call a logger. The logger is the runtime object that actually does the logging. When you call the logger, you’re going to specify the log data as well as a log level. In this case, the message would be something like, “Button pressed”. The logger aggregates the data, the level, and anything you’ve already specified should be added to each record – like a timestamp or the user – and turns that into a log record.\nNow, you’ve got to do something with the log record. The logging software does this via a handler (called an appender in the log4r package). There are three general options for what might happen to your log record. The first (and most common) option is to save the log record to a file on disk. This file is saved somewhere where you can access it later. Many logging consumers are very comfortable watching a file somewhere and aggregating lines of the log as they are written to the file.\nYou may additionally – or instead – want your log emitted somewhere on the system. In Linux, there are two common places to emit logs: stdout (standard output) and stderr (standard error). These are the typical places that messages are emitted – normal ones to stdout and errors to stderr.\n\n\n\n\n\n\nEmitting to stdout\n\n\n\nThese days, the most common reason to emit to standard outputs rather than save to a file on disk is because you’re running inside a Docker container. As we’ll get into more in Chapter 5, anything that lives inside a Docker container is ephemeral. This is obviously bad if it’s possible that the things you’re trying to log will be important in the event the Docker container is unexpectedly killed.\nFor running inside a Docker container (including in Kubernetes), a common pattern is to emit logs on stdout and stderr and have a service outside the container that consumes them and writes them somewhere more permanent and/or aggregates them.\n\n\nIf you are emitting logs to file, you may also want to consider how long those logs stay around. Log rotation is the process of storing logs for a period of time and then deleting the old ones. A common log rotation pattern is to have a 24-hour log file. Each day, the system automatically sets up a new log file and deletes the oldest one (30 days is a typical retention period). The Python logging library does log rotation itself. log4r does not, but there is a Linux library called logrotate that you can use in concert with log4r.1\nLastly, it’s possible you want to do something else completely different with your logs. This is most typical when you want logs of a certain level of criticality sent to an outside system immediately. For example, maybe you want to send the log as a slack message or text in the event a production system goes down.\nAs the log record is written, it has to be formatted using a formatter or layout. In most cases, the default log format is in plain text. So if you log “Button pressed” at the info level, your log record might look something like this in the file:\n2022-11-18 21:57:50 INFO Button Pressed\nBut if you’re shipping your logs off to have them consumed by some other service, you might prefer to have a highly structured log file. The most common structured logging format is JSON, though it’s also possible to use something like YAML or XML. If you used JSON logging, the same record might be emitted as:\n{\n  \"time\": \"2022-11-18 21:57:50\",\n  \"level\": \"INFO\", \n  \"data\": \"Button Pressed\"\n}\nMost logging libraries have 5-7 levels of logging. Both the Python logging library and log4r use the five levels below:\n\nCritical: an error so big that the app itself shuts down. For example, if your app cannot run without a connection to an outside service, you might log an inability to connect as a Critical error.\nError: an issue that will make an operation not work, but that won’t bring down your app. In the language of software engineering, you might think of this as a caught exception. An example might be a user submitting invalid input.\nWarn/Warning: an unexpected application issue that isn’t fatal. For example, you might include having to retry doing something or noticing that resource usage is high. If something were to go wrong later, these might be helpful breadcrumbs to look at.\nInfo: something normal happened in the app. These record things like starting and stopping, successfully making database and other connections, and configuration options that are used.\nDebug: a deep record of what the app was doing. The debug log is meant for people who understand the app code to be able to understand what functions ran and with what arguments.\n\n\n4.2.1.1 A Shiny App that logs\nHere’s a silly function. It counts normally up to 5, but returns NaN starting at 5 and then forevermore if you use the output as the new input.\ncounter <- function(n) {\n  # Do some math\n  n <- n %% 5 # Do a modulous for no reason\n  n <- n + n/n # Add n/n (it's always 1, right?)\n  n\n}\nAnd here’s a Shiny app that renders a number and a button you can use to “Count to Infinity!” Unfortunately, it uses our fatally flawed counter function, so it will stop working after 5.\nYou can see in the app that I’ve already implemented some logging. At the beginning, I create a logger using the standard defaults and then log that the app is starting. You can also see that I log every time the button gets pressed.\nlibrary(shiny)\nlibrary(log4r)\n\n# Create log object\nlog <- logger()\n# Log startup\ninfo(log, paste(\"App starting\"))\nn <- reactiveVal(1)\n\nui <- fluidPage(\n  titlePanel(\n    \"Count to Infinity!\"\n  ),\n  mainPanel(\n    actionButton(\n      \"button\", \"Next Number\"\n    ),\n    textOutput(\"number\")\n  )\n)\n\nserver <- function(input, output) {\n  observeEvent(\n    input$button, {\n      info(log, \"Button Pressed\")\n      # Increment value of count reactive\n      n(counter(n(), log))\n    })\n\n  output$number <- renderText(n())\n}\n\n# Run the application\nshinyApp(ui = ui, server = server)\nIf you’re eagle-eyed, you’ll also notice that I’m passing the log object into the counter function. I’ve modified it to do some logging, allowing me to debug log the value of n after I take the modulo and logging an error if n isn’t a number.\ncounter <- function(n, log) {\n  # Do some math\n  n <- n %% 5 # Do a modulous for no reason\n  debug(log, paste(\"n:\", n))\n  n <- n + n/n # Add n/n (it's always 1, right?)\n\n  # Error if it's not a number\n  if (is.nan(n)) error(log, \"n is not a number\")\n\n  n\n}\nSo if I launch this app, here’s what comes out in the logs. Note that by default, it’s just printing to the console. In a production setting, I might want to configure where the log goes in the call to the logger function.\n> runApp()\nINFO  [2022-11-19 12:06:43] App starting\nINFO  [2022-11-19 12:06:46] Button Pressed\nINFO  [2022-11-19 12:06:47] Button Pressed\nINFO  [2022-11-19 12:06:48] Button Pressed\nINFO  [2022-11-19 12:06:50] Button Pressed\nERROR [2022-11-19 12:06:50] n is not a number\nIf this was deployed in production and I got a call that my app was consistently returning NaN, this is pretty helpful. I can see that the app started normally, the button got pressed a few times just fine and then it started returning something that isn’t a number.\nYou’ll notice that my debug printing of n doesn’t appear anywhere though. That’s because it’s standard for logging packages only to print down to the INFO level by default. If I want DEBUG logging, I’ll have to turn that on by changing the logger creator line to read\nlog <- logger(\"DEBUG\")\n\n\n\n\n\n\nNote\n\n\n\nOne nice thing about having a dev environment like we discussed in Chapter 1 is that you can have that environment configured to always do debug logging using a config setting to set the log level.\n\n\nNow when I run my app, I get the following logs:\n> runApp()\nINFO  [2022-11-19 12:18:57] App starting\nINFO  [2022-11-19 12:19:00] Button Pressed\nDEBUG [2022-11-19 12:19:00] n: 2\nINFO  [2022-11-19 12:19:00] Button Pressed\nDEBUG [2022-11-19 12:19:00] n: 3\nINFO  [2022-11-19 12:19:01] Button Pressed\nDEBUG [2022-11-19 12:19:01] n: 4\nINFO  [2022-11-19 12:19:01] Button Pressed\nDEBUG [2022-11-19 12:19:01] n: 0\nERROR [2022-11-19 12:19:01] n is not a number\nAha! Now it might be obvious to me that I’m dividing 0 by 0 in my code and I can go fix it, all without having to actually step through my code line-by-line.\n\n\n\n4.2.2 How to emit metrics\nEmitting metrics is somewhat more complicated than emitting logs, and for many use cases it won’t be necessary. As I mentioned above, in many cases just scraping logs for any metrics you need is good enough. But if you need to do more complex metrics emission and aggregation, you’ve still got great options.\n\n\n\n\n\n\nMetrics for ModelOps\n\n\n\nIt’s worth noting that the most common reasons Data Scientists need metrics is if they’re running ML models in production. As I’m writing, there are a variety of frameworks and tools vying to be the top in this arena.\nEach of the major cloud providers has their own model hosting solution that includes monitoring capabilities, and there are many other paid options as well.\nFor open source usage, I am (perhaps unsurprisingly) partial to the vetiver package being developed for both R and python by my colleagues at Posit.\n\n\nThese days, most people use the Prometheus/Grafana stack for open source monitoring. Prometheus is an open source monitoring tool that makes it easy to store metrics data, query that data, and alert based on it. Grafana is an open source dashboarding tool that sits on top of Prometheus to do visualization of the metrics. They are usually used together to do monitoring and visualization of metrics.\nIt’s very easy to monitor the server your data science asset might sit on. Prometheus includes something called a node exporter, which makes it extremely easy to monitor the system resources of your server.\nThe way this works is that your Data Science asset emits metrics in a format that Prometheus recognizes, Prometheus watches the metrics and stores them and alerts if necessary, and you have a Grafana dashboard available for visualizing those metrics as needed.\nPython has an official Prometheus client you can use for emitting metrics from a Python asset, and the openmetrics package in R makes it easy to emit metrics from a Plumber API or (experimentally) a Shiny app.\nQUESTION FOR READERS - do I need to do an example that includes emitting metrics?\n\n\n4.2.3 Consuming Logs and Metrics\nOnce you’ve got your asset instrumented and your logging going, you’ll want to use them. If you’re using a professional product like Posit Connect to host your asset, you will probably get some degree of support for finding and reading relevant logs out of the box.\nIf not, you’ll probably be making good use of the skills we’ll review in Chapter 10 to watch logs and parse them.\nYou also can configure the Prometheus and Grafana stack to do both metrics monitoring and log aggregation. Grafana provides a generous free tier that allows you to use Prometheus and Grafana to do your monitoring without having to set up your own server. You can just set up their service and point your app to it.\nThere’s a great Get Started with Grafana and Prometheus doc on the GrafanaLabs website if you want to actually try it out."
  },
  {
    "objectID": "chapters/sec1/1-4-monitor-log.html#comprehension-questions",
    "href": "chapters/sec1/1-4-monitor-log.html#comprehension-questions",
    "title": "4  Logging and Monitoring",
    "section": "4.3 Comprehension Questions",
    "text": "4.3 Comprehension Questions\n\nWhat is the difference between monitoring and logging? What are the two halves of the monitoring and logging process?\nIn general, logging is good, but what are some things you should be careful not to log?\nAt what level would you log each of the following events:\n\nSomeone clicks on a particular tab in your Shiny app.\nSomeone puts an invalid entry into a text entry box.\nAn http call your app makes to an external API fails.\nThe numeric values that are going into your computational function."
  },
  {
    "objectID": "chapters/sec1/1-4-monitor-log.html#portfolio-exercise-adding-logging",
    "href": "chapters/sec1/1-4-monitor-log.html#portfolio-exercise-adding-logging",
    "title": "4  Logging and Monitoring",
    "section": "4.4 Portfolio Exercise: Adding Logging",
    "text": "4.4 Portfolio Exercise: Adding Logging\nStart with the app you built for Chapter 3.\nUsing the appropriate package for whichever language you wrote in, add robust logging to both the API and the App you built.\nFor an extra challenge, consider trying to emit Prometheus metrics as well and consume them using Prometheus and Grafana."
  },
  {
    "objectID": "chapters/sec1/1-5-docker.html#containers-are-a-packaging-tool",
    "href": "chapters/sec1/1-5-docker.html#containers-are-a-packaging-tool",
    "title": "5  Docker for Data Science",
    "section": "5.1 Containers are a Packaging Tool",
    "text": "5.1 Containers are a Packaging Tool\nLet’s tell a story that might feel familiar. A collaborator sends you a piece of code. You go to run it on your machine and an error pops up Python 3.7 not found. So you spend an hour on Stack Overflow, figuring out how to install version 3.7 of Python.\nThen you try to run the code again, which creates some maps, and this time get an error System library gdal not found. “Augh!” you cry, “Why is there not a way to include all of these dependencies with the code?!?”\nYou have just discovered one of the primary use cases for a container.\nContainers are a way to package up some code with all of its dependencies, making it easy to run the code later, share it with someone else for collaboration, or put it onto a production server – all while being reasonably confident that you won’t ever have to say, “well, it runs on my machine”.\nDocker is by far the most popular open-source containerization platform. So much so that for most purposes container is a synonym for Docker container.1 In this chapter, containers will exclusively refer to Docker containers.\nIn addition to making it easy to get all of the dependencies with an app, Docker also makes it easy to run a bunch of different isolated apps without having them interfere with each other.\nVirtual machines of various sorts have been around since the 1960s, and are still used for many applications. In contrast to a virtual machine, Docker is much more lightweight. Once a container has been downloaded to your machine, it can start up in less than a second.\nThis is why Docker – not the only, or even the first open source containerization system – was the first to hit the mainstream, as much as any esoteric code-development and deployment tool can be said to “hit the mainstream”.\nThis means that – for the most part – anything that can run in a Docker container in one place can be run on another machine with very minimal configuration.\n\n\n\n\n\n\nNote\n\n\n\nThere are exceptions. Until recently, a huge fraction of laptop CPUs were of a particular architecture called x86.\nApple’s recent M1 and M2 chips run on an ARM64 architecture, which had previously been used almost exclusively for phones and tablets. The details aren’t super important, but the upshot is that getting containers working on Apple silicon may not be trivial.\n\n\nDocker doesn’t completely negate the need for other sorts of IT tooling, because you still have to provision the physical hardware somehow, but it does make everything much more self-contained. And if you’ve already got a laptop, you can easily run Docker containers with just a few commands (we’ll get to that below)."
  },
  {
    "objectID": "chapters/sec1/1-5-docker.html#containers-for-data-science",
    "href": "chapters/sec1/1-5-docker.html#containers-for-data-science",
    "title": "5  Docker for Data Science",
    "section": "5.2 Containers for Data Science",
    "text": "5.2 Containers for Data Science\nIn a data science context, there are two main ways you might use containers – as a way to package a development environment for someone else to use, and as a way to package a finished app for archiving, reproducibility, and production.\n\n\n\n\n\n\nThe Data Science Reproducibility Stack\n\n\n\nA reminder from the reproducibility chapter:\nThe data science reproducibility stack generally includes 6 elements:\n\nCode\nData\nR + Python Packages\nR + Python Versions\nOther System Libraries\nOperating System\n\n\n\nIf you’re running RStudio Server or JupyterHub on a centralized server, Docker can be a great way to maintain that server. In my opinion, maintaining a Docker container is one of the easiest ways to start on an infrastructure-as-code journey.\nWe’re not going to get terribly deep into this use case, as creating the overwhelming majority of the work involved is standard IT/Admin tasks for hosting a server - things like managing networking, authentication and authorization, security, and more.\n\n\n\n\n\n\nNote\n\n\n\nIf you’re thinking about hosting a data science workbench in Docker, you should think carefully about whether you want to deal with standalone containers, or whether you’re really looking for container orchestration using Kubernetes.\n\n\nIn this chapter, I’ll suggest trying to stand up RStudio Server in a container on your desktop, but don’t let the ease fool you. The majority of difficulties with administering a server are the same, even if you put your application stack into a Docker container. Section II of this book will have a lot more on those challenges, and I suggest you check it out if you’re interested.\nInstead, we’re going to stick with talking about how actual data scientists would want to use Docker: to archive and share completed data science assets.\n\nIn this pattern, you’ll put your whole reproducibility stack inside the container itself – perhaps minus your data."
  },
  {
    "objectID": "chapters/sec1/1-5-docker.html#container-gotchas",
    "href": "chapters/sec1/1-5-docker.html#container-gotchas",
    "title": "5  Docker for Data Science",
    "section": "5.3 Container Gotchas",
    "text": "5.3 Container Gotchas\nDocker containers are great for certain purposes, but there are also some tradeoffs that it’s worth being aware of.\nThe first is the tradeoff of Docker’s strength: a container only gets access to the resources it has specifically been allowed to access.\nThis is a great feature for security and process isolation, but it means you may run into some issues with networking and access to system resources, like your files. You’ll have to develop a reasonably good mental model of the relationship of the Docker container to the rest of your machine in order to be able to develop effectively.\nIt’s worth noting that in some environments – especially highly-regulated ones – a Docker container may not be a sufficient level of reproducibility. Differences between machines at the physical hardware level could potentially mean that numeric solutions could differ across machines, even with the same container. You probably know if you’re in this kind of environment and you have to maintain physical machines.\nThere are also several antipatterns that using a container could facilitate.\nThe biggest reproducibility headache for most data scientists is managing R and Python package environments. While you can just install a bunch of packages into a container, save the container state, and move on, this really isn’t a good solution.\nIf you do this, you’ve got the last state of your environment saved, but it’s not really reproducible. If you come back next year and need to add a new package, you’ll have no way to do it without potentially breaking the whole environment.\nThe obvious solution is to write down the steps for creating your Docker container – in a file called a Dockerfile. Here, it’s tempting to create a Dockerfile that looks like:\n...\nRUN /opt/R/4.1.0/bin/R install.packages(c(\"shiny\", \"dplyr\"))\n...\nBut this is also completely non-reproducible. Whenever you rebuild your container, you’ll install the newest versions of Shiny and Dplyr afresh, potentially ruining the reproducibility of your code. For that reason, the best move is to still use R- and Python-specific libraries for capturing package state – like renv and rig in R and virtualenv , conda , and pyenv in Python – rather than relying on Docker for that job. There’s more on those topics in the Chapter 2 on environments."
  },
  {
    "objectID": "chapters/sec1/1-5-docker.html#trying-out-docker",
    "href": "chapters/sec1/1-5-docker.html#trying-out-docker",
    "title": "5  Docker for Data Science",
    "section": "5.4 Trying out Docker",
    "text": "5.4 Trying out Docker\nIf you’ve read this far, you probably have a reasonably good mental model of when you might want to use Docker to encapsulate your data science environment or when you might not. The rest of this chapter will be a hands-on intro to using Docker to run a finished app.\n\n5.4.1 Prerequisites\nIn order to get started with Docker, you’ll need to know a few things. The first is that you’ll have to actually have Docker installed on an environment you can use. The easiest way to do this is to install Docker Desktop on your laptop, but you can also put Docker on a server environment.\n\n\n\n\n\n\nNote\n\n\n\nWe’re going to use Docker from a terminal on your machine. I’ll give you all the commands you’ll need, but you need to know how to find and open a terminal.\nIf that’s new to you, you might want to skip ahead and check out the beginning of Chapter 8 so you can at least open a terminal.\n\n\n\n\n5.4.2 Getting Started\nLet’s get started with an example that demonstrates the power of Docker right off the bat.\nOnce you’ve got Docker Desktop installed and running, type the following in your terminal:\ndocker run --rm -d \\\n  -p 8000:8000 \\\n  --name palmer-plumber \\\n  alexkgold/plumber\nOnce you type in this command, it’ll take a minute to pull, extract, and start the container.\nOnce the container starts up, you’ll see a long sequence of letters and numbers. Now, navigate to http://localhost:8000/__docs__/ in your browser (this URL has to be exact!), and you should see the documentation for an R language API that lets you explore the Palmer Penguins data set\nThat was probably pretty uninspiring. It took a long time to download and get started. In order to show the real power of Docker, let’s now kill the container with\ndocker kill palmer-plumber\nYou can check that the container isn’t running by trying to visit that URL again. You’ll get an error.\nLet’s bring the container back up by running the docker run command above again.\nThis time is should be quick – probably less than a second – now that you’ve got the container downloaded. THIS is the power of Docker.\n\nAs you click around, seeing penguin stats and seeing plots, you might notice that nothing is showing up on the command line…but what if I want logs of what people are doing? Or I need to look at the app code?\nYou can get into the container to poke around using the command\ndocker exec -it palmer-plumber /bin/bash\nOnce you’re in, try cat api/plumber.R to look at the code of the running API.\nWhen you need to get out, you can leave by typing exit.\ndocker exec is a general purpose command for executing a command inside a running container. The overwhelming majority of the time I use it, it’s to get a terminal inside a running container so I can poke around.\nYou can spend a lot of time getting deep into why the command works, but just memorizing (or, more likely, repeatedly googling) docker exec -it <container> /bin/bash will get you pretty far.\n\n\n\n\n\n\nNote\n\n\n\nIf you’re used to running things on servers, you might be in the habit of SSH-ing in, poking around, and fixing things that are broken. This isn’t great for a lot of reasons, but it’s a huge anti-pattern in Docker land.\nContainers are stateless and immutable. This means that anything that happens in the container stays in the container – even when the container goes away. If something goes wrong in your running container, you may need to exec in to poke around, but you should fix it by rebuilding and redeploying your image, not by changing the running container.\n\n\nOne nicety of Docker is that it gives you quick access to the most common reason you’d probably exec into the container – looking at logs.\nAfter you’ve clicked around a little in the API, try running:\ndocker logs palmer-plumber\nWe’re done with this container now. Feel free to kill it before you move on.\n\nGreat! We’ve played around with this container pretty thoroughly.\nBefore we get into how this all works, let’s try one more example.\nGo back into your terminal and navigate to a directory you can play around in (the cd command is your friend here, see Chapter 8 if you’re not familiar). Run the following in your terminal:\ndocker run \\\n-v ${PWD}:/project-out \\\nalexkgold/batch:0.1\nIt’ll take a minute to download – this container is about 600Mb. You may need to grant the container access to a directory on your machine when it runs. This container will take a few moments to run. If you go to the directory in file browser, you should be able to open hello.html in your web browser – it should be a rendered version of a Jupyter Notebook.\nThis notebook is just a very basic visualization, but you can see how it’s nice to be able to render a Jupyter Notebook locally without having to worry about making sure you had any of the dependencies installed. This is good both for running on demand, and also for archival purposes.\n\nNow that we’ve got Docker working for you, let’s take a step back, explain what we just did, and dive deeper into how this can be helpful.\nHopefully these two examples are exciting – in the first, we got an interactive web API running like a server on our laptop in just a few seconds – and without installing any of the packages or even a version of R locally. In the second, we rendered a Jupyter Notebook using the quarto library – again, without worrying about downloading it locally."
  },
  {
    "objectID": "chapters/sec1/1-5-docker.html#container-lifecycle",
    "href": "chapters/sec1/1-5-docker.html#container-lifecycle",
    "title": "5  Docker for Data Science",
    "section": "5.5 Container Lifecycle",
    "text": "5.5 Container Lifecycle\nBefore we dig into the nitty-gritty of how that all worked – and how you might change it for your own purposes, let’s spend just a minute clarifying the lifecycle of a Docker container.\nThis image explains the different states a Docker container can be in, and the commands you’ll need to move them around.\n\nA container starts its life as a Dockerfile. A Dockerfile is a set of instructions for how to build a container. Dockerfiles are usually stored in a git repository, just like any other code, and it’s common to build them on push via a CI/CD pipeline.2\nA working Dockerfile gets built into a Docker image with the build command. Images are immutable snapshots of the state of the container at a given time.\nIt is possible to interactively build a container as you go and snapshot to create an image, but for the purposes of reproducibility, it’s generally preferable to build the image from a Dockerfile, and adjust the Dockerfile if you need to adjust the image.\nUsually, the image is going to be the thing that you share with other people, as it’s the version of the container that’s compiled and ready to go.\nDocker images can be shared directly like any other file, or via sharing on an image registry via the push and pull commands.\nIf you’re familiar with git, the mental model for Docker is quite similar. There is a public Docker Hub you can use, and it’s also possible to run private image registries. Many organizations make use of the image registries as a service offerings from cloud providers. The big 3’s are Amazon’s Elastic Container Registry (ECR), Azure Container Registry, and Google Container Registry.\nOnce you’ve got an image downloaded locally, you can run it with the run command. Note that you generally don’t have to pull before running a container, as it will auto-pull if it’s not available.\nNow that you’re all excited, let’s dig in on how the docker run command works, and the command line flags we used here, which are the ones you’ll use most often."
  },
  {
    "objectID": "chapters/sec1/1-5-docker.html#understanding-docker-run",
    "href": "chapters/sec1/1-5-docker.html#understanding-docker-run",
    "title": "5  Docker for Data Science",
    "section": "5.6 Understanding docker run",
    "text": "5.6 Understanding docker run\nAt it’s most basic, all you need to know is that you can run a Docker image locally using the docker run <name> command. However, Docker commands usually use a lot of command line flags – so many that it’s easy to miss what the command actually is.\n\n\n\n\n\n\nNote\n\n\n\nA command line flag is an argument passed to a command line program.\nThere’s a lot more about using command line tools in Chapter 8.\n\n\nLet’s pull apart the two commands we just used, which use the command line flags you’re most likely to need.\n\n5.6.1 Parsing container names\nTo start with, let’s parse the name of the container. In this example, you used two different container names – alexkgold/plumber and alexkgold/batch:0.1. All containers have an id, and they may also have a tag. If you’re using the public DockerHub registry, like I am, container ids are of the form <user>/<name>. This should look very familiar if you already use a git repository.\nIn addition to an id, containers can also have a tag. For example, for the alexkgold/batch image, we specified a version: 0.1. If you don’t specify a tag when pulling or pushing an image, you’ll automatically create or get latest – the newest version of a container that was pushed to the registry.\nUsers often create tags that are relevant to the container – often versions of the software contained within. For example, the rocker/r-ver container, which is a container pre-built with a version of R in it uses tags for the version of R.\nAll these examples use the public DockerHub. Many organizations use a private image registry, in which case you can prefix the container name with the URL of the registry.\n\n\n5.6.2 docker run flags\nIn this section we’re going to go through the docker run flags we used in quite a bit of detail.\n\n\n\n\n\n\nNote\n\n\n\nIf you just want a quick reference later, there’s a cheatsheet in [Appendix @docker-cheat].\n\n\nLet’s first look at how we ran the container with the plumber API in it.\nFor this container, we used the --rm flag, the -d flag, the -p flag with the argument 8000:8000, and the --name flag with the argument plumber-palmer.\nThe --rm flag removes the container after it finishes running. This is nice when you’re just playing around with a container locally because then you can use the same container name repeatedly, but it’s a flag you’ll almost never use in production because it removes everything from the container, including logs.\nYou can check this by running docker kill palmer-plumber to make sure the container is down and then try to get to the logs with docker logs palmer-plumber. But they don’t exist because they got cleaned up!\nFeel free to try running to container without the --rm flag, playing around, killing the container, and then looking at the logs. Before you’re able to bring back another container with the same name, you’ll have to remove the container with docker rm palmer-plumber.\nThe -d flag instructs the container to run in detached mode so the container won’t block the terminal session. You can feel free to run the container attached – but you’ll have to quit the container by aborting the command from inside the terminal (Ctrl + c), or opening another terminal to docker kill the container.\nThe -p flag publishes a port from inside the container to the host machine. So by specifying -p 8000:8000, we’re taking whatever’s available on the port 8000 inside the container and making it available at the same port on the localhost of the machine that’s hosting the container.\nTODO: picture of ports\nPort forwarding is always specified as <host port>:<container port>. Try playing around with changing the values to make the API available on a different port, perhaps 9876. For a more in-depth treatment of ports, see Chapter 11.\nThe --name flag gives our container a name. This is really just a convenience so that you could do commands like docker kill in terms of the container name, rather than the container ID, which will be different for each person who runs the command.\nIn a lot of cases, you won’t bother with a name for the container.\nYou can find container ID using the docker ps command to get the process status. In the case below, I could control the container with the name palmer-plumber, or with the container ID. You can abbreviate container IDs as long as they’re unique – I tend to use the first three characters.\n❯ docker ps                                                         [12:23:13]\n\nCONTAINER ID   IMAGE               COMMAND                  CREATED          STATUS          PORTS                    NAMES\n\n35bd54e44015   alexkgold/plumber   \"R -e 'pr <- plumber…\"   29 seconds ago   Up 28 seconds   0.0.0.0:8000->8000/tcp   palmer-plumber\n\nNow let’s head over to the batch document rendering, where we only used one command line flag -v ${PWD}:/project-out, short for volume. To demonstrate what this argument does, navigate to a new directory on your command line and re-run the container without the argument.\nWait…where’d my document go?\nRemember – containers are completely ephemeral. What happens in the container stays in the container. This means that when my document is rendered inside the container, it gets deleted when the container ends its job.\nBut that’s not what I wanted – I wanted to get the output back out of the container.\nThe solution – making data outside the container available to the container and vice-versa – is accomplished by mounting a volume into the container using the -v flag. Like with mounting a port, the syntax is -v <directory outside container>:<directory inside container>.\n\nThis is an essential concept to understand when working with containers. Because containers are so ephemeral, volumes are the way to get anything from your host machine in, and to persist anything that you want to outlast the lifecycle of the container.\nIn this case, we actually used a variable ${PWD}, which will be evaluated to the current working directory to be the directory project-out inside the container, so the rendered document can be persisted after the container goes away."
  },
  {
    "objectID": "chapters/sec1/1-5-docker.html#build-your-own-with-dockerfiles",
    "href": "chapters/sec1/1-5-docker.html#build-your-own-with-dockerfiles",
    "title": "5  Docker for Data Science",
    "section": "5.7 Build your own with Dockerfiles",
    "text": "5.7 Build your own with Dockerfiles\nSo far, we’ve just been running containers based on images I’ve already prepared for you. Let’s look at how those images were created so you can try building your own.\nA Dockerfile is just a set of instructions that you use to build a Docker image. If you have a pretty good idea how to accomplish something on a running machine, you shouldn’t have too much trouble building a Dockerfile to do the same, as long as you remember two things:\nTODO: Image of build vs run time\n\nThe difference between build time and run time. There are things that should happen at build time – like setting up the versions of R and Python, copying in the code you’ll run, and installing the system requirements. That’s very different from the thing I want to have happen at run time – rendering the notebook or running the API.\nDocker containers only have access to exactly the resources you provide to them at both build and runtime. That means that they won’t have access to libraries or programs unless you give them access, and you also won’t have access to files from your computer unless you make them available.\n\nThere are many different commands you can use inside a Dockerfile, but with just a handful, you’ll be able to build most images you might need.\nHere are the important commands you’ll need for getting everything you need into your images.\n\nFROM – every container starts from a base image. In some cases, like in my Jupyter example, you might start with a bare bones container that’s just the operating system (ubuntu:20.04). In other cases, like in my shiny example, you might start with a container that’s almost there, and all you need to do is to copy in a file or two.\nRUN – run any command as if you were sitting at the command line inside the container. Just remember, if you’re starting from a very basic container, you may need to make a command available before you can run it (like wget in my container below).\nCOPY – copy a file from the host filesystem into the container. Note that the working directory for your Dockerfile will be whatever your working directory is when you run your build command.\n\nOne really nice thing about Docker containers is that they’re built in layers. Each command in the Dockerfile defines a new layer. If you make changes below a given layer in your Dockerfile, rebuilding will be easy, because Docker will only start rebuilding at the layer with changes.\nIf you’re mainly building containers for finished data science assets to be re-run on demand, there’s only one command you need:\n\nCMD - Specifies what command to run inside the container’s shell at runtime. This would be the same command you’d use to run your project from the command line.\n\nIf you do much digging, you’ll probably run into the ENTRYPOINT command, which can take a while to tell apart from CMD. If you’re building containers to run finished data science assets, you shouldn’t need ENTRYPOINT. If you’re building containers to – for example – accept a different asset to run or allow for particular arguments, you’ll need to use ENTRYPOINT to specify the command that will always run and CMD to specify the default arguments to ENTRYPOINT, which can be overridden on the command line.3\nHere’s the Dockerfile I used to build the container for the Jupyter Notebook rendering. Look through it. Can you understand what it’s doing?\n# syntax=docker/dockerfile:1\nFROM ubuntu:20.04\n\n# Copy external files\nRUN mkdir -p /project/out/\n\nCOPY ./requirements.txt /project/\nCOPY ./hello.ipynb /project/\n\n# Install system packages\nRUN apt-get update && apt-get install -y \\\n  wget python3 python3-pip\n\n# Install quarto CLI + clean up\nRUN wget https://github.com/quarto-dev/quarto-cli/releases/download/v0.9.83/quarto-0.9.83-linux-amd64.deb\nRUN dpkg -i ./quarto-0.9.83-linux-amd64.deb\nRUN rm -f ./quarto-0.9.83-linux-amd64.deb\n\n# Install Python requirements\nRUN pip3 install -r /project/requirements.txt\n\n# Render notebook\nCMD cd /project && \\\n  quarto render ./hello.ipynb && \\\n  # Move output to correct directory\n  # Needed because quarto requires relative paths in --output-dir: \n  # https://github.com/quarto-dev/quarto-cli/issues/362\n  rm -rf /project-out/hello_files/ && \\\n  mkdir -p /project-out/hello_files && \\\n  mv ./hello_files/* /project-out/hello_files/ && \\\n  mv ./hello.html /project-out/\nOnce you’ve created your Dockerfile, you build it into an image using docker build -t <image name>. You can then push that to DockerHub or another registry using docker push."
  },
  {
    "objectID": "chapters/sec1/1-5-docker.html#comprehension-questions",
    "href": "chapters/sec1/1-5-docker.html#comprehension-questions",
    "title": "5  Docker for Data Science",
    "section": "5.8 Comprehension Questions",
    "text": "5.8 Comprehension Questions\n\nWhat does using a Docker container for a data science project make easier? What does it make harder?\nDraw a mental map of the relationship between the following: Dockerfile, Docker Image, Docker Registry, Docker Container\nWhen would you want to use each of the following flags for docker run? When wouldn’t you?\n\n-p, --name, -d, --rm, -v\n\nWhat are the most important Dockerfile commands?"
  },
  {
    "objectID": "chapters/sec1/1-5-docker.html#lab-running-a-container-locally",
    "href": "chapters/sec1/1-5-docker.html#lab-running-a-container-locally",
    "title": "5  Docker for Data Science",
    "section": "5.9 Lab: Running a Container Locally",
    "text": "5.9 Lab: Running a Container Locally\nTODO: move palmer penguins example down here\nCONSIDER – should the portfolio example instead be dockerizing the app you created in section 1-3?"
  },
  {
    "objectID": "chapters/sec1/1-5-docker.html#portfolio-example-run-rstudio-server-in-a-container",
    "href": "chapters/sec1/1-5-docker.html#portfolio-example-run-rstudio-server-in-a-container",
    "title": "5  Docker for Data Science",
    "section": "5.10 Portfolio Example: Run RStudio Server in a Container",
    "text": "5.10 Portfolio Example: Run RStudio Server in a Container\nWrite a blog post about getting RStudio Server running in a container on your desktop using the rocker/rstudio container. Can you access your home directory from RStudio Server?\n\nHint 1: You’ll probably need all of the docker run flags you’ve learned and one more – the -e KEY=value flag provides an environment variable to the running container.\nHint 2: The default port for RStudio Server is 8787.\nHint 3: If you’re running a laptop with Apple Silicon (M1 or M2), you may need to try a different container. amoselb/rstudio-m1 worked for me.4\n\n\n5.10.1 Running a service in a container\nConsider reviewing the containers section if you’re not generally familiar with how to run a container.\nIf you want to run a service, like RStudio Server, out of a container, the pattern is very similar to running an interactive app. You’ll find an appropriate container, bring it up, and do some port mapping to make it available to the outside world.\nThe rocker organization makes available a number of containers related to R and RStudio. So if you want a container running RStudio Server on your laptop, it’s as easy as running\ndocker run \\\n  --rm -d \\\n  -p 8787:8787 \\\n  --name rstudio \\\n  rocker/rstudio\nNow, when you go to http://localhost:8787 on your laptop, you should see the the RStudio login screen…but what’s the password? Luckily, the wonderful folks who built the rocker/rstudio container made it easy to supply a password for the default rstudio user.\nWe do this by supplying an environment variable to the container named PASSWORD using the -e flag.\nSo, docker kill rstudio and try again by adding a password when you start:\ndocker run \\\n  --rm -d \\\n  -p 8787:8787 \\\n  --name rstudio \\\n  -e PASSWORD=my-rstudio-pass\n  rocker/rstudio\nNow you should be looking at the RStudio IDE! Hurray!\nThis is great for standing up a quick sandbox…but before you go standing up an RStudio Server on your laptop and spreading it around the world, there are a few things you’ll want to think about.\nIt’s totally possible to run a service like RStudio Server in a docker container, but you’ll need to take all the same steps to make it available to the outside world in terms of hosting it on a server, routing traffic properly, and making sure that you’re using HTTPS to secure your traffic. See chapters XX-XX for more on all that.\nNow, let’s say I wanted to create another user on the server (docker exec)\nBut there’s also one more concern that’s particular to Docker.\nOne of the best things about a Docker container is how ephemeral it is. Things come up in moments, and when they’re gone you don’t have to worry about them. But that’s also very dangerous if you want things to persist.\nThe best way to fix this is to mount external volumes that will maintain the state should the container die or should you want to replace it. We went over how to do that in the last section.\nAt a minimum, you’ll want to mount the user home directories that store all the data and code you can see in RStudio Server. You may also want to mount other bits of state, like wherever you’ve installed your version of R, and the config file you’re using to maintain the server.\nNote that, unlike on a server (where you restart the process of the server), the pattern with a container is generally to kill and restart the container, and let changes come up with the new container."
  },
  {
    "objectID": "chapters/sec2/2-0-sec-intro.html",
    "href": "chapters/sec2/2-0-sec-intro.html",
    "title": "Tools of the IT/Admin",
    "section": "",
    "text": "The last section was all about how to make use of DevOps technologies as a data scientist. The next section introduces a baseline of IT/Admin knowledge.\nThis section is the bridge between the two.\nThis section is designed to get you comfortable with the tools you need to administer a remote server over SSH if you’ve never done so before. This section includes an intro to the terminal and the command line and how to use SSH to connect to a remote server.\nI think of these tools solidly as IT/Admin tools, though I think facility on the command line can also be useful as a solo data scientist.\nIf you’re comfortable using SSH on a remote server, it may well be that this section is skippable for you. I might recommend skimming – there might be a useful trick or two in here. If you’re not sure, review the questions at the end of each chapter. If you’ve got the answers down you can probably skip the chapter with no issue."
  },
  {
    "objectID": "chapters/sec2/2-1-terminal.html#what-makes-up-the-command-line",
    "href": "chapters/sec2/2-1-terminal.html#what-makes-up-the-command-line",
    "title": "6  The Terminal",
    "section": "6.1 What makes up the command line?",
    "text": "6.1 What makes up the command line?\nIt is possible to spend a lotof time customizing your terminal to be exactly what you like. Some might argue it wouldn’t be the best use of your time to do so.\nSuch people are no fun, and having a terminal that’s super customized to what you like is great. Plus you get to feel like a real hacker.\nOne of the confusing things about customizing your command line is understanding what program you’re actually interacting with and where it’s customized. So here’s a little intro.\nThere are three programs that sit on top of each other when you interact with the command line – the terminal, the shell, and the operating system.\nThe terminal is the visual program where you’ll type in commands. The terminal program you use will dictate the colors and themes available for the window, how tabs and panes work, and the keyboard shortcuts you’ll use to manage them.\nThe shell is the program you’re interacting with as you’re typing in commands. It’s what matches the words you type to actual commands or programs on your system. Depending on which shell you choose, you’ll get different options for autocompletion, options for plugins for things like git, and coloring and theming of the actual text in your terminal.\nThere is some overlap of things you can customize via the terminal vs the shell, so mix and match to your heart’s content.\nLastly, the operating system is what actually runs the commands you’re typing in. So the set of commands available to you will differ by whether you’re using Windows or Mac or Linux.\n\n\n\n\ngraph LR\n    A[A Human] --> |Types| B[Commands]\n    A --> |Opens| E\n    E[Terminal] --> |Opens| C\n    C[Shell] --> |Dispatches| B\n    D[Operating System] --> |Defines the set of| B\n    D[Operating System] --> |Runs| B\n\n\n\n\n\n\n\n\nIn the next few sections of this chapter, we’ll get into how to set up your terminal and shell so that it looks and behaves exactly the way you want.\n\n\n\n\n\n\nNote\n\n\n\nI haven’t used a Windows machine in many years. I’ve collected some recommendations here, but I can’t personally vouch for them the way I can my Mac recommendations."
  },
  {
    "objectID": "chapters/sec2/2-1-terminal.html#choose-your-terminal",
    "href": "chapters/sec2/2-1-terminal.html#choose-your-terminal",
    "title": "6  The Terminal",
    "section": "6.2 Choose your terminal",
    "text": "6.2 Choose your terminal\n\nMacOSWindows\n\n\nIf you’re using a Mac, you can use the built-in terminal app, conveniently called Terminal. It’s fine.\nIf you’re going to be using your terminal more than occasionally, I’d recommend downloading and switching to the the free iTerm2, which adds a bunch of niceties like better theming and multiple tabs.\n\n\nIf you’re using Windows, there are a variety of alternative terminals you can try, but the built-in terminal is the favorite of many users. Experiment if you like, but feel free to stick with the default."
  },
  {
    "objectID": "chapters/sec2/2-1-terminal.html#choosing-and-configuring-your-shell",
    "href": "chapters/sec2/2-1-terminal.html#choosing-and-configuring-your-shell",
    "title": "6  The Terminal",
    "section": "6.3 Choosing and configuring your shell",
    "text": "6.3 Choosing and configuring your shell\n\nMacOSWindows\n\n\nThe default shell for MacOS (and Linux) is called bash. It’s pretty great shell. There’s nothing to really replace bash, but there are bash alternatives that extend bash in various ways.\nThe most popular bash alternatives include zsh, Ksh, and Fish. If you don’t already have a favorite, I recommend zsh.1\nIt has a few advantages over bash out of the box, like better autocompletion. It also has a huge ecosystem of themes and plugins that can make your shell way prettier and more functional. There are plugins that do everything from displaying your git status on the command line to controlling your Spotify playlist.\nThere are two popular plugin managers for zsh – OhMyZsh and Prezto. I prefer and recommend Prezto, but the choice is really up to you.\nI’m not going to go through the steps of installing these tools – there are numerous online walkthroughs and guides that you can google.\nBut it is a little confusing to know what to customize where, so here’s the high level overview if you’ve installed iTerm2, zsh, and prezto. You’ll customize the look of the window and the tab behavior in the iTerm2 preferences and customize the text theme and plugins via prezto. You can mostly skip any customization of zsh in the .zshrc since you’ll be doing that in Prezto.\n\n\nWindows comes with two shells built in, the Command shell (cmd) and the PowerShell.\nThe command shell is older and has been superseded by PowerShell. If you’re just getting started, you absolutely should just work with PowerShell. If you’ve been using Command shell on a Windows machine for a long time, most Command shell command work in PowerShell, so it may be worth switching over.\nOnce you’ve installed PowerShell, many people like customizing it with Oh My Posh."
  },
  {
    "objectID": "chapters/sec2/2-1-terminal.html#comprehension-questions",
    "href": "chapters/sec2/2-1-terminal.html#comprehension-questions",
    "title": "6  The Terminal",
    "section": "6.4 Comprehension Questions",
    "text": "6.4 Comprehension Questions\n\nDraw a mental map that includes the following: terminal, shell, operating system, my laptop"
  },
  {
    "objectID": "chapters/sec2/2-2-ssh.html#how-ssh-works",
    "href": "chapters/sec2/2-2-ssh.html#how-ssh-works",
    "title": "7  Connecting Securely with SSH",
    "section": "7.1 How SSH works",
    "text": "7.1 How SSH works\nSSH allows you to directly access the command line on a remote host from anywhere that can connect to it over a network. It is the main way to administer a server.\nSSH works via the exchange of cryptographic keys. You will create an SSH key, which comes in two parts – the public key and the private key.\n\n\n\n\n\n\nNote\n\n\n\nI believe the terms public key and private key are a little bit of a misnomer. The analogy to the real world is a little clearer by thinking of the private key as the key and the public key as the lock.\nYou’re the only one who has the key, but you can hand copies of the lock around so they can always verify that your key is the real one.\n\n\nAs the name might suggest, you keep the private key secret. The best practice is to never move it once it has been created. You can give the public key out to anywhere you might need to access using SSH. Popular targets include remote servers you’ll need to SSH into as well as remote git hosts, like GitHub.\nSSH works on the basis of public key cryptography, which is really cool. It also defies common sense a little bit – it is a little strange that you create this two-part thing and it’s absolutely fine to hand one half around but really bad if you mix them up.\nA short digression about the mathematics of public key cryptography may help clarify.\nPublic key cryptography relies on mathematical operations that are easy in one direction, but really hard to reverse. This means that if I’ve got the public key, it’s really hard to reverse-engineer the private key, but really easy to check that the private key is right if I’m given it up front.\nAn example of an operation like this is multiplying prime numbers together. Having a public key is just like being told a number – say \\(91\\). Even if you know it’s the product of two primes, it’ll probably take you a few moments to figure out the right primes are \\(7\\) and \\(13\\).\nBut if you already have \\(91\\) and I tell you that the right primes are \\(7\\) and \\(13\\), it’s super quick to check that those are indeed the right ones.\nThe biggest difference between multiplying \\(7 * 13 = 91\\) and modern encryption algorithms is the size of the number. Public key cryptography doesn’t use small numbers like 91. It uses numbers with 91 or 9,191 digits.\nModern encryption methods also use substantially more convoluted mathematical operations than simple multiplication – but the idea is completely the same, and prime numbers are equally important.\nThe point is that SSH public keys are very big numbers, so while someone could try to reverse-engineer the private keys by brute force, it’d take more time than we have left before the heat death of the universe at current computing speeds.\nThis is why you can give your public key to a server or service that you might not fully control. Someone who has your public key can verify that your private key is the one that fits that public key – but it’s basically impossible to reverse engineer the private key with the public key in hand.\nHowever, it is totally possible to compromise the security of an SSH connection by being sloppy with your private keys. So while SSH is cyptographically super secure, the whole system is only as secure as you. Always keep your private keys securely in the place where they were created and share only the public keys."
  },
  {
    "objectID": "chapters/sec2/2-2-ssh.html#practical-ssh-usage",
    "href": "chapters/sec2/2-2-ssh.html#practical-ssh-usage",
    "title": "7  Connecting Securely with SSH",
    "section": "7.2 Practical SSH usage",
    "text": "7.2 Practical SSH usage\nBefore SSH will work, your keypair needs to be created and the public key needs to be shared. There are tons of guides online to creating an SSH key for your operating system – google one when you need it.\n\n\n\n\n\n\nDebugging SSH\n\n\n\nSSH has one of my favorite debugging modes.\nIf something’s not working when you try to connect, just add a -v to your command for verbose mode. If that’s not enough information, add another v for -vv, and even another!\nEvery v you add (up to 3) will make the output more verbose.\n\n\nThe way you register a keypair as valid on a server you control is by creating a user on that server and adding the public key to the end of the .ssh/authorized_keys file inside the user’s their home directory. More on server users and home directories in Chapter 10.\nIf you’re the server admin, you’ll have your users create their SSH keys, share the public keys with you, and you’ll put them into the right place on the server.\nIf you need to SSH from the server to another server or to a service that uses SSH, like GitHub, you’ll create another SSH key on the server and use that public key on the far end of the connection.\nIf you follow standard instructions for creating a key, it will use the default name, probably id_ed25519.1 I’d recommend sticking with the default name if you’ve only got one. This is because the ssh command will just use the keys you’ve created if they have the default name.\nIf you don’t want to use the default name for some reason, you can specify a particular key with the -i flag.\nIf you’re using SSH a lot on the same servers, I’d recommend setting up an SSH config file. You can include usernames and addresses in a config file so instead of typing ssh alexkgold@do4ds-lab.shop I can just type ssh lab.\nA google search should return good instructions for setting up your SSH config when you get there."
  },
  {
    "objectID": "chapters/sec2/2-2-ssh.html#comprehension-questions",
    "href": "chapters/sec2/2-2-ssh.html#comprehension-questions",
    "title": "7  Connecting Securely with SSH",
    "section": "7.3 Comprehension Questions",
    "text": "7.3 Comprehension Questions\n\nUnder what circumstances should you move or share your SSH private key?\nWhat is it about SSH public keys that makes them safe to share?"
  },
  {
    "objectID": "chapters/sec2/2-3-cmd-line.html#the-structure-of-bash-commands",
    "href": "chapters/sec2/2-3-cmd-line.html#the-structure-of-bash-commands",
    "title": "8  Getting comfortable on the Command Line",
    "section": "8.1 The structure of bash commands",
    "text": "8.1 The structure of bash commands\nbash and its derivatives provide small programs that each do one small thing well called a command.\nA command is the program you want to run, usually an abbreviation of the word for what you want to do. For example, the command to list the contents of a directory is ls.\nArguments tell the command what to run on. They come after the command with a space in between. For example, if I want to run ls on the directory /home/alex, I can run ls /home/alex on the command line.\nMany commands have default arguments. For example, ls runs by default on the directory I’m currently in. So if I’m in /home/alex, running ls and running ls /home/alex would return the same thing.\nOptions or flags modify how the command operates. Flags are denoted by having one or more dashes before them. For example, the ls command, which lists files, has the optional flag -l, which indicates that the files should be displayed as a list.\nFlags always come in between the command and any arguments to the command. So, for example, if I want to get the files in /home/alex as a list, I can run ls -l /home/alex or navigate to /home/alex and run ls -l.\nSome flags themselves have arguments. So, for example, if you’re using the -l flag on ls, you can also use the -D flag to format the datetime when the file was last updated.\nSo, for example, running ls -l -D %Y-%m-%dT%H:%M:%S /home/alex will list all the files in /home/alex with the date-time of the last update formatted in ISO 8601 format (which is always the correct format for dates).\nIt’s nice that this structure is standard. You always know that a bash command will be formatted as <command> <flags + flag args> <command args>. The downside is that having the main argument come all the way at the end, after all the flags, can make it really hard to mentally parse commands if you don’t know them super well.\nBecause there can be so many arguments, bash commands can get long. Sometimes you’ll see bash commands split them over multiple lines. You can tell bash you want it to keep reading after a line break by ending the line with a space and a \\. It’s often nice to include one flag or argument per line.\nFor example, here’s that ls command more nicely formatted:\n> ls -l \\\n  -D %Y-%m-%dT%H:%M:%S \\\n  /home/alex\nThis is at least a little easier to parse. There is also help available!\nAll of the flags and arguments for commands can be found in the program’s man page (short for manual). You can access the man page for any command with man <command>. You can scroll the man page with arrow keys and exit with q.\nIf you ever can’t figure out how to quit, ctrl + c will generally quit from any command line situation.\nTODO: is this alt on windows, or also ctrl?\n\n\n\n\n\n\n\nSymbol\nWhat it is\n\n\n\n\nman\nmanual\n\n\nq\nQuit man pages (and many other situations)\n\n\n\\\nContinue command on new line"
  },
  {
    "objectID": "chapters/sec2/2-3-cmd-line.html#linux-directories-and-files",
    "href": "chapters/sec2/2-3-cmd-line.html#linux-directories-and-files",
    "title": "8  Getting comfortable on the Command Line",
    "section": "8.2 Linux directories and files",
    "text": "8.2 Linux directories and files\nIn Linux, directories define where you can be. A directory is just a container for files and other directories.\nIn Linux, the entire file system is a tree (or perhaps an upside-down tree). The root directory, / is the base of the tree and a / in between two directories means that it’s a sub-directory. So the directory /home/alex is the alex directory, which is contained in home, which is in the root directory /.\nIn Linux, every directory is a sub-directory of / or a sub-directory of a sub-directory of / or…you get the picture. The sequence of directories that defines a location is called a file path.\nEvery Linux command happens at a particular file path – called the working directory. In some cases the commands you’re allowed to run or what they do will vary a lot based on where you are when they run.1\nFile paths can be either absolute – specified relative to the root – or relative to the working directory. Absolute file paths always start with / so they’re easy to identify.\nDepending on what you’re doing, either absolute or relative paths make more sense. In general, absolute file paths make more sense when you want to access the same resource regardless of where the command is run, and relative file paths make more sense when you want to access a resource specific to where you run it.\nAt any time, you can get the full path to your working directory with the pwd command, which is an abbreviation for print working directory. When you’re writing out a file path, the current working directory is at ..\nGoing back to the ls command, you can now see that the default argument to ls is .. You can test this for yourself by comparing the output of ls and ls .. They should be identical.\nAside from / and ., there are two other special directories.\n.. is the parent of the directory you’re in, so you can move to the parent of your current directory using cd .. and to it’s parent with cd ../...\n~ is the home directory of your user (assuming it has one). We’ll get more into what that means in a bit.\n\n\n\n\n\n\nHow does / compare to C:?\n\n\n\nIf you’re a Windows person, you might think this is analogous to C:. You’re not wrong, but the analogy is imprecise.\nIn Linux, everything is a sub-directory of /, irrespective of the configuration of physical or virtual drives that houses the storage. Frequently, people will put extra drives on their server – a process called mounting – and house them at /mnt (short for…you guessed it).\nThat’s different from Windows. In Windows you can have multiple roots, one for each physical or logical disk you’ve got. That’s why your machine may have a D: drive, or if you have network shares, those will often be on M: or N: or P:.\n\n\nAlong with being able to inspect directories, it’s useful to be able to change your working directory with the cd command, short for change directory.\nRecap of commands in this section\n\n\n\n\n\n\n\n\n\nCommand\nWhat it does/is\nHelpful options\nExample\n\n\n\n\n/\nsystem root\n\n\n\n\n.\ncurrent working directory\n\n\n\n\nls\nlist objects in a directory\n-l - format as list\n-a - all (include hidden files)\n$ ls .\n$ ls -la\n\n\npwd\nprint working directory\n\n$ pwd\n\n\ncd\nchange directory\n\n$ cd ~ / Documents\n\n\n~\nhome directory of the current user\n\n$ ls ~"
  },
  {
    "objectID": "chapters/sec2/2-3-cmd-line.html#reading-text-files",
    "href": "chapters/sec2/2-3-cmd-line.html#reading-text-files",
    "title": "8  Getting comfortable on the Command Line",
    "section": "8.3 Reading text files",
    "text": "8.3 Reading text files\nBeing comfortable opening and navigating around text files is an essential IT/Admin skill.\ncat is the command to print a file, starting at the beginning.\nSometimes you’ve got a really big file and you want to see just part. less starts at the top with the ability to scroll down. head prints the first few lines and quits. It is especially useful to peer at the beginning of a plain text data file (like csv) as it prints the first few rows and exits – so you can preview the beginning of a very large data file very quickly.\ntail skips right to the end of a file. Log files usually are written so the newest part is last – so much so that “tailing a log file” is a synonym for looking at it. In some cases, you’ll want to tail a file as the process is still running and writing information to the log. You can get a live view of the end of the file using the -f flag (for follow).\nSometimes you want to search around inside a text file. You’re probably familiar with the power of regular expressions (regex) to search for specific character sequences in text strings. The Linux command to do regex searches is grep, which returns results that match the regex pattern you specify.\nThe true power of grep is unlocked in combination with the pipe. The Linux pipe operator – | – takes the output of the previous command and sends it into the next one.\n\n\n\n\n\n\nHaven’t I seen the pipe before?\n\n\n\nThe pipe should feel extremely familiar to R users.\nThe {magrittr} pipe, %>%, has become extremely popular as part of the tidyverse since its introduction in 2013. A base R pipe, |>, was released as a part of R 4.1 in 2021.\nThe original pipe in {magrittr} took inspiration from both the Unix pipe and the pipe operator in the F# programming langauge.\n\n\nA combination I do all the time is to pipe the output of ls into grep when searching for a file inside a directory. So if I was searching for a file whose name contained the word data, that might look something like ls ~/projects/my-project | grep data.\n\n\n\n\n\n\n\n\nCommand\nWhat it does\nNotes + Helpful options\n\n\n\n\ncat\nPrints a file.\n\n\n\nless\nPrints a file, but just a little.\nCan be very helpful to look at a few rows of csv.\nLazily reads lines, so can be much faster than cat for big files.\n\n\ntail\nLook at the end of a file.\nUseful for logs, where the newest part is last.\nThe -f flag is useful for a live view.\n\n\nhead\nLook at the beginning of a file.\nDefaults to 10 lines, can specify a different number with -n <n>.\n\n\ngrep\nSearch a file using regex.\nWriting regex can be a pain. I suggest testing expressions on regex101.com.\nOften useful in combination with the pipe.\n\n\n|\nthe pipe"
  },
  {
    "objectID": "chapters/sec2/2-3-cmd-line.html#copying-moving-and-removing-files-and-directories",
    "href": "chapters/sec2/2-3-cmd-line.html#copying-moving-and-removing-files-and-directories",
    "title": "8  Getting comfortable on the Command Line",
    "section": "8.4 Copying, moving and removing files and directories",
    "text": "8.4 Copying, moving and removing files and directories\nYou can copy a file from one place to another using the cp command. cp leaves behind the old file and adds the new one at the specified location. You can move a file with the mv command, which does not leave the old file behind.\nIf you want to remove a file entirely, you can use the rm command. There is also a version to remove a directory, rmdir.\n\n\n\n\n\n\nWarning\n\n\n\nBe very careful with the rm command.\nUnlike on your desktop there’s no recycle bin! Things that are deleted are instantly deleted forever.\n\n\nIf you want to make a directory, mkdir makes a directory at the specified filepath. mkdir will only work if it’s creating the entire file path specified, so the -p flag can be handy to create only the parts of the path that don’t exist.\nSometimes it’s useful to operate on every file inside a directory. You can get every file that matches a pattern with the wildcard, *. You can also do partial matching with the wildcard to get all the files that match part of a pattern.\nFor example, let’s say I have a /data directory and I want to put a copy of only the .csv files inside into a new sub-directory. I could do the following:\n> mkdir -p /data/data-copy\n> cp /data/*.csv /data/data-copy\n\n\n\n\n\n\n\n\n\nCommand\nWhat it does/is\nNotes + Helpful Options\nExample\n\n\n\n\nrm\nremove – deletes i mmediately and p ermantenly\n-r - r ecursively remove everything below a file path\n-f - force - dont ask for each file\n$ rm  - rf old_docs/\nBE VERY CAREFUL WITH -rf\n\n\ncp\ncopy\n\n\n\n\nmv\nmove\n\n\n\n\n*\nwildcard\n\n\n\n\nmkdi r/rmdir\nm ake/remove directory\n-p - create any parts that don’t exist"
  },
  {
    "objectID": "chapters/sec2/2-3-cmd-line.html#moving-things-to-and-from-the-server",
    "href": "chapters/sec2/2-3-cmd-line.html#moving-things-to-and-from-the-server",
    "title": "8  Getting comfortable on the Command Line",
    "section": "8.5 Moving things to and from the server",
    "text": "8.5 Moving things to and from the server\nIt’s very common to have a file on your server you want to move to your desktop or vice versa.\nIt’s generally easier to move a single file rather than a whole bunch. The tar command turns a set of files or whole directory into a single archive file, usually with the file suffix .tar.gz. Creating an archive also does some compression when it creates the archive file. The amount depends on the content.\nThe tar command is used to both create and unpack (extract) archive files and telling it which one requires the use of several flags. I never remember them – this is a command I google every time I use it. The flags you’ll use most often are in the cheat sheet below.\nOnce you’ve created an archive file, you’ve got to move it. The scp command is the way to do this. scp – short for secure copy – is basically a combo of SSH and copy.2 scp is particularly nice because it uses the syntax you’re used to from using cp.\nSince scp establishes an SSH connection, you need to make the request to somewhere that is accepting SSH connections. Hopefully your server is accepting SSH connections and your laptop is not.\nYou’ll almost certainly have the experience at some point of being on your server and wanting to scp something to or from your laptop. You need to do the scp command from a regular terminal on your laptop, not one that’s already SSH-ed into your server.\n\n\n\n\n\n\n\n\nCommand\nWhat it does\nNotes + Helpful options\n\n\n\n\ntar\ncompress/decompress file/directory\nAlmost always used with flags.\nCreate is usually\ntar -czf  <a rc h i ve name> <file(s)>\nExtract is usually\nt ar - xfv <archive name>\n\n\nscp\nCopy across ssh\nCan use most ssh flags (like -i and -v)"
  },
  {
    "objectID": "chapters/sec2/2-3-cmd-line.html#writing-files-on-the-command-line",
    "href": "chapters/sec2/2-3-cmd-line.html#writing-files-on-the-command-line",
    "title": "8  Getting comfortable on the Command Line",
    "section": "8.6 Writing files on the command line",
    "text": "8.6 Writing files on the command line\nTODO – add > and >>\nThere will be many situations where writing into a text file will be handy while administering your server – for example, when changing config files. When you’re on the command line, you’ll use a command line tool for writing into those files – meaning you’ll only have your keyboard to navigate, save, and exit.\nThere are times when you want to make files or directories with nothing in them – the touch command makes a blank file at the specified file path.\nYou also may want to take some text and make it into a file. You can do this with the > command. >> does the same thing, but appends it to the end of the file.\nA common reason you might want to do this is to add something to the end of your .gitignore. You can’t just type a word on the command line and have it treated like a string – so you may need the echo command to have something you type treated as a string.\nFor example, if you want to add your .Rprofile file to your .gitignore, you could do that with echo .Rprofile >> .gitignore.\n\n\n\n\n\n\n\n\nCommand\nWhat it does\nNotes\n\n\n\n\ntouch\nCreates file if doesn’t already exist.\nUpdates timestamp to current time if it does exist\n\n\n>\nOverwrite file contents\nCreates a new file if it doesn’t exist\n\n\n>>\nConcatenate to end of file\nCreates a new file if it doesn’t exist\n\n\n\n\n8.6.1 Command line text editors\nThere are two command line text editors you’ll probably encounter – both extremely powerful text editing tools: nano and vi/vim.3\nYou can open a file in either by typing nano <filename> or vi <filename>. Unfortunately for many newbie Linux Admins it’s extremely easy to get stuck inside a file with no hint of how to get out!\nIn nano there will be helpful prompts along the bottom to tell you how to interact with the file, so you’ll see once you’re ready to go, you can exit with ^x. But what is ^? On most keyboards, you can insert the caret character, ^, by pressing Shift + 6. But that’s not what this is.\nIn this case, the ^ caret is short for Ctrl on Windows and for Command (⌘) on Mac. Phew!\nWhere nano gives you helpful – if obscure – hints, vim leaves you all on your own. It doesn’t even tell you you’re inside vim!\nThis is where many people get stuck and end up having to just exit and start a new terminal session. It’s not the end of the world if you do, but knowing a few vim commands can help you avoid that fate.\nOne of the most confusing things about vim is that you can’t edit the file when you first enter. That’s because vim keybindings were (1) developed before keyboards uniformly had arrow keys and (2) are designed to minimize how much your hands need to move.\nIf you feel like taking the time, learning vim keybindings can make navigating and editing text (code) files easier. Plus it just feels really cool. I recommend spending some time trying. In this section, I’m just going to help you get the minimum amount of vim you need to be safe.\nWhen you enter, you’re in normal mode, which is for navigating through the file. Typing things on your keyboard won’t type into the document, but will do other things.\nPressing the i key activates insert mode. For those of us who are comfortable in a word processor like Word or Google Docs, insert mode will feel very natural. You can type and words will appear and you can navigate with the arrow keys.\nOnce you’re done writing, you can go back to normal mode by pressing the escape key. In addition to navigating the file, normal mode allows you to do file operations like saving and quitting.\nFile operations are prefixed with a colon :. The two most common commands you’ll use are save (write) and quit. You can combine these together, so you can save and quit in one command using :wq.\nSometimes you may want to exit without saving. If you’ve made changes and try to exit with :q, you’ll find yourself in an endless loop of warnings that your changes won’t be saved. You can tell vim you mean it with the exclamation mark and exit using :q!.\n\n\n\n\n\n\n\n\nVim Command\nWhat it does\nNotes + Helpful options\n\n\n\n\n^\nPrefix for file command in nano editor.\nIts the ⌘ or Ctrl key, not the caret symbol.\n\n\ni\nEnter insert mode in vim\n\n\n\nescape\nEnter normal mode in vim.\n\n\n\n:w\nWrite the current file in vim (from normal mode)\nCan be combined to save and quit in one, :wq\n\n\n:q\nQuit vim (from normal mode)\n:q! quit without saving"
  },
  {
    "objectID": "chapters/sec2/2-3-cmd-line.html#comprehension-questions",
    "href": "chapters/sec2/2-3-cmd-line.html#comprehension-questions",
    "title": "8  Getting comfortable on the Command Line",
    "section": "8.7 Comprehension Questions",
    "text": "8.7 Comprehension Questions\n\nIf you don’t know the real commands for them, make up what you think the bash commands might be to do the following. So if you think you’d create a command called cmd with a flag -p and an argument arg, you’d write cmd -p <what p does> <arg>. In the next chapter you’ll get to see how close you got to the real thing:\n\nChange Directories, the only argument is where to go\nMaking a Directory, with an optional flag to make parents as you go. The only argument is the directory to make.\nRemove files, with flags to do so recursively and to force it without checking in first. The only argument is the file or directory to remove."
  },
  {
    "objectID": "chapters/sec3/3-0-sec-intro.html",
    "href": "chapters/sec3/3-0-sec-intro.html",
    "title": "The DIY Data Science Environment",
    "section": "",
    "text": "The first section of this book focused on how to move your data science practices closer to the DevOps ideal – and mostly focused on the Dev side of the coin.\nThis section focuses more on Ops.\nIn this section, you’ll learn how to manage a server-based data science environment reasonably safely and securely. We’re going to get into how to make use of Amazon Web Services (AWS) to host data science assets on the web.\nYou’re going to learn standard patterns to do simple administrative tasks for servers and apply those to a data science use-case.\nBut before you get too excited, it’s also worth talking about the limits of this section.\nThere’s a reason IT Administration is a career all to itself. There’s so much to know about servers, networking, managing users, storage, and more. And it’s scary – it’s easy to run into pitfalls that cause security vulnerabilities, system instability, and general annoyance.\nMoreover, hosting servers isn’t your competitive advantage as a data scientist. You’re probably great at writing R or Python code, cleaning and managing data, or building models. Managing servers isn’t in your core skillset.\nSo this section is really targeting the student who wants to host a project for portfolio purposes, the hobbyist who’s hosting a toy project, or the data science leader who has no choice but to host things for themselves because they just can’t get IT/Admin support.\nThe good news is that if that’s you, you’re in the right place.\nThis section has five chapters, building towards having a data science workbench with R and Python in the cloud you could actually use for yourself or a small team.\nIn addition to hosting those development environments, we’ll also deploy a Shiny app on the server using Shiny Server Open Source and we’ll host a vetiver-generated API using a Docker container.\nSo how are we going to get there?\nIn each of the five chapters, we’ll go through a conceptual overview of an important aspect of acquiring, hosting, and managing a data science environment in the cloud. Once you’ve read the conceptual overview, we’ll do a hands-on lab where you’ll get to practice actually accomplishing something in AWS.\nIt’s worth saying that the way we’re doing this is going to work with the most basic primitives of cloud computing. The first reason for that is that it’s cheaper. Understandably, using simpler cloud services is cheaper than using more expensive ones. If you follow this all the way to the end, it won’t be zero cost, but you could pay less than $10.\nThe second reason is that it’s better for comprehension. All of cloud computing relies on the primitives we’ll use here (at it’s inception, the goal of AWS was to create an Operating System for the internet). Once you understand these primitives, you’ll have no trouble using other services to accomplish the same ends if you’d prefer a less DIY experience.\nLet’s get started."
  },
  {
    "objectID": "chapters/sec3/3-1-cloud.html#layers-of-cloud-services",
    "href": "chapters/sec3/3-1-cloud.html#layers-of-cloud-services",
    "title": "9  Getting Started with the Cloud",
    "section": "9.1 Layers of cloud services",
    "text": "9.1 Layers of cloud services\nWhile the basic premise of the cloud is rent a server, there are layers and layers of services on top of that basic premise. Depending on your organization’s needs, it may make sense to rent a very basic service, or a higher level one.\nLet’s talk about cakes to help make the levels clear. This year, I’m planning to bake and decorate a beautiful cake for my friend’s birthday. It’s going to be a round layer cake with white frosting, and it’s going to say “Happy Birthday!” in teal with giant lavender frosting rosettes.\nNow that I’ve decided what I’m making, I have a few different options for how I get there. If I’m a real DIY kind of person, I could buy all the ingredients from the grocery store and make everything from scratch. Or I could buy a cake mix – reducing the likelihood I’ll buy the wrong ingredients or end up with unnecessary leftovers. Or maybe I don’t want to bake at all – I could just buy a premade cake already covered in white frosting and just do the writing and the rosettes.\nThe choice of how much to DIY my friend’s birthday cake and how much to get from a bakery is very much akin to your choices when buying server capacity from the cloud.\nTODO: Image of getting cake w/ IaaS, PaaS, SaaS\nIn the US, a huge fraction of server workloads run on servers rented from one of the “big three clouds” – Amazon Web Services (AWS), Microsoft Azure, and Google Cloud Platform (GCP). There are also numerous smaller cloud providers, many of which target more specific use cases.\nIt’s totally possible to just rent the raw ingredients from these cloud providers – they’ll happily rent you the most basic cloud services like servers, networking, and storage and let you put them together yourself. This kind of cloud service – the equivalent of baking from scratch – is called Infrastructure as a Service (IaaS, pronounced eye-az).\nBut then I’m responsible for managing all of the reproducibility stack – I have to make sure the servers are up to date with new operating systems and security patches, while also ensuring there are modern versions of R and Python on the system, and making sure there’s RStudio and Jupyter Notebooks present.\nBut just as you might not want to have to buy eggs, milk, and sugar just to end up with a pretty cake, IT/Admin organizations are increasingly taking advantage of the cloud equivalent of cake mixes and premade blank sheet cakes.\nIf I want to offload more of the work, I can think about a Platform as a Service (PaaS – pronounced pass) solution. This would give me the ability to somehow define an environment and send it somewhere to run without worrying about the underlying servers. For example, I might want to build a docker container with Python and Jupyter Notebook and host it somewhere that autoscales for the number of users and the amount of other resources needed.\nThere are a bunch of different entities you can run in this way, and services to match each. For example, if you want to run a general app, you can use something like AWS’s Elastic Beanstalk to run a user-facing app that dynamically scales (TODO: Azure/GCP equivalents), if you want to run a container, you might use XXXX, and if you want to get a Kubernetes cluster as a service, you might use XXXX. If you want to just run a function, there’s always AWS Lambda.\n\n\n\n\n\n\nDanger\n\n\n\nI am not recommending you run a data science workbench on a service like Elastic Beanstalk. This actually tends not to work very well, for reasons we’ll get into in a bit.\n\n\nIf I want to go one level more abstracted, I might want to go with a Software as a Service(SaaS - pronounced sass) solution. This would be something like one of the cloud providers hosted machine learning environment where you can train, deploy, and monitor machine learning models. AWS has SageMaker, Azure has Azure Machine Learning, and Google’s has Vertex AI. And there are organizations that host popular data science tools like Posit.cloud and Saturn Cloud that you can use.\nSometimes you’ll hear people describe PaaS or SaaS solutions called going serverless.\nThe first thing to understand about serverless computing is that there is no such thing as serverless computing.\nEvery bit of cloud computation runs on a server - the question is whether you have to deal with the server or if you just deal with a preconfigured service.\nIn this book, we’ll be working exclusively with IaaS services – taking the most basic services and building up a working data science environment. Once you understand how these pieces fit together, you’ll be in a much better place if it turns out your organization wants to leverage more abstracted versions of those services."
  },
  {
    "objectID": "chapters/sec3/3-1-cloud.html#common-services-for-data-science-usage",
    "href": "chapters/sec3/3-1-cloud.html#common-services-for-data-science-usage",
    "title": "9  Getting Started with the Cloud",
    "section": "9.2 Common services for data science usage",
    "text": "9.2 Common services for data science usage\nIf you’re not familiar with cloud provider terminology, it can be very hard to tell what service you might need from a cloud provider, and they don’t really help the matter. It’s very common (especially in AWS) to have many different services that fulfill similar needs, and it can be really hard to concretely tell the difference.\nMaking the issue even more difficult, many companies go out of their way to make their services sound grand and important and don’t just say, “this is a ___ you can rent”.\nIt’s helpful to keep in mind that at the very bottom, all you’re doing is renting servers and storage and managing networking and permissions for the servers and storage. Every other service is just combination of server, storage, networking, and permissions that comes with software preinstalled or configured to make your life easier.4\nWe’re going to talk through some of the basic services that are offered by each of the three major cloud platforms.\n\n\n\n\n\n\nCloud service naming\n\n\n\nAzure and GCP tend to name their offerings pretty literally, while AWS chooses cutesy names that have no relationship to the task at hand. I’m going to try to name the services for each of the purposes I’m talking about – but it’s worth noting that feature sets aren’t exactly parallel across cloud providers.\nThis makes AWS names a little harder to learn, but much easier to recall once you’ve learned them. In this section – contrary to standard practice – I’m going to use the common abbreviated names for AWS services and put the full name in parentheses as these are just trivia.\n\n\nRemember, at the bottom of all cloud services are servers and each of them has a service that is “just rent me a server”. AWS has EC2 (Elastic Cloud Compute) and Azure has Azure VMs, and Google has Google Compute Engine.\nAlong with servers, there are two main kinds of storage you’ll rent. The first is file storage, where you’ll store things in a file directory like on your laptop. These are AWS’s EBS (Elastic Block Store), Azure Managed Disk, and Google Persistent Disk.\nThe major cloud providers all also have blob (Binary Large Object) storage. Blob storage allows you to store individual objects somewhere and recall them to any other machine that has access to the blob store. The major blob stores are AWS Simple Storage Service (S3), Azure Blob Storage, and Google Cloud Storage.\nThere are also two important networking services for each of the clouds – a way to make a private network and a way to do DNS routing. If you don’t know what these mean, there will be a lot more detail in Chapter 11. For now, it’s enough to know that private networking is done in AWS’s VPC (Virtual Private Cloud), Azure’s Virtual Network, and Google’s Virtual Private Cloud. DNS is done in AWS’s Route 53, Azure DNS, and Google Cloud DNS.\nOnce you’ve got all this stuff up and running, you need to make sure that permissions are set in the right way. AWS has IAM (Identity andAccess Management), Azure has Azure Active Directory, and Google has Identity Access Management.\nNow, there are a variety of things you might want to do past these basic tasks of server, storage, networking, and access management. Here are a few more services you’ll likely hear about over time.\n\n\n\n\n\n\n\n\n\nService\nAWS\nAzure\nGCP\n\n\n\n\nKubernetes cluster\nEKS (Elastic Kubernetes Service)\nAKS (Azure Kubernetes Service)\nGKE (Google Kubernetes Engine)\n\n\nRun a function as an API\nLambda\nAzure Functions\nGoogle Cloud Functions\n\n\nDatabase\nRDS/Redshift5\nAzure Database\nGoogle Cloud SQL\n\n\nML Platform\nSageMaker\nAzure ML\nVertex AI"
  },
  {
    "objectID": "chapters/sec3/3-1-cloud.html#lab",
    "href": "chapters/sec3/3-1-cloud.html#lab",
    "title": "9  Getting Started with the Cloud",
    "section": "9.3 Lab",
    "text": "9.3 Lab\nWelcome to the lab portion. In this lab, we’re going to get you up and running with an AWS account and show you how to manage, start, and stop EC2 instances in AWS.\nAt a high level, you can do all of this with any cloud provider. Feel free to try if you prefer Azure of GCP, but the details on how to get started and service names will be different.\nWe will be standing up a server in AWS’s free tier – so there will be no cost involved as long as you haven’t used up all your AWS free tier credits before now.\n\n9.3.1 Login to the AWS Console\nWe’re going to start by logging into AWS. If you already know how, you can skip ahead to standing up an instance.\nIf not, go to https://aws.amazon.com and click Sign In to the Console .\nIf you’ve never set up an AWS account before, click Create a New AWS account and follow the instructions to create an account. Note that even if you’ve got an Amazon account for ordering stuff online and watching movies, an AWS account is separate.\nOnce you’ve logged in, you’ll be confronted by the AWS console. There are a ton of things here – feel free to poke around if you want – come back and continue when you’re ready.\n\n\n9.3.2 Stand up an instance\nAWS’s “rent a server” is called EC2.6 It is the most basic AWS service, and it’s what we’re going to use to get started.\n\n\n\n\n\n\nNote\n\n\n\nI’m going to walk you through setting up an instance from the AWS console. The exact buttons and text may change by the time you read this, but the general overview will be the same.\n\n\nThere are five things we have to configure before launching the server. I’m not going to walk you through configuring each one on this page, because Amazon is constantly updating the exact layout of the page and the text, but I’ll explain the choices and then you can make them yourself – or even make similar choices if you’re in GCP or Azure.\nThe first choice is instance name and tags. None of this is required – it’s for convenience and organization. I’d suggest you name the server something like do4ds-lab.\nNext, you’ll have to choose the image. All clouds have a concept of server images. In AWS, they’re called AMIs (pronounced like you’re reading the letters individually, short for Amazon Machine Image).\nAn image is the set of software that’s preinstalled when you start the server. Images can range from just a bare operating system to a running RStudio instance that’s suspended in between two computations. There are many paid images that come preinstalled with a bunch of software ready to go.\nSince we’re going to work on configuring the server from the ground up, we’re going to choose an AMI that’s just the operating system. Choose the most basic Ubuntu one. At the time of this writing, that’s using Ubuntu 22.04.\nIf you want to use a different operating system, that’s fine, but you may need to adjust the commands in this chapter and subsequent to match.\nYou’ll have to choose an Instance Type. In AWS, there are two components to instance type – the family and the size. The family is the category of server that you’re using. In AWS, families are denoted by letters and numbers, so there are T2s and T3s, C4s and C5s, R5s and many more.\nWithin each family, there are different sizes. The sizes vary by the family, but generally range from things below small nano to multiples of xlarge like 24xlarge.\nWe’ll get into choosing the right family and size for your team’s use case in Chaper 14.\nFor now, I’d recommend you get the largest server that is free tier eligible, which is a t2.micro with 1 CPU and 1 Gb of memory as of this writing.\n\n\n\n\n\n\nServer sizing for the lab\n\n\n\nA t2.micro with 1 CPU and 1 Gb of memory is a very small server.\nIf all you’re doing is walking through the lab, it should be sufficient, but if you actually want to do any data science work, you’ll need a substantially larger server.\nIt is possible to rack up really large AWS bills, so be careful.\nThat said, a modestly-sized server is still pretty cheap if you’re only putting it up for a short amount of time.\nI’m writing this on a 2021 M1 Macbook Pro with 10 CPUs and 32 Gb of memory. If you wanted that same computational power from an AWS server, it’s roughly comparable to a t3.2xlarge – with 8 CPUs and 32Gb of memory. That server costs is $0.33 an hour. So a full year running full time for an instance is nearly $3,000, but if you’re only running that instance for a little while – say the few hours it’ll take you to complete this lab, it will probably only be a few dollars.\n\n\nNext, you’ll need to make sure you have a keypair. We’ll get into what this key is and how to use it in Chapter 8. For now, you’ll need to create a keypair if you don’t already have one. I’d suggest naming it do4ds-lab-key because I’ll use that name, so you can just copy/paste commands if you use the same name.\nI’d recommend creating a directory for this lab, perhaps something like do4ds-lab and putting your keypair there. If you’re not going to do that, just make sure you keep track of where you downloaded it.\nWhether you’re on Windows or Mac, download the pem version of the key.\nYou shouldn’t need to change any networking settings from the defaults. Make sure it’s configured to allow SSH traffic.\nYou’ll need to configure storage. By default, the server comes just with root storage. Root storage is where the operating system will be, as well as storing most of the executables you’ll need.\nYou’ll want another volume where you’ll store the actual work you’re doing – data files, code, and more. You can attach volumes quite easily. In AWS, they’re called EBS (short for Elastic Block Store). While it’s not strictly necessary to configure an extra EBS volume to complete the labs in this book, make sure to add one since it’s necessary if you ever want to stand up an actual environment to do work.\nYou can feel free to look through the Advanced Details, but you shouldn’t need to adjust any of them.\nWhen you go to the summary, it should look something like this, assuming you followed the instructions here:\n\nClick Launch Instance. AWS is now creating a virtual server just for you.\nIf you go back to the EC2 page and click on Instances you can see your instance as it comes up. You may need to remove the filter for State: Running since it’ll take a few moments to be Running.\n\n\n9.3.3 Grab the address of your server\nIf you click on the actual instance ID in blue, you can see all the details of your server.\nThe instance ID and public IP addresses were auto-assigned.\nGrab the Public IPv4 DNS address, which starts with ec2- and ends with amazonaws.com. Copy it somewhere easy to grab. That’s going to be the way we access the server.\nFor example, as I write this, my server has the address ec2-54-159-134-39.compute-1.amazonaws.com. In the commands, I’ll include the variable SERVER_ADDRESS. If you’d like to be able to copy commands verbatim, you can set the variable SERVER_ADDRESS to be your server address using SERVER_ADDRESS=ec2-54-159-134-39.compute-1.amazonaws.com.\n\n\n\n\n\n\nTip\n\n\n\nIf you’re used to R, where it’s best practice to put spaces around =, notice that assigning variables in bash requires no spaces around =.\n\n\n\n\n9.3.4 Stopping or burning it down\nWhenever you’re stopping for the day, you may want to suspend your server so you’re not paying for it overnight or using up your free tier hours.\nYou can suspend an instance in the state it’s in so it can be restarted later. Depending on how much data you’re storing, it may not be free, but storage costs are generally very modest.\nWhenever you want to suspend your instance, go to the EC2 page for your server. Under the Instance State drop down in the upper right, choose Stop Instance.\nAfter a couple minutes the instance will stop and you won’t get charged for it. Before you come back to the next lab, you’ll need to start the instance back up so it’s ready to go.\nIf you want to completely delete the instance at any point, you can choose to Terminate Instance from that same Instance State dropdown."
  },
  {
    "objectID": "chapters/sec3/3-1-cloud.html#comprehension-questions",
    "href": "chapters/sec3/3-1-cloud.html#comprehension-questions",
    "title": "9  Getting Started with the Cloud",
    "section": "9.4 Comprehension Questions",
    "text": "9.4 Comprehension Questions\n\nWhat is the difference between PaaS, IaaS, and SaaS? What’s an example of each that you’re familiar with?\nWhat are the names for AWS’s services for: renting a server, file system storage, blob storage"
  },
  {
    "objectID": "chapters/sec3/3-2-linux-admin.html#linux-is-an-operating-system-with-a-long-history",
    "href": "chapters/sec3/3-2-linux-admin.html#linux-is-an-operating-system-with-a-long-history",
    "title": "10  Basic Linux SysAdmin",
    "section": "10.1 Linux is an operating system with a long history",
    "text": "10.1 Linux is an operating system with a long history\nA computer’s operating system (OS) defines how applications – like Microsoft Word, RStudio, and Minecraft – interact with the underlying hardware to actually do computation. OSes define how files are stored and accessed, how applications are installed and can connect to networks, and more.\nThese days, basically all computers run on one of a few different operating systems – Windows, MacOS, or Linux for laptops and desktops; Windows or Linux for servers, Android (a flavor of Linux) or iOS for phones and tablets, and Linux for other kinds of embedded systems (like ATMs and the chips in your car).\nWhen you stand up a server, you’re going to be choosing from one of a few versions of Linux. If you’re unfamiliar with Linux, the number of choices can seem overwhelming, so here’s a quick primer on the history of operating systems. Hopefully it’ll help it all make sense.\nBefore the early 1970s, the market for computer hardware and software looked nothing like it does now. Computers released in that era had extremely tight linking between hardware and software. There were no standard interfaces between hardware and software, so each hardware manufacturer also had to release the software to use with their machine.\nIn the early 1970s, Bell Labs released Unix – the first operating system.\nOnce there was an operating system, the computer market started looking a lot more familiar to 2020s eyes. Hardware manufacturers would build machines that ran Unix and software companies could write applications that ran on Unix. The fact that those applications would run on any Unix machine was a game-changer.\nIn the 1980s, programmers wanted to be able to work with Unix themselves, but didn’t necessarily want to pay Bell Labs for Unix, so they started writing Unix-like operating systems. Unix-like OSes or Unix clones behaved just like Unix, but didn’t actually include any code from Unix itself.1\nIn 1991, Linus Torvalds – then a 21 year-old Finnish grad student – released Linux, an open source Unix clone via a amusingly nonchalant newsgroup posting.2\nSince then, Linux has seen tremendous adoption. A large majority of the world’s servers run on Linux.3 Along with most of the world’s servers, almost all of the world’s embedded computers – in ATMs, cars and planes, TVs, and most other gadgets and gizmos – run on Linux. If you have an Android phone or a Chromebook – that’s Linux. Basically all of the world’s supercomputers use Linux.\nAs you might imagine, running Linux in so many different places has necessitated the creation of many different kinds of Linux. For example, a full-featured Linux server is going to require a very different operating system than the barebones operating system running on an ATM with extremely modest computational power.\nThese different versions are called distributions (distros for short) of Linux. They have a variety of technical attributes and also different licensing models.\nSome versions of Linux, like Ubuntu, are completely open source. Others, like Red Hat Enterprise Linux (RHEL), are paid. Most paid Linux OSes have closely-related free and open source versions – like CentOS and Fedora for RHEL.4\nMany organizations have a standard Linux distro they use – most often RHEL/CentOS or Ubuntu. Increasingly, organizations deploying in AWS are using Amazon Linux, which is independently maintained by Amazon but was originally a RHEL derivative. There are also some organizations that use SUSE (pronunced soo-suh), which has both open source and enterprise versions."
  },
  {
    "objectID": "chapters/sec3/3-2-linux-admin.html#a-tiny-intro-to-linux-administration",
    "href": "chapters/sec3/3-2-linux-admin.html#a-tiny-intro-to-linux-administration",
    "title": "10  Basic Linux SysAdmin",
    "section": "10.2 A tiny intro to Linux administration",
    "text": "10.2 A tiny intro to Linux administration\nWe’ll get into how to administer a Linux server just below, but before we get there, let’s introduce what you’ll be doing as a Linux server admin. There are three main things you’ll manage as a Linux server admin:\n\nSystem resources Each server has a certain amount of resources available. In particular, you’ve got CPU, RAM, and storage. Keeping track of how much you’ve got of these things, how they’re being used, and making sure no one is gobbling up all the resources is an important part of system administration.\nNetworking Your server is only valuable if you and others can connect to it, so managing how your server can connect to the environment around it is an important part of Linux administration.\nPermissions Servers generally exist to allow a number of people to access the same machine. Creating users and groups and managing what they’re allowed to do them is a huge part of server administration.\nApplications Generally you want to do something with your server, so being able to interact with applications that are running, debug issues, and fix things that aren’t going well is an essential Linux admin skill.\n\nWhen you log into a Linux server, you’ll be interacting exclusively via the command line, so all of the commands in this chapter are going to be terminal commands. If you haven’t yet figured out how to open the terminal on your laptop and got it themed and customized so it’s perfect, I’d advise going back to Chapter 8 to get it all configured.5\n\n\n\n\n\n\nWindows, Mac, and Linux\n\n\n\nMacOS is based on BSD, a Unix clone, so any terminal commands you’ve used before will be very similar to Linux commands.\nWindows, on the other hand, is basically the only popular operating system that isn’t a Unix clone. Over time, the Windows command line has gotten more Unix like, so the differences aren’t as big as they used to be, but there will be some differences in the commands that work on Windows vs Linux. There will be some differences in the exact commands that work on Windows vs on Linux.\nThe most obvious difference is the types of slashes used in file paths. Unix-like systems use forward slashes / to denote file hierarchies, while Windows uses back slashes \\."
  },
  {
    "objectID": "chapters/sec3/3-2-linux-admin.html#managing-who-can-do-what",
    "href": "chapters/sec3/3-2-linux-admin.html#managing-who-can-do-what",
    "title": "10  Basic Linux SysAdmin",
    "section": "10.3 Managing who can do what",
    "text": "10.3 Managing who can do what\nWhenever you’re doing something in Linux, you’re doing that thing as a particular user.\nOn any Unix-like system, you can check your active user at any time with the whoami command. For example, here’s what it looks like on my MacBook.\n ❯ whoami                                                       \nalexkgold\nwhoami returns the username of my user.\nUsernames have to be unique on the system – but their not the true identifier for a Linux user. A user is uniquely (and permanently) identified by their user id (uid). All other attributes including username, password, home directory, groups, and more are malleable – but uid is forever.\nMany of the users on a Linux server correspond to actual humans. But there are more users than that. Most programs that run on a Linux server run as a service account that represent the set of permissions allowed to that program.\nFor example, installing RStudio Server will create a user with username rstudio-server. Then when rstudio-server goes to do something – start an R session for example – it will do so as rstudio-server.\n\n\n\n\n\n\nA few details on UIDs\n\n\n\nuids are just numbers from 0 to over 2,000,000,000. uids are assigned by the system at the time the user is created. You should probably keep uids below 2,000,000 or so if you happen to be assigning uids manually – some programs can’t deal with uids any bigger.\n10,000 is the the lowest uid that’s available for use by a user account. Everything below that is reserved for predefined system accounts or application accounts.\n\n\nIn addition to users, Linux has a notion of groups. A group is a collection of users. Each user has exactly one primary group and can be a member of secondary groups.6 By default, each user’s primary group is the same as their username.\nLike a user has a uid a group has a gid. User gids start at 100.\nYou can see a user’s username, uid, groups, and gid with the id command.\n ❯ id                                                                \nuid=501(alexkgold) gid=20(staff) groups=20(staff),12(everyone),61(localaccounts),79(_appserverusr),80(admin),81(_appserveradm),98(_lpadmin),701(com.apple.sharepoint.group.1),33(_appstore),100(_lpoperator),204(_developer),250(_analyticsusers),395(com.apple.access_ftp),398(com.apple.access_screensharing),400(com.apple.access_remote_ae)\nOn my laptop, I’m a member of a number of different groups.\nThere’s one extra special user – called the admin, root, sudo, or super user. They get the ultra-cool uid 0. That user has permission to do anything on the system. You almost never want to actually log in as the root user. Instead, you make users and add them to the admin or sudo group so that they have the ability to temporarily assume those admin powers.\nThe easiest way to make users is with the useradd command. Once you have a user, you may need to change the password, which you can do at any time with the passwd. Both useradd and passwd start interactive prompts, so you don’t need to do much more than run those commands.\n\n\n\n\n\n\n\nCommand\nWhat it does\n\n\n\n\nsu <username>\nChange to be a different user.\n\n\nwhoami\nGet username of current user.\n\n\nid\nGet full user + group info on current user.\n\n\npasswd\nChange password.\n\n\nuseradd\nAdd a new user.\n\n\n\n\n10.3.1 File Permissions\nEvery object in Linux is just a file. Every log – file. Every picture – file. Every program – file. Every system setting – file.\nSo determining whether a user can take a particular action is really a question of whether they have the right permissions on a particular file.\n\n\n\n\n\n\nNote\n\n\n\nThe question of who’s allowed to do what – authorization – is an extremely deep one. There’s a chapter all about authorization, how it differs from authentication, and the different ways your IT/Admins might want to manage it later in the book.\nThis is just going to be a high-level overview of basic Linux authorization.\n\n\nThere are three permissions you can have: read, write, and execute. Read means you’re allowed to see the contents of a file, write means you can save a changed version of a file, and execute means you’re allowed to run the file as a program.\nThe execute permission really only makes sense for some kinds of files - what would it mean to execute a csv file? But Linux doesn’t care – you can assign any combination of these three permissions for any file.\nHow are these permissions assigned? Every file has an owner and an owning group.\nSo you can think of permissions in Linux as being assigned in a 3x3 grid. The owner, the owning group, and everyone else can have permissions to read, write, or execute the file.\nTODO: change to graphic\n\n\n\n\nOwner\nGroup\nEveryone Else\n\n\n\n\nRead\n✅/❌\n✅/❌\n✅/❌\n\n\nWrite\n✅/❌\n✅/❌\n✅/❌\n\n\nExecute\n✅/❌\n✅/❌\n✅/❌\n\n\n\nTo understand better, let’s look at the permissions on an actual file.\nRunning ls -l on a directory gives you the list of files in that directory, along with their permissions. The first few columns of the list give you the full set of file permissions – though they can be a little tricky to read.\nSo, for example, here’s a few lines of the output of running ls -l on a python project I’ve got.\n❯ ls -l                                                           \n-rw-r--r--  1 alexkgold  staff     28 Oct 30 11:05 config.py\n-rw-r--r--  1 alexkgold  staff   2330 May  8  2017 credentials.json\n-rw-r--r--  1 alexkgold  staff   1083 May  8  2017 main.py\ndrwxr-xr-x 33 alexkgold  staff   1056 May 24 13:08 tests\nThis readout has a series of 10 characters: the file permissions, followed by a number, then the file’s owner, and the file’s group.7 Let’s learn how to read these.\nThe file’s owner and group are the easiest to understand. In this case, I alexkgold own all the files, and the group of all the files is staff.\nThe 10 character file permissions are relative to that user and group.\nThe first character indicates the type of file – - for normal and d for a directory.\nThe next 9 characters are indicators for the three permissions – r for read, a w for write, and a x for execute or - for not – first for the user, then the group, then any other user on the system.\nSo, for example, my config.py file with permissions of rw-r-r-- indicates the user (alexkgold) can read and write the file, and everyone else – including in the file’s group staff – has read only.\nIn the course of administering a server, you will probably need to change a file’s permissions. You can do so using the chmod command.\nFor chmod, you permissions are indicated with only 3 numbers – one for the user, group, and everyone else. The way this works is pretty clever – you just sum up the permissions as follows: 4 for read, 2 for write, and 1 for execute. You can check for yourself, but any set of permissions can be uniquely identified by a number between 1 and 7.8\nSo chmod 765 <filename> would give the user full permissions, read and write to the group, and read and execute to everyone else. This would be a strange set of permissions to give a file, but it’s a perfectly valid chmod command.\n\n\n\n\n\n\nNote\n\n\n\nIf you spend any time administering a Linux server, you almost certainly will at some point find yourself running into a problem and frustratedly applying chmod 777 to rule out a permissions issue.\nI can’t in good faith tell you not to do this – we’ve all been there. But if it’s something important, be sure you change it back once you’re finished figuring out what’s going on.\n\n\nIn some cases you might actually want to change the owner or group of a file. You can change users or groups with either names or ids. You can do so using the chown command. If you’re changing the group, the group name gets prefixed with a colon.\nIn some cases, you might not be the correct user to take a particular action. You might not want to change the file permissions, but instead to change who you are. In that case, you can switch users with the su command.\nSome actions are also reserved for the admin user. For example, let’s take a look at this configuration file:\n ❯ ls -l /etc/config/my-config                      \n-rw-r--r--  1 root  system  4954 Dec  2 06:37 config.conf\nAs you can see, all users can read this file to check the configuration settings. But this file is owned by root, and only the owner has write permissions. So I could run cat config.conf to see it. Or I could go into it with vim config.conf, but I’d find myself stuck if I wanted to make changes.\nSo if I want to change this configuration file, I’d need to temporarily assume my root powers to make changes. Instead of switching to be the root user, I would run sudo vim config.conf and open the file for editing with root permissions.\n\n\n\n\n\n\n\n\nCommand\nWhat it does\nHelpful options + notes\n\n\n\n\n\n\n\n\n\nchm o d <pe r m i s sions> <file>\nModifies permissions on a file.\nNumber indicates permissions for user, group, others: add 4 for read, 2 for write, 1 for execute, 0 for nothing.\n\n\nch o wn <u s e r / group> <file>\nChange the owner of a file.\nCan be used for user or group. , e.g. :my-group.\n\n\nsu <username>\nChange active user.\n\n\n\ns udo <command>\nAdopt super user permissions for the following command."
  },
  {
    "objectID": "chapters/sec3/3-2-linux-admin.html#installing-stuff",
    "href": "chapters/sec3/3-2-linux-admin.html#installing-stuff",
    "title": "10  Basic Linux SysAdmin",
    "section": "10.4 Installing Stuff",
    "text": "10.4 Installing Stuff\nThere are several different ways to install programs for Linux, and you’ll see a few of them throughout this book.\nJust as CRAN and PyPI are repositories for R and Python packages, Linux distros also have their own repositories. For Ubuntu, the apt command is used for accessing and installing .deb files from the Ubuntu repositories. For CentOS and RedHat, the yum command is used for installing .rpm files.\n\n\n\n\n\n\nNote\n\n\n\nThe examples below are all for Ubuntu, since that’s what we use in the lab for this book. Conceptually, using yum is very similar, though the exact commands differ somewhat.\n\n\nWhen you’re installing packages in Ubuntu, you’ll often see commands prefixed with apt-get update && apt-get upgrade y. This command makes your machine update the list of available packages it knows about on the server and upgrade everything to the latest version.\nPackages are installed with apt-get install <package>. Depending on which user you are, you may need to prefix the command with sudo.\nYou can also install packages that aren’t from the central package repository. Doing that will generally involve downloading a file directly from a URL – usually with wget and then installing it from the file you’ve downloaded – often with the gdebi command.\n\n\n\n\n\n\n\nCommand\nWhat it does\n\n\n\n\napt-g e t u p d ate && apt-get upgrade y\nFetch and install upgrades to system packages\n\n\na pt-get install <package>\nInstall a system package.\n\n\nwget\nDownload a file from a URL.\n\n\ngdebi\nInstall local .deb file."
  },
  {
    "objectID": "chapters/sec3/3-2-linux-admin.html#debugging-and-troubleshooting",
    "href": "chapters/sec3/3-2-linux-admin.html#debugging-and-troubleshooting",
    "title": "10  Basic Linux SysAdmin",
    "section": "10.5 Debugging and troubleshooting",
    "text": "10.5 Debugging and troubleshooting\nThere are three main resources you’ll need to manage as the server admin – CPU, RAM, and storage space. There’s more on all three of these and how to make sure you’ve got enough in Chapter 14.\nFor now, we’re just going to go over how to check how much you’ve got, how much you’re using, and getting rid of stuff that’s misbehaving.\n\n10.5.1 Storage\nA common culprit for weird server behavior is running out of storage space. There are two handy commands for monitoring the amount of storage you’ve got – du and df. These commands are almost always used with the -h flag to put file sizes in human-readable formats.\ndu, short for disk usage, gives you the size of individual files inside a directory. This can be helpful for finding your largest files or directories if you think you might need to clean up things. It’s particularly useful in combination with the sort command.\nFor example, here’s the result of running du on the chapters directory where the text files for this book live.\n ❯ du -h chapters | sort -h                                      \n 44K    chapters/sec2/images-servers\n124K    chapters/sec3/images-scaling\n156K    chapters/sec2/images\n428K    chapters/sec2/images-traffic\n656K    chapters/sec1/images-code-promotion\n664K    chapters/sec1/images-docker\n1.9M    chapters/sec1/images-repro\n3.4M    chapters/sec1\n3.9M    chapters/sec3/images-auth\n4.1M    chapters/sec3\n4.5M    chapters/sec2/images-networking\n5.3M    chapters/sec2\n 13M    chapters\nSo if I were thinking about cleaning up this directory, I could see that my images-networking directory in sec2 is the biggest single bottom-level directory. If you find yourself needing to find big files on your Linux server, it’s worth spending some time with the help pages for du. There are lots of really useful options.\ndu is useful for identifying large files and directories on a server. df, for disk free, is useful for diagnosing issues that might be a problem for a directory. If you’re struggling to write into a directory – perhaps getting out of space errors, df can help you diagnose.\ndf answers the question – given a file or directory, what device is it mounted on and how full is that device?\nSo here’s the result of running the df command on that same chapters directory.\n ❯ df -h chapters                                                    \nFilesystem     Size   Used  Avail Capacity iused      ifree %iused  Mounted on\n/dev/disk3s5  926Gi  163Gi  750Gi    18% 1205880 7863468480    0%   /System/Volumes/Data\nSo you can see that the chapters folder lives on a disk called /dev/disk3s5 that’s a little less than 1Tb and is 18% full – no problem. On a server this can be really useful to know, because it’s quite easy to switch a disk out for a bigger one in the same spot.\n\n\n\n\n\n\n\n\nCommand\nWhat it does\nHelpful options\n\n\n\n\ndu\nCheck size of files.\nMost likely to be used du  -h <dir> | sort -h 9\nAlso useful to combine with head.\n\n\ndf\nCheck storage space on device.\n-h\n\n\n\n\n\n10.5.2 Monitoring processes\nEvery program your computer runs is a process. For example, when you type python on the command line to open a REPL, that’s a process. Running more complicated programs usually involves more than one process.\nFor example, running RStudio involves (at minimum) one process for the IDE itself and one for the R session that it uses in the background. The relationships between these different processes is mostly hidden from you – the end user.\nAs a server admin, finding runaway processes, killing them, and figuring out how to prevent it from happening again is a pretty common task. Runaway processes usually misbehave by using up the entire CPU, filling up the entire machine’s RAM.\nLike users and groups have ids, each process has a numeric process id (pid). Each process also has an owner – this can be either a service account or a real user. If you’ve got a rogue process, the pattern is to try to find the process and make note of its pid. Then you can immediately end the process by pid with the kill command.\nSo, how do you find a troublesome process?\nThe top command is a good first stop. top shows the top CPU-consuming processes in real time. Here’s the top output from my machine as I write this sentence.\nPID    COMMAND      %CPU TIME     #TH    #WQ  #PORT MEM    PURG   CMPRS PGRP\n0      kernel_task  16.1 03:56:53 530/10 0    0     2272K  0B     0B    0\n16329  WindowServer 16.0 01:53:20 23     6    3717  941M-  16M+   124M  16329\n24484  iTerm2       11.3 00:38.20 5      2    266-  71M-   128K   18M-  24484\n29519  top          9.7  00:04.30 1/1    0    36    9729K  0B     0B    29519\n16795  Magnet       3.1  00:39.16 3      1    206   82M    0B     39M   16795\n16934  Arc          1.8  18:18.49 45     6    938   310M   144K   61M   16934\n16456  Messages     1.7  06:58.27 4      1    603   138M   2752K  63M   16456\n1      launchd      1.7  13:41.03 4/1    3/1  3394+ 29M    0B     6080K 1\n573    diagnosticd  1.4  04:31.97 3      2    49    2417K  0B     816K  573\n16459  zoom.us      1.3  66:38.37 30     3    2148  214M   384K   125M  16459\n16575  UniversalCon 1.3  01:15.89 2      1    131   12M    0B     2704K 16575\nIn most instances, the first three columns are the most useful. You’ve got the name of the command and how much CPU they’re using. Right now, nothing is using very much CPU. If I were to find something concerning – perhaps an R process that is using 500% of CPU – I would want to take notice of its pid to kill it with kill.\n\n\n\n\n\n\nSo much CPU?\n\n\n\nFor top (and most other commands), CPU is expressed as a percent of single core availability. So on a modern machine, it’s very common to see CPU totals well over 100%. Seeing a single process using over 100% of CPU is rarer.\n\n\nThe top command takes over your whole terminal. You can exit with Ctrl + c.\nAnother useful command for finding runaway processes is ps aux.10 It lists all processes currently running on the system, along with how much CPU and RAM they’re using. You can sort the output with the --sort flag and specify sorting my cpu with --sort -%cpu or by memory with --sort -%mem.\nBecause ps aux returns every running process on the system, you’ll probably want to pipe the output into head.\nAnother useful way to use ps aux is in combination with grep. If you pretty much know what the problem is – often this might be a runaway R or Python process – ps aux | grep <name> can be super useful to get the pid.\nFor example, here are the RStudio processes currently running on my system.\n > ps aux | grep \"RStudio\\|USER\"                                                                                      [10:21:18]\nUSER               PID  %CPU %MEM      VSZ    RSS   TT  STAT STARTED      TIME COMMAND\nalexkgold        23583   0.9  1.7 37513368 564880   ??  S    Sat09AM  17:15.27 /Applications/RStudio.app/Contents/MacOS/RStudio\nalexkgold        23605   0.5  0.4 36134976 150828   ??  S    Sat09AM   1:58.16 /Applications/RStudio.app/Contents/MacOS/rsession --config-file none --program-mode desktop \n\n\n\n\n\n\nTip\n\n\n\nThe grep command above looks a little weird because I used a little trick. I wanted to keep the header in the output, so the regex I used matches both a the header line (USER) and the thing I actually care about (RStudio).\n\n\n\n\n\n\n\n\n\n\nCommand\nWhat it does\nHelpful options\n\n\n\n\ntop\nSee what’s running on the system.\n\n\n\nps aux\nSee all system processes.\nConsider using --sort and pipe into head or grep\n\n\nkill\nKill a system process.\n-9 to force kill immediately\n\n\n\n\n\n10.5.3 Managing networking\nNetworking is a complicated topic, which we’ll approach with great detail in Chapter 11. For now, it’s important to be able to see what’s running on your server that is accessible from the outside world on a particular port.\nThe main command to help you see what ports are being used and by what services is the netstat command. netstat returns the services are running and the ports associated. netstat is generally most useful with the -tlp flags to show programs that are listening and the programs associated.\nTODO: get netstat example\n\n\n\n\n\n\n\n\nCommand\nWhat it does\nHelpful options\n\n\n\n\nnetstat\nSee ports and services using them.\nUsually used with -tlp\n\n\n\nSometimes you know you’ve got a service running on your machine, but you just can’t seem to get the networking working. It can be useful to access the service directly without having to deal with networking. You can do this with – port forwarding, also called tunneling.\nSSH port forwarding allows you to take the output of a port on a remote server, route it through SSH, and display it as if it were on a local port. For example, let’s say I’ve got RStudio Server running on my server. Maybe I don’t have networking set up yet, or I just can’t get it working. If I’ve got SSH to my server working properly, I can double check that the service is working as I expect and the issue really is somewhere in the network.\nI find that the syntax for port forwarding completely defies my memory and I have to google it every time I use it. For the kind of port forwarding you’ll use most often in debugging, you’ll use the -L flag.\nssh -L <local port>:<remote ip>:<remote port> <ssh hostname>\nWhen you’re doing ssh forwarding, local is the place you’re ssh-ed into (aka your server) and the remote is another location – usually your laptop.\nSince the “remote” is my laptop, I almost always want to use localhost as the remote IP, and I usually want to use the same port remotely and locally – unless the local service is on a reserved port.\nSo let’s say I’ve got RStudio Server running on my server at my-ds-workbench.com on port 3939. Then I could run ssh -L 3939:localhost:3939 my-user@my-ds-workbench.com. With this command, I can bypass networking and just access whatever is at port 3939 on my server (hopefully RStudio Workbench!) by just going to localhost:3939 in my laptop’s browser.\n\n\n10.5.4 Understanding PATHs\nLet’s say you want to open R on your command line. Once you’ve got everything properly configured, you can just type R and have it open right up.\n ❯ R                                                       \n\nR version 4.2.0 (2022-04-22) -- \"Vigorous Calisthenics\"\nCopyright (C) 2022 The R Foundation for Statistical Computing\nPlatform: x86_64-apple-darwin17.0 (64-bit)\n\nR is free software and comes with ABSOLUTELY NO WARRANTY.\nYou are welcome to redistribute it under certain conditions.\nType 'license()' or 'licence()' for distribution details.\n\n  Natural language support but running in an English locale\n\nR is a collaborative project with many contributors.\nType 'contributors()' for more information and\n'citation()' on how to cite R or R packages in publications.\n\nType 'demo()' for some demos, 'help()' for on-line help, or\n'help.start()' for an HTML browser interface to help.\nType 'q()' to quit R.\n\n>\nBut how does the operating system know what you mean when you type R? If you’ve been reading carefully, you’ve realized that running the command means opening a particular runnable file, and R isn’t a file path on my system.\nYou can actually just type the a complete filename of a runnable binary into your command line. For example, on my MacBook, my version of R is at /usr/bin/local/R, so I could open an R session by typing that full path. Sometimes it can be handy to be precise about exactly which executable you’re opening (looking at you, multiple versions of Python), so you may want to use full paths to executables.\nIf you ever want to check which actual executable is being used by a command, you can use the which command. For example, on my system this is the result of which R.\n ❯ which R                                                    \n/usr/local/bin/R\nMost of that time you don’t want to have to bother with full paths for executables. You want to just type R on the command line and have R open. Moreover, there are cases where functionality relies on another executable being able to find R and run it – think of running RStudio Server, which starts a version of R under the hood.\nThe operating system knows how to find the actual runnable programs on your system via something called the path. When you type R into the command line, it searches along the path to find a version of R it can run.\nYou can check your path at any time by just echoing the PATH environment variable with echo $PATH. On my MacBook, this is what the path looks like.\n ❯ echo $PATH                                                      \n/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/System/Cryptexes/App/usr/bin:/usr/bin:/bin:/usr/sbin:/sbin\nLater, when we get into running versions of programs that aren’t the system versions, we may have to append locations to the path so that we can run them easily."
  },
  {
    "objectID": "chapters/sec3/3-2-linux-admin.html#comprehension-questions",
    "href": "chapters/sec3/3-2-linux-admin.html#comprehension-questions",
    "title": "10  Basic Linux SysAdmin",
    "section": "10.6 Comprehension Questions",
    "text": "10.6 Comprehension Questions\n\nCreate a mind map of the following terms: Operating System, Windows, MacOS, Unix, Linux, Distro, Ubuntu\nWhen you initially SSH-ed into your server using ubuntu@$SERVER_ADDRESS, what user were you and what directory did you enter? What about when you used test_user@$SERVER_ADDRESS?\nWhat are the 3x3 options for Linux file permissions? How are they indicated in an ls -l command?\nHow would you do the following?\n\nFind and kill the process IDs for all running rstudio-server processes.\nFigure out which port JupyterHub is running on.\nCreate a file called secrets.txt, open it with vim, write something in, close and save it, and make it so that only you can read it."
  },
  {
    "objectID": "chapters/sec3/3-2-linux-admin.html#lab-a-working-data-science-workbench",
    "href": "chapters/sec3/3-2-linux-admin.html#lab-a-working-data-science-workbench",
    "title": "10  Basic Linux SysAdmin",
    "section": "10.7 Lab: A working Data Science Workbench",
    "text": "10.7 Lab: A working Data Science Workbench\nIn this lab, we’re going to take your server and take it from an empty server that you can access to one with users and useful software installed and running – even if it’s not yet accessible to the outside world.\n\n10.7.0.1 Step 1: Log on with the .pem key\nThe .pem key you downloaded when you set up the server is the skeleton key – it will automatically let you in with complete admin privleges. In Chapter 10, we’ll set up a user with SSH on the server with more limited permissions.\nIn the meantime, we’re going to use the .pem key to get started on the server, but be extremely careful with the power of the .pem key.\nBecause the keypair is so powerful, AWS requires that you restrict the access pretty severely (more on what that means in Chapter 10). If you try to use the keypair without first changing the permissions, you’ll be unable to and get a warning that looks something like:\n\n\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\n\n\\@ WARNING: UNPROTECTED PRIVATE KEY FILE! \\@\n\n\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\n\nPermissions 0644 for 'do4ds-lab-key.pem' are too open.\n\nIt is required that your private key files are NOT accessible by others.\n\nThis private key will be ignored.\n\nLoad key \"do4ds-lab-key.pem\": bad permissions\n\nubuntu\\@ec2-54-159-134-39.compute-1.amazonaws.com: Permission denied (publickey).\nBefore we can use it to open the server, we’ll need to make a quick change to the permissions on the key.\n[QUESTION – does this happen on Windows? If so, what are commands to alter permissions?]\nSo let’s change the file permissions.\nWe’ll get into the details of how to use these commands in just a minute. For now, you’ll need to open a terminal window, navigate to the directory where the key is and change the file permissions.\nOn my machine that looks like:\n$ cd ~/Documents/do4ds-lab\n$ chmod 600 do4ds-lab-key.pem\nYou can sub in the path to where your key is and the name you used for your key.\nIn your terminal type the following\n$ ssh -i do4ds-lab-key.pem ubuntu@$SERVER_ADDRESS\n\n10.7.0.1.1 SSH on Windows\nFor a long time, Windows didn’t come with a built in SSH client, so you had to use PuTTY to do SSH from a Windows machine. Microsoft brought a native SSH client to Windows 10 in 2015, and it has been enabled by default since 2018.\nIf you run into any trouble using SSH commands on Windows, double check that you’ve enabled the OpenSSH Client.\nType yes when prompted, and you’re now logged in to your server!\n\n\n\n10.7.1 Step 2: Create a non-root user\nThe first thing we’re going to do is create a user so that you can login without running as root all the time. In general, if you’ve got a multitenant server, you’re going to want users for each actual human who’s accessing the system.\nI’m going to use the username test-user. If you want to be able to copy/paste commands from the online book, I’d advise doing the same. If you were creating users based on real humans, I’d advise using their names.\nLet’s create a user using the adduser command. This will walk us through a set of prompts to create a new user with a home directory and a password. Feel free to add any information you want – or to leave it blank – when prompted.\nsudo adduser test-user\nWe want this new user to be able to adopt root privileges. Remember that the way that is determined is whether the user is part of the sudo group.\nsudo usermod -aG sudo test-user\nHopefully it’s reasonably intuitive that -aG stands for add to group.\n\n\n10.7.2 Step 3: Add an SSH Key for your user\nIf you’ve already got an SSH key you use, feel free to use that one. If you don’t have one, google how to create an SSH key and do it on your laptop.\nThe first step is going to be adding your public key to the end of the authorized_users file.\nFirst, you need to get your public key to the server. I might recommend using scp.11\nFor me, the command looks something like this\nscp -i do4ds-lab-key.pem \\ # scp w/ pem key\n  ~/.ssh/id_ed25519.pub \\ # local public key\n  ubuntu@ec2-54-159-134-39.compute-1.amazonaws.com:/home/ubuntu # where to on server\nNow our public key is on the server, but it’s in the ubuntu user’s home directory. We’re going to add it as an authorized key for the test-user so that we can SSH in as that user.\nHere are the commands to do so:\nssh -i \nubuntu@ip-172-31-2-42:~$ sudo mv /home/ubuntu/id_ed25519.pub /home/test-user/ # move key\nubuntu@ip-172-31-2-42:~$ sudo chown test-user /home/test-user/id_ed25519.pub # give it to test-user\nubuntu@ip-172-31-2-42:~$ su test-user #change user\nPassword:\ntest-user@ip-172-31-2-42:/home/ubuntu$ cd ~ #go to home dir\ntest-user@ip-172-31-2-42:~$ mkdir -p .ssh #create .ssh directory\ntest-user@ip-172-31-2-42:~$ chmod 700 .ssh # Lock directory from other users\ntest-user@ip-172-31-2-42:~$ cat id_ed25519.pub >> .ssh/authorized_keys #add public key to end of authorized_keys file\ntest-user@ip-172-31-2-42:~$ chmod 600 .ssh/authorized_keys #set permissions\nNow we’re all set up with SSH, and you can log in as a normal user from your laptop just using ssh test-user@$SERVER_ADDRESS.\nIf you want to set up an SSH config for this server, I’d advise waiting until we’ve got a permanent URL for it in the next chapter.\nNow that we’re all set up, you should store the pem key somewhere safe and never use it to log in again.\nWhen you ever want to exit SSH and get back to your machine, you can just type exit.\n\n\n10.7.3 Step 4: Install R and Python\nEverything until now has been generic server administration. Now let’s get into some data science specific work – setting up R and Python.\n\n\n\n\n\n\nTip\n\n\n\nIf you run into trouble assuming sudo with your new user, try exiting SSH and coming back. Sometimes these changes aren’t picked up until you restart the shell.\n\n\n\n10.7.3.1 Installing R\nThere are a number of ways to install R on your server including installing it from source, from the system repository, or using R-specific tooling.\nYou can use the system repository version of R, but then you just get whatever version of R happens to be current when you run sudo apt-get install R. My preferred option is to use rig, which is an R-specific installation manager.\n\n\n\n\n\n\nNote\n\n\n\nAs of this writing, rig only supports Ubuntu. If you want to install on a different Linux distro, you will have to install R a different way.\nPosit makes R binaries available for a variety of different operating systems, including Ubuntu and RedHat. You can find instructions at https://docs.posit.co/resources/install-r/.\nTODO: confirm that installing into /opt/R works for RStudio Server OSS.\n\n\nThere are good instructions on downloading rig and using it to install R on the GitHub repo – https://github.com/r-lib/rig.\nOnce you’ve got R installed on your server, you can check that it’s running by just typing R into the command line. If that works, you’re good to move on to the next step.\n\n\n\n10.7.4 Step 5: Installing RStudio Server\nOnce you’ve got R installed, let’s download and install RStudio Server. This should be a very easy process. The basic process is to install the gdebi package from apt, which is used for installing downloaded packages, download the RStudio Server package, and install it.\nI’m not going to reproduce the commands here because the RStudio Server version numbers change frequently and you probably want the most recent one.\nYou can find the exact commands on the Posit website at https://posit.co/download/rstudio-server/. Make sure to pick the version that matches your operating system. Since you’ve already installed R, you can skip down to actually installing the server.\nOnce you’ve installed, you can check the status with sudo systemctl status rstudio-server. If it says running, you’re good to go!\nBut knowing it’s good to go isn’t nearly as fun as actually trying it. We don’t have a stable public URL for the server yet, so we can’t just access it from our browser. This is a perfect use case for an SSH tunnel.\nBy default, RStudio Server is on port 8787, so we’ll tunnel port 8787 on the server to localhost:8787 on our laptop. The command to do that is ssh -L 8787:localhost:8787 test-user@$SERVER_ADDRESS.\nNow, if you go to localhost:8787 in your browser, you should be able to access RStudio Server and login with the username test-user and password you set on the server.\n\n\n10.7.5 Step 6: Installing JupyerHub + JupyterLab\nRStudio and R are system libraries. So when RStudio runs, it calls and owns the R process that you’ll use inside RStudio Server. In contrast, JupyterHub and JupyterLab are Python programs, so we install them inside a Python installation.\nPeople pretty much only ever install R to do data science. In contrast, Python is one of the world’s most popular programming languages for general purpose computing. Contrary to what you might think, this actually makes configuring Python harder than configuring R.\nThe reason is that your system comes with a version of Python installed, but we don’t want to use that version. Given the centrality of that version to normal server operations, we want to leave it alone. It turns out that installing one or more other versions of Python and then ignoring the system version of Python isn’t totally trivial to do. Then we’re going to want to create a standalone virtual environment that’s just for running JupyterHub so it doesn’t get messed up later.\nTODO: diagram of relationships of system python, DS python, Jupyter Python.\nIt’s very likely that the version of Python on your system is old. Generally we’re going to want to install a newer Python for doing data science work, so let’s start there. As of this writing, Python 3.10 is a relatively new version of Python, so we’ll install that one.\nLet’s start by actually installing Python 3.10 on our system. We can do that with apt.\nTODO: finish these instructions\n> sudo su\n> apt install python3.10-venv\nNow that we’ve installed Python, we can create a standalone virtual environment for running JupyterHub.\n> python3 -m venv /opt/jupyterhub\n> source /opt/jupyterhub/bin/activate\nNow we’re going to actually get JupyterHub up and running inside the virtual environment we just created. JupyterHub produces docs that you can use to get up and running very quickly. If you have to stop for any reason, make sure to come back, assume sudo, and start the JupyterHub virtual environment we created.\nHere were the installation steps that worked for me:\nnpm install -g configurable-http-proxy\napt-get install npm nodejs\npython3 -m pip install jupyterhub jupyterlap notebook\n\nln -s /opt/jupyterhub/bin/jupyterhub-singleuser /usr/local/bin/jupyterhub-singleuser # symlink in singleuser server, necessary because we're using virtual environment\n\njupyterhub\nIf all went well, you’ll now have JupyterHub up and running on port 8000!\nIf you want to confirm, tunnel in with ssh -L 8000:localhost:8000 test-user@$SERVER_ADDRESS.\n\n10.7.5.1 Running JupyterHub as a service\nAs I mentioned above, JupyterHub is a Python process, not a system process. This is ok, but it means that we’ve got to remember the command to start it if we have to restart it, and that it won’t auto restart if it were to fail for any reason.\nA program that runs in the background on a machine, starting automatically, and controlled by systemctl is called a daemon. Since we want JupyterHub to be a daemon, we’re got to add it as a system daemon, which isn’t hard.\nWe don’t need it right now, but it’ll be easier to manage JupyterHub later on from a config file that’s in /etc/jupyterhub.\nLet’s create a default config file and move it into the right place using\n> jupyterhub --generate-config\n> mkdir -p /etc/jupyterhub\n> mv jupyterhub_config.py /etc/jupyterhub\nNow we’ve got to daemon-ize JupterHub. There are two steps – create a file describing the service for the server’s daemon, and then start the service.\nTo start with, end the existing JupyterHub process. If you’ve still got that terminal open, you can do so with ctrl + c. If not, you can use your ps aux and grep skills to find and kill the JupyterHub processes.\nOn Ubuntu, adding a daemon file uses a tool called systemd and is really straightforward.\nFirst, add the following to /etc/systemd/system/jupyterhub.service. If you’re reading this book in hard copy, you can go to the online version to copy/paste or get this file on the book’s Git repo at TODO.\n\n\n/etc/systemd/system/jupyterhub.service\n\n[Unit]\nDescription=Jupyterhub\nAfter=syslog.target network.target\n\n[Service]\nUser=root\nEnvironment=\"PATH=/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/opt/jupyterhub/bin\"\nExecStart=/opt/jupyterhub/bin/jupyterhub -f /etc/jupyterhub/jupyterhub_config.py\n\n[Install]\nWantedBy=multi-user.target\n\nHopefully this file is pretty easy to parse. Two things to notice – the Environment line adds /opt/jupyterhub/bin to the path – that’s where our virtual environment is.\nSecond, the ExecStart line is the startup command and includes our -f /etc/jupyterhub/jupyterhub_config.py – this is the command to start JupyterHub with the config we created a few seconds ago.\nNow we just need to reload the daemon tool so it picks up the new service it has available and start JupyterHub!\nsystemctl daemon-reload\nsystemctl start jupyterhub\nYou should now be able to see that JupyterHub is running using systemctl status jupyterhub and can see it again by tunneling to it.\nTo set JupyterHub to automatically restart when the server restarts, run systemctl enable jupyterhub.\n\n\n\n10.7.6 Step 7: Running a Plumber API in a Container\nIn addition to running a development workbench on your server, you might want to run a data science project. If you’re running a Shiny app, Shiny Server is easy to configure along the lines of RStudio Server.\nHowever, if you put an API in a container like we did in Chapter 5, you might want to just deploy that somewhere on your server as a running container.\nThe first step is just to install docker on your system with sudo apt-get install docker.io. You can check that you can run docker with docker ps. You may need to adopt sudo privileges to do so.\nOnce we’ve got docker installed, getting the API running is almost trivially easy using the command we used back in Chapter 5 to run our container.\ndocker run --rm -d \\\n  -p 8555:8000 \\\n  --name palmer-plumber \\\n  alexkgold/plumber\nThe one change you might note is that I’ve changed the port on the server to be 8555, since we’ve already got JupyterHub running on 8000.\nNow, we can SSH tunnel into our server and view our running API at localhost:8555/__docs__/.\nThis was easy to get something up and running quickly. But you should notice that this isn’t daemonized – if we restart the server or the container dies for any reason, it won’t auto-restart.\nIt’s not generally a best practice to daemon-ize a docker container by just putting the run command into systemd. Instead, you should use a container management system like Docker Compose or Kubernetes that are designed specifically to manage running containers. Getting deeper into those is beyond the scope of this book.\n\n\n10.7.7 Questions for Alex\nWe didn’t actually make use of the EBS volume we mounted for home dirs. Should we do that? https://www.tecmint.com/move-home-directory-to-new-partition-disk-in-linux/\n\n\n10.7.8"
  },
  {
    "objectID": "chapters/sec3/3-3-networking.html#tcpip-and-the-mail",
    "href": "chapters/sec3/3-3-networking.html#tcpip-and-the-mail",
    "title": "11  Intro to Computer Networks",
    "section": "11.1 TCP/IP and the mail",
    "text": "11.1 TCP/IP and the mail\nLet’s start by imagining you have a penpal, who lives in an apartment building across the country.1\nLet’s say you’ve got some juicy gossip to share with your penpal. Because this gossip is so juicy, you’re not going to put it on a postcard. You’d write your letter, put it inside an envelope, address the letter to the right spot, and put it in the mail.\nYou trust the postal service to take your letter, deliver it to the correct address, and then your friend will be able to read your letter.\nThe process of actualy getting data from one computer to another is governed by the TCP/IP protocol and is called packet switching.2\nWhen a computer has data to send to another computer, it takes that information and packages it up into a bundle called a packet.3\nThe data in the packet is like the contents of your letter.\nJust as the postal service defines permissible envelope sizes and address formats, the TCP/IP protocol defines what a packet has to look like from the outside, including what constitutes a valid address.\nA uniform resource locator (URL) is the way to address a specific resource on a network.\nA full URL includes 4 pieces of information: \\[\n\\overbrace{\\text{https://}}^\\text{protocol}\\overbrace{\\text{example.com}}^\\text{address}\\overbrace{\\text{:443}}^\\text{port}\\overbrace{\\text{/}}^\\text{resource}\n\\]\nThe protocol starts the URL. It is separated from the rest of the URL by ://.\nEach of the rest of the pieces of the URL is needed to get to a specific resource and has a real-world analog.\nThe address is like the street address of your penpal’s building. It specifies the host where your data should go.4\nA host is any entity on a network that can receive requests and send responses. So a server is a common type of host on a network, but so is a router, your laptop, and a printer.\nThe port is like your friend’s apartment. It specifies which service on the server to address.\nLastly, the resource dictates what resource you want on the server. It’s like the name on the address of the letter, indicating that you’re writing to your penpal, not their mom or sister.\nThis full URL may look a little strange to you. You’re probably used to just putting a standalone domain like \\(google.com\\) into your browser when you want to go to a website. That’s because https, port 443, and / are all defaults, so modern browsers don’t make you specify them.\nBut what if you make it explicit? Try going to https://google.com:443/. What do you get?"
  },
  {
    "objectID": "chapters/sec3/3-3-networking.html#ports-get-you-to-the-right-service",
    "href": "chapters/sec3/3-3-networking.html#ports-get-you-to-the-right-service",
    "title": "11  Intro to Computer Networks",
    "section": "11.2 Ports get you to the right service",
    "text": "11.2 Ports get you to the right service\nA port is a location on a server where a network connection is possible. It’s like the apartment number for the mail. Each port has a unique number 1 to just over 65,000. By default, the overwhelming majority of the ports on the server are closed for security reasons.\nYour computer automatically opens ports to make outgoing connections, but if you want to allow someone to make inbound connections – like to access RStudio or a Shiny app on a server – you need to manually configure and open a port.\nAny program that is running on a server and that you intend to be accessible from the outside is called a service. For example, we set up RStudio, JupyterHub, and a Plumber API as services in the lab in Chapter 10. Each service lives on a unique port on the server.\nSince each service running on a server needs a unique port, it’s common to choose a somewhat random relatively high-numbered port. That makes sure it’s unlikely that the port will conflict with another service running on the server.\nFor example RStudio Server runs on port 8787 by default. According to JJ Allaire, there’s no special meaning to this port. It was chosen because it was easy to remember and not 8888, which some other popular projects had taken.\n\n11.2.1 Special Port Cheatsheet\nAll ports below 1024 are reserved for common server tasks, so you can’t assign services to low-numbered ports.\nThere are also three common ports that will come up over and over. These are handy because if you’re using the relevant service, you don’t have to indicate if it’s using the default port.\n\n\n\nProtocol\nDefault Port\n\n\n\n\nHTTP\n80\n\n\nHTTPS\n443\n\n\nSSH\n22"
  },
  {
    "objectID": "chapters/sec3/3-3-networking.html#assigning-addresses",
    "href": "chapters/sec3/3-3-networking.html#assigning-addresses",
    "title": "11  Intro to Computer Networks",
    "section": "11.3 Assigning addresses",
    "text": "11.3 Assigning addresses\nThe proper address of a server on a network is defined by the Internet Protocol (IP) and is called an IP Address. IP addresses are mapped to human-friendly domains like \\(google.com\\) with the Domain Name Service (DNS).\n\n\n\n\n\n\nNote\n\n\n\nIn this chapter, we’re going to set DNS aside and talk exclusively about IP Addresses.\n?sec-dns-ssl is all about DNS.\n\n\nEvery host on the internet has a public IP address.\nA public IP address is an IP address that is valid across the entire internet. That means that each public IP address is unique across the entire internet. You can think of a public IP address like the public street address of a building.\nBut many hosts are not publicly accessible on the internet. Many are housed in private networks. A private network is one where the hosts aren’t directly accessible to the public. You can think of a host in a private network like a building inside a gated community. You may still be able to get there from the public, but you can’t just walk up to the building from the street. Instead you need to come in through the specific gates that have been permitted and approach only on the roads that are allowed.\nTODO: Image of public IPs like street address, private like cul-de-sac\nThere are many different kinds of private networks. Some are small and enforced by connection to a physical endpoint, like the private network your WiFi router controls that houses your laptop, phone, TV, Xbox, and anything else you allow to connect to your router. In other cases, the network is a software network. Many organizations have virtual private networks (VPNs). In this case, you connect to the network via software. There may be resources you can only connect to inside your VPN and there also might be limitations about what you can go out and get.\nThere are a few different reasons for this public/private network split. The first is security. If you’ve got a public IP address, anyone on the internet can come knock on your virtual front door. That’s actually not so bad. What’s more problematic is that they can go all the way around the building looking for an unlocked side door. Putting your host inside a private network is a way to ensure that people can only approach the host through on the pathways you intend.\nThe second reason is convenience.\nPrivate networks provide a nice layer of abstraction for network addresses.\nYou probably have a variety of network-enabled devices in your home, from your laptop and phone to your TV, Xbox, washing machine, and smart locks. But from the outside, your house has only one public IP address – the address of the router in your home. That means that your router has to keep track of all the devices you’ve got, but they don’t need to register with any sort of public service just to be able to use the internet.\nAs we’ll get into in Chapter 12, keeping track of IP Addresses is best left to machines. If you’re managing a complex private network, it’s really nice to give hostnames to individual hosts. A nice feature of a private hostname compared to a public address is that you don’t have to worry if the hostname is unique across the entire internet – it just has to be unique inside your private network.\n\n11.3.1 Firewalls, allow-lists, and other security settings\nOne of the most basic ways to keep a server safe is to not allow traffic from the outside to come in. Generally that means that in addition to keeping the server ports themselves closed, you’ll also have a firewall up that defaults to all ports being closed.\nIn AWS, the basic level of protection for your server is called the security group. If you remember, we used a default security group in launching your server. When you want to go add more services to your server, you’ll have to open additional ports both on the server and in the security group.\nIn addition to keeping particular ports closed, you can also set your server to only allow incoming traffic from certain IP addresses. This is generally a very coarse way to do security, and rather fragile. For example, you could configure your server to only accept incoming requests from your office’s IP address, but what if someone needs to access the server from home or the office’s IP address is reassigned?\nOne thing that is not a security setting is just using a port that’s hard to guess. For example, you might think, “Well, if I were to put SSH on port 2943 instead of 22, that would be safer because it’s harder to guess!” I guess so, but it’s really just an illusion of security. There are ways to make your server safer. Choosing esoteric port numbers really isn’t it."
  },
  {
    "objectID": "chapters/sec3/3-3-networking.html#how-packets-are-routed",
    "href": "chapters/sec3/3-3-networking.html#how-packets-are-routed",
    "title": "11  Intro to Computer Networks",
    "section": "11.4 How packets are routed",
    "text": "11.4 How packets are routed\nThe way packets travel from one computer to another is called routing. A router is a hardware or software device that route packets.\nYou can think of routers and public networks as existing in trees. Each router knows about the IP addresses downstream of it and also the single upstream default address.5\nTODO: diagram of routers in trees\nFor example, the router in your house just keeps track of the actual devices that are attached to it. So if you were to print something from your laptop, the data would just go to your router and then to your printer.\nOn the other hand, when you look at a picture on Instagram, that traffic has to go over the public network. The default address for your home’s router is probably one owned by your internet service provider (ISP) for your neighborhood. And that router’s default address is probably also owned by your ISP, but for a broader network.\nSo your packet will get passed upstream to a sufficiently general network and then back downstream to the actual address you’re trying to reach.\nMeanwhile, your computer just waits for a response. Once the server has a response to send, it comes back using the same technique. Obviously a huge difference between sending a letter to a penpal and using a computer network is the speed. Where sending a physical letter takes a minimum of days, sending and receiving packets over a network is so fast that the delay is imperceptible when you’re playing a multiplayer video game or collaborating on a document online."
  },
  {
    "objectID": "chapters/sec3/3-3-networking.html#how-to-recognize-an-ip-address",
    "href": "chapters/sec3/3-3-networking.html#how-to-recognize-an-ip-address",
    "title": "11  Intro to Computer Networks",
    "section": "11.5 How to recognize an IP address",
    "text": "11.5 How to recognize an IP address\nYou’ve probably seen IPv4 addresses many times. They’re four blocks of 8-bit fields (numbers between 0 and 255) with dots in between, so they look something like 65.77.154.233.\nIf you do the math, you’ll realize there are “only” about 4 billion of these. While we can stretch those 4 billion IP addresses to many more devices since most devices only have private IPs, we are indeed running out of public IPv4 addresses.\nThe good news is that smart people started planning for this a while ago. In the last few years, adoption of the new IPv6 standard has started. IPv6 addresses are eight blocks of hexadecimal (0-9 + a-f) digits separated by colons, with certain rules that allow them to be shortened, so 4b01:0db8:85a3:0000:0000:8a2e:0370:7334 or 3da4:66a::1 are both examples of valid IPv6 addresses.\nIPv6 will coexist with IPv4 for a few decades and we’ll eventually switch entirely to IPv6. There’s no worry about running out of IPv6 addresses any time soon, because the total quantity of IPv6 addresses is a number with 39 zeroes.\n\n11.5.1 Reserved IP Addresses\nMost IPv4 addresses are freely available to be assigned, but there are a few that you’ll see in particular contexts and it’s useful to know what they are.\nThe first IP address you’ll see a lot is 127.0.0.1, also known as localhost or loopback. This is the way a machine refers to itself.\nFor example, if you open a Shiny app in RStudio Desktop, the app will pop up in a little window along with a notice that says\nListening on http://127.0.0.1:6311\nThat http://127.0.0.1 is indicating that your computer is serving the Shiny app to itself on the localhost address.\nThere are also a few blocks of addresses that are reserved for use on private networks, so they’re never assigned in public.\n\n\n\n\n\n\n\nCode\nMeaning\n\n\n\n\n127.0.0.1\nlocalhost or loopback – the machine that originated the request\n\n\n192.168.x.x\n172.16.x.x.x\n10.x.x.x\nProtected address blocks used for private IP addresses.\n\n\n\nYou don’t really need to remember these, but it’s very likely you’ve seen an address like 192.168.0.1 or 192.168.1.1 if you’ve ever tried to configure a router or modem for your home wifi.\nNow you know why."
  },
  {
    "objectID": "chapters/sec3/3-3-networking.html#basic-network-troubleshooting",
    "href": "chapters/sec3/3-3-networking.html#basic-network-troubleshooting",
    "title": "11  Intro to Computer Networks",
    "section": "11.6 Basic network troubleshooting",
    "text": "11.6 Basic network troubleshooting\nNetworking can be difficult to manage because there are so many layers where it can go awry. Let’s say you’ve configured a service on your server, but you just can’t seem to access it.\nThe ping command can be useful for checking whether your server is reachable on the network. For example, here’s what happens when I ping the domain where this book sits.\n> ping -o do4ds.com                                                                        \nPING do4ds.com (185.199.110.153): 56 data bytes\n64 bytes from 185.199.110.153: icmp_seq=0 ttl=57 time=13.766 ms\n\n--- do4ds.com ping statistics ---\n1 packets transmitted, 1 packets received, 0.0% packet loss\nround-trip min/avg/max/stddev = 13.766/13.766/13.766/0.000 ms\nThis looks great – it sent 1 packet to the server and got one back. That’s exactly what I want. Seeing an unreachable host or packet loss would be an indication that my networking probably isn’t configured correctly somewhere between me and the server. I generally like to use ping with the -o option for sending just one packet – as opposed to continuously trying.\nIf ping fails, it means that my server isn’t reachable. The things I’d want to check is that I have the URL for the server correct, that DNS is configured correctly (see Chapter 12), that I’ve correctly configured any firewalls to have the right ports open (Security Groups in AWS), and that any intermediate networking devices are properly configured (see more on proxies in Chapter 16).\nIf ping succeeds but I still can’t access the server, curl is good to check. curl actually attempts to fetch the website at a particular URL. It’s often useful to use curl with the -I option so it just returns a simple status report, not the full contents of what it finds there.\nFor example, here’s what I get when I curl CRAN from my machine.\n > curl -I https://cran.r-project.org/                                                         \n \nHTTP/1.1 200 OK\nDate: Sun, 15 Jan 2023 15:34:19 GMT\nServer: Apache\nLast-Modified: Mon, 14 Nov 2022 17:33:06 GMT\nETag: \"35a-5ed71a1e393e7\"\nAccept-Ranges: bytes\nContent-Length: 858\nVary: Accept-Encoding\nContent-Type: text/html\nThe important thing here is that first line. The server is returning a 200 HTTP status code, which means all is well. For more on HTTP status codes and how to interpret them, see ?sec-apis.\nIf ping succeeds, but curl does not, it means that the server is up at the expected IP address, but the service is not accessible. At that point, you might check whether the right ports are accessible – it’s possible to (for example) have port 443 or 80 accessible on your server, but not the port you actually need for your service. You also might check on the server itself that the service is running and that it is running on the port you think it is.\nIf you’re running inside a container, you should check that you’ve properly configured the port inside container to be forwarded to the outside."
  },
  {
    "objectID": "chapters/sec3/3-3-networking.html#comprehension-questions",
    "href": "chapters/sec3/3-3-networking.html#comprehension-questions",
    "title": "11  Intro to Computer Networks",
    "section": "11.7 Comprehension Questions",
    "text": "11.7 Comprehension Questions\n\nWhat are the 4 components of a URL? What’s the significance of each?\nWhat are the two things a router keeps track of? How does it use each of them?\nAre there any inherent differences between public and private IP addresses?\nWhat is the difference between an IP address and a port?\nLet’s say you’ve got a server at 54.33.115.12. Draw a mind map of what happens when you try to SSH into the server. Your explanation should include the terms: IP Address, port, 22, default address, router, sever."
  },
  {
    "objectID": "chapters/sec3/3-3-networking.html#lab-making-it-accessible-in-one-place",
    "href": "chapters/sec3/3-3-networking.html#lab-making-it-accessible-in-one-place",
    "title": "11  Intro to Computer Networks",
    "section": "11.8 Lab: Making it accessible in one place",
    "text": "11.8 Lab: Making it accessible in one place\n\n\n\n\n\n\nNote\n\n\n\nThis lab is somewhat advanced. If you just wanted to run one service on your server – say just RStudio or just the Plumber API, you could skip this lab and just configure the service to be available on port 80 and/or 443.\n\n\nIn this lab, we’re going to go from having to SSH tunnel to be able to use your data science workbench to making it available over the internet. That means we’re going to have to do 2 things: configure the networking to allow HTTP traffic and configure a real domain for our server.\nRight now RStudio Server is ready to serve traffic inside our server on port 8787, JupyterHub is on 8000, and our Palmer Penguins API is on 8555. But right now nothing can get to them. So the first step is to allow traffic in.\nThe easiest thing we could do would be to just open up ports 8787 and 8000 to the world. It’s not the right answer, but it will “work”.\n\n\n\n\n\n\nTip\n\n\n\nIf you do want to try it to prove to yourself, go to the settings for your server’s security group and just add a custom TCP rule allowing access to ports 8787, 8000, and 8555 from anywhere. If you visit $SERVER_ADDRESS:8787 you should get RStudio, similarly with JupyterHub at $SERVER_ADDRESS:8000, and the API at $SERVER_ADDRESS:8555.\nOk, now close those ports back up so we can do this the right way.\n\n\nIf you did it this way, your users would have to remember these arbitrary ports to use your server. It’s also insecure because there isn’t a good way to secure these ports with SSL.6\nThe common solution to wanting to serve multiple services off of one server is to use a proxy. A proxy is a network device that reroutes traffic in various ways. Chapter 16 has more detail on proxies and why they’re a pain.\nFor the purpose of this lab, I’m going to give you a pre-built proxy to use.\nHere are the steps to take on your server:\n\nInstall nginx with sudo apt install nginx.\nSave a backup of nginx.conf, cp /etc/nginx/nginx.conf /etc/nginx/nginx-backup.conf.7\nEdit the nginx configuration with sudo vim /etc/nginx/nginx.conf and replace it with:\n\n\n\n/etc/nginx/nginx.conf\n\nhttp {\n  \n  \\# Enable websockets (needed for Shiny)\n  map \\$http_upgrade \\$connection_upgrade { \n    default upgrade; '' close; \n  }\n  \n  server { listen 80;\n    \n    location /rstudio/ {\n      # Needed only for a custom path prefix of /rstudio\n      rewrite ^/rstudio/(.*)$ /$1 break;\n      \n      # Use http here when ssl-enabled=0 is set in rserver.conf\n      proxy_pass http://localhost:8787;\n      \n      proxy_http_version 1.1;\n      proxy_set_header Upgrade $http_upgrade;\n      proxy_set_header Connection $connection_upgrade;\n      proxy_read_timeout 20d;\n      \n      # Not needed if www-root-path is set in rserver.conf\n      proxy_set_header X-RStudio-Root-Path /rstudio;\n      \n      # Optionally, use an explicit hostname and omit the port if using 80/443\n      proxy_set_header Host $host:$server_port;\n    }\n    \n    location /jupyter/ {\n      # NOTE important to also set base url of jupyterhub to /jupyter in its config\n      proxy_pass http://127.0.0a.1:8000;\n      \n      proxy_redirect   off;\n      proxy_set_header X-Real-IP $remote_addr;\n      proxy_set_header Host $host;\n      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n      proxy_set_header X-Forwarded-Proto $scheme;\n      \n      # websocket headers\n      proxy_set_header Upgrade $http_upgrade;\n      proxy_set_header Connection $connection_upgrade;\n    }\n    \n    location /palmer/ {\n    #TODO\n    }\n  }\n\n\nTest that your configuration is valid sudo nginx -t.\nStart nginx sudo systemctl start nginx. If you see nothing all is well.\n\nIf you need to change anything, update the config and then restart with sudo systemctl restart nginx.\nThere’s one more thing you’ll have to do, which is to let RStudio and JupyterHub know that they’re on a subpath. Complex web apps like RStudio and JupyterHub frequently send people to a different subpath internally. In order to do so properly, they need to know to prepend all of those requests with the subpath.\nRStudio accepts a header from the proxy that lets it know what path it’s on (see the X-RStudio-Root-Path header line in the nginx config). Jupyter needs to be explicitly told.\nIf you recall, we autogenerated the JupyterHub config with\n jupyterhub --generate-config\n sudo mkdir /etc/jupyterhub\n sudo mv jupyterhub_config.py /etc/jupyterhub\nSo you can go edit it with sudo vim /etc/jupyterhub/jupyterhub_config.py.\nFind the line that reads # c.JupyterHub.bind_url = 'http://:8000'.\n\n\n\n\n\n\nTip\n\n\n\nYou can search in vim from normal mode with / <thing you're searching for>. Go to the next hit with n.\n\n\nUncomment that line and add a /jupyter on the end, so it reads c.JupyterHub.bind_url = 'http://:8000/jupyter'.\nStart jupyterhub with a specific config file.\nWe already daemonized JupyterHub, you restarting it with sudo systemctl restart jupyterhub should pick up the new config."
  },
  {
    "objectID": "chapters/sec3/3-4-dns.html#how-dns-lookups-work",
    "href": "chapters/sec3/3-4-dns.html#how-dns-lookups-work",
    "title": "12  DNS allows for human-readable addresses",
    "section": "12.1 How DNS lookups work",
    "text": "12.1 How DNS lookups work\nIn Chapter 11, we discussed how packets gets routed to successively higher-level routers until there’s one that knows where the packet is going and sends it back down. It turns out that this process essentially happens twice. The first time is to get the IP address for the domain in a process called DNS resolution. The second time is when it actually sends the information to that IP address.\nYou can see the basic structure of a DNS lookup using the terminal command nslookup. Here’s the DNS routing for \\(google.com\\):\n ❯ nslookup google.com                                              \nServer:     192.168.86.1\nAddress:    192.168.86.1#53\n\nNon-authoritative answer:\nName:   google.com\nAddress: 142.251.163.113\nName:   google.com\nAddress: 142.251.163.100\nName:   google.com\nAddress: 142.251.163.138\nName:   google.com\nAddress: 142.251.163.101\nName:   google.com\nAddress: 142.251.163.102\nName:   google.com\nAddress: 142.251.163.139\nThese addresses do change periodically, so they may no longer be valid by the time you read this. But if you want to go do an nslookup yourself, you can visit one of those IP addresses and see that it takes you right to the familiar \\(google.com\\).1\nDNS resolution is actually quite a complex process because every computer in the world needs to be able to resolve a DNS entry to the same IP address in a timely fashion. The simplest form of DNS lookup would be easy – there would just be one nameserver with the top-level domains and then one server for each top-level domain.\nThere are two problems with this theoretical DNS lookup – it wouldn’t be very resilient because of a lack of redundancy and it would be slow. In order to speed things up, the DNS system is highly decentralized.\nAlong with decentralization, DNS resolution relies on a lot of cacheing. When your computer or an intermediate DNS server looks up an IP address for you, it caches it. This is because its likely that if you’ve looked up a domain once, you’re going to do it again soon.\nThis is great if you are using the internet and don’t want to wait for DNS lookups, but when you’re changing the domains on servers you control, there are thousands of public DNS servers that a request could get routed to, and many of them may have outdated cache entries. DNS changes can take up to 24 hours to propagate.\nThat means that if you make a change and it’s not working, you have no idea whether you made a mistake or it just hasn’t propagated yet. It’s very annoying.\nSometimes, using a private browsing window will cause DNS cache refreshes, but not always."
  },
  {
    "objectID": "chapters/sec3/3-4-dns.html#how-dns-is-configured",
    "href": "chapters/sec3/3-4-dns.html#how-dns-is-configured",
    "title": "12  DNS allows for human-readable addresses",
    "section": "12.2 How DNS is configured",
    "text": "12.2 How DNS is configured\nFrom the perspective of someone trying to set up their own website, there’s only one DNS server that matters to you personally – the DNS server for your domain name registrar.\nDomain name registrars are the companies that actually own domains. You can buy or rent one from them in order to have a domain on the internet.\nYour first stop would be a domain name registrar where you’d find an available domain you like and pull out your credit card.\nCosts for domain names vary widely. Buying a meaningless domain in a less popular top-level domain, say ladskfmlsdf.me can cost as little as $3 per year. On the other hand, buying a .com domain that’s a real word or phrase can be a few thousand dollars. There are articles every few years about some major company accidentally allowing their domain name to lapse and ransoming it back for stupid amounts of money.\nConfiguration of DNS is done by way of records, of which there are a menagerie of types you can configure. Luckily, most simple configurations only need CNAME and A records.\nHere’s an imaginary DNS record table for the domain example.com:\n\n\n\nPath/Host\nType\nTarget\n\n\n\n\n@\nA\n143.122.8.32\n\n\nwww\nCNAME\nexample.com\n\n\n*\nA\n143.122.8.33\n\n\n\nLet’s go through how to read this table.\nSince we’re configuring example.com, the paths/hosts in this table are relative to example.com.\nIn the first row we’re configuring an A record to go to the target IP address. A records (or their IPv6 cousin AAAA records) map a domain to an actual IP address. The path @ is a special symbol meaning exact match. So by this configuration, any traffic to example.com will be passed straight through to the specified IP address.\nThe second row deals with traffic to the www subdomain. CNAME records alias sub-domains. They’re most frequently used to map sub-domains to the main domain. Since this is a CNAME record for example.com, this record indicates that traffic to www.example.com should be treated exactly like traffic to example.com. Some domain providers do automatic redirection of www traffic, and so this row may not be necessary in some configurations.\nThe last record uses the wildcard symbol * to send all subdomain traffic that’s not already spoken for – say blog.example.com or info.example.com directly to the IP address specified. In this case, I’m sending all of those subdomains to a different IP address, maybe a 404 (not found) page – or maybe I’m serving all the subdomains off a different server.\nSo what happens is that your query goes through several layers of public DNS servers to get to the DNS entry for your domain name registrar. In many cases, you’ll directly configure your domain name registrar to point to your website or server – but you also can configure the domain name registrar to point at another set of DNS servers you actually control with an NS record.\nIf you’re setting up your own server, this probably isn’t the case, but some large enterprises do run their own private DNS servers."
  },
  {
    "objectID": "chapters/sec3/3-4-dns.html#comprehension-questions",
    "href": "chapters/sec3/3-4-dns.html#comprehension-questions",
    "title": "12  DNS allows for human-readable addresses",
    "section": "12.3 Comprehension Questions",
    "text": "12.3 Comprehension Questions\nTODO"
  },
  {
    "objectID": "chapters/sec3/3-4-dns.html#lab-configuring-dns-for-your-server",
    "href": "chapters/sec3/3-4-dns.html#lab-configuring-dns-for-your-server",
    "title": "12  DNS allows for human-readable addresses",
    "section": "12.4 Lab: Configuring DNS for your server",
    "text": "12.4 Lab: Configuring DNS for your server\n\n\n\n\n\n\nWarning\n\n\n\nIn Chapter 13, we’ll secure your server with an SSL certificate and https. You absolutely should not use your server for anything real until you’ve configured SSL.\n\n\n\n12.4.1 Step 1: Allocate Elastic IP\nIf you’ve restarted your server, you’ve probably noticed that the IP address changed!\nThis can be a real pain if you have to reconfigure your DNS every time. AWS offers a service called Elastic IP that gives you an IP address that won’t change that you can use with your instance, even if you restart it.\nAs of this writing, Elastic IPs are free as long as it’s associated with a single running EC2 instance. You get charged if they’re not active – basically AWS doesn’t want you hoarding Elastic IPs. Great – just make sure to give back the elastic IP if you take down your instance.\n\n\n\n\n\n\nNote\n\n\n\nIf you were doing this for real, and not in a somewhat arbitrary order that’s grouped by topic, it would’ve made sense to set up the elastic IP as soon as you brought the server up so you only would’ve used that IP address.\n\n\nGo to Elastic IP and click allocate. Once it has been allocated, you’ll need to associate it with the existing Instance you created. Choose the instance and the default private IP address.\nNote that once you make this change, your server will no longer be available at its old IP address, so you’ll have to ssh in at the new one. If you have SSH terminals open when you make the change, they will break.\n\n\n12.4.2 Step 2: Buy a domain\nThis part will not be free, but it can be very cheap. The easiest way to do this is via AWS’s Route53 service. You can get domains on Route53 for as little as $9 per year – but there are even cheaper services. For example, I was able to get the domain \\(do4ds-lab.shop\\) for $1.98 for a year on \\(namecheap.com\\).\n\n\n12.4.3 Step 3: Configure DNS\nOnce you’ve got your domain, you have to configure your DNS. You’ll have to create 2 A records – one each for the @ host and the * host pointing to your IP and one for the CNAME at the www with the value being your bare domain.\nSo in NameCheap, my Advanced DNS configuration looks like this:\n\n\n\n12.4.4 Step 4: Wait an annoyingly long time\nNow you just have to be patient. Unfortunately DNS takes time to propagate. After a few minutes (or hours?), your server should be reachable at your domain.\nIf it’s not (yet) reachable, try seeing if an incognito browser works. If it doesn’t, wait some more. When you run out of patience, try reconfiguring everything and check if it works now."
  },
  {
    "objectID": "chapters/sec3/3-5-ssl.html#comprehension-questions",
    "href": "chapters/sec3/3-5-ssl.html#comprehension-questions",
    "title": "13  SSL/HTTPS are the most basic network security tools",
    "section": "13.1 Comprehension Questions",
    "text": "13.1 Comprehension Questions\n\nWrite down the step-by-step procedure that is followed when you sit down at your laptop and type google.com into the search bar. Your explanation should include the following terms: URL, domain, DNS, protocol, HTTP, HTTPS, Default Address, Router\nCreate a mind map of the following terms: Application Layer Protocol, HTTP, HTTPS, SSH, Port, 22, 80, 443\nIn the following URLs, what are the domain, top-level domain, path, subdomain, query parameters?\n\nhttps://blog.example.com/2022-10-30\nfacebook.com?search=alex.gold\nhttps://alexkgold.space/mfy.html\n\nWrite a mind map of the following terms: HTTP, HTTPS, SSL, TLS, CA, Public Certificate, Private Certificate, 443"
  },
  {
    "objectID": "chapters/sec3/3-5-ssl.html#lab-configure-ssl",
    "href": "chapters/sec3/3-5-ssl.html#lab-configure-ssl",
    "title": "13  SSL/HTTPS are the most basic network security tools",
    "section": "13.2 Lab: Configure SSL",
    "text": "13.2 Lab: Configure SSL\nThere’s a wonderful service called letsencrypt that is trying to make the world more secure, acting as a free CA, and handing out free SSL certificates to anyone who needs one.\nThey have a utility called certbot, which you just install on your server and it can autogenerate an SSL certificate, install it on the server, and even update your nginx configuration!\nFor anyone who’s never dealt with self-signing certificates in the past, let me tell you, this is magical!\nIn our nginx configuration, we'll need to add a line certbot will use to know which site to generate the certificate for.\nSo inside the nginx configuration’s server block, add\nserver_name do4ds-lab.shop www.do4ds-lab.shop;\nsubstituting in your domain for mine. Make sure to do both the bare domain and the www subdomain.\nAt that point, you can just install certbot and let it go! As of this writing, that was as simple as running\nsudo su apt-get install certbot apt-get install python3-certbot-nginx systemctl restart nginx sudo certbot –nginx -d do4ds-lab.shop -d www.do4ds-lab.shop\nIf you google “configure nginx with letsencrypt”, there’s a great article by someone at Nginx walking you through the process if the commands above change.\nI’d recommend you take a moment and inspect the /etc/nginx/nginx.conf file to look at what certbot added. You’ll notice two things – one is inside the server block. You’ll notice that the listen 80 is gone. We’re no longer listening for HTTP traffic. Instead there’s a listen 443 – the default port for SSL, and a bunch of stuff that says # managed by Certbot that tells nginx where to find the SSL certificate.\nScrolling down a little, there’s a new server block that is listening on 80. This block returns a 301, which you might recall is a redirect code (specifically for a permanent redirect) sending all HTTP traffic to HTTPS.\nBefore we exit and test it out, let’s do one more thing. RStudio does a bunch of sending traffic back to itself. For that reason, the /rstudio proxy location also needs to know to upgrade traffic from HTTP to HTTPS.\nSo add the following line to the nginx config:\nproxy_set_header X-Forwarded-Proto https; #redirect URLs back to https\nProto here is short for protocol. So this is letting RStudio Server know that when it forwards or redirects traffic, it should do so to HTTPS, not HTTP.\n\nOk -- let's try out our newly-secured workbench!\nGo to the URL your server is hosted at, and you’ll find that…it’s broken again.\nBefore you read along, think for just a moment. Why is it broken? Maybe scroll up to the image of the different layers of networking. Which one still isn’t open?\nIf you thought of the AWS instance security group, you're right! You'll remember that by default it was open to SSH traffic on port `22`, and we opened it to `HTTP` traffic on `80`. But now that we're sending it `HTTPS` traffic on `443`, you'll have to open it to that traffic as well.\nAdd a rule to allow HTTPS traffic on 443 from anywhere.\nNOW you should be able to go to <your-domain>/rstudio and get to RStudio and <your-domain>/jupyter to get to JupyterHub! Voila!\nNote that you *also* can now SSH into `ssh test-user@<domain>`, so you don't ever need to remember IP Addresses for day-to-day operations."
  },
  {
    "objectID": "chapters/sec3/3-6-servers.html#computers-are-addition-factories",
    "href": "chapters/sec3/3-6-servers.html#computers-are-addition-factories",
    "title": "14  Choosing the right server for you",
    "section": "14.1 Computers are addition factories",
    "text": "14.1 Computers are addition factories\nAs a data scientist, the amount of computational theory it’s really helpful to understand in your day-to-day can be summarized in three sentences:\n\nComputers can only add.\nModern ones do so very well and very fast.\nEverything a computer “does” is just adding two (usually very large) numbers, reinterpreted.1\n\nI like to think of computers as factories for doing addition problems.\n\nWe see meaning in typing the word umbrella or jumping Mario over a Chomp Chain and we interpret something from the output of some R code or listening to Carly Rae Jepsen’s newest bop, but to your computer it’s all just addition.\nEvery bit of input you provide your computer is homogenized into addition problems. Once those problems are done, the results are reverted back into something we interpret as meaningful. Obviously the details of that conversion are complicated and important – but for the purposes of understanding what your computer’s doing when you clean some data or run a machine learning model, you don’t have to understand much more than that.\n\n14.1.1 Compute\nThe addition assembly line itself – where the work actually gets done – is referred to as compute. It’s where 2+2 gets turned into 4, and where 345619912 + 182347910 gets turned into 527967822. The heart of the factory in your computer is the central processing unit (CPU).\nThere are two elements to the total speed of your compute – the total number of cores, which you can think of as an individual conveyor belt doing a single problem at a time, and the speed at which each belt is running.\nThese days, most consumer-grade laptops have between 4 and 16 cores, and may have additional capabilities that effectively doubles that number. So most laptop CPUs can do between 4 and 32 simultaneous addition problems.\nIn your computer, the basic measure of conveyor belt speed is single-core “clock speed” in hertz (hz) – operations per second. The cores in your laptop probably run between 2-5 gigahertz (GHz): 2-5 billion operations per second.\n\nA few decades ago, there was a steady march in increases in clock speed – increasing by approximately 10x during the 90s, by 2-3x in the 2000s, and somewhere between not at all and 1.5x in the 2010s. Computers have gotten much faster through this entire time, but mostly from sources other than boosts in single-core clock speeds.\nFor example, in 2020, Apple released a series of Macs with its new M1 chip. Real-world performance of the M1 chip was dramatically better than earlier models, despite modest improvements in clock speeds.\n\n\n14.1.1.1 GPU Computing\nWhile compute usually just refers to the CPU, it’s not completely synonymous. Computers can offload some problems to a graphical processing unit (GPU). GPUs are specialized chips used for tasks like editing photo or videos, rendering video game graphics, some kinds of machine learning, and (yes) Bitcoin mining.\nWhere the CPU has a few fast cores, the GPU takes the opposite approach, with many slower cores. Where a consumer-grade CPU has 4-16 cores, mid-range GPUs have 700-4,000, but each one runs between 1% and 10% the speed of a CPU core.\nFor GPU-centric tasks, the overwhelming parallelism of a GPU is more important than the speed of any individual core, and GPU computation can be dramatically faster. For the purposes of data science, many popular machine learning techniques – including neural networks, XGBoost, and other tree-based models – potentially run much much faster on GPUs relative to CPUs.\n\n\n\n14.1.2 Memory (RAM)\nYour computer’s random access memory (RAM) is its short term storage. Your computer uses RAM to store addition problems it’s going to tackle soon, and results it thinks it might need again in the near future.\nThe benefit of RAM is that it’s very fast to access. The downside is that it’s temporary. When your computer turns off, the RAM gets wiped.2\n\nYou probably know this, but memory and storage is measured in bytes. These days, most consumer hard drives are between 1/2 to a few terrabytes (trillion bytes). Some enterprise data stores run on the scales of thousands of terrabytes (pettabytes) or even thousands of pettabytes (yottabytes).\n\nModern consumer-grade laptops come with somewhere between 4 and 16 Gb of memory.\n\n\n14.1.3 Storage (Hard Drive/Disk)\nYour computer’s storage is like a freezer. It keeps things permanently, whether the factory is operating or not. The tradeoff is that things there are cold, and take some time to warm up before they can be used.\nA few years ago, all hard drives were (HDDs), and were basically the same. HDDs have a bunch of magnetic disks that spin very fast (5,400 and 7,200 RPM are common speeds). Magnetized read/write heads move among the disks and save and read your data.\nIn the last few years, solid-state drives (SSDs) have been taking over. SSDs, which are collections of flash memory chips, are up to 15x faster than HDDs. They also can take a wider variety of shapes and sizes, and are more reliable and durable because they have no moving parts. The main drawback is that they’re usually more expensive per byte, but prices are still quite reasonable.\nMany consumer laptops have only an SSD at this point. Some desktops and high-end laptops combine a smaller SSD with a larger HDD."
  },
  {
    "objectID": "chapters/sec3/3-6-servers.html#choosing-the-right-data-science-machine",
    "href": "chapters/sec3/3-6-servers.html#choosing-the-right-data-science-machine",
    "title": "14  Choosing the right server for you",
    "section": "14.2 Choosing the right data science machine",
    "text": "14.2 Choosing the right data science machine\nIn my experience as a data scientist and talking to IT/DevOps organizations trying to equip data scientists, the same questions about choosing a computer come up over and over again. Here are the guidelines I often share.\n\n14.2.1 Get as much RAM as feasible\nIn most cases, R and Python have to load all of your data into memory. Thus, the size of the data you can use is limited to the size of your machine’s RAM. Most other limits of your machine will just result in things being slower than you’d really want, but trying to load too much data into memory will result in a session crash, and you won’t be able to do your analysis.\n\nYou can get around the in-memory limitation by using a database or libraries that facilitate on-disk operations like Apache Arrow or dask.\n\nIt’s easy to say that you’ll always want more RAM, but a rough rule of thumb for whether you’ve got enough is the following:\n\nAmount of RAM = max amount of data * 3\n\nBecause you’ll often be doing some sort of transformation that results in invisible data copies and your computer can’t devote all of its memory, you’ll want to leave plenty of room over your actual data size. I’ve never run into problems with a scaling factor of 3, but this is definitely only a rough rule-of-thumb.\n\n\n14.2.2 Go for fewer, faster cores in the CPU\nR and Python are single-threaded. Unless you’re using special libraries for parallel processing, you’ll end up red-lining a single CPU core while the other just look on in silence. Therefore, single core clock speed matters more than the number of cores, and fewer, faster cores are usually preferable to many slower\nIf you’re buying a laptop or desktop, there usually aren’t explicit choices between a few fast cores and more slow cores. Most modern CPUs are pretty darn good, and you should just get one that fits your budget. If you’re standing up a server, you often have an explicit choice between more slower cores and fewer faster ones.\n\n\n14.2.3 Get a GPU…maybe…\nIf you’re doing machine learning that can be improved by GPU-backed operations, you might want a GPU. In general, only highly parallel machine learning problems like training a neural network or tree-based models will benefit from GPU computation.\nOn the other hand, GPUs are expensive, non-machine learning tasks like data processing don’t benefit from GPU computation, and many machine learning tasks are amenable to linear models that run well CPU-only.\n\n\n14.2.4 Get a lot of storage, it’s cheap\nAs for storage – get a lot – but don’t think about it too hard, because it’s cheap. Both a 1TB SSD and a 4TB HDD are around $100. Storage is cheap enough these days that it is almost always more cost efficient to buy more storage rather than making a highly-paid professional spend their time trying to figure out how to move things around.\nOne litmus test of an IT organization that is well-equipped to support data science is whether they understand this. Smart organizations know that just getting more storage is easily worth the cost in terms of the time of admins and data scientists.\n\n\n14.2.5 AWS Instance Classes for Data Science\nTODO: base storage vs mounted\nt3 – good b/c of instance credits, limited size\nCs – good b/c fast CPUs\nR - good b/c high amount of RAM\nP - have GPUs, but also v expensive\nInstance scale linearly w/ number of cores (plot?)"
  },
  {
    "objectID": "chapters/sec3/3-6-servers.html#lab-upgrading-instance-size",
    "href": "chapters/sec3/3-6-servers.html#lab-upgrading-instance-size",
    "title": "14  Choosing the right server for you",
    "section": "14.3 Lab: Upgrading Instance Size",
    "text": "14.3 Lab: Upgrading Instance Size\nOk, now we’re going to experience the real magic of the cloud – flexibility.\n\n\n\n\n\n\nNote\n\n\n\nNow that we’ve got JupyterHub and RStudio Server up and running at stable paths, you might want to just use the terminals inside those environments rather than SSH-ing in. Up to you!\nYou get exactly the same terminal by logging in as test-user and using the terminal as you would from SSH-ing in.\n\n\nWe’re going to upgrade the size of our server in just a minute or two.\nFirst, let’s confirm what we’ve got available. You can check the number of CPUs you’ve got with lscpu in a terminal. Similarly, you can check the amount of RAM with free -h. This is just so you can prove to yourself later that the instance really changed.\nNow, you can go to the instance page in the AWS console. The first step is to stop (not terminate!) the instance. This means that changing instance type does require some downtime for the instance, but it’s quite limited.\nOnce the instance has stopped, you can change the instance type under Actions > Instance Settings. Then start the instance. It’ll take a few seconds to start the instance.\nAnd that’s it.\nSo, for example, I changed from a t2.micro to a t2.small. Both only have 1 CPU, so I won’t see any difference in lscpu, but running free -h before and after the switch reveals the difference in the total column:\ntest-user@ip-172-31-53-181:~$ free -h\n               total        used        free      shared  buff/cache   available\nMem:           966Mi       412Mi       215Mi       0.0Ki       338Mi       404Mi\nSwap:             0B          0B          0B\ntest-user@ip-172-31-53-181:~$ free -h\n               total        used        free      shared  buff/cache   available\nMem:           1.9Gi       225Mi       1.3Gi       0.0Ki       447Mi       1.6Gi\nSwap:             0B          0B          0B\nI got twice as much RAM!\nThere are some rules around being able to change from one instance type to another…but this is an amazing superpower if you’ve got variable workloads or a team that’s growing. The ability to scale a machine so easily is a game-changer.\nA lot of the things we did – adding the elastic IP and daemonizing JupyterHub are the reason that we’re able to bring it back up so smoothly.\nYou can do the same thing with attached volumes, resizing them on-the-fly. It’s worth noting that you can only revise volume sizes up, and that after resizing, you’ll have to adjust the Linux filesystem so it knows it can take advantage of the new space available. AWS has good walkthroughs available on how to resize the file system, so I won’t go into it in any detail other than to say it has to be done."
  },
  {
    "objectID": "chapters/sec3/3-6-servers.html#comprehension-questions",
    "href": "chapters/sec3/3-6-servers.html#comprehension-questions",
    "title": "14  Choosing the right server for you",
    "section": "14.4 Comprehension Questions",
    "text": "14.4 Comprehension Questions\n\nThink about the scenarios below – which part of your computer would you want to upgrade to solve the problem?\n\nYou try to load a big csv file into pandas in Python. It churns for a while and then crashes.\nYou go to build a new ML model on your data. You’d like to re-train the model once a day, but it turns out training this model takes 26 hours on your laptop.\nYou design an visualization Matplotlib , and create a whole bunch in a loop, you want to parallelize the operation. Right now you’re running on a t2.small with 1 CPU.\n\nDraw a mind map of the following: CPU, RAM, Storage, Operations Per Second, Parallel Operations, GPU, Machine Learning\nWhat are the architectural differences between a CPU and a GPU? Why does this make a GPU particularly good for Machine Learning?"
  },
  {
    "objectID": "chapters/sec4/4-0-sec-intro.html#the-deal-with-security",
    "href": "chapters/sec4/4-0-sec-intro.html#the-deal-with-security",
    "title": "Making it Enterprise-Grade",
    "section": "The deal with security",
    "text": "The deal with security\nSome organizations have standalone security teams – I tend to think this is an anti-pattern. For example, your server could be unavailable because a bad actor broke in and took it offline, someone used up all the resources and it crashed, or because the networking broke and it can’t be reached.\nAs a data scientist, you probably have a getting-things-done kind of mindset. And that’s great!\nBut if you’re having to interact with security professionals at your organization, you’re probably finding that they have a whole different set of concerns – perhaps concerns that you’re struggling to communicate with them about.\nAs a data scientist, you’re probably thinking that you’ve secured your server. After all, we protected the traffic to and from the server with SSL/HTTPS so no one can snoop on it or jump in the middle and we configured authentication in the server, so only your authorized users can get in.\nYou’re not wrong, but that’s not how security professionals think about security. IT security professionals think about security in layers. And while you’ve done a good job setting your server up to comply with basic security best practices, there are no layers. That server front door is open to the internet. Literally anyone in the world can come to that authentication page for your RStudio Server or JupyterHub and start trying out passwords. That means you’re just one person choosing the password password away from a bad actor getting access to your server.\nLest you think you’re immune because you’re not an interesting target, there are plenty of bots out there randomly trying to break in to every existing IP address, not because they care about what’s inside, but because they want to co-opt your resources for their own purposes like crypto mining or virtual attacks on Turkish banks.1\nMoreover, security and IT professionals aren’t just concerned with bad actors from outside (called outsider threat) or even someone internal who decides to steal data or resources (insider threat). They are (or at least should be) also concerned with accidents and mistakes – these are just as big a threat to accessibility and availability.\nFor example, many basic data science workbenches give root access to everyone – it’s easy and simple! That obviously exposes the possibility that someone with access to your server could decide to steal data for their own ends (insider threat). But it also opens up the possibility that someone makes a mistake! What if they mean to just delete a project directory, but forget to add the whole path in and wipe your whole server. Yikes!\nIn this section, we’re going to address the topics that most often come up around the security and stability of a platform to do open source data science in an enterprise context.\n\n\n\n\n\n\nWhat is enterprise?\n\n\n\nEnterprise is a term that gets thrown around a lot. In this context, Enterprise roughly translates to “a large organization with well-defined IT/Admin responsibilities and roles”. If you’re not in an Enterprise, you likely have the rights and ability to just stand up a data science workbench and use it, like we did in the last section.\nIf you are in an Enterprise, you probably had to do all that on a personal account, and almost certainly couldn’t connect to your real data sources.\nDoing open source data science in an Enterprise almost certainly means having to work across teams to get your workbench built and having to convince other people that you’ll be a good actor at the end of the day.\n\n\nSection Outline (just for Alex, delete later):\nOpen Source in the enterprise\n\nManaging versions of R + Python\nManaging packages\n\nStaging environments + Infra as Code\n\nData stance\nSecurity + Sandboxing\n\nResourcing/Scaling\nAuth\nSecure Networking\n\nProper Resourcing\nIf you’re a small data science team, you might not be too concerned if someone accidentally knocks your data science workbench offline for 30 minutes because they tried to run a job that was too big. You’re probably all sitting in the same room and you can learn something from the experience.\nThat’s not the case when you get to enterprise-grade tooling. An enterprise-grade data science workbench probably supports dozens or hundreds of professionals across multiple teams. The server being down isn’t a sorta funny occurrence you can all fix together – it’s a problem that must be fixed immediately – or even better avoided altogether.\nThat’s why the third chapter in this section is going to get deep into how IT/Admins think about scaling a server to avoid hitting resource constraints. We’ll also get into some of the tools, like load balancers and Kubernetes that they use to scale the servers so you can have an intelligent conversation with them about what you need.\n\n\nTooling to fit it all together\nBack in section one, we learned about environments as code – using code to make sure that our data science environments are reproducible and can be re-created as needed.\nThis idea isn’t original – in fact, DevOps has it’s own set of practices and tooling around using code to manage DevOps tasks, broadly called Infrastructure As Code (IaC). This chapter will get broadly into some of the things you can do with IaC tooling, and will introduce some key concepts and some of the popular tooling options for doing IaC you can try out.\nTODO: developing relationships with IT/Admins"
  },
  {
    "objectID": "chapters/sec4/4-1-os-ds-in-ent.html#data-science-sandboxes",
    "href": "chapters/sec4/4-1-os-ds-in-ent.html#data-science-sandboxes",
    "title": "15  Open Source Data Science in the Enterprise",
    "section": "15.1 Data Science Sandboxes",
    "text": "15.1 Data Science Sandboxes\nIn the chapter on environment as code tooling, we discussed how you want to create a version of your package environment in a safe place and then use code to share it with others and promote it into production.\nThe IT/Admin group at your organization probably is thinking about something similar, but at a more basic level. They want to make sure that they’re providing a great experience to you – the users of the platform.\nSo they probably want to make sure they can upgrade the servers you’re using, change to a newer operating system, or upgrade to a newer version of R or Python without interrupting your use of the servers.\nThese are the same concerns you probably have about the users of the apps and reports you’re creating. In this case, I recommend a two-dimensional grid – one for promoting the environment itself into place for IT/Admins and another for promoting the data science assets into production.\nIn order to differentiate the environments, I often call the IT/Admin development and testing area staging, and use dev/test/prod language for the data science promotion process.\n\n15.1.1 Creating Dev/Test/Prod\nIn Chapter 1 on code promotion, we talked about wanting to create separate dev/test/prod environments for your data science assets. In many organizations, your good intentions to do dev/test/prod work for your data science assets are insufficient.\nIn the enterprise, it’s often the case that IT/Admins are required to ensure that your dev work can’t wreak havoc on actual production systems are data. In many cases, this is actually a good thing!\nIn this case, I recommend you work with your IT/Admins to create a data science sandbox - a place where you can have free access to the data and packages you need to develop new assets to your heart’s content.\nThen you need a promotion process to get into “higher lanes”.\nThere are two items that are likely to raise friction between you and the IT/Admins as you’re trying to create a data science sandbox. The first is access to data, and the second is access to packages.\nMany IT/Admins are familiar with building workbenches for other sorts of software development tasks, which are fundamentally different from doing data science. Because so much of data science is exploratory, you need a safe place where you can work with real data, explore relationships in the data, and try out ways of modeling and visualizing the data to see what works.\nBut for many software development tasks, access to fake or mock data is sufficient. Helping IT/Admin understand that you need access to real data, even if just a sample or in read-only mode is likely to be one of the highest hurdles to getting a data science sandbox that’s actually useful. In data science, the dev process looks less like test and prod – in software engineering you’re generally iterating on an app, writing code, and then promoting it into a testing lane. In data science, the dev activities are entirely different from the test or prod activities – you can’t start building a model or shiny app until you’ve really explored the data in depth.\nIn many cases, creating a data science sandbox with read-only access to data is a great solution. If you’re creating a tool that also writes data, you can write the data only within the dev environment.\nThe other issue that can be a tough sell is getting freer access to packages in the dev environment. As you probably know, part of the exploratory data process is trying out new packages – is it best to use this modeling package or that, best to use one type of visualization or another. If you can convince your IT/Admins to give you freer access to packages in dev, that’s ideal.\nYou can work with them to figure out what the promotion process will look like. It’s easy to generate a list of the packages you’ll need to promote apps or reports into production with tools like renv or venv. Great collaborations with IT/Admin are possible if you can develop a procedure where you give them package manifests that they can compare to allow-lists and then make those packages available.\n\n\n15.1.2 Infrastructure As Code Tooling\nThere are many, many varieties of infrastructure as code tooling. There are many books on infrastructure as code tooling and I won’t be covering them in any depth here. Instead, I’ll share a few of the different “categories” (parts of the stack) of infrastructure as code tooling and suggest a few of my favorites.\nTo get from “nothing” to a usable server state, there are (at minimum) two things you need to do – provision the infrastructure you need, and configure that infrastructure to do what you want.\nFor example, let’s say I’m standing up a server to deploy a simple shiny app. In order to get that server up, I’ll need to stand up an actual server, including configuring the security settings and networking that will allow the proper people to access the server. Then I’ll need to install a version of R on the server, the Shiny package, and a piece of hosting software like Shiny Server.\nSo, for example, you might use AWS’s CloudFormation to stand up a virtual private cloud (VPC), put an EC2 server instance inside that VPC, attach an appropriately-sized storage unit, and attach the correct networking rules. Then you might use Chef to install the correct software on the server and get your Shiny app up-and-running.\nIn infrastructure as code tooling, there generally isn’t a clear dividing line between tools that do provisioning and tools that do configuration…but most tools lean one way or the other.\nBasically any tool does provisioning will directly integrate into the APIs of the major cloud providers to make it easy to provision cloud servers. Each of the cloud providers also has their own IaC tool, but many people prefer to use other tools when given the option (to be delicate).\nThe other important division in IaC tools is declarative vs imperative. In declarative tooling, you simply enumerate the things you want, and the tool makes sure they get done in the right order. In contrast, an imperative tool requires that you provide actual instructions to get to where you want to go.\nIn many cases, it’s easy to be declarative with provisioning servers, but it’s often useful to have a way to fall back to an imperative mode when configuring them because there may be dependencies that aren’t obvious to the provisioning tool, but are easy to put down in code. If the tool does have an imperative mode, it’s also nice if it’s compatible with a language you’d be comfortable with.\nOne somewhat complicated addition to the IaC lineup is Docker and related orchestration tools. There’s a whole chapter on containerization and docker, so check that out if you want more details. The short answer is that docker can’t really do provisioning, but that you can definitely use docker as a configuration management IaC tool, as long as you’re disciplined about updating your Dockerfiles and redeployment when you want to make changes to the contents.\nBasically none of these tools will save you from your own bad habits, but they can give you alternatives.\nIn short, exactly which tool you’ll need will depend a lot on what you’re trying to do. Probably the most important question in choosing a tool is whether you’ll be able to get help from other people at your organization on it. So if you’re thinking about heading into IaC tooling, I’d suggest doing a quick survey of some folks in DevOps and choosing something they already know and like."
  },
  {
    "objectID": "chapters/sec4/4-1-os-ds-in-ent.html#foss-in-enterprise",
    "href": "chapters/sec4/4-1-os-ds-in-ent.html#foss-in-enterprise",
    "title": "15  Open Source Data Science in the Enterprise",
    "section": "15.2 FOSS in Enterprise",
    "text": "15.2 FOSS in Enterprise\nSucceeding with Open Source in an enterprise context means forging a good partnership with IT/Admin. Many IT/Admins have only limited experience with FOSS at all and even if they’re used to FOSS, they probably have almost no experience with FOSS for data science, which looks a good bit different from many other applications of FOSS.\n\n15.2.1 Legal issues with open source\nWhen using Free and Open Source Software (FOSS) in the enterprise, legal concerns may arise about the legality of using this software.\nFOSS is made such by the license applied to it. There are a variety of different open source licenses with different implications for the organization that wants to make use of them. I am not a lawyer and this is not legal advice, but here’s a little context on the issues at hand.\nThere are some open source licenses that are called “permissive” because they allow broad use of the software licensed under them. For example, the MIT license allows anyone to, “use, copy, modify, merge, publish, distribute, sublicense, and/or sell” software that is licensed under MIT – and it doesn’t even require attribution!\nThis means that you are broadly allowed to use something MIT-licensed and do what you want with it, without having to worry too much about the implications.1 Other licenses (like some of the BSD variants) allow you to do basically whatever you want, but do require attribution.\nOn the other hand, there are more restrictive or “copyleft” licenses – popular ones are GPL and AGPL. Aside from restrictions these licenses may place on the usage of the software, they also require that derivative works are released under the same license.\nR is released under GPL, a copyleft licenses. Python is released under a specialized Python Software Foundation (PSF) license, which is a permissive license.\nThis means, for example, that you can not take the base R language, rewrite a section of it and create a proprietary version of R you want people to pay for.2\nHowever, there is disagreement among lawyers about the scope of copyleft licenses. Many copyright lawyers believe that the extent of a derivative work for the purpose of a copyleft license is literally repackaging a bit of code and using it.\nBeyond the base languages themselves, packages in R and Python are released under whatever licenses the package authors decide.\nSo, for example, the popular ggplot2 R package for plotting is released under an MIT license. But wait, you say, R is GPL, doesn’t that mean that derivative works also have to be a copyleft license? This is allowed because ggplot2 is uses R, but the source code of the R language doesn’t actually appear in the ggplot2 package, so it isn’t bound by R’s GPL license.3\nThere is a grey area about using these packages. Say I create an app or plot in R and then share that plot with the public – is that app or plot bound by the license as well? Do I now have to release my source code to the public? Many would argue no – it uses R, but doesn’t derive from R in any way. Others have stricter concerns. Most interpretations of the very strict AGPL license is that running a service based on AGPL licensed code requires releasing the source code. Understandably, organizations that themselves sell software tend to be a little more concerned about these issues.\nThere are disagreements on this among lawyers, and you should be sure to talk to a lawyer at your organization if you have concerns.\n\n\n15.2.2 Package Management in the Enterprise\nIn the chapter on environments as code, we went in depth on how to manage a per-project package environment that moves around with that project. This is a best practice and you should do it.\nBut in some Enterprise environments, there may be further requirements around how packages get into the environment, which packages are allowed, and how to validate those packages.\nThe biggest difference between libraries and repositories is how many versions of packages are allowed. Because libraries can house (at most) one version of any given package, they should live as close to where they’re used as possible.\nIn contrast, it’s nice to manage repositories at as general of a level as possible for convenience’s sake.\nIn some cases, enterprises are happy to allow data scientists open access to any packages on CRAN, PyPI, GitHub, and elsewhere. But most enterprises have somewhat more restrictive stances for security reasons.\nIn these cases, they have to do two things - make sure that public repositories are not available to users of their data science platforms, and use one of these repository tools to manage the set of packages that are available inside their environment. It often takes a bit of convincing, but a good division of labor here is generally that the IT/Admins manage the repository server and what’s allowed into the environment and the individual teams manage their own project libraries.\n\n\n\n\n\n\nAmount of Space for Packages\n\n\n\nWhen admins hear about a package cache per-project, they start getting worried about storage space. I have heard this concern many times from admins who haven’t yet adopted this strategy, and almost never heard an admin say they were running out of storage space because of package storage.\nThe reality is that most R and Python packages are very small, so storing many of them is reasonably trivial.\nAlso, these package storage tools are pretty smart. They have a shared package cache across projects, so each package only gets installed once, but can be used in a variety of projects.\nIt is true that each user then has their own version of the package. Again, because packages are small, this tends to be a minor issue. It is possible to make the package cache one that is shared across users, but the (small) risk this introduces of one user affecting other users on the server is probably not worth the very small cost of provisioning enough storage that this just isn’t an issue.\n\n\nMany enterprises run some sort of package repository software. Common package repositories used for R and Python include Jfrog Artifactory, Sonatype Nexus, Anaconda Business, and Posit Package Manager.\nArtifactory and Nexus are generalized library and package management solutions for all sorts of software, while Anaconda and Posit Package Manager are more narrowly tailored for data science use cases.\nThere are two main concerns that come up in the context of managing packages for the enterprise. The first is how to manage package security vulnerabilities.\nIn this context, the question of how to do security scanning comes up. What exactly security professionals mean by scanning varies widely, and what’s possible differs a good bit from language to language.\nIt is possible to imagine a security scanner that actually reads in all of the code in a package and identifies potential security risks – like usage of insecure libraries, calls to external web services, or places where it accesses a database. The existence of tools at this level of sophistication exist roughly in proportion to how popular the language is and how much vulnerability there is.\nSo javascript, which is both extremely popular and also makes up most public websites, has reasonably well-developed software scanning. Python, which is very popular, but is only rarely on the front end of websites has fewer scanners, and R, which is far less popular has even fewer. I am unaware of any actual code scanners for R code.\nOne thing that can be done is to compare a packaged bit of software with known software vulnerabilities.\nNew vulnerabilities in software are constantly being identified. When these vulnerabilities are made known to the public, the CVE organization attempts to catalog them all. One basic form of security checking is looking for the use of libraries with known CVE records inside of packages.\nThe second thing your organization may care about is the licenses software is released under. They may want to disallow certain licenses – especially aggressive copyleft licenses – from being present in their codebases."
  },
  {
    "objectID": "chapters/sec4/4-1-os-ds-in-ent.html#comprehension-questions",
    "href": "chapters/sec4/4-1-os-ds-in-ent.html#comprehension-questions",
    "title": "15  Open Source Data Science in the Enterprise",
    "section": "15.3 Comprehension Questions",
    "text": "15.3 Comprehension Questions\n\nWhat is the purpose of creating a data science sandbox? Who benefits most from the creation of a sandbox?\nWhy is using infrastructure as code an important prerequisite for doing Dev/Test/Prod?\nWhat is the difference between permissive and copyleft open source licenses? Why are some organizations concerned about using code that includes copyleft licenses?\nWhat are the key issues to solve for open source package management in the enterprise?"
  },
  {
    "objectID": "chapters/sec4/4-2-secure-networking.html#the-role-of-proxies",
    "href": "chapters/sec4/4-2-secure-networking.html#the-role-of-proxies",
    "title": "16  Upgraded Networking",
    "section": "16.1 The role of proxies",
    "text": "16.1 The role of proxies\nA proxy is a server that exists solely as an intermediary to manage the traffic coming to it. Proxies can serve a variety of different roles including acting as a firewall, routing traffic to different places, doing authentication into the servers behind, or caching data for better performance on frequent queries.\nFor an IT/Admin, managing proxies is an everyday activity.\nHopefully for you as a data scientist, you almost never have to know much about the proxies themselves, but being able to ask intelligent questions – and know how they might get in the way of what you’re trying to go – can relieve some headaches before they occur.\n\n\n\n\n\n\nNetwork Debugging Tip\n\n\n\nIf you’re experiencing weird behavior in your data science environment – files failing to upload or download, sessions getting cutoff strangely, or data not transferring right – issues with a proxy are probably the first thing you should check.\n\n\nIf you’re talking to your IT/Admin about the network where your data science environment sits, one of the most important questions is whether that network has a proxy. There are two different kinds of proxies that are important to know about – forward and reverse. Proxies are discussed from the perspective of being inside the network, so a forward proxy is one that intercepts and does something to outbound traffic, while a reverse proxy is one that does something to inbound traffic. Personally, I much prefer the terms inbound and outbound to forward and reverse, because it’s much easier to remember which is which.\nIn most cases, proxies sit right on the edge of your network – in the DMZ (public subnet) between your private servers and the open internet. In some cases, there are also proxies inside different segments of the private network. These are generally for the purposes of isolating components as a security measure – like having watertight bulkheads between sections of a ship. If you’ve got a proxy – for example – between your app development environment and where it’s deployed, that’s a reasonably common cause of issues along the way.\nDeveloping a good mental model of where network connections originate is really important in terms of understanding why proxies might be causing you trouble. One of the most helpful things you can do when talking to your IT/Admin about networking issues is help them understand when your data science environment requires an inbound connection and when it requires an outbound connection.\nTODO: image inbound vs outbound connection\nAs we went over in chapter 2-4, network traffic always operates on a call and response model. So whether your traffic is inbound or outbound is dependent on who makes the call. Inbound means that the call is coming from a computer outside the private network directed to a server inside the private network, and outbound is the opposite.\nSo basically, anything that originates on your laptop – including the actual session into the server is an inbound connection, while anything that originates on the server – including everything in code that runs on the server is an outbound connection.\n\n16.1.1 What proxies do\nThere are a number of different functions proxies can do – here are a few of the most common.\nProxies – especially reverse/inbound – often do redirection. This means that the proxy is generally the what’s actually available at the public URL of your server. It then redirects people along to the actual hostname for your server. Inside your private network, you can name your server whatever you want, so if you’ve got a server that’s just named rstudio inside your network, your proxy would know that anyone going to my-org.com/rstudio ends up at the host named rstudio.\nOne other nice thing proxies can do along with redirection is managing ports more securely. Many services run on a high-numbered port by default to avoid conflicts with other services. For example, RStudio Server runs on port 8787 by default. But remembering what weird port has to be open for every service can be kinda a pain. Thus it can be much easier to just keep standard ports (80 for HTTP, 443 for HTTPS, and 22 for SSH) open on your proxy and have the proxy just redirect the traffic coming into it on 80 to 8787 on the server with RStudio Server.\nThere’s a special kind of reverse proxy called a load-balancer. This is a kind of proxy that redirects traffic coming to a single URL to not just one – but one of a pool of servers. In the scaling chapter, we’ll get more into how to think about pools of servers and load-balancing work, but the networking part is handled by a load-balancer.\n[TODO: image of path rewriting + load-balancing]\nSometimes proxies also terminate SSL. Because the proxy is the last server that is accessible from the public network, many organizations don’t bother to implement SSL/HTTPS inside the private network so they don’t have to worry about managing SSL certificates inside their private network. This is getting rarer as tooling for managing SSL certificates gets better, but it’s common enough that you might start seeing HTTP addresses if you’re doing server-to-server things inside the private network.\nTODO: image of SSL termination\nOccasionally proxies also do authentication. In most cases, proxies pass along any traffic that comes in to where it’s supposed to go. If there’s authentication, it’s often at the server itself. Sometimes the proxy is actually where authentication happens, so you have to provide the credentials at the edge of the network. Once those credentials have been supplied, the proxy will let you through. Depending on the configuration, the proxy may also add some sort of token or header to your incoming traffic to let the servers inside know that your authentication is good and to pass along identification for authorization purposes.\nTODO: image of auth at proxy\nThe last thing that proxies can do is just block traffic that isn’t explicitly allowed. If you’ve got a forward proxy in your environment, you may have to work with your IT/Admins to make sure that you’re able to access the resources you need to get your work done. If that’s not an option, you may have to think about how to operate offline, which we’ll address towards the end of the chapter.\n\n\n16.1.2 Inbound vs outbound proxies\nThe things you’ll have to think about are very different depending on whether your VPC has a outbound/forward or an inbound/reverse proxy. If you’ve got a reverse proxy (and most enterprise networks do), the main thing you probably have to consider is path-rewriting – generally the proxy is what’s actually hosted at the public URL of the server, and then it passes people along to the internal hostname of the actual server.\nThe other big concern is what kinds of connections your proxy supports. For example, many data science app frameworks (including Shiny and Streamlit) use a technology called Websockets for maintaining the connection between the user and the app session. Most modern proxies support Websockets – but some don’t and you’ll have to figure out workarounds if you can’t get Websockets enabled.\nAdditionally, some inbound proxies have limitations that can make things weird for data science use cases – the most common are limiting file size for uploads and downloads and implementing timeouts on file uploads, downloads, and sessions. It’s reasonably common for organizations to have standard file size limits or timeouts that don’t work well in a data science contexts. In data science contexts, files tend to be big and session lengths long. If you’re trying to work in a data science context and weird things are happening with file uploads or downloads or sessions ending unexpectedly, checking on inbound proxy settings is a good first hunch.\nInbound proxies are what you’re thinking about when you’re wondering if anything is going wrong for your connection to your server. If you can’t upload or download the files you need directly to the server or your RStudio session keeps timing out, a reverse proxy is a likely candidate.\nOn the other hand, forward/outbound proxies are the likely culprit if your code is having trouble running. In general, outbound proxies are simpler than inbound. The most common thing they do is simply block traffic from leaving the private network. Many organizations have these proxies to reduce the risk of someone getting in and then being able to exfiltrate valuable resource.\nThe most common reasons you’d hit a forward/outbound proxy in your code is when you’re installing packages from a public repository or when you’re trying to make use of an API or other web service that’s outside your private network.\nIn some cases, ameliorating these issues is as easy as talking to your IT/Admin and asking them to open the outbound proxy to the right server. Especially if it’s a URL protected by HTTPS and that’s for only one thing – for example CRAN, PyPI, or public RStudio Package Manager, it’s generally pretty safe and many organizations are happy to allow-list a limited number of outbound addresses.\nIf not, the next section is for you."
  },
  {
    "objectID": "chapters/sec4/4-2-secure-networking.html#fully-offlineairgapped-operations",
    "href": "chapters/sec4/4-2-secure-networking.html#fully-offlineairgapped-operations",
    "title": "16  Upgraded Networking",
    "section": "16.2 Fully Offline/Airgapped operations",
    "text": "16.2 Fully Offline/Airgapped operations\nOffline or airgapped environments are quite common in highly regulated industries with strong requirements around data security and governance.\nThe term airgapped comes from the notion that there is a physical gap – air – between the internet and the environment. In these instances, the servers where your data science environment exists is physically disconnected from the outside world, and if you need to move something into the environment, you’ll have to load it onto a physical drive outside the environment and then walk into where the environment is.\nHowever, environments that are this thoroughly airgapped are quite rare. Most airgapped environments are airgapped by proxies.\nThis means that they sharply limit where inbound connections are allowed to come from – for example perhaps only from machines in the physical building where your company sits.1 The way you get into your airgapped environment usually involves either being in a certain physical location to get on the network or logging into a VPN. If your organization requires offline operations, they almost certainly have ways to give you access to the network. Talk to your IT/Admin.\nTODO: image of VPC inside VPN\nIf your server is offline, it’s likely that they strictly limit or completely disallow outbound connections from the servers inside the private network. This is where your organization probably doesn’t have standard practices, and you getting clear on what you need with your IT/Admins will really help.\nHere are the four most common reasons you’ll need to make outbound connections from inside your data science environment.\n\nDownloading Packages Downloading a package requires a network connection to the repository – usually CRAN, BioConductor, public RStudio Package Manager, Conda, PyPI, or GitHub. If you can get narrow exceptions for these, that’s great! If not, you’ll need to figure out how to run a package repository inside your data science environment like we discussed in the last chapter.\nData Science Tasks In many organizations, you don’t need internet access at all for the work you’re doing. You’re just working on data from databases or files inside your private network and don’t really need access to data or resources outside. On the other hand, if you’re consuming data from public APIs or scraping data from the web, that may require external connections.\nSystem Libraries In addition to the R and Python packages, there are also system libraries you’ll need installed, like the versions of R and Python themselves, and other packages used by the system. Generally it’ll be the IT/Admin managing and installing these, so they probably have a strategy for doing it. This may come up specifically in the context of data science if you’re using R or Python packages that are basically just wrappers around system libraries, like the sf package in R, or the GDAL python package which wraps the GDAL system library for geospatial work.\nSoftware Licensing If you’re using all open source software, this probably won’t be an issue. But if you’re buying licenses to a professional product, you’ll have to figure out how to activate the software licensing, which generally operates by reaching out to servers owned by the software vendor. They should have a method for activating servers that can’t reach the internet, but your IT/Admins will appreciate if you’ve done your homework on this before asking them to activate some new software.\n\nBefore you go ahead treating your environment as truly offline/airgapped, it’s almost always worth asking if narrow exceptions can be made to a network that is offline/airgapped. The answer may surprise you.\nIf you are truly offline, you probably won’t be able to move things on or off your private servers. Instead, when you need things, the IT/Admin will either connect to a server in the DMZ that has permission to access both the public internet and the private network to load things, or they’ll actually have to download things to their laptop from the internet, connect to the server in the offline environment, and upload them.\nTODO: drawing of offline operations"
  },
  {
    "objectID": "chapters/sec4/4-2-secure-networking.html#comprehension-questions",
    "href": "chapters/sec4/4-2-secure-networking.html#comprehension-questions",
    "title": "16  Upgraded Networking",
    "section": "16.3 Comprehension Questions",
    "text": "16.3 Comprehension Questions\n\nWhat is the advantage of adopting a more complex networking setup than a server just deployed directly on the internet? Are there advantages other than security?\nDraw a mental map with the following entities: inbound traffic, outbound traffic, proxy, DMZ, private subnet, public subnet, VPC\nLet’s say you’ve got a private VPC that hosts an instance of RStudio Server, an instance of JupyterHub, and a Shiny Server that has an app deployed. Here are a few examples of traffic – are they outbound, inbound, or within the network?\n\nSomeone connecting to and starting a session on RStudio Server.\nSomeone SFTP-ing an app and packages from RStudio Server to Shiny Server.\nSomeone installing a package to the Shiny Server.\nSomeone uploading a file to JupyterHub.\nA call in a Shiny app using httr2 or requests to a public API that hosts data.\nAccessing a private corporate database from a Shiny for Python app using sqlalchemy.\n\nWhat are the most likely pain points for running a data science workbench that is fully offline/airgapped?"
  },
  {
    "objectID": "chapters/sec4/4-3-auth.html#the-many-flavors-of-auth-or-what-does-sso-mean",
    "href": "chapters/sec4/4-3-auth.html#the-many-flavors-of-auth-or-what-does-sso-mean",
    "title": "17  Logging in with auth",
    "section": "17.1 The many flavors of auth (or what does SSO mean?)",
    "text": "17.1 The many flavors of auth (or what does SSO mean?)\nSingle Sign On (SSO) is a slippery term, so it is almost always necessary to clarify what is meant by the term when you hear it. At some organizations, identity management isn’t centralized at all. This means that usernames and passwords are unique to each service, onboarding and offboarding of users has to be handled independently for each service, and users have to login frequently. In short, it’s often not a great system. This is never referred to as SSO.\n\nMost organizations of a meaningful size have centralized identity management. This means that identities, credentials, authorization, onboarding, and offboarding are handled centrally. However, you may still need to independently login to each system. For example, in this system, every service might take the same username and password as your credentials, but if you go to RStudio Server followed by JupyterHub, you’ll need to provide that username and password independently to each service. This system is often facilitated by PAM, and LDAP/AD. Some organizations call this SSO, because there’s only one set of credentials.\nIn true SSO, users login once and are given a token or ticket.1 Then, when they go to the next service, they don’t have to login again because that service can just look at the token or ticket to do auth for that user. For example, in this system, I could go to RStudio Server and login, and then go to JupyterHub and get in without being prompted again for my password. This type of auth is facilitated by Kerberos, SAML, or OAuth."
  },
  {
    "objectID": "chapters/sec4/4-3-auth.html#auth-techniques",
    "href": "chapters/sec4/4-3-auth.html#auth-techniques",
    "title": "17  Logging in with auth",
    "section": "17.2 Auth Techniques",
    "text": "17.2 Auth Techniques\nIf you have five data scientists in your group, and the only shared resource you have is an RStudio Server instance, you probably don’t need to think terribly hard about auth. It’s pretty straightforward to just make users on a server and give them access to everything.\nBut as organizations get larger with hundreds or thousands of users, there’s constant churn of people joining and leaving. The number of services can creep into the dozens or hundreds and people may have very different authorization levels to different services. Trying to manage auth on the individual services is a nightmare – as is trying to keep that many usernames and passwords straight for users. That is why almost all organizations with more than a few users have centrally managed auth.\n\n17.2.1 You get a permission and you get a permission!\nFor the most part, we think of people being authenticated and authorized into services. However, it’s sometimes useful to consider the broader class of entities that could do auth. There are two common non-human entities that are included in auth systems that are worth considering.\nService Accounts are accounts given to non-human entities when you want it to be able to do something on its own behalf. For example, maybe you’ve got a Shiny app that users use to visualize data that’s in a database. Very often, you don’t want the app to have the same permisions as the app’s author, or to inherit the permissions of the people viewing the app. Instead, you want the app to be able to have permissions to do certain database operations. In that case, you would create a service account to give to the Shiny app that has exactly those permissions.\nThere are also times where it’s useful to go one level up and give permissions to an entire instance or service. In that case, you might assign permissions to an instance. For example, you could make it the case that anyone who is logged into the JupyterHub server is allowed to read from the database.\nInstance permissions are rather broad, and so they are usually only applied when you’ve got multiple resources inside a private network. In that case, authentication and authorization are only done at a single point and authorization is pretty broad.\n\n\n17.2.2 Authorization is kinda hard\nFrom a management perspective, authentication is pretty simple. A person is given a set of credentials, and they have to supply those credentials when prompted to prove they are who they say they are.\nAuthorization is a whole other can of worms. There is a meaningful literature on varieties of authorization and how they work. We’re not going to get too deep into the weeds, other than to define some common terms and how they’re used.\nThe atomic basis for authorization is a permission. Permissions are a binary switch that answers the question is this person allowed to do the thing they are trying to do?2\nThe simplest way of assigning permissions is called an access control list (ACL). In systems that use ACLs, each piece of content has a list of users who are allowed access. Sometimes, ACLs are also assigned to groups, which are simply sets of users – think data-scientists.\n\nOne ACL implementation with which you may be familiar is file permissions on a Linux server. For example, if you have a Mac or are on a Linux server, you can open your terminal, navigate to a directory and do the following:\n$ ls -l\n-rwxr-xr-x   1 alexkgold  staff   2274 May 10 12:09 README.md\nThat first set of characters describes the ACL for the README.md file. The first character - indicates this is a file, as opposed to a directory of files (which would be d). Then there are three sets of 3 characters, rwx, which are short for read, write, and execute, with the first group for the owner, alexkgold, the second group for anyone else in the group staff, and the third set for anyone else.\nSo you can read -rwxr-xr-x as, this is a file that alexkgold can read, write or execute, and anyone else can read or execute, but not edit.\nACLs are pretty intuitive, but it turns out that when you are managing a lot of users across a lot of files, directories, and services, they can get pretty difficult to manage, so many organizations use Role Based Access Control (RBAC).\nRBAC adds a layer of abstraction between users and permissions, which makes it a little harder to understand, but ultimately results in a much more flexible system. In RBAC, permissions are not assigned to individual pieces of content or to users or groups. Instead, permissions are assigned to roles, and roles are given to users or groups.3\n\nThere are also further iterations on the RBAC model, like Attribute Based Access Control (ABAC) or Policy Based Access Control (PBAC) in which there’s a long list of attributes that could be considered for a user to compute their permissions for a given service."
  },
  {
    "objectID": "chapters/sec4/4-3-auth.html#auth-technologies",
    "href": "chapters/sec4/4-3-auth.html#auth-technologies",
    "title": "17  Logging in with auth",
    "section": "17.3 Auth Technologies",
    "text": "17.3 Auth Technologies\n\n17.3.1 Username + Password\nMany pieces of software come with integrated authentication. When you use those system, the product stores encrypted username and password pairs in a database.\nThese setups are often really easy from an admin perspective – you just set up individual users on the server. However, the flip side is that users have one more username and password to remember, which is annoying for them. Moreover, if you have more than a few users, or the system is one of more than a few, it’s hard to manage users on a lot of different systems. It can be a real pain to create accounts on a ton of different systems when a new person joins the organization, or to remove their permissions one-by-one when they leave.\nFor this reason, most IT/Admin organizations strongly prefer using some sort of centralized identity store.\n\n\n17.3.2 PAM\nPluggable Authentication Modules (PAM) is a Linux system for doing authentication. As of this writing, PAM is the default authentication method for both RStudio Server and JupyterHub.\n\nConceptually PAM is pretty straightforward. You install a service on a Linux machine and configure it to use PAM authentication from the underlying host. By default, PAM just authenticates against the users configured on the Linux server, but it can also be configured to use other sorts of “modules” to authenticate against other systems – most commonly LDAP/AD or Kerberos. PAM can also be used to do things when users login – the most common being initializing tokens or tickets to other systems, like a database.\nPAM is often paired with System Security Services Daemon (SSSD), which is most commonly used to automatically create Linux users on a server based on the identities stored in an LDAP/AD instance.\nThough conceptually simple, reading, writing, and managing PAM modules is kinda painful.\n#TODO: Add PAM example\n\n\n17.3.3 LDAP/AD\nLightweight Directory Access Protocol (LDAP) is a relatively old, open, protocol used for maintaining a set of entities and their attributes. To be precise, LDAP is actually a protocol for maintaining and accessing entities and their attributes in a tree. It happens that this is a really good structure for maintaining permissions and roles of users at an organization, and it’s the main thing LDAP is used for.\nActive Directory (AD) is Microsoft’s implementation of LDAP, and is by-far the most common LDAP “flavor” out there. AD so thoroughly owns the LDAP enterprise market, that LDAP is often referred to as LDAP/AD. There are other implementations you may run across, the most common being OpenLDAP.\n\nAzure, Microsoft’s cloud provider, offers an authentication service called Azure Active Directory (AAD). Confusingly, AAD is usually used in combination with SAML, not LDAP.\n\nIt’s worth distinguishing the use of LDAP as an identity store from its use as an authentication technology. As a tree-based database, LDAP is uniquely well-suited to storing the identities, and other attributes of people at the organization. However, as discussed below, using LDAP to authenticate into actual services has security and convenience drawbacks, and many organizations consider it outdated and insecure.\nA lot of organizations are moving away from LDAP for authentication in favor of token-based technologies like SAML or OAuth, but many are keeping LDAP as their identity “source of truth” that is referenced by the SAML or OAuth Identity Provider.\nLDAP has three main disadvantages relative to other technologies. First, LDAP requires that your credentials (username and password, usually) actually be provided to the service you’re trying to use. This is fundamentally insecure relative to a system where your credentials are provided only to the identity provider, and the service just gets a token verifying who you are. In token-based systems, adding additional requirements like MFA or biometrics are easy, as they’re simply added at the IdP layer. In contrast, doing those things in LDAP would require the service to implement them, which usually is not the case, so you’re usually limited to username and password.\nThe second disadvantage of LDAP is that it does not allow for central administration of permissions. LDAP directly records only objects and their attributes. Say, for example, you want only users of a particular group to have access to a certain resource. In LDAP, you would have to specify in that resource that it should only allow in users of that group. This is in contrast to SAML/OAuth, where the authorization is centrally managed.\nLastly, LDAP authentication is based on each service authenticating. Once you authenticate, the service might give you a cookie so that your login persists, but there is no general-purpose token that will allow you to login to multiple services.\n\n17.3.3.1 How LDAP Works\nWhile the technical downsides of LDAP are real, the technical operations of LDAP are pretty straightforward. In short, you try to login to a service, the service collects your username and password, sends it off to the LDAP server, and checks that your username and password are valid.\n\nNote that LDAP is purely for authentication. When you’re using LDAP, authorization has to be handled separately, which is one of the disadvantages.\n\n\n17.3.3.2 Deeper Than You Need on LDAP\nLDAP is a tree-based entity and value store. This means that LDAP stores things and their attributes, which include a name and one or more values. For example, my entry in a corporate LDAP directory might look like this:\ncn: Alex Gold\nmail: alex.gold@example.com\nmail: alex.gold@example.org\ndepartment: solutions\nmobile: 555-555-5555\nobjectClass = Person\nMost of these attributes should be pretty straightforward. cn is short for common name, and is part of the way you look up an entity in LDAP (more on that below). Each entity in LDAP must have an objectClass, which determines the type of entity it is. In this case, I am a Person , as opposed to a device, domain, organizationalRole, or room – all of which are standard objectClasses.\nLet’s say that your corporate LDAP looks like the tree below:\n\n#TODO: make solutions an OU in final\nThe most common way to look up LDAP entities is with their distinguished name (DN), which is the path of names from the point you’re starting all the way back to the root of the tree. In the tree above, my DN would be cn=alex,ou=solutions,dc=example,dc=com.\nNote that you read the DN from right to left to work your way down the tree. Aside from cn for common name, other common fields include ou for organizational unit, and dc for domain component.\n\n\n17.3.3.3 Trying out LDAP\nNow that we understand in theory how LDAP works, let’s try out an actual example.\nTo start, let’s stand up LDAP in a docker container:\n#TODO: update ldif\ndocker network create ldap-net\ndocker run -p 6389:389 \\\n  --name ldap-service \\\n  --network ldap-net \\\n  --detach alexkgold/auth\nldapsearch is a utility that lets us run queries against the LDAP tree. Let’s try it out against the LDAP container we just stood up.\nLet’s say I want to return everything in the subtree under example.org. In that case, I would run ldapsearch -b dc=example,dc=org, where b indicates my search base, which is a dn. But in order to make this actually work, we’ll need to include a few more arguments, including\n\nthe host where the LDAP server is, indicated by -H\nthe bind DN we’ll be using, flagged with -D\nthe bind password we’ll be using, indicated by -w\n\nSince we’re testing, we’re also going to provide the flag -x to use whatever certificate is present on the server. Putting it altogether, along with the commands to reach the docker container, let’s try:\nldapsearch -x -H ldap://localhost:6389 -b dc=example,dc=org -D \"cn=admin,dc=example,dc=org\" -w admin\n\n# extended LDIF\n#\n# LDAPv3\n# base <dc=example,dc=org> with scope subtree\n# filter: (objectclass=*)\n# requesting: ALL\n#\n\n# example.org\ndn: dc=example,dc=org\nobjectClass: top\nobjectClass: dcObject\nobjectClass: organization\no: Example Inc.\ndc: example\n\n# admin, example.org\ndn: cn=admin,dc=example,dc=org\nobjectClass: simpleSecurityObject\nobjectClass: organizationalRole\ncn: admin\ndescription: LDAP administrator\nuserPassword:: e1NTSEF9d3IyVFp6SlAyKy9xT2RsQ0owTDYzR0RzNFo0NUFrQ00=\n\n# search result\nsearch: 2\nresult: 0 Success\n\n# numResponses: 3\n# numEntries: 2\nYou should be able to read what got returned pretty seamlessly. One thing to notice is that the user password is returned, so it can be compared to a password provided. It is encrypted, so it doesn’t appear in plain text.\nNote that ldap is a protocol – so it takes the place of the http you’re used to in normal web operations. Like there’s https, there is also a protocol called LDAPS, which is ldap + tls for the same reason you’ve got https. LDAP is (almost) always running in the same private network as the service, so many organizations don’t require using LDAPS, but others do require it.\nRunning the ldapadmin\ndocker run -p 6443:443 \\\n        --name ldap-admin \\\n        --env PHPLDAPADMIN_LDAP_HOSTS=ldap-service \\\n        --network ldap-net \\\n        --detach osixia/phpldapadmin\ndn for admin cn=admin,dc=example,dc=org pw: admin\nhttps://localhost:6443\n# Replace with valid license\nexport RSC_LICENSE=XXXX-XXXX-XXXX-XXXX-XXXX-XXXX-XXXX\n\n# Run without persistent data and using default configuration\ndocker run -it --privileged \\\n    --name rsc \\\n    --volume $PWD/rstudio-connect.gcfg:/etc/rstudio-connect/rstudio-connect.gcfg \\\n    -p 3939:3939 \\\n    -e RSC_LICENSE=$RSC_LICENSE \\\n    --network ldap-net \\\n    rstudio/rstudio-connect:latest\n\n\n17.3.3.4 Single vs Double Bind\nThere are two different ways to establish a connection between your server and the LDAP server. The first method is called Single Bind. In a single bind authentication, the user credentials are used both to authenticate to the LDAP server, and to query the server.\nIn a Double Bind configuration, there is a separate administrative service account, used to authenticate to the LDAP server. Once authentication is complete, then the user is queried in the system.\nSingle bind configurations are often more limited than double bind ones. For example, in most cases you’ll only be able to see the single user as well as the groups they’re a part of. This can limit application functionality in some cases. On the other hand, there need be no master key maintained on your server, and some admins may prefer it for security reasons.\nWe can see this really concretely. In the example above, you used a double bind by supplying admin credentials to LDAP. Let’s say instead, you just provide a single user’s credentials. In that case, I don’t get anything back if I just do a general search.\nldapsearch -x -H ldap://localhost:6389 -b dc=example,dc=org -D \"cn=joe,dc=engineering,dc=example,dc=org\" -w joe                                       \n# extended LDIF\n#\n# LDAPv3\n# base <dc=example,dc=org> with scope subtree\n# filter: (objectclass=*)\n# requesting: ALL\n#\n\n# search result\nsearch: 2\nresult: 32 No such object\n\n# numResponses: 1\nBut just searching for information about Joe does return his own information.\nldapsearch -x -H ldap://localhost:6389 -b cn=joe,dc=engineering,dc=example,dc=org -D \"cn=joe,dc=engineering,dc=example,dc=org\" -w joe                    32 ✘\n# extended LDIF\n#\n# LDAPv3\n# base <cn=joe,dc=engineering,dc=example,dc=org> with scope subtree\n# filter: (objectclass=*)\n# requesting: ALL\n#\n\n# joe, engineering.example.org\ndn: cn=joe,dc=engineering,dc=example,dc=org\ncn: joe\ngidNumber: 500\ngivenName: Joe\nhomeDirectory: /home/joe\nloginShell: /bin/sh\nmail: joe@example.org\nobjectClass: inetOrgPerson\nobjectClass: posixAccount\nobjectClass: top\nsn: Golly\nuid: test\\joe\nuidNumber: 1000\nuserPassword:: e01ENX1qL01raWZrdk0wRm1sTDZQM0MxTUlnPT0=\n\n# search result\nsearch: 2\nresult: 0 Success\n\n# numResponses: 2\n# numEntries: 1\n\n\n\n17.3.4 Kerberos Tickets\nKerberos is a relatively old ticket-based auth technology. In Kerberos, encrypted tickets are passed around between servers. Because these tickets live entirely on servers under the control of the organization, they are generally quite secure.\nThough Kerberos is freely available, it was widely adopted along with Active Directory, and it’s used almost exclusively in places that are running a lot of Microsoft products. A frequent use of Kerberos tickets is to establish database connections.\nBecause the tickets are passed around from server to server, Kerberos can be used to create a true SSO experience for users.\n\n17.3.4.1 How Kerberos Works\nAll of Kerberos works by sending information to and from the central Kerberos Domain Controller (KDC). In Kerberos, authentication and authorization are handled independently.\nWhen a Kerberos session is initialized, the service sends the users credentials off to the KDC and requests something called the Ticket Granting Ticket (TGT) from the KDC. TGTs have a set expiration period. When they expire, the client has to request an updated TGT. This is one reason why Kerberos is considered quite secure - even if someone managed to steal a TGT, they’d only be able to use it for a little while before it went stale and could be revoked.\nWhen the user wants to actually do something, they send the TGT back to the KDC again and get a session key (sometimes referred to as a service ticket) that allows access to the service, usually with a specified expiration period.\n\n\n\n17.3.4.2 Try out Kerberos\n#TODO\n\n\n\n17.3.5 SAML\nThese days Security Assertion Markup Language (SAML) is probably the most common system that provides true SSO – including single login and centrally-managed permissions. SAML does this by passing around XML tokens.4\nThe way this generally works is that a user attempts to login to a Service Provider (SP). The SP redirects the user to an Identity Provider (IdP), which checks either for a preexisting token in the users browser, or verifies the users credentials. The IdP checks for the user’s authorization to access the SP in question, and sends an authorization token back to the SP.\n\nRelative to LDAP/AD, which is from the early 1990s, SAML is a new kid on the block. SAML 1.0 was introduced in 2002, and SAML 2.0, which is the current standard, came out in 2005. Many large enterprises are switching their systems over to use SAML or have already done so.\nOne superpower of SAML IdPs is that many of them can federate identity management to other systems. So, it’s pretty common for large enterprises to maintain their user base in one or more LDAP/AD system, but actually use a SAML IdP to do authentication and authorization. In fact, this is what Azure Active Directory (AAD), which is Microsoft Azure’s hosted authentication offering does. It is possible to use LDAP/AD with AAD, but most organizations use it with SAML.\nOne of the nice things about SAML is that credentials are never shared directly with the SP. This is one of the ways in which SAML is fundamentally more secure than LDAP/AD – the users credentials are only ever shared with the IdP.\nThere are two different ways logins can occur – starting from the SP, or starting from the IdP.\nIn SAML, the XML tokens that are passed back and forth are called assertions.\n\n17.3.5.1 Try SAML\nWe’re going to use a simple SAML IdP to try out SAML a bit. This container only supports a single SP. Any IdP that might be used in an enterprise environment is going to support many SPs simultaneously.\nLet’s go through the environment variables we’re providing to this docker run command. We’re providing three different arguments:\n\nThe SP_ENTITY_ID is the URL of the\nSP_ASSERTION_CONSUMER_SERVICE is the URL of the SP that is prepared to receive the authorized responses coming back from the SAML IdP.\nSP_SINGLE_LOGOUT_SERVICE is the URL where the SP will receive a logout command once someone has been logged out at the IdP level. Many SPs do not implement single logout.\n\ndocker run --name=saml_idp \\\n-p 8080:8080 \\\n-p 8443:8443 \\\n-e SIMPLESAMLPHP_SP_ENTITY_ID=http://app.example.com \\\n-e SIMPLESAMLPHP_SP_ASSERTION_CONSUMER_SERVICE=http://localhost/simplesaml/module.php/saml/sp/saml2-acs.php/test-sp \\\n-e SIMPLESAMLPHP_SP_SINGLE_LOGOUT_SERVICE=http://localhost/simplesaml/module.php/saml/sp/saml2-logout.php/test-sp \\\n-d kristophjunge/test-saml-idp:1.15\nhttp://localhost:8080/simplesaml\nadmin/secret\n\n\n\n17.3.6 OIDC/OAuth2.0\nOIDC/OAuth is slightly newer than SAML, created in 2007 by engineers at Google and Twitter. OAuth 2.0 – the current standard was released in 2012. If you’re being pedantic, OAuth is a authorization protocol, and OpenID Connect (OIDC) is an authorization protocol that uses OAuth. In most cases, people will just call it OAuth.\n\n#TODO: this picture is bad\nIn an enterprise context, OAuth/OIDC is conceptually very similar to SAML – but instead of passing around XML tokens, it’s based on JSON Web Tokens (JWT, usually pronounced “jot”).\n\nThe fact that JSON is much more human-readable than XML is one of the big advantages of OIDC/OAuth compared to SAML.\n\n#TODO: try it out\n\n17.3.6.1 OAuth/OIDC vs SAML\nFrom a practical perspective, the biggest difference between OAuth/OIDC and SAML is that SAML is quite strict about what SPs are allowed. Each SP needs to be registered at a specific web address that the IdP knows it’s allowed to receive requests from.\nIn contrast, OAuth/OIDC was designed to be used to delegate authentication and authorization to different kinds of services that might be widely available on the internet. If you’ve ever allowed a website to Login with Apple/Google/Facebook/Github, that has been an application of OAuth/OIDC.\nBecause the set of allowable SPs is fixed under SAML, it’s more common in enterprise settings. Some admins consider SAML more secure for that reason as well.\nIn some situations, SAML is used for authentication and OAuth is used for access to other services. Most commonly in the data science world, this can come up when a user logs into a service like RStudio Server and is then authorized to a database using an OAuth JWT.\nResources: https://www.okta.com/identity-101/saml-vs-oauth/ https://www.okta.com/identity-101/whats-the-difference-between-oauth-openid-connect-and-saml/ https://phoenixnap.com/blog/kerberos-authentication https://www.dnsstuff.com/rbac-vs-abac-access-control"
  },
  {
    "objectID": "chapters/sec4/4-3-auth.html#comprehension-questions",
    "href": "chapters/sec4/4-3-auth.html#comprehension-questions",
    "title": "17  Logging in with auth",
    "section": "17.4 Comprehension Questions",
    "text": "17.4 Comprehension Questions\n\nWhat is the difference between authentication and authorization?\nWhat are some different ways to manage permissions? What are the advantages and drawbacks of each?\nWhat is some advantages of token-based auth? Why are most organizations adopting it? Are there any drawbacks?\nFor each of the following, is it a username + password method or a token method? PAM, LDAP, Kerberos, SAML, ODIC/OAuth"
  },
  {
    "objectID": "chapters/sec4/4-4-scaling.html#k8s",
    "href": "chapters/sec4/4-4-scaling.html#k8s",
    "title": "18  Scaling",
    "section": "18.1 Container Deployment + Orchestration",
    "text": "18.1 Container Deployment + Orchestration\nOne tool that comes up increasingly frequently when talking about scaling is Kubernetes (sometimes abbreviated as K8S).4 Kubernetes is the way people orchestrate Docker containers in production settings.5 So basically that it’s the way to put containers into production when you want more than one to interact – say you’ve got an app that separately has a database and a front end in different containers, or, like in this chapter, multiple load-balanced instances of the same containers.\nWhile the operational details of Kubernetes are very different from the horizontal scaling patterns we’ve discussed so far in this chapter, the conceptual requirements are the same.\nTODO: Diagram of K8S\nMany people like Kubernetes because of its declarative nature. If you recall from the section on Infrastructure as Code, declarative code allows you to make a statement about what the thing is you want and just get it, instead of specifying the details of how to get there.\nOf course, in operation this all can get much more complicated, but once you’ve got the right containers, Kubernetes makes it easy to say, “Ok, I want one instance of my load balancer container connected to three instances of my compute container with the same volume connected to all three.”\n\n\n\n\n\n\nKubernetes Tripwire!\n\n\n\nIf you’re reading this and are extremely excited about Kubernetes – that’s great! Kubernetes does make a lot of things easy that used to be hard. Just know, networking configuration is the place you’re likely to get tripped up. You’ve got to deal with networking into the cluster, networking among the containers inside the cluster, and then networking within each container.\nComplicated kubernetes networking configurations are not for the faint of heart.\n\n\nFor individual data scientists, Kubernetes is usually overkill for the type of work you’re doing. If you find yourself in this territory, it’s likely you should try to work with you organization’s IT/Admin group.\nOne of the nice abstraction layers Kubernetes provides is that in Kubernetes, you provide declarative statements of the containers you want to run, and any requirements you have. You separately register actual hardware with the cluster, and Kubernetes takes care of placing the conatiners onto the hardware depending on what you’ve got available.\nIn practice, unless you’re part of a very sophisticated IT organization, you’ll almost certainly use Kubernetes via one of the cloud providers’ Kubernetes clusters as a service. AWS’s is called Elastic Kubernetes Service (EKS).6\nOne really nice thing about using these Kubernetes clusters as a service is that adding more compute power to your cluster is generally as easy as a few button clicks. On the other hand, that also makes it dangerous from a cost perspective.\nIt is possible to define a Kubernetes cluster “on the fly” and deploy things to a cluster in an ad hoc way. I wouldn’t recommend this for any production system. Helm is the standard tool for defining kubernetes deployments in code, and Helmfile is a templating system for Helm.\nSo, for example, if you had a standard “Shiny Server” that was one load balancer containers, two containers each running a Shiny app, and a volume mounted to both, you would define that cluster in Helm. If you wanted to be able to template that Helm code for different clusters, you’d use Helmfile."
  },
  {
    "objectID": "chapters/sec4/4-4-scaling.html#comprehension-questions",
    "href": "chapters/sec4/4-4-scaling.html#comprehension-questions",
    "title": "18  Scaling",
    "section": "18.2 Comprehension Questions",
    "text": "18.2 Comprehension Questions\n\nWhat is the difference between horizontal and vertical scaling? For each of the following examples, which one would be more appropriate?\n\nYou’re the only person using your data science workbench and run out of RAM because you’re working with very large data sets in memory.\nYour company doubles the size of the team that will be working in your data science workbench. Each person will be working with reasonably small data, but there’s going to be a lot more of them.\nYou have a big modeling project that’s too large for your existing machine. The modeling you’re doing is highly parallelizable.\n\nWhat is the role of the load balancer in horizontal scaling? When do you really need a load balancer and when can you go without?\nWhat are the biggest strengths of Kubernetes as a scaling tool? What are some drawbacks?"
  },
  {
    "objectID": "chapters/append/docker-cheatsheet.html#general-docker-commands",
    "href": "chapters/append/docker-cheatsheet.html#general-docker-commands",
    "title": "Appendix A — Docker Cheatsheet",
    "section": "A.1 General Docker Commands",
    "text": "A.1 General Docker Commands\n\n\n\n\n\n\n\n\nCommand\nPurpose\nExample\n\n\n\n\ndocker run\nRun an image as a container\ndocker run me/my-image\n\n\ndocker ps\nList all containers\ndocker ps\n\n\ndocker kill\nKill a container\ndocker kill my-container\n\n\ndocker exec\nRun a command inside a running container\ndocker exec -it /bin/bash\n\n\ndocker build\nBuild a Dockerfile\ndocker built -t me/my-image .\n\n\ndocker logs\nGet logs from a container\ndocker logs my-container\n\n\ndocker pull\nPull a container from a registry\ndocker pull me/my-image\n\n\ndocker push\nPush a container to a registry\ndocker push me/my-image"
  },
  {
    "objectID": "chapters/append/docker-cheatsheet.html#docker-run-command-flags",
    "href": "chapters/append/docker-cheatsheet.html#docker-run-command-flags",
    "title": "Appendix A — Docker Cheatsheet",
    "section": "A.2 docker run command flags",
    "text": "A.2 docker run command flags\n\n\n\n\n\n\n\n\nFlag\nPurpose\nExample\n\n\n\n\n-d\nRun in “detached” mode that doesn’t block your terminal\ndocker run -d ...\n\n\n--rm\nRemove the container on stop\nReminder: don’t use in prod\ndocker run --rm …\n\n\n-p\nPublish ports from container to host\ndocker run -p 8000:8000 …\n\n\n-v\nMount a volume into the container\ndocker run -v $(pwd):/data\n\n\n--name\nGive container a human-friendly name\ndocker run --name my-container\n\n\n\nReminder - -p and -v order is <host>:<container>"
  },
  {
    "objectID": "chapters/append/docker-cheatsheet.html#dockerfile-commands",
    "href": "chapters/append/docker-cheatsheet.html#dockerfile-commands",
    "title": "Appendix A — Docker Cheatsheet",
    "section": "A.3 Dockerfile Commands",
    "text": "A.3 Dockerfile Commands\nThese are the commands that go in a Dockerfile when you’re building it.\n\n\n\n\n\n\n\n\nCommand\nPurpose\nExample\n\n\n\n\nFROM\nIndicate base container\nFROM rocker/r-ver:4.1.0\n\n\nRUN\nRun a command when building\nRUN apt-get update\n\n\nCOPY\nCopy from the working directory into the container\nCOPY . /app/\n\n\nCMD\nSpecify the command to run when the container starts\nCMD quarto render ."
  }
]