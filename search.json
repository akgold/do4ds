[
  {
    "objectID": "index.html#software-information-and-conventions",
    "href": "index.html#software-information-and-conventions",
    "title": "DevOps for Data Science",
    "section": "Software information and conventions",
    "text": "Software information and conventions\nI used the knitr package (Xie 2015) and the quarto package (quarto?) to compile my book. My R session information is shown below:\n\nxfun::session_info()\n\nR version 4.0.2 (2020-06-22)\nPlatform: x86_64-pc-linux-gnu (64-bit)\nRunning under: Ubuntu 20.04.3 LTS\n\nLocale:\n  LC_CTYPE=C.UTF-8       LC_NUMERIC=C          \n  LC_TIME=C.UTF-8        LC_COLLATE=C.UTF-8    \n  LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   \n  LC_PAPER=C.UTF-8       LC_NAME=C             \n  LC_ADDRESS=C           LC_TELEPHONE=C        \n  LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   \n\nPackage version:\n  base64enc_0.1.3 compiler_4.0.2  digest_0.6.25  \n  evaluate_0.14   fastmap_1.1.0   glue_1.4.2     \n  graphics_4.0.2  grDevices_4.0.2 highr_0.8      \n  htmltools_0.5.2 jquerylib_0.1.3 jsonlite_1.7.1 \n  knitr_1.36      magrittr_1.5    methods_4.0.2  \n  renv_0.15.2     rlang_0.4.10    rmarkdown_2.11 \n  stats_4.0.2     stringi_1.4.6   stringr_1.4.0  \n  tinytex_0.35    tools_4.0.2     utils_4.0.2    \n  xfun_0.28       yaml_2.2.1     \n\n\nPackage names are in bold text (e.g., rmarkdown), and inline code and filenames are formatted in a typewriter font (e.g., knitr::knit('foo.Rmd')). Function names are followed by parentheses (e.g., bookdown::render_book())."
  },
  {
    "objectID": "index.html#about-the-author",
    "href": "index.html#about-the-author",
    "title": "DevOps for Data Science",
    "section": "About the Author",
    "text": "About the Author\nAlex Gold is a manager on the Solutions Engineering team at RStudio.\nHe works with customers of RStudio’s professional software to help them deploy, configure, and use RStudio’s professional software and open source tooling in R and Python.\nIn his free time, he enjoys landscaping, handstands, and Tai Chi.\nHe occasionally blogs about data, management, and leadership at alexkgold.space."
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "DevOps for Data Science",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nA lot of people are helping me write this book.\nThis book is published to the web using GitHub Actions from rOpenSci."
  },
  {
    "objectID": "index.html#color-palette",
    "href": "index.html#color-palette",
    "title": "DevOps for Data Science",
    "section": "Color palette",
    "text": "Color palette\nTea Green: CAFFDO Steel Blue: 3E7CB1 Kombu Green: 273c2c Bright Maroon: B33951 Sandy Brown: FCAA67\n\n\n\n\nXie, Yihui. 2015. Dynamic Documents with R and Knitr. 2nd ed. Boca Raton, Florida: Chapman; Hall/CRC. http://yihui.name/knitr/."
  },
  {
    "objectID": "chapters/intro.html#moving-data-science-to-a-server",
    "href": "chapters/intro.html#moving-data-science-to-a-server",
    "title": "Introduction",
    "section": "Moving Data Science to A Server",
    "text": "Moving Data Science to A Server\nIn recent years, as data science has become more central to organizations, many have been moving their operations off of individual contributors’ laptops and onto centralized servers. Depending on your organization, the centralization of data science operations can make your life way easier – or it can be kinda a bummer.\nServer migrations can work well regardless of whether they’re instigated by the data science or the IT organization. The biggest determinant is how well the data science and IT/DevOps teams can collaborate.\nData scientists are good at manipulating and using data, but most have little expertise in SysAdmin work, and aren’t really that interested. On the flip side, IT/DevOps organizations usually don’t really understand data science workflows, the data science development process, or how data scientists use R and Python.\nOften, migrations to a server are instigated by the data scientists themselves – usually because they’ve run out of horsepower on their laptops. If you, or one of your teammates, enjoys and is good as SysAdmin work, this can be a great situation! You get the hardware you need for your project quickly and with minimal interference.\nOn the other hand, most data scientists don’t really want to be SysAdmins, and these systems are often fragile, isolated from other corporate systems, and potentially susceptible to security vulnerabilities.\nOther organizations are moving to servers as well, but led by the IT group. For many IT groups, it’s way easier to maintain a centralized server environment, as opposed to helping each data scientist maintain their own environment on their laptop.\nHaving just one platform makes it much easier to give shared access to more powerful computing platforms, to data sources that require some configuration, and to R and Python packages that wrap around system libraries and can be a pain to configure (looking at you, rJava).\nThis can be a great situation for data scientists! If the platform is well-configured and scoped, you can get instant access through their web browser to more compute resources, and don’t have to worry about maintaining local installations of data science tools like R, Python, RStudio, and Jupyter, and you don’t need to worry about how to connect to important data sources – those things are just available for use.\nBut this can also be a bad experience. Long wait times for hardware or software updates, overly restrictive policies – especially around package management – and misunderstandings of what data scientists are trying to do on the platforms can lead to servers going largely unused.\nSo much of whether the server-based experience is good or not depends on the relationship between the data science and IT/Admin group. In organizations where these groups work together smoothly, this can be a huge win for everyone involved. However, there are some organizations where IT/Admins are so concerned with stability and security that they make it impossible to do data science, and the data scientists spend all their time playing cat-and-mouse games to try to get work done behind IT/Admin’s backs.\nIf you work at such a place, it’s frankly hard to get much done on the server. It’s probably worth investing some time into improving your relationship with your favorite person on the IT/Admin team. Hopefully, this book will help you understand a little of what’s on the minds of people in the IT group, and a sense of how to talk to them better."
  },
  {
    "objectID": "chapters/intro.html#intro-reprise-to-edit",
    "href": "chapters/intro.html#intro-reprise-to-edit",
    "title": "Introduction",
    "section": "Intro (reprise – to edit)",
    "text": "Intro (reprise – to edit)\nMuch data science takes place on the laptops of data scientists the world over.\nHowever, workloads are increasingly outgrowing the capacity of laptops, data science apps and assets are becoming more and more mission-critical, and organizations are centralizing their data science functions. As a result, more and more data science is taking place in ways that interact with tools and systems traditionally within the domain of the IT group.\nSometimes this group is called DevOps, and increasingly there are standalone groups for MLOps or DataOps.\nIn some organizations, the DevOps group wants to support data scientists, but may not be sure how. In other organizations, data scientists are on their own to figure out how to operate.\nI lead the Solutions Engineering team at RStudio. My team sits exactly in this odd space in between the data scientists and the IT/Admin/DevOps organization. We constantly sit on calls to help the IT/Admin group understand what the data scientists need and help the data scientists ask the right questions of the IT/Admin team.\nThis book is split into three parts.\nPart one is about tools and strategies DevOps professionals know all about that data scientists might want to use in their everyday work as they “go to production”. This will include topics like developing and testing apps as they go, using CI/CD to deploy, and more.\nPart two is for data scientists who might have to set up their own server-based data science workbenches, and will provide both conceptual understanding and hands-on knowledge of how to set up your own data science workbench.\nThe final part is for people who have the support of a DevOps/IT/Admin function at their organization, but don’t know how to communicate. These are tools and technologies that are commonplace at many large organizations – and are probably totally foreign to most data scientists. The second half of this book is less hands-on. I don’t expect many readers of this book to be setting up their own LDAP server for authentication or setting up a high-availability cluster of servers.\nBut you might have to collaborate with your organization’s IT/Admin group on such things. If you do, hopefully this book will help you understand the important questions to ask, the things you need to explain, and maybe make that relationship a little easier to manage."
  },
  {
    "objectID": "chapters/intro.html#book-outline-remove-later",
    "href": "chapters/intro.html#book-outline-remove-later",
    "title": "Introduction",
    "section": "Book Outline (Remove Later)",
    "text": "Book Outline (Remove Later)\nI. DevOps for Data Scientists\n\nCommand Line + SSH\nCode Promotion: Dev/Test/Prod\n\nGit\nCI/CD\n\nReproducibility + Package Management\nData Access\nContainers, Docker, and Kubernetes\nUnderstanding the Cloud\n\nII. Your Own Data Science Workbench\n\nUnderstanding Servers\nIntro Walkthrough\nUnderstanding Network Traffic\nPublic URLs + HTTPS\nInternal Network Config\nInfrastructure as Code Tooling\n\nIII. Communicating with IT/Admin\n\nAuthentication\nOffline Operations\nScaling"
  },
  {
    "objectID": "chapters/sec1/cmd-line.html#what-is-ssh",
    "href": "chapters/sec1/cmd-line.html#what-is-ssh",
    "title": "1  The Command Line + SSH",
    "section": "2.1 What is SSH?",
    "text": "2.1 What is SSH?\n\nStands for secure (socket) shell\nWay to remotely access a different (virtual) machine\nWorkhorse of doing work on another server"
  },
  {
    "objectID": "chapters/sec1/cmd-line.html#how-does-ssh-work",
    "href": "chapters/sec1/cmd-line.html#how-does-ssh-work",
    "title": "1  The Command Line + SSH",
    "section": "2.2 How does SSH work?",
    "text": "2.2 How does SSH work?\n\nVia Public Key Encryption\n\nPublic/Private Key\nKnown Hosts [Diagram: SSH Keys] > Sidebar on public key encryption\n\nBy default, available on port 22"
  },
  {
    "objectID": "chapters/sec1/cmd-line.html#basic-ssh-use",
    "href": "chapters/sec1/cmd-line.html#basic-ssh-use",
    "title": "1  The Command Line + SSH",
    "section": "2.3 Basic SSH Use",
    "text": "2.3 Basic SSH Use\n\nThe terminal\n3 step process\n\ngenerate public/private keys ssh-keygen\nplace keys in appropriate place\nuse ssh to do work\n\nPermissions on key"
  },
  {
    "objectID": "chapters/sec1/cmd-line.html#getting-comfortable-in-your-own-setup",
    "href": "chapters/sec1/cmd-line.html#getting-comfortable-in-your-own-setup",
    "title": "1  The Command Line + SSH",
    "section": "2.4 Getting Comfortable in your own setup",
    "text": "2.4 Getting Comfortable in your own setup\n\nUsing ssh-config"
  },
  {
    "objectID": "chapters/sec1/cmd-line.html#advanced-ssh-tips-tricks",
    "href": "chapters/sec1/cmd-line.html#advanced-ssh-tips-tricks",
    "title": "1  The Command Line + SSH",
    "section": "2.5 Advanced SSH Tips + Tricks",
    "text": "2.5 Advanced SSH Tips + Tricks\n\nSSH Tunneling/Port Forwarding\n-vvvv, -i, -p [Diagram: Port Forwarding]"
  },
  {
    "objectID": "chapters/sec1/cmd-line.html#exercises",
    "href": "chapters/sec1/cmd-line.html#exercises",
    "title": "1  The Command Line + SSH",
    "section": "2.6 Exercises",
    "text": "2.6 Exercises\n\nDraw your own diagram of the sequence of things that occur when you execute ssh user@server.company.com in the terminal.\nStand up a new server on one of the major cloud providers. Try logging in using the provided key file. Create a new user on the server.\nGenerate a new SSH key locally and copy the correct key onto the server (think for a moment about which key is the correct one – consult your diagram from step 1 if necessary).\nSet up an SSH alias so you can SSH into your server using just ssh testserver (hint: look at your SSH config).\nCreate a local text file and SFTP it to the server. SSH in, edit the file, and SFTP it back.\n[Advanced] Stand up a nginx server on your remote instance. Don’t open the port to the world, but SSH port forward the server page to your local browser."
  },
  {
    "objectID": "chapters/sec1/code-promotion.html#devtestprod",
    "href": "chapters/sec1/code-promotion.html#devtestprod",
    "title": "2  Code Promotion Workflows",
    "section": "2.1 Dev/Test/Prod",
    "text": "2.1 Dev/Test/Prod\nConceptually, the use of each environment is quite straightforward. Dev is a development environment – a sandbox where new things can be developed freely with no risk. In a data science context, this generally means that dev and test environments may need to be connected to input data, but that output needs to be somehow mocked or redirected so it doesn’t leak back into prod.\nThe test environment is for (shocker) testing. This likely includes functional testing that things work as intended, but also might include UAT. In the context of data science assets, UAT usually answers questions like whether labels and buttons are clear and whether plots and graphs answer exactly the intended questions.\nThe production environment is for production – meaning use by people and machines outside the development process. Changes are never made in prod, except when they’ve already been validated in the test environment, and then the only thing that happens in prod is a quick update to the newest testing version.\nThis obviously requires that the dev, test, and prod environments be very close mirrors of each other. For example, doing test on a Windows laptop and then going to prod on a Linux server introduces a potential that things that worked in test suddenly don’t when going to prod. For that reason, making all three (or at least test and prod) match as precisely as possible is essential (more in chapter 1.3). The need to match these three environments so precisely is one reason for data science workloads moving onto servers.\nIn a data science context, the dev/test/prod sequence looks a little different from in an IT context. In an IT context, dev/test/prod means three identical environments. This is generally not the case in a data science context.\nFor a data scientist, dev is often a different environment than test or prod. For example, it’s likely that you would do your development of your asset in your favorite R or Python data science IDE – perhaps RStudio, JupyterHub, Spyder, or PyCharm. This environment might be server-based, or it might be on your laptop.\nThe biggest reason for this difference is that pure DevOps involves building a known product according to an identified need. Data science is different. The early parts of a data science project are about discovery and iteration – that means that you can’t start “development” right off the bat. There’s first some research and discovery that happens in an environment well-suited to such things before promotion happens into a “deployment” environment.\nIn contrast, traditional software development rarely starts with discovery. Instead, it starts with producing a minimal set of features that has been identified ahead of time. This difference causes much friction and misunderstanding between data science and IT/Admin/DevOps teams.\nAt that point, you’ll promote your asset into test, which might be a docker container, straight onto a server, or a deployment platform like RStudio Connect. Then, once everything is ready to go, you’d promote it into prod. This is somewhat different than a traditional dev/test/prod setup where all three environments are very precise mirrors of each other. In some cases, like if you’re using a product that hosts your apps for you, test and prod may even be on the same physical server, which would be very unusual in the software development world.\nDepending on how dependable your prod environment has to be, it can be useful to maintain a “two dimensional” dev/test/prod setup as in the graphic below.\n\nIn this setup, you would select the IT configuration that works for your organization and maintain one or two copies of the entire environment. I often call this a staging environment to differentiate it from the dev/test/prod environments for the data science assets.\nSo when you wanted to make a chance to the underlying servers or their architecture, that would be tested in the staging environment and then deployed to production. Data scientists would never work in the staging environment (except as testers), that’s purely for IT/Admin testing. The staging environment would include all of the environments data scientists would use – dev, test, and prod.\nThen, data science code promotion through dev/test/prod would be distinct from how server changes get made."
  },
  {
    "objectID": "chapters/sec1/code-promotion.html#cattle-not-pets",
    "href": "chapters/sec1/code-promotion.html#cattle-not-pets",
    "title": "2  Code Promotion Workflows",
    "section": "2.2 Cattle, Not Pets",
    "text": "2.2 Cattle, Not Pets\nIn the IT world, there’s a phrase that servers should be cattle, not pets. The idea here is that servers should be unremarkable and that each one should be more-or-less interchangeable. This matters, for example, in making sure your test and prod environments look exactly the same.\nA bad pattern then would look like this – I develop an update to an important Shiny or Dash app in my local environment and then move it onto a server. At that point, the app doesn’t quite work and I make a bunch of manual changes to the environment – say adjusting file paths or adding R or Python packages. Those manual changes end up not really being documented anywhere. A week later, when I go to update the app in prod, it breaks on first deploy, because the server state of the test and prod servers drifted out of alignment.\nThe main way to combat this kind of state drift is to religiously use state-maintaining infrastructure as code (IaC) tooling. That means that all changes to the state of your servers ends up in your IaC tooling and no “just login and make it work” shenanigans are allowed in prod.\nIf something breaks, you reproduce the error in staging, muck around until it works, update your IaC tooling to fix the broken thing, test that the thing is fixed, and then (and only then) push the updated infrastructure into prod directly from your IaC tooling.\n\n2.2.1 Infrastructure As Code Tooling\nThere are many, many different varieties of infrastructure as code tooling. There are many books on infrastructure as code tooling and I won’t be covering them in any depth here. Instead, I’ll share a few of the different “categories” (parts of the stack) of infrastructure as code tooling and suggest a few of my favorites.\nTo get from “nothing” to a usable server state, there are (at minimum) two things you need to do – provision the infrastructure you need, and configure that infrastructure to do what you want.\nFor example, let’s say I’m standing up a server to deploy a simple shiny app. In order to get that server up, I’ll need to stand up an actual server, including configuring the security settings and networking that will allow the proper people to access the server. Then I’ll need to install a version of R on the server, the Shiny package, and a piece of hosting software like Shiny Server.\nSo, for example, you might use AWS’s CloudFormation to stand up a virtual private cloud (VPC), put an EC2 server instance inside that VPC, attach an appropriately-sized storage unit, and attach the correct networking rules. Then you might use Chef to install the correct software on the server and get your Shiny app up-and-running.1\nIn infrastructure as code tooling, there generally isn’t a clear dividing line between tools that do provisioning and tools that do configuration…but most tools lean one way or the other.\nBasically any tool does provisioning will directly integrate into the APIs of the major cloud providers to make it easy to provision cloud servers. Each of the cloud providers also has their own IaC tool, but many people prefer to use other tools when given the option (to be delicate).\nThe other important division in IaC tools is declarative vs imperative. In declarative tooling, you simply enumerate the things you want, and the tool makes sure they get done in the right order. In contrast, an imperative tool requires that you provide actual instructions to get to where you want to go.\nIn many cases, it’s easy to be declarative with provisioning servers, but it’s often useful to have a way to fall back to an imperative mode when configuring them because there may be dependencies that aren’t obvious to the provisioning tool, but are easy to put down in code. If the tool does have an imperative mode, it’s also nice if it’s compatible with a language you’d be comfortable with.2\nOne somewhat complicated addition to the IaC lineup is Docker and related orchestration tools. There’s a whole chapter on containerization and docker, so check that out if you want more details. The short answer is that docker can’t really do provisioning, but that you can definitely use docker as a configuration management IaC tool, as long as you’re disciplined about updating your Dockerfiles and redeployment when you want to make changes to the contents.\nBasically none of these tools will save you from your own bad habits, but they can give you alternatives.\nIn short, exactly which tool you’ll need will depend a lot on what you’re trying to do. Probably the most important question in choosing a tool is whether you’ll be able to get help from other people at your organization on it. So if you’re thinking about heading into IaC tooling, I’d suggest doing a quick survey of some folks in DevOps and choosing something they already know and like."
  },
  {
    "objectID": "chapters/sec1/code-promotion.html#cicd-and-the-mechanics-of-code-promotion",
    "href": "chapters/sec1/code-promotion.html#cicd-and-the-mechanics-of-code-promotion",
    "title": "2  Code Promotion Workflows",
    "section": "2.3 CI/CD and The mechanics of code promotion",
    "text": "2.3 CI/CD and The mechanics of code promotion\nThe common term for the mechanics of code promotion are Continuous Integration/Continuous Deployment (CI/CD). The actual practice of CI/CD relates deeply to the philosophy of DevOps and agile software development in ways that are beyond the scope of this book.3\nIn this book I’m mainly going to focus on a few workflows and a few tools that I’ve found particularly useful in a data science context.\nThe most concrete way that CI/CD practices get expressed is through integrations into source control software – usually Git. If you’re not already familiar, I’d suggest spending some time learning git. I will admit that learning git is nontrivial. People who say git is easy are either lying to look smarter or learned so long ago that they have forgotten the early-to-git sense that you’re likely to mess up your entire workflow at any moment.\n\nIf you don’t already know git and want to learn, I’d recommend HappyGitWithR by Jenny Bryan. It’s a great on-ramp to learn git.\nEven if you’re a Python user, the sections on getting started with git, on basic git concepts, and on workflows will be useful since they approach git from a data science perspective.\n\nFor the purposes of this section, I’m going to assume you at least conceptually understand what git branches are and what a merge is – as much of your CI/CD pipeline will be based on what happens when you merge.\nFor production data science assets, I generally recommend long-running dev (or test) and prod branches, with feature branches for developing new things. The way this works is that new features are developed in a feature branch, merged into dev for testing, and then promoted to prod when you’re confident it’s ready.\nFor example, if you had two new plots you were adding to an existing dashboard, your git commit graph might look like this:\n\n\\[TODO: change dev to test to match above, label merges\\]\nCI/CD adds a layer on top of this. CI/CD allows you to integrate functional testing by automatically running those tests whenever you do something in git. These jobs can run when a merge request is made, and are useful for tasks like spellchecking, linting, and running tests.\nFor the purposes of CI/CD, the most interesting jobs are those that do something after there’s a commit or a completed merge, often deploying the relevant asset to its designated location.\nSo a CI/CD integration using the same git graph as above would have released 3 new test versions of the app and 2 new prod versions. Note that in this case, the second test release revealed a bug, which was fixed and tested in the test version of the app before a prod release was completed.\nIn years past, the two most popular CI/CD tools were called Travis and Jenkins. By all accounts, these tools were somewhat unwieldy and difficult to get set up. More recently, GitHub – the foremost git server – released GitHub Actions (GHA), which is CI/CD tooling directly integrated into GitHub that’s free for public repositories and free up to some limits for private ones.\nIt’s safe to say GHA is eating the world of CI/CD.4\nFor example, if you’re reading this book online, it was deployed to the website you’re currently viewing using GHA. I’m not going to get deep into the guts of GHA, but instead talk generally about the pattern for deploying data science assets, and then go through how I set up getting this book to run on GHA.\n\n2.3.1 Using CI/CD to deploy data science assets\nIn general, using a CI/CD tool to deploy a data science asset is pretty straightforward. The mental model to have is that the CI/CD tool stands up a completely empty server for you, and runs some code on it.\nThat means that you’re just doing something simple like spellchecking, you can probably just specify to run spellcheck. If you’re doing something more complicated, like rendering an R Markdown document or Jupyter Notebook and then pushing it to a server, you’ll have to take a few extra steps to be sure the right version of R or Python is on the CI/CD server, that your package environment is properly reproduced, and that you have the right code to render your document.\nFeel free to take a look through the code for the GitHub Action for this book. It’s all YAML, so it’s pretty human-readable.\nHere’s what happens every time I make a push to the main branch of the repository for this book:5\n\nCheckout the current main branch of the book.\nUse the r-lib action to install R.\nUse the r-lib action to setup pandoc (a required system library for R Markdown to work).\nGet the cached renv library for the book.\nRender the book.\nPush the book to GitHub Pages, where this website serves from.\n\nYou’ll see that it uses a mixture of pre-defined actions created for general use, pre-defined actions created by people in the R community, and custom R code I insert to restore an renv library and render the book itself."
  },
  {
    "objectID": "chapters/sec1/code-promotion.html#exercises",
    "href": "chapters/sec1/code-promotion.html#exercises",
    "title": "2  Code Promotion Workflows",
    "section": "2.4 Exercises",
    "text": "2.4 Exercises\n\nCreate something and add GHA integration \\[TODO: What to use as example?\\]\nStand up some virtual environments using ___ \\[TODO: Which ones to try?\\]"
  },
  {
    "objectID": "chapters/sec1/repro-pkgs.html#the-environment",
    "href": "chapters/sec1/repro-pkgs.html#the-environment",
    "title": "3  Environments and Reproducibility",
    "section": "3.1 The Environment",
    "text": "3.1 The Environment\nThe example above talked about reproducibility as a function of what you did in your code – did you bother to set a seed for your random number generator or not?\nBut for the purpose of this chapter, that’s the last we’re going to talk about code reproducibility. Instead, we’re going to discuss re-creating the environment. You might well ask: well, what is the environment?\nFor the purposes of data science, I think of everything that isn’t code you’ve written or the data you’re using. The environment is the mixing bowl where those two things meet to do your work. In a lot of cases, data scientists can get away without thinking about the environment…at least for a while.\n\nOften, you’ve got an environment, probably on your laptop, where things work well enough. The problem comes later. Sometimes this comes up when collaborating on a piece of work – how many times have you told a collaborator, “Well, it works on my machine…” Other times it comes up when you get to think about taking an app or report into production – thinking about how to make sure your app or report will reliably run for an extended period of time. And sometimes it comes up in the context of archiving what you’re doing - if you need to re-run this later to verify results, are you confident that it will run cleanly and return the same results.\nAs I said above, there is no perfect answer to reproducing an environment, but there are some general considerations to keep in mind, and a few tools and strategies I’ve found that are useful.\nIn computational terms, the environment is everything that isn’t the code or data themselves. There are layers of environment, one atop the other, right down to the physical hardware.\n[TODO: redo graphic as layers, add hardware]\n\nAgain, depending on your requirements, there may be different answers to your reproducibility issues. In almost all cases, it’s worthwhile to make sure you can easily reproduce your language and package environments. This can solve a lot of headaches down the road, and I recommend it in almost all cases that aren’t just a one-off quick analysis (and sometimes even then!).\nBut there are also highly-regulated industries where you have to ensure the reproducibility of results down to the level of hardware-specific machine instructions and your version of reproducibility might involve just keeping hardware running for a decade to make sure the environment is truly and completely reproducible."
  },
  {
    "objectID": "chapters/sec1/repro-pkgs.html#two-layers-of-environment-management",
    "href": "chapters/sec1/repro-pkgs.html#two-layers-of-environment-management",
    "title": "3  Environments and Reproducibility",
    "section": "3.2 Two Layers of Environment Management",
    "text": "3.2 Two Layers of Environment Management\nIn most cases, you won’t have to persist your physical hardware – and if you’re in the cloud that’s not really even an option. In that case, I generally recommend two “layers” of tooling to ensure environmental reproducibility.\n\nIf you need to maintain your hardware, that’s beyond the scope of this book. But I am going to suggest two sets of tools for managing the R and Python package environments you might have, as well as the underlying versions of R and Python, the system libraries, and the operating system.\n\n3.2.1 Reproducing Package Environments\nFor data science projects, it’s generally worth using a language-specific tool to manage the set of packages you want in your environment. In R, {renv} is the standard on this front. In Python, there are many different options. In the context of production data science, I recommend {virtualenv}/{venv} and related tools.\n\nA sidebar on Conda\nMany data scientists love Conda for managing their Python environments.\nConda is a great tool for its main purpose – allowing you to create a data science environment on your local laptop, especially when you don’t have root access to your laptop because it’s a work machine that’s locked down by the admins.\nIn the context of a production environment, Conda smashes together the language version, the package management, and (sometimes) the system library management. This has the benefit of being conceptually simple and easy-to-use. But I’ve often seen it go awry in production environments, and I generally recommend people use a tool that’s just for package management, like {virtualenv}, as opposed to an all-in-one tool like Conda.\n\nIn both R and Python, these tools accomplish two main tasks for you:\n\nThey create a standalone package environment that’s independent of anything else you do on your computer. That means that you can have another project with different package requirements that won’t mess up this one.\nThey make it easy to record the exact package requirements for your project.\n\nOn a high level, the workflows for these tasks are similar between R and Python. However, there are some meaningful differences in tooling – especially because virtually every computer arrives with a system version of Python installed, while R is only ever installed by a user trying to do data science tasks. At the end of the day, this actually makes it harder to use Python because you do not want to use your system Python for your data science work…but sometimes it accidentally gets into the mix.\nA general suggestion of workflows for data science package management, whether in R or Python – this should be independently done for every project:\n\nCreate a new directory for the project.\nIf you’re in R, make sure you’ve got renv install.packages(\"renv\")\nCreate a standalone library for the project.\n\nrenv::init()\npython -m venv .venv\n\nInstall packages into your project\n\n(note R done from inside a session, python done from command line)\nRenv does some clever caching so if you use the same package in multiple projects, installs will be fast\nTODO: does venv do the same?\n\ninstall.packages(…)\npython -m pip install …\n\n\nSnapshot your package state\n\nrenv::snapshot()\npip freeze > requirements.txt\n\n\nWhen you come back to your project later, you just reactivate these environment, and you’re back into your isolated project environment.\n\nRun the appropriate activation command.\n\nrenv::activate() (automatically in project-level .Rprofile)\nsource .venv/bin/activate\n\n\nThen, when you want to collaborate or move this project to a different machine, you just move the renv lockfile or the requirements.txt. In general, these tools come with default .gitignores that move just the file that describes the package set and not the packages themselves. This is a good thing!\nIn general, packages are specific to the underlying machine – in particular to the operating system and language version, so the optimal thing is to move just the file recording the required versions and then restore those versions wherever you end up, as opposed to moving live package binaries that may or may not work in a new location.\n\nMove your renv lockfile or requirements.txt into place.\nCreate an renv or virtual env\n\nrenv::init()\npython -m venv .venv\n\nRestore the captured set of packages from the lockfile/requirements\n\nrenv::restore()\npip install -r requirements.txt\n\n\nTODO: add section on inspecting lockfiles to understand\n\n\n3.2.2 Reproducing the rest of the stack\nSometimes, just recording the package environment and moving that around is sufficient. In many cases, old versions of R and Python are retained in the environment, and that’s sufficient.\nThere are times where you need to reproduce elements further down the stack. In some highly-regulated industries, you’ll need to go further down the stack because of requirements for numeric reproducibility. Numeric routines in both R and Python call on system-level libraries, often written in C++ for speed. While it’s unlikely that upgrades to these libraries would cause changes to the numeric results you get, it can happen, and it may be worth maintaining parts of the stack.\nIn other cases, your R or Python library might basically just be a wrapper for system libraries. For example, many popular packages for geospatial analysis are just thin language wrappers that call out to the system libraries. In this case, it might be important to be able to maintain a particular version of the underlying system library to ensure that your code runs at all in the future.\nThere are many tools you can use to record and reproduce the R and Python versions you’re using, the system libraries, and the operating system itself. Many of these fall into the category of Infrastructure-as-Code configuration tools.\nThese days, the clear leader of the pack on this front is Docker. It has become an increasingly popular way to create, maintain, and use standalone environments – and for good reason! In fact, the next chapter is going to be all about the use of Docker in data science. However, it’s worth keeping in mind that if you’re working in the context of a formally-supported IT organization, they may have other tooling they prefer to create and maintain environments, and they can be equally valid."
  },
  {
    "objectID": "chapters/sec1/docker.html#container-orchestration",
    "href": "chapters/sec1/docker.html#container-orchestration",
    "title": "4  Containers, Docker, and Orchestration",
    "section": "4.1 Container Orchestration",
    "text": "4.1 Container Orchestration\nKubernetes (K8S) – software for deploying and managing containers\nHelm is the standard tool for defining kubernetes deployments.\nHelmfile is a templating system for helm.\nThere are other competitors, most notably docker swarm, but K8S is by far the biggest\nThe line is fuzzy though – there are container orchestration services that aren’t K8S or even abstract a level up from K8S.\n[TODO: Graphic of K8S/Docker]"
  },
  {
    "objectID": "chapters/sec1/docker.html#exercises",
    "href": "chapters/sec1/docker.html#exercises",
    "title": "4  Containers, Docker, and Orchestration",
    "section": "4.2 Exercises",
    "text": "4.2 Exercises\n\nPut a shiny app in a container, run it on your desktop.\nPut that container into a container registry.\nUse a local K8S distribution to run several instances of that container."
  },
  {
    "objectID": "chapters/sec1/data.html#options-for-data-storage",
    "href": "chapters/sec1/data.html#options-for-data-storage",
    "title": "5  Choosing and Using A Data Architecture",
    "section": "5.1 Options for Data Storage",
    "text": "5.1 Options for Data Storage\n\n3 basic types of data stores: flat file (csv, pickle/rds), flat file w/ interactive read (arrow), database\nImplications how to configure access\nDatabase is standalone option, usually requires configuring separate server\nOther two can be configured two main ways:\n\nFile system\nBucket Storage\n\nData Lake – usually for raw data, basically bucket storage writ large, sometimes include interacticve read capabilities\nData Warehouse – usually a database"
  },
  {
    "objectID": "chapters/sec1/data.html#how-to-pick",
    "href": "chapters/sec1/data.html#how-to-pick",
    "title": "5  Choosing and Using A Data Architecture",
    "section": "5.2 How to pick",
    "text": "5.2 How to pick\n\nFlat files are simplest\nFlat files w/ interactive read are great for med-large data\nDatabases often already exist\nIf not, a few tips:\n\nSQL-based databases are the backbone of analytics work\nDatabase as a service is a basic cloud provider\nThere are SO many options TODO: How much more in depth am I going here?"
  },
  {
    "objectID": "chapters/sec1/data.html#working-with-file-systems",
    "href": "chapters/sec1/data.html#working-with-file-systems",
    "title": "5  Choosing and Using A Data Architecture",
    "section": "5.3 Working with File Systems",
    "text": "5.3 Working with File Systems\n\nWorking on your laptop file system isn’t great\nWorking in a cloud provider is different than on your laptop\n\nVolumes are separate from compute\nOften have automated snapshots/backups\nVolumes can also be shared across multiple servers\n\nProcess of putting a volume on a server is called mounting"
  },
  {
    "objectID": "chapters/sec1/data.html#database",
    "href": "chapters/sec1/data.html#database",
    "title": "5  Choosing and Using A Data Architecture",
    "section": "5.4 Working with Databases",
    "text": "5.4 Working with Databases\n\nTwo main ways to work with databases from R/Python - direct connection or w/ connector/driver (JDBC/ODBC)\nhttps://docs.google.com/presentation/d/1wJ3YnB4ob3AkNRDLpvYy3B-JMUaVsPjl3AA5QkN7fII/edit\n\n\n5.4.1 Security\nRow-level security Kerberos JWT IAM roles"
  },
  {
    "objectID": "chapters/sec1/data.html#exercises",
    "href": "chapters/sec1/data.html#exercises",
    "title": "5  Choosing and Using A Data Architecture",
    "section": "5.5 Exercises",
    "text": "5.5 Exercises\n\nConnect to and use an S3 bucket from the command line and from R/Python.\nStand up a container with Postgres in it, connect using isql, then from R/Python using ODBC/JDBC.\nStand up an EC2 instance w/ EBS volume, change EBS to different instances.\nStand up an NFS instance and mount onto a server."
  },
  {
    "objectID": "chapters/sec1/cloud.html#the-rise-of-services",
    "href": "chapters/sec1/cloud.html#the-rise-of-services",
    "title": "6  The Cloud and Cloud Providers",
    "section": "6.1 The Rise of Services",
    "text": "6.1 The Rise of Services\nLike much of the rest of the economy, server provisioning and use has gone the way of services. Instead of buying, owning, and maintaining a physical object, a huge proportion of the world’s server hardware is rented.\n[TODO: quote in paragraph below: https://www.srgresearch.com/articles/cloud-market-growth-rate-nudges-amazon-and-microsoft-solidify-leadership]\nIn the US, a huge fraction runs on servers rented from one of three organizations (in order of how significant they are) – Amazon Web Services (AWS), Microsoft Azure, and Google Cloud Platform (GCP). These three companies account for a huge proportion of what we think of as “The Cloud”. There are other smaller players, and also companies that are popular for particular tasks, like Netlify for hosting static websites.\nHowever, in many cases, the cloud doesn’t just refer to renting a server itself. There are also layers and layers of services on top of the “rent me a server” business.\nIn general, the “rent me a server” model is called Infrastructure as a Service (IaaS - pronounced eye-az). So when you stood up an EC2 instance from AWS in the first chapter, you were using AWS’s IaaS offering.\nIn general, people split the layers on top of IaaS into two categories – Platform as a Service (PaaS – pronounced pass), and Software as a Service (SaaS pronounced sass).\nPaaS is where you rent an environment in which to do development. On the other hand, SaaS is renting software as an end user.\nOne common way to explain the difference is using a baking metaphor. Consider making a cake. An on-prem install would be where you make the cake completely from scratch. A IaaS experience would be buying cake mix and baking at home. PaaS would be buying a premade blank cake that you decorate yourself, and SaaS would be just buying a store-bought cake.\nI find these categories and this metaphor sorta helpful in the abstract, but when getting down to concrete real-world examples, the distinctions get fuzzier, and you have to be careful which perspective you’re talking about.\nFor example, RStudio Cloud is a service where you can get an environment with RStudio preconfigured and ready to use. From the perspective of a data scientist, this is clearly a PaaS offering. RStudio is providing a platform where you can learn or do work as a data scientist.\nBut from the perspective of an IT/Admin considering how to set up a server-based data science environment inside their company, RStudio Cloud is clearly a SaaS offering – you just getting the software configured and ready to use.\nMaking the issue even more difficult, many companies go out of their way to make their services sound grand and important and don’t just say, “this is ___ as a service”. Moreover, it’s very common (especially in AWS) to have many different services that fulfill similar needs, and it can be really hard to concretely tell the difference.\nFor example, if you go to AWS’s database offerings for a “database as a service”, your options include Redshift, RDS, Aurora, DynamoDB, ElastiCache, MemoryDB for Redis, DocumentDB, Keyspaces, Neptune, Timestream, and more.\nThere’s a reason why there’s a meaningful industry of people whose full time job is to consult on which AWS service your company needs and how to take advantage of the pricing rules to make sure you get a good deal.\nThere are a few reasons why organizations are moving to the cloud. Primary among them is that maintaining physical servers is often not the core competency of IT/Admin organizations. They’d rather manage higher-level abstractions than physical servers – or increasingly even virtual servers.\nOne reason that people cite, but very rarely comes to pass, is cost. In theory, the flexibility of the cloud should allow organizations to stand up servers as needed and spin them down when they’re not needed. This flexibility is real, there are times when it’s super useful to be able to bring up a server for a particular project – it’s often far quicker and easier than buying and installing a server of your own.\nIn reality, the engineering needed to stand up and spin down these servers at the right time is really difficult and costly – enough so that most organizations could probably substantailly save money if they repatriated their cloud workloads.\nFor more established organizations, running workloads in the cloud may, in fact, be substantially more costly than just bringing those workloads on prem."
  },
  {
    "objectID": "chapters/sec1/cloud.html#serverless-computing",
    "href": "chapters/sec1/cloud.html#serverless-computing",
    "title": "6  The Cloud and Cloud Providers",
    "section": "6.2 Serverless Computing",
    "text": "6.2 Serverless Computing\nIn the past few years, there has been a rise in interest in “serverless” computing. This is a buzzword and there’s no one shared definition of what serverless means. It’s worth making clear that there is no such thing as truly serverless computing. Every bit of cloud computation runs on a server - the question is whether you have to deal with the server or if someone else is doing it for you.\nHowever, there are two distinct things happening that can meaningfully be described as serverless…but they’re completely different.\nOne is the rise of containerization. In chapter XXXX, we’ll get deep into the weeds on using docker yourself. Docker is a very cool tool that makes software much more portable, because you can bring the environment – all the way down to the operating system – around with you very easily. This is kinda a superpower, and many organizations are moving towards using docker containers as the atomic units of their IT infrastructure, so the IT organization doesn’t manage any servers directly, and instead just manages docker containers.\nIn some sense, this is meaningfully serverless. You’ve moved the level of abstraction up a layer from servers and virtual machines to docker containers. And managing docker containers is often meaningfully easier than managing virtual machines directly. However, you still face a lot of the same problems like versioning operating systems, dealing with storage and networking yourself, and more.\nThere is another, stronger, use of serverless that is rising and is also pretty cool, but is super different. In these services, you just hand off a function, written in soem programming langauge to a cloud provider, and they run that function as a service. In a trivial example, imagine a service that adds two numbers together. You could write a Python or R function that does this addition and returns it.\nIt is possible to just deploy this function to a Cloud provider’s environment and then only pay for the actual compute time needed to complete your function calls. This is obviously very appealing because you really don’t have to manage anything at the server level. The disadvantage is that this works only for certain types of operations."
  },
  {
    "objectID": "chapters/sec1/cloud.html#common-services-that-will-be-helpful-to-know-offhand",
    "href": "chapters/sec1/cloud.html#common-services-that-will-be-helpful-to-know-offhand",
    "title": "6  The Cloud and Cloud Providers",
    "section": "6.3 Common Services That will be helpful to know offhand",
    "text": "6.3 Common Services That will be helpful to know offhand\nLuckily, if you’re thinking about setting up and running a standard data science platform in one of the major cloud providers, you’re likely to use one of a few reasonably standard tools.\nIt’s helpful to keep in mind that at the very bottom, there are four basic cloud services: renting servers, configuring networking, identity management, and renting storage. All the other services are recombinations and software installed on top of that.1\nAzure and GCP tend to name their offerings pretty literally. AWS, on the other hand, uses names that have little relationship to the actual task at hand, and you’ll just need to learn them.\n\n6.3.1 IaaS\n\nCompute - AWS EC2, Azure VMs, Google Compute Engine\nStorage\n\nfile - EBS, Azure managed disk, Google Persistent Disk\nNetwork drives - EFS, Azure Files, Google Filestore\nblock/blob - S3, Azure Blob Storage, Google Cloud Storage\n\nNetworking:\n\nPrivate Clouds: VPC, Virtual Network, Google Virtual Private Cloud\nDNS - Route53, Azure DNS + Traffic Manager, Google Cloud DNS\n\n\n\n\n6.3.2 Not IaaS\n\nContainer Hosting - ECS, Azure Container Instances + Container Registry\nK8S cluster as a service - EKS, AKS, GKE\nRun a function as a service - Lambda, Azure Functions, Google Cloud Functions\nDatabase - RDS/Redshift, Azure Database, Google Cloud SQL\nSageMaker - ML platform as a service, Azure ML, Google Notebooks\n\nhttps://docs.microsoft.com/en-us/azure/architecture/aws-professional/services#networking https://cloud.google.com/free/docs/aws-azure-gcp-service-comparison"
  },
  {
    "objectID": "chapters/sec1/cloud.html#cloud-tooling",
    "href": "chapters/sec1/cloud.html#cloud-tooling",
    "title": "6  The Cloud and Cloud Providers",
    "section": "6.4 Cloud Tooling",
    "text": "6.4 Cloud Tooling\n\nIdentity MGMT - IAM, Azure AD\nBilling Mgmt\n\nIaaC tooling"
  },
  {
    "objectID": "chapters/sec1/cloud.html#exercises",
    "href": "chapters/sec1/cloud.html#exercises",
    "title": "6  The Cloud and Cloud Providers",
    "section": "6.5 Exercises",
    "text": "6.5 Exercises\n\nExample of something you’d want to build – for each of the 3 providers, which services would you use if you wanted an IaaS solution, a PaaS solution?"
  },
  {
    "objectID": "chapters/sec2/servers.html#computers-are-addition-factories",
    "href": "chapters/sec2/servers.html#computers-are-addition-factories",
    "title": "7  Computers, Computing, and Servers",
    "section": "7.1 Computers are addition factories",
    "text": "7.1 Computers are addition factories\nAs a data scientist, the amount of computational theory it’s really helpful to understand in your day-to-day can be summarized in three sentences:\n\nComputers can only add.\nModern ones do so very well and very fast.\nEverything a computer “does” is just adding two (usually very large) numbers, reinterpreted.1\n\nI like to think of computers as factories for doing addition problems.\n\nWe see meaning in typing the word umbrella or jumping Mario over a Chomp Chain and we interpret something from the output of some R code or listening to Carly Rae Jepsen’s newest bop, but to your computer it’s all just addition.\nEvery bit of input you provide your computer is homogenized into addition problems. Once those problems are done, the results are reverted back into something we interpret as meaningful. Obviously the details of that conversion are complicated and important – but for the purposes of understanding what your computer’s doing when you clean some data or run a machine learning model, you don’t have to understand much more than that.\n\n7.1.1 Compute\nThe addition assembly line itself – where the work actually gets done – is referred to as compute. It’s where 2+2 gets turned into 4, and where 345619912 + 182347910 gets turned into 527967822. The heart of the factory in your computer is the central processing unit (CPU).\nThere are two elements to the total speed of your compute – the total number of cores, which you can think of as an individual conveyor belt doing a single problem at a time, and the speed at which each belt is running.\nThese days, most consumer-grade laptops have between 4 and 16 cores, and may have additional capabilities that effectively doubles that number. So most laptop CPUs can do between 4 and 32 simultaneous addition problems.\nIn your computer, the basic measure of conveyor belt speed is single-core “clock speed” in hertz (hz) – operations per second. The cores in your laptop probably run between 2-5 gigahertz (GHz): 2-5 billion operations per second.\n\nA few decades ago, there was a steady march in increases in clock speed – increasing by approximately 10x during the 90s, by 2-3x in the 2000s, and somewhere between not at all and 1.5x in the 2010s. Computers have gotten much faster through this entire time, but mostly from sources other than boosts in single-core clock speeds.\nFor example, in 2020, Apple released a series of Macs with its new M1 chip. Real-world performance of the M1 chip was dramatically better than earlier models, despite modest improvements in clock speeds.\n\n\n7.1.1.1 GPU Computing\nWhile compute usually just refers to the CPU, it’s not completely synonymous. Computers can offload some problems to a graphical processing unit (GPU). GPUs are specialized chips used for tasks like editing photo or videos, rendering video game graphics, some kinds of machine learning, and (yes) Bitcoin mining.\nWhere the CPU has a few fast cores, the GPU takes the opposite approach, with many slower cores. Where a consumer-grade CPU has 4-16 cores, mid-range GPUs have 700-4,000, but each one runs between 1% and 10% the speed of a CPU core.\nFor GPU-centric tasks, the overwhelming parallelism of a GPU is more important than the speed of any individual core, and GPU computation can be dramatically faster. For the purposes of data science, many popular machine learning techniques – including neural networks, XGBoost, and other tree-based models – potentially run much much faster on GPUs relative to CPUs.\n\n\n\n7.1.2 Memory (RAM)\nYour computer’s random access memory (RAM) is its short term storage. Your computer uses RAM to store addition problems it’s going to tackle soon, and results it thinks it might need again in the near future.\nThe benefit of RAM is that it’s very fast to access. The downside is that it’s temporary. When your computer turns off, the RAM gets wiped.2\n\nYou probably know this, but memory and storage is measured in bytes. These days, most consumer hard drives are between 1/2 to a few terrabytes (trillion bytes). Some enterprise data stores run on the scales of thousands of terrabytes (pettabytes) or even thousands of pettabytes (yottabytes).\n\nModern consumer-grade laptops come with somewhere between 4 and 16 Gb of memory.\n\n\n7.1.3 Storage (Hard Drive/Disk)\nYour computer’s storage is like a freezer. It keeps things permanently, whether the factory is operating or not. The tradeoff is that things there are cold, and take some time to warm up before they can be used.\nA few years ago, all hard drives were (HDDs), and were basically the same. HDDs have a bunch of magnetic disks that spin very fast (5,400 and 7,200 RPM are common speeds). Magnetized read/write heads move among the disks and save and read your data.\nIn the last few years, solid-state drives (SSDs) have been taking over. SSDs, which are collections of flash memory chips, are up to 15x faster than HDDs. They also can take a wider variety of shapes and sizes, and are more reliable and durable because they have no moving parts. The main drawback is that they’re usually more expensive per byte, but prices are still quite reasonable.\nMany consumer laptops have only an SSD at this point. Some desktops and high-end laptops combine a smaller SSD with a larger HDD."
  },
  {
    "objectID": "chapters/sec2/servers.html#choosing-the-right-data-science-machine",
    "href": "chapters/sec2/servers.html#choosing-the-right-data-science-machine",
    "title": "7  Computers, Computing, and Servers",
    "section": "7.2 Choosing the right data science machine",
    "text": "7.2 Choosing the right data science machine\nIn my experience as a data scientist and talking to IT/DevOps organizations trying to equip data scientists, the same questions about choosing a computer come up over and over again. Here are the guidelines I often share.\n\n7.2.1 Get as much RAM as feasible\nIn most cases, R and Python have to load all of your data into memory. Thus, the size of the data you can use is limited to the size of your machine’s RAM. Most other limits of your machine will just result in things being slower than you’d really want, but trying to load too much data into memory will result in a session crash, and you won’t be able to do your analysis.\n\nYou can get around the in-memory limitation by using a database or libraries that facilitate on-disk operations like Apache Arrow or dask.\n\nIt’s easy to say that you’ll always want more RAM, but a rough rule of thumb for whether you’ve got enough is the following:\n\nAmount of RAM = max amount of data * 3\n\nBecause you’ll often be doing some sort of transformation that results in invisible data copies and your computer can’t devote all of its memory, you’ll want to leave plenty of room over your actual data size. I’ve never run into problems with a scaling factor of 3, but this is definitely only a rough rule-of-thumb.\n\n\n7.2.2 Go for fewer, faster cores in the CPU\nR and Python are single-threaded. Unless you’re using special libraries for parallel processing, you’ll end up red-lining a single CPU core while the other just look on in silence. Therefore, single core clock speed matters more than the number of cores, and fewer, faster cores are usually preferable to many slower\nIf you’re buying a laptop or desktop, there usually aren’t explicit choices between a few fast cores and more slow cores. Most modern CPUs are pretty darn good, and you should just get one that fits your budget. If you’re standing up a server, you often have an explicit choice between more slower cores and fewer faster ones.\n\n\n7.2.3 Get a GPU…maybe…\nIf you’re doing machine learning that can be improved by GPU-backed operations, you might want a GPU. In general, only highly parallel machine learning problems like training a neural network or tree-based models will benefit from GPU computation.\nOn the other hand, GPUs are expensive, non-machine learning tasks like data processing don’t benefit from GPU computation, and many machine learning tasks are amenable to linear models that run well CPU-only.\n\n\n7.2.4 Get a lot of storage, it’s cheap\nAs for storage – get a lot – but don’t think about it too hard, because it’s cheap. Both a 1TB SSD and a 4TB HDD are around $100. Storage is cheap enough these days that it is almost always more cost efficient to buy more storage rather than making a highly-paid professional spend their time trying to figure out how to move things around.\nOne litmus test of an IT organization that is well-equipped to support data science is whether they understand this. Smart organizations know that just getting more storage is easily worth the cost in terms of the time of admins and data scientists."
  },
  {
    "objectID": "chapters/sec2/servers.html#is-a-server-different",
    "href": "chapters/sec2/servers.html#is-a-server-different",
    "title": "7  Computers, Computing, and Servers",
    "section": "7.3 Is a server different?",
    "text": "7.3 Is a server different?\nNo.\nBut also yes.\nAt its core, a server is exactly the same sort of addition factory as your laptop, and the same mental model of what is happening under the hood will serve you well.\n\nThe big difference is in how the input and output is done. While you interact directly with a computer through keyboard and mouse/touchpad, servers generally don’t have built in graphical interfaces – by default all interaction occurs via command line tools.\nOne of the reasons is that the overwhelming majority of the world’s servers run the Linux operating system, as opposed to the Windows or Mac OS your laptop probably runs.3 There are many different distributions (usually called “distros”) of Linux. For day-to-day enterprise server use, the most common of these are Ubuntu, CentOS, Red Hat Enterprise Linux (RHEL), SUSE Enterprise Linux.\nAlong with most of the world’s servers, almost all of the world’s embedded computers – in ATMs, cars and planes, TVs, and most other gadgets and gizmos – run on Linux. If you have an Android phone or a Chromebook – that’s Linux. Basically all of the world’s supercomputers use Linux."
  },
  {
    "objectID": "chapters/sec2/servers.html#exercises",
    "href": "chapters/sec2/servers.html#exercises",
    "title": "7  Computers, Computing, and Servers",
    "section": "7.4 Exercises",
    "text": "7.4 Exercises\n\nThink about the scenarios below – which part of your computer would you want to upgrade to solve the problem?\n\nYou try to load a big csv file into pandas in Python. It churns for a while and then crashes.\nYou go to build a new ML model on your data. You’d like to re-train the model once a day, but it turns out training this model takes 26 hours on your laptop.\nYou design an visualization Matplotlib , and create a whole bunch in a loop/"
  },
  {
    "objectID": "chapters/sec2/aws-walkthrough.html#exercises",
    "href": "chapters/sec2/aws-walkthrough.html#exercises",
    "title": "8  Lab 1: Get your own server",
    "section": "8.1 Exercises",
    "text": "8.1 Exercises\nTry standing up a new server and installing R and RStudio Server Open Source, or Python and JupyterHub.\n\nHint 1: Remember that your instance only allows traffic to SSH in on port 22 by default. You access RStudio on port 8787 by default and JupyterHub on port 8000. You control what ports are open via the Security Group.\nHint 2: You’ll need to create a user on the server. The adduser command is your friend.\n\n#TODO: test out JupyterHub"
  },
  {
    "objectID": "chapters/sec2/understanding-traffic.html#computer-communication-packets-traversing-networks",
    "href": "chapters/sec2/understanding-traffic.html#computer-communication-packets-traversing-networks",
    "title": "9  Computer Networks and the Internet",
    "section": "9.1 Computer communication = packets traversing networks",
    "text": "9.1 Computer communication = packets traversing networks\nThe virtual version of the processes that get your letter from your house to your penpal’s is called packet switching, and it’s really not a bad analogy. Like the physical mail, your computer dresses up a message with an address and some other details, sends it on its way, and waits for a response.1 The set of rules – called a protocol – that defines a valid address, envelope type, and more is called TCP/IP.\nUnderneath these protocols is a bunch of hardware, which we’re basically going to ignore.\nEach computer network is governed by a router. For the purposes of your mental model, you can basically think of your router as doing two things – maintaining a table of the IP addresses it knows, and following this algorithm over and over again.\n#TODO: Turn into visual tree – also visual of networks and sub-networks\n\nDo I know where this address is?\n\nYes: Send the packet there.\nNo: Send the packet to the default address and cross fingers.\n\n\nIn general, routers only know about the IP addresses of sub-networks and devices, so if you’re printing something from your laptop to the computer in the next room, the packet just goes to your router and then straight to the printer.\n\nIn your home’s local area network (LAN), your router does one additional thing – as devices like your phone, laptop, or printer attach to the network, it assigns them IP addresses based on the addresses available in a process called Dynamic Host Configuration Protocol (DHCP).\n\nOn the other hand, if you’re sending something to a website or server that’s far away, your computer has no idea where that IP address is. Clever people have solved this problem by setting the default address in each router to be an “upstream” router that is a level more general.\nSo immediately upstream of your router is probably a router specific to your ISP for a relatively small geographic area. Upstream of that is probably a router for a broader geographic area. So your packet will get passed upstream to a sufficiently general network and then back downstream to the actual IP address you’re trying to reach.\n#TODO: Image of computer network w/ upstream and downstream networks\nWhen the packets are received and read – something happens. Maybe you get to watch your show on Netflix, or your document gets printed – or maybe you get an error message back. In any event, the return message will be transmitted exactly the same way as your initial message, though it might follow a different path."
  },
  {
    "objectID": "chapters/sec2/understanding-traffic.html#more-details-about-ip-addresses",
    "href": "chapters/sec2/understanding-traffic.html#more-details-about-ip-addresses",
    "title": "9  Computer Networks and the Internet",
    "section": "9.2 More details about IP Addresses",
    "text": "9.2 More details about IP Addresses\nIP addresses are, indeed, addresses. They are how one computer or server finds another on a computer network, and they are unique within that network.\nMost IP addresses you’ve probably seen before are IPv4 addresses. They’re four blocks of 8-bit fields, so they look something like 65.77.154.233, where each of the four numbers is something between 0 and 255.\nSince these addresses are unique, each server and website on the internet needs a unique IP address. If you do the math, you’ll realize there are “only” about 4 billion of these. It turns out that’s not enough for the public internet and we’re running out.\nIn the last few years, adoption of the new standard, IPv6, has started. IPv6 addresses are eight blocks of hexadecimal (0-9 + a-f) digits separated by colons, with certain rules that allow them to be shortened, so 4b01:0db8:85a3:0000:0000:8a2e:0370:7334 or 3da4:66a::1 are both examples of valid IPv6 addresses.\nIPv6 will coexist with IPv4 for a few decades, and we’ll eventually switch entirely to IPv6. There’s no worry about running out of IPv6 addresses any time soon, because the total number of IPv6 addresses is a number 39 digits long.\n\n9.2.0.1 Special IP Addresses\nAs you work more with IP addresses, there are a few you’ll see over and over. Here’s a quick cheatsheet:\n\n\n\n\n\n\n\nCode\nMeaning\n\n\n\n\n127.0.0.1\nlocalhost or loopback – the machine that originated the request\n\n\n192.168.x.x, 172.16.x.x.x,\n10.x.x.x\nProtected address blocks used for private IP addresses. More on public vs private addresses in chapter XX."
  },
  {
    "objectID": "chapters/sec2/understanding-traffic.html#application-layer-protocols-define-valid-messages",
    "href": "chapters/sec2/understanding-traffic.html#application-layer-protocols-define-valid-messages",
    "title": "9  Computer Networks and the Internet",
    "section": "9.3 Application Layer Protocols define valid messages",
    "text": "9.3 Application Layer Protocols define valid messages\nIf we think of the TCP/IP protocol defining valid addresses, package sizes and shapes, and how the mail gets routed, then application layer protocols are one layer down – they define what are valid messages to put inside the envelope.\nThere are numerous application layer protocols. Some you will see in this book include SSH for direct server access, (S)FTP for file transfers, SMTP for email, LDAP(S) for authentication and authorization, and websockets for persistent bi-directional communication – used for interactive webapps created by the Shiny package in R and the Streamlit package in Python.\nWe’ll talk more about some of those other protocols later in the book. For now, let’s focus on the one you’ll spend most of your time thinking about – http.\n\n9.3.1 http is the most common application layer protocol\nHyptertext transfer protocol (http) is the protocol that underlies a huge fraction of internet traffic. http defines how a computer can initiate a session with a server, request the server do something, and receive a response.\nSo whenever you go to a website, http is the protocol that defines how the underlying interactions that happen as your computer requests the website and the server sends back the various assets that make up the web page, which might include the HTML skeleton for the site, the CSS styling, interactive javascript elements, and more.\n\nIt’s worth noting that these days, virtually all http traffic over the internet is in the form of secured https traffic. We’ll get into what the s means and how it’s secured in the next chapter.\n\nThere are a few important elements to http requests and responses:\n\nRequest Method – getting deep into HTTP request methods is beyond the scope of this book, but there are a variety of different methods you might use to interact with things on the internet. The most common are GET to get a webpage, POST or PUT to change something, and DELETE to delete something.\nStatus Code - each HTTP response includes a status code indicating the response category. Some special codes you’ll quickly learn to recognize are below. The one you’ll (hopefully) see the most is 200, which is a successful response.\nResponse and Request Headers – headers are metadata included with the request and response. These include things like the type of the request, the type of machine you’re coming from, cookie-setting requests and more. In some cases, these headers include authentication credentials and tokens, and other things you might want to inspect.\nBody - this is the content of the request or response.\n\nIt’s worth noting that GET requests for fetching something generally don’t include a body. Instead, any specifics on what is to be fetched are specified through query parameters, the part of the URL that shows up after the ?. They’re often something like, ?first_name=alex&last_name=gold\n\n\n\n\n9.3.2 Understand http traffic by inspecting it\nThe best way to understand http traffic is to take a close look at some. Luckily, you’ve got an easy tool – your web browser!\nOpen a new tab in your browser and open your developer tools. How this works will depend on your browser. In Chrome, you’ll go to View > Developer > Developer Tools and then make sure the Network tab is open.\nNow, navigate to a URL in your browser (say google.com).\nAs you do this, you’ll see the traffic pane fill up. These are the requests and responses going back and forth between your computer and the server.\nIf you click on any of them, there are a few useful things you can learn.\n\nAt the top, you can see the timing. This can be helpful in debugging things that take a long time to load. Sometimes it’s helpful to see what stage in the process bogs down.\nIn the pane below, you can inspect the actual content that is going back and forth between your computer and the server you’re accessing including the request methods, status codes, headers, and bodies.\n\n9.3.2.1 Special HTTP Codes\nAs you work more with http traffic, you’ll learn some of the common codes. Here’s a cheatshet for some of the most frequent you’ll see.\n\n\n\n\n\n\n\nCode\nMeaning\n\n\n\n\n200\nEveryone’s favorite, a successful response.\n\n\n3xx\nYour query was redirected somewhere else, usually ok.\n\n\n4xx and 5xx\nErrors with, respectively, the request itself and the server.\n\n\n\nParticular Error Codes\n\n\n400\nBad request. This isn’t a request the server can understand.\n\n\n401 and 403\nUnauthorized or forbidden. Often means required authentication hasn’t been provided.\n\n\n404\nNot found. There isn’t any content at the address you’re trying to access.\n\n\n504\ngateway timeout. This means that a proxy or gateway between you and the server you’re trying to access timed out before it got a response from the server."
  },
  {
    "objectID": "chapters/sec2/understanding-traffic.html#exercises",
    "href": "chapters/sec2/understanding-traffic.html#exercises",
    "title": "9  Computer Networks and the Internet",
    "section": "9.4 Exercises",
    "text": "9.4 Exercises\n#TODO"
  },
  {
    "objectID": "chapters/sec2/using-urls.html#using-ports-to-get-to-the-right-service",
    "href": "chapters/sec2/using-urls.html#using-ports-to-get-to-the-right-service",
    "title": "10  Getting a real URL",
    "section": "10.1 Using ports to get to the right service",
    "text": "10.1 Using ports to get to the right service\nLet’s say you want to run software on a server. One of the big differences between server software, and software on your laptop is that server software needs to be able to interact with the outside world to be useful.\nFor example, when you want to use Microsoft Word on your computer, you just click on the button and then it’s ready to go. But say I want to use RStudio Server. I don’t have a desktop where I click to open RStudio Server. Instead, I go to a particular URL and I expect that RStudio will be there, ready to listen."
  },
  {
    "objectID": "chapters/sec2/using-urls.html#special-ip-addresses-and-ports",
    "href": "chapters/sec2/using-urls.html#special-ip-addresses-and-ports",
    "title": "10  Getting a real URL",
    "section": "10.2 Special IP Addresses and Ports",
    "text": "10.2 Special IP Addresses and Ports\nAll ports below 1024 reserved.\n80 - HTTP default\n443 - HTTPS default\n22 - SSH default\nNormally, you’ll see a URL written something like this:\n\\[\nexample.com\n\\]\nIt doesn’t seem like this little sni\n\\[\n\\overbrace{https://}^{\\text{Protocol}}\\overbrace{\\underbrace{www}_{\\text{Subdomain}}.\\underbrace{example}_{\\text{Primary Domain}}.\\underbrace{com}_{\\text{Top-Level Domain}}}^{\\text{Domain}}/\\overbrace{engineering}^{\\text{Path}}\n\\]\nEven worse, IP addresses generally aren’t permanent – they can change when individual servers are replaced, or if you were to change the server architecture (say by adding and load-balancing a second instance – see chapter XX).\nAnatomy of a URL\nIn order to have something human-friendly and permanent, we access internet resources at uniform resource locators (URLs), like google.com, rather than an IP address."
  },
  {
    "objectID": "chapters/sec2/using-urls.html#getting-your-own-domain",
    "href": "chapters/sec2/using-urls.html#getting-your-own-domain",
    "title": "10  Getting a real URL",
    "section": "10.3 Getting your own domain",
    "text": "10.3 Getting your own domain\nIn the last chapter, we spent most of the time talking about server locations in terms of IP addresses. And it’s true – the “real” address of any server is its IP address. But we generally don’t access websites or other resources at IP addresses – they’re hard to remember, and they can also change over time.\nInstead, we generally use domains for websites, and hostnames for individual servers. We’ll get into hostnames later on – for now we’re going to focus on domains.\nA domain is simply a convenient alias for an IP address. The domain name system (DNS) is the decentralized internet phonebook that translates back and forth between domains and IP addresses. The details of how DNS resolution works are quite intricate – but the important thing to know is that there are layers of DNS servers that eventually return an IP address to your computer for where to find your website.\nFrom the perspective of someone trying to set up their own website, there’s only one DNS server that matters to you personally – the DNS server for your domain name registrar.\nDomain name registrars are the companies that actually own domains. You can buy or rent one from them in order to have a domain on the internet. So let’s say you take the data science server you set up in lab 1 and decide that you want to host it at a real domain.\nYour first stop would be a domain name registrar where you’d find an available domain you like and pull out your credit card.\nCosts for domain names vary widely. Buying a meaningless domain in a less popular top-level domain, say ladskfmlsdf.me can cost as little as $3 per year. On the other hand, buying a .com domain that’s a real word or phrase can be a few thousand dollars – and there are articles every few years about some major company accidentally allowing their domain name to lapse and ransoming it back for stupid amounts of money.\nSo, conceptually, it’s easy to understand how a domain comes to stand in for an IP address, with DNS being the directory that ties the two together.\n\n10.3.1 Configuring DNS to connect IP addresses and Domains\nThe harder part is the nitty gritty of how you accomplish that mapping yourself, which we’ll get into now.\nConfiguration of DNS is done by way of records, of which there are a menagerie of types you can configure. Luckily, most simple configurations only need CNAME and A records.\nHere’s an imaginary DNS record table for the domain example.com:\n\n\n\nPath/Host\nType\nTarget\n\n\n\n\n@\nA\n143.122.8.32\n\n\nwww\nCNAME\nexample.com\n\n\n*\nA\n143.122.8.33\n\n\n\nLet’s go through how to read this table.\nSince we’re configuring example.com, the paths/hosts in this table are relative to example.com.\nIn the first row we’re configuring an A record to go to the target IP address. A records (or their IPv6 cousin AAAA records) map a domain to an actual IP address. The path @ is a special symbol meaning exact match. So by this configuration, any traffic to example.com will be passed straight through to the specified IP address.\nThe second row deals with traffic to the www subdomain. CNAME records alias sub-domains. They’re most frequently used to map sub-domains to the main domain. Since this is a CNAME record for example.com, this record indicates that traffic to www.example.com should be treated exactly like traffic to example.com. Some domain providers do automatic redirection of www traffic, and so this row may not be necessary in some configurations.\nThe last record uses the wildcard symbol * to send all subdomain traffic that’s not already spoken for – say blog.example.com or info.example.com directly to the IP address specified.\nSo what happens is that your query goes through several layers of public DNS servers to get to the DNS entry for your domain name registrar. In many cases, you’ll directly configure your domain name registrar to point to your website or server – but you also can configure the domain name registrar to point at another set of DNS servers you actually control with an NS record.\nIf you’re setting up your own server, this probably isn’t the case, but some large enterprises do run their own private DNS servers.\n\nYou should always configure your domain provider as narrowly as possible – and you should configure your website or server first.\n#TODO: why?\n\n\n\n10.3.2 Learning to Hate DNS\nAs you get deeper into using servers, you will learn to hate DNS with a fiery passion. While it’s necessary so we’re not running around trying to remember incomprehensible IP addresses, it’s also very hard to debug as a server admin.\nLet’s say I’ve got the public domain example.com, and I’m taking down the server and putting up a new one. I’ve got to alter the public DNS record so that everyone going to example.com gets routed to the new IP address, and not the old one.\nThe thing that makes it particularly challenging is that the DNS system is decentralized. There are thousands of public DNS servers that a request could get routed to, and many of them may need updating.\nObviously, this is a difficult problem to solve, and it can take up to 24 hours for DNS changes to propagate across the network. So making changes to DNS records and checking if they’ve worked is kinda a guessing game of whether enough time has passed that you can conclude that your change didn’t work right, or if you should just wait longer.\nTo add an additional layer of complexity, DNS lookups are slow, so your browser caches the results of DNS lookups it has done before. That means that it’s possible you’ll still get an old website even once the public DNS record has been updated. If a website has ever not worked for you and then worked when you tried a private browser, DNS caching is likely the culprit. Using a private browsing window sidesteps your main DNS cache and forces lookups to happen afresh.\n\n\n10.3.3 Trying it out\nGo through hosting this book somewhere."
  },
  {
    "objectID": "chapters/sec2/using-urls.html#exercises",
    "href": "chapters/sec2/using-urls.html#exercises",
    "title": "10  Getting a real URL",
    "section": "10.4 Exercises",
    "text": "10.4 Exercises\n\nFind a cheap domain you like and buy it.\nPut an EC2 server back up with the Nginx hello-world example.\nConfigure your server to be available at your new domain.\n\nHint: In AWS, Route 53 is the service that handles incoming networking. They can serve as a domain name registrar, or you can buy a domain elsewhere and just configure the DNS using Route 53."
  },
  {
    "objectID": "chapters/sec2/using-urls.html#securing-traffic-with-https",
    "href": "chapters/sec2/using-urls.html#securing-traffic-with-https",
    "title": "10  Getting a real URL",
    "section": "10.5 Securing Traffic with https",
    "text": "10.5 Securing Traffic with https\nWhen you go to a website on the internet, you’ll see the URL prefixed by the https (though it’s sometimes hidden by your browser because it’s assumed). https is actually a mashup that is short for http with secure sockets layer (SSL).\nThese days, almost everyone actually uses the successor to SSL, transport layer security (TLS). However, because the experience of configuring TLS is identical to SSL, admins usually just talk about configuring SSL even when they mean TLS.\nThese days, almost every bit of internet traffic is actually https traffic. You will occasionally see http traffic inside private networks where encryption might not be as important – but more and more organizations are requiring end-to-end use of SSL.\nSecuring your website or server using SSL/TLS is one of the most basic things you can do to make sure your website traffic is safe. You should always configure https – full stop.\nSSL/TLS security is accomplished by configuring your site or server to use a SSL certificate (often abbreviated to cert). We’ll go through the details of how to get and configure an SSL certificate in this chapter – but first a little background on how SSL/TLS works.\n\n10.5.1 How SSL/TLS Enhances Security\nSSL accomplishes two things for you – identity validation and traffic encryption.\nWhen you go to a website, SSL/TLS is the technology that verifies that you’re actually reaching the website you think you’re reaching. This prevents something called a man-in-the-middle attack where a malicious actor manages to get in between the server and the client of network traffic. So, for example, you might think you’re putting your bank login information into your normal bank website, but there’s a hacker sitting in the middle, reading all of the traffic back and forth.\n[TODO: Image of man-in-the-middle]\nYou can see this in action in your web browser. When you go to a website protected by https, you’ll see a little lock icon to the left of the URL. That means that this website’s SSL certificate matches the website and therefore your computer can verify you’re actually at the website you mean to be at.\nBut how does your computer know what a valid SSL certificate is? Your computer has a list of trusted Certificate Authorities (CAs) who create, sell, and validate SSL/TLS certificates. So when you navigate to a website, the website sends back a digital signature. Your computer checks the signature against the indicated CA to verify that it was issued to the site in question.\n[TODO: image of SSL validation]\nThe second type of scary scenario SSL prevents is a snooping/sniffing attack. Even if you’re getting to the right place, your traffic travels through many different channels along the way – routers, network switches, and more. This means that someone could theoretically look at all your traffic along the way to its meaningful destination.\nWhen your computer gets back the digital signature to verify the site’s identity, it also prompts an exchange of encryption keys. These keys are used to encrypt traffic back and forth between you and the server so anyone snooping on your message will just see garbled nonsense and not your actual content. You can think of the SSL/TLS encryption as the equivalent of writing a message on a note inside an envelope, rather than on a postcard anyone could read along the way.\n\n\n10.5.2 Getting a cert of your own\nIn order to configure your site or server with SSL, there are three steps you’ll want to take: getting an SSL certificate, putting the certificate on the server, and making sure the server only accepts https traffic.\nYou can either buy an SSL certificate or make one yourself, using what’s called a self-signed cert.\nThere are a variety of places you can buy an SSL/TLS certificate, in many cases, your domain name registrar can issue you one when you buy your domain.\nWhen you create or buy your cert, you’ll have to choose the scope. A basic SSL certificate covers just the domain itself, formally known as a fully qualified domain name (FQDN). So if you get a basic SSL certificate for www.example.com, www.blog.example.com will not be covered. You can get a wildcard certificate that would cover every subdomain of *.example.com.\n\nNote that basic SSL/TLS certification only validates that when you type example.com in your browser, that you’ve gotten the real example.com. It doesn’t in any way validate who owns example.com, whether they’re reputable, or whether you should trust them.\nThere are higher levels of SSL certification that do validate that, for example, the company that owns google.com is actually the company Google.\n\nBut sometimes it’s not feasible to buy certificates. While a basic SSL certificate for a single domain can cost $10 per year or less, wildcard certificates will all the bells and whistles can cost thousands per year. This can get particularly expensive if you’ve got a lot of domains for some reason.\nMoreover, there are times when you can’t buy a certificate. If you’re encrypting traffic inside a private network, you will need certificates for hosts or IP addresses that are only valid inside the private network, so there’s no public CA to validate them.\nThere are two potential avenues to follow. In some cases, like inside a private network, you want SSL/TLS for the encryption, but don’t really care about the identity validation part. In this case, it’s usually possible to skip that identity validation part and automatically trust the certificate for encryption purposes.\nIt’s also possible to create your own private CA, which would verify all your SSL certificates. This is pretty common in large organizations. At some point, every server and laptop needs to have the private CA added to its set of trusted certificate validators.\nA warning: it is deceptively easy to generate and configure a self-signed SSL certificate. It’s usually just a few lines of shell commands to create a certificate, and adding the certificate to your server or website is usually just a copy/paste affair.\nHowever, it’s pretty common to run into problems with self-signed certs or private CAs. Making sure the certificate chain is correct, or running into a piece of software that doesn’t ignore the identity validation piece right is pretty common. This shouldn’t dissuade you from using SSL/TLS. It’s an essential, and basic, component of any security plan – but using a self-signed cert probably isn’t as easy as it seems.\nWhen you configure your site or server, there will likely be an option to redirect all http traffic to https traffic. If your server or site is open to the internet, you should set this option."
  },
  {
    "objectID": "chapters/sec2/private-network-config.html#finding-the-right-server-with-hostnames",
    "href": "chapters/sec2/private-network-config.html#finding-the-right-server-with-hostnames",
    "title": "11  Private Networking Configuration",
    "section": "11.1 Finding the right server with hostnames",
    "text": "11.1 Finding the right server with hostnames\nLike we configured a URL to have an abstraction layer between our IP address and our server, we also will usually configure a hostname rather than using IP addresses."
  },
  {
    "objectID": "chapters/sec2/private-network-config.html#the-role-of-forward-and-reverse-proxies",
    "href": "chapters/sec2/private-network-config.html#the-role-of-forward-and-reverse-proxies",
    "title": "11  Private Networking Configuration",
    "section": "11.2 The role of forward and reverse proxies",
    "text": "11.2 The role of forward and reverse proxies"
  },
  {
    "objectID": "chapters/sec2/private-network-config.html#configuring-the-private-network",
    "href": "chapters/sec2/private-network-config.html#configuring-the-private-network",
    "title": "11  Private Networking Configuration",
    "section": "11.3 Configuring the private network",
    "text": "11.3 Configuring the private network\nA private network is defined by a range of IP addresses, called CIDR blocks. A CIDR block defines the sub-addresses available inside an IP address using a /n notation. So the CIDR block 10.0.0.0/16 is the largest possible CIDR block and includes all IP addresses from 10.0.0.0 to 10.255.255.255.\nThe numbers after the / are related to powers of 2 (binary something blah), so you can fit 2 /x+1 CIDR blocks in a single /x block.\nSo, for example, there are 128 IP addresses in a /25 CIDR block, and 256 in a /24 CIDR block, so 10.0.0.0/24 could have two subnets, 10.0.0.0/25 and 10.0.0.128/25.\n\n11.3.1 Restricting Outbound Connections\nIn AWS, you’ll configure egress from private subnets using a NAT Gateway. Egress from the public subnets goes through the Internet Gateway. Egress is actually needed less than you might think - in our current setup, it’s needed for package updates to RSPM (the RSPM service, not RSP to RSPM), to do google OAuth to the google servers (rstudioservices only), get stuff from github, update products, and get content from apt/yum repos.\nIn Colorado, it’s not needed for RSC because the SAML token is in the browser. It is needed for RSP because Onelogin is outside the VPC. This is generally not the case for customers, who are mostly using on-prem AD servers."
  },
  {
    "objectID": "chapters/sec2/private-network-config.html#bastion-hosts",
    "href": "chapters/sec2/private-network-config.html#bastion-hosts",
    "title": "11  Private Networking Configuration",
    "section": "11.4 Bastion Hosts",
    "text": "11.4 Bastion Hosts"
  },
  {
    "objectID": "chapters/sec2/private-network-config.html#exercises",
    "href": "chapters/sec2/private-network-config.html#exercises",
    "title": "11  Private Networking Configuration",
    "section": "11.5 Exercises",
    "text": "11.5 Exercises\nConsider going to the website google.com. Draw a diagram of how the following are incorporated: TCP/IP, DNS, HTTP, HTTPS.\nSet up a free-tier EC2 instance and put an NGINX server up. Figure out how to allow your computer to access the server, but not your phone. Try accessing it on a non-default port.\nTry to HTTP into a fresh EC2 with the default security group. Take a look at the inbound security group rules. Hint: is there an inbound rule on a default HTTP port?\nSSH into your EC2 instance and try to reach out to something on the internet (curl…). See if you can change security group rules to shut down access.\nCan you do it by changing the IP address range it’s accepting connections from?\nCan you do it by changing the listening ports?"
  },
  {
    "objectID": "chapters/sec3/auth.html#the-many-flavors-of-auth-or-what-does-sso-mean",
    "href": "chapters/sec3/auth.html#the-many-flavors-of-auth-or-what-does-sso-mean",
    "title": "12  How login works",
    "section": "12.1 The many flavors of auth (or what does SSO mean?)",
    "text": "12.1 The many flavors of auth (or what does SSO mean?)\nSingle Sign On (SSO) is a slippery term, so it is almost always necessary to clarify what is meant by the term when you hear it. At some organizations, identity management isn’t centralized at all. This means that usernames and passwords are unique to each service, onboarding and offboarding of users has to be handled independently for each service, and users have to login frequently. In short, it’s often not a great system. This is never referred to as SSO.\n\nMost organizations of a meaningful size have centralized identity management. This means that identities, credentials, authorization, onboarding, and offboarding are handled centrally. However, you may still need to independently login to each system. For example, in this system, every service might take the same username and password as your credentials, but if you go to RStudio Server followed by JupyterHub, you’ll need to provide that username and password independently to each service. This system is often facilitated by PAM, and LDAP/AD. Some organizations call this SSO, because there’s only one set of credentials.\nIn true SSO, users login once and are given a token or ticket.1 Then, when they go to the next service, they don’t have to login again because that service can just look at the token or ticket to do auth for that user. For example, in this system, I could go to RStudio Server and login, and then go to JupyterHub and get in without being prompted again for my password. This type of auth is facilitated by Kerberos, SAML, or OAuth."
  },
  {
    "objectID": "chapters/sec3/auth.html#auth-techniques",
    "href": "chapters/sec3/auth.html#auth-techniques",
    "title": "12  How login works",
    "section": "12.2 Auth Techniques",
    "text": "12.2 Auth Techniques\nIf you have five data scientists in your group, and the only shared resource you have is an RStudio Server instance, you probably don’t need to think terribly hard about auth. It’s pretty straightforward to just make users on a server and give them access to everything.\nBut as organizations get larger with hundreds or thousands of users, there’s constant churn of people joining and leaving. The number of services can creep into the dozens or hundreds and people may have very different authorization levels to different services. Trying to manage auth on the individual services is a nightmare – as is trying to keep that many usernames and passwords straight for users. That is why almost all organizations with more than a few users have centrally managed auth.\n\n12.2.1 You get a permission and you get a permission!\nFor the most part, we think of people being authenticated and authorized into services. However, it’s sometimes useful to consider the broader class of entities that could do auth. There are two common non-human entities that are included in auth systems that are worth considering.\nService Accounts are accounts given to non-human entities when you want it to be able to do something on its own behalf. For example, maybe you’ve got a Shiny app that users use to visualize data that’s in a database. Very often, you don’t want the app to have the same permisions as the app’s author, or to inherit the permissions of the people viewing the app. Instead, you want the app to be able to have permissions to do certain database operations. In that case, you would create a service account to give to the Shiny app that has exactly those permissions.\nThere are also times where it’s useful to go one level up and give permissions to an entire instance or service. In that case, you might assign permissions to an instance. For example, you could make it the case that anyone who is logged into the JupyterHub server is allowed to read from the database.\nInstance permissions are rather broad, and so they are usually only applied when you’ve got multiple resources inside a private network. In that case, authentication and authorization are only done at a single point and authorization is pretty broad.\n\n\n12.2.2 Authorization is kinda hard\nFrom a management perspective, authentication is pretty simple. A person is given a set of credentials, and they have to supply those credentials when prompted to prove they are who they say they are.\nAuthorization is a whole other can of worms. There is a meaningful literature on varieties of authorization and how they work. We’re not going to get too deep into the weeds, other than to define some common terms and how they’re used.\nThe atomic basis for authorization is a permission. Permissions are a binary switch that answers the question is this person allowed to do the thing they are trying to do?2\nThe simplest way of assigning permissions is called an access control list (ACL). In systems that use ACLs, each piece of content has a list of users who are allowed access. Sometimes, ACLs are also assigned to groups, which are simply sets of users – think data-scientists.\n\nOne ACL implementation with which you may be familiar is file permissions on a Linux server. For example, if you have a Mac or are on a Linux server, you can open your terminal, navigate to a directory and do the following:\n$ ls -l\n-rwxr-xr-x   1 alexkgold  staff   2274 May 10 12:09 README.md\nThat first set of characters describes the ACL for the README.md file. The first character - indicates this is a file, as opposed to a directory of files (which would be d). Then there are three sets of 3 characters, rwx, which are short for read, write, and execute, with the first group for the owner, alexkgold, the second group for anyone else in the group staff, and the third set for anyone else.\nSo you can read -rwxr-xr-x as, this is a file that alexkgold can read, write or execute, and anyone else can read or execute, but not edit.\nACLs are pretty intuitive, but it turns out that when you are managing a lot of users across a lot of files, directories, and services, they can get pretty difficult to manage, so many organizations use Role Based Access Control (RBAC).\nRBAC adds a layer of abstraction between users and permissions, which makes it a little harder to understand, but ultimately results in a much more flexible system. In RBAC, permissions are not assigned to individual pieces of content or to users or groups. Instead, permissions are assigned to roles, and roles are given to users or groups.3\n\nThere are also further iterations on the RBAC model, like Attribute Based Access Control (ABAC) or Policy Based Access Control (PBAC) in which there’s a long list of attributes that could be considered for a user to compute their permissions for a given service."
  },
  {
    "objectID": "chapters/sec3/auth.html#auth-technologies",
    "href": "chapters/sec3/auth.html#auth-technologies",
    "title": "12  How login works",
    "section": "12.3 Auth Technologies",
    "text": "12.3 Auth Technologies\n\n12.3.1 Username + Password\nMany pieces of software come with integrated authentication. When you use those system, the product stores encrypted username and password pairs in a database.\nThese setups are often really easy from an admin perspective – you just set up individual users on the server. However, the flip side is that users have one more username and password to remember, which is annoying for them. Moreover, if you have more than a few users, or the system is one of more than a few, it’s hard to manage users on a lot of different systems. It can be a real pain to create accounts on a ton of different systems when a new person joins the organization, or to remove their permissions one-by-one when they leave.\nFor this reason, most IT/Admin organizations strongly prefer using some sort of centralized identity store.\n\n\n12.3.2 PAM\nPluggable Authentication Modules (PAM) is a Linux system for doing authentication. As of this writing, PAM is the default authentication method for both RStudio Server and JupyterHub.\n\nConceptually PAM is pretty straightforward. You install a service on a Linux machine and configure it to use PAM authentication from the underlying host. By default, PAM just authenticates against the users configured on the Linux server, but it can also be configured to use other sorts of “modules” to authenticate against other systems – most commonly LDAP/AD or Kerberos. PAM can also be used to do things when users login – the most common being initializing tokens or tickets to other systems, like a database.\nPAM is often paired with System Security Services Daemon (SSSD), which is most commonly used to automatically create Linux users on a server based on the identities stored in an LDAP/AD instance.\nThough conceptually simple, reading, writing, and managing PAM modules is kinda painful.\n#TODO: Add PAM example\n\n\n12.3.3 LDAP/AD\nLightweight Directory Access Protocol (LDAP) is a relatively old, open, protocol used for maintaining a set of entities and their attributes. To be precise, LDAP is actually a protocol for maintaining and accessing entities and their attributes in a tree. It happens that this is a really good structure for maintaining permissions and roles of users at an organization, and it’s the main thing LDAP is used for.\nActive Directory (AD) is Microsoft’s implementation of LDAP, and is by-far the most common LDAP “flavor” out there. AD so thoroughly owns the LDAP enterprise market, that LDAP is often referred to as LDAP/AD. There are other implementations you may run across, the most common being OpenLDAP.\n\nAzure, Microsoft’s cloud provider, offers an authentication service called Azure Active Directory (AAD). Confusingly, AAD is usually used in combination with SAML, not LDAP.\n\nIt’s worth distinguishing the use of LDAP as an identity store from its use as an authentication technology. As a tree-based database, LDAP is uniquely well-suited to storing the identities, and other attributes of people at the organization. However, as discussed below, using LDAP to authenticate into actual services has security and convenience drawbacks, and many organizations consider it outdated and insecure.\nA lot of organizations are moving away from LDAP for authentication in favor of token-based technologies like SAML or OAuth, but many are keeping LDAP as their identity “source of truth” that is referenced by the SAML or OAuth Identity Provider.\nLDAP has three main disadvantages relative to other technologies. First, LDAP requires that your credentials (username and password, usually) actually be provided to the service you’re trying to use. This is fundamentally insecure relative to a system where your credentials are provided only to the identity provider, and the service just gets a token verifying who you are. In token-based systems, adding additional requirements like MFA or biometrics are easy, as they’re simply added at the IdP layer. In contrast, doing those things in LDAP would require the service to implement them, which usually is not the case, so you’re usually limited to username and password.\nThe second disadvantage of LDAP is that it does not allow for central administration of permissions. LDAP directly records only objects and their attributes. Say, for example, you want only users of a particular group to have access to a certain resource. In LDAP, you would have to specify in that resource that it should only allow in users of that group. This is in contrast to SAML/OAuth, where the authorization is centrally managed.\nLastly, LDAP authentication is based on each service authenticating. Once you authenticate, the service might give you a cookie so that your login persists, but there is no general-purpose token that will allow you to login to multiple services.\n\n12.3.3.1 How LDAP Works\nWhile the technical downsides of LDAP are real, the tradeoff is that the technical operations of LDAP are pretty straightforward. In short, you try to login to a service, the service collects your username and password, sends it off to the LDAP server, and checks that your username and password are valid.\n\nNote that LDAP is purely for authentication. When you’re using LDAP, authorization has to be handled separately, which is one of the disadvantages.\n\n\n12.3.3.2 Deeper Than You Need on LDAP\nLDAP is a tree-based entity and value store. This means that LDAP stores things and their attributes, which include a name and one or more values. For example, my entry in a corporate LDAP directory might look like this:\ncn: Alex Gold\nmail: alex.gold@example.com\nmail: alex.gold@example.org\ndepartment: solutions\nmobile: 555-555-5555\nobjectClass = Person\nMost of these attributes should be pretty straightforward. cn is short for common name, and is part of the way you look up an entity in LDAP (more on that below). Each entity in LDAP must have an objectClass, which determines the type of entity it is. In this case, I am a Person , as opposed to a device, domain, organizationalRole, or room – all of which are standard objectClasses.\nLet’s say that your corporate LDAP looks like the tree below:\n\n#TODO: make solutions an OU in final\nThe most common way to look up LDAP entities is with their distinguished name (DN), which is the path of names from the point you’re starting all the way back to the root of the tree. In the tree above, my DN would be cn=alex,ou=solutions,dc=example,dc=com.\nNote that you read the DN from right to left to work your way down the tree. Aside from cn for common name, other common fields include ou for organizational unit, and dc for domain component.\n\n\n12.3.3.3 Trying out LDAP\nNow that we understand in theory how LDAP works, let’s try out an actual example.\nTo start, let’s stand up LDAP in a docker container:\n#TODO: update ldif\ndocker network create ldap-net\ndocker run -p 6389:389 \\\n  --name ldap-service \\\n  --network ldap-net \\\n  --detach alexkgold/auth\nldapsearch is a utility that lets us run queries against the LDAP tree. Let’s try it out against the LDAP container we just stood up.\nLet’s say I want to return everything in the subtree under example.org. In that case, I would run ldapsearch -b dc=example,dc=org, where b indicates my search base, which is a dn. But in order to make this actually work, we’ll need to include a few more arguments, including\n\nthe host where the LDAP server is, indicated by -H\nthe bind DN we’ll be using, flagged with -D\nthe bind password we’ll be using, indicated by -w\n\nSince we’re testing, we’re also going to provide the flag -x to use whatever certificate is present on the server. Putting it altogether, along with the commands to reach the docker container, let’s try:\nldapsearch -x -H ldap://localhost:6389 -b dc=example,dc=org -D \"cn=admin,dc=example,dc=org\" -w admin\n\n# extended LDIF\n#\n# LDAPv3\n# base <dc=example,dc=org> with scope subtree\n# filter: (objectclass=*)\n# requesting: ALL\n#\n\n# example.org\ndn: dc=example,dc=org\nobjectClass: top\nobjectClass: dcObject\nobjectClass: organization\no: Example Inc.\ndc: example\n\n# admin, example.org\ndn: cn=admin,dc=example,dc=org\nobjectClass: simpleSecurityObject\nobjectClass: organizationalRole\ncn: admin\ndescription: LDAP administrator\nuserPassword:: e1NTSEF9d3IyVFp6SlAyKy9xT2RsQ0owTDYzR0RzNFo0NUFrQ00=\n\n# search result\nsearch: 2\nresult: 0 Success\n\n# numResponses: 3\n# numEntries: 2\nYou should be able to read what got returned pretty seamlessly. One thing to notice is that the user password is returned, so it can be compared to a password provided. It is encrypted, so it doesn’t appear in plain text.\nNote that ldap is a protocol – so it takes the place of the http you’re used to in normal web operations. Like there’s https, there is also a protocol called LDAPS, which is ldap + tls for the same reason you’ve got https. LDAP is (almost) always running in the same private network as the service, so many organizations don’t require using LDAPS, but others do require it.\nRunning the ldapadmin\ndocker run -p 6443:443 \\\n        --name ldap-admin \\\n        --env PHPLDAPADMIN_LDAP_HOSTS=ldap-service \\\n        --network ldap-net \\\n        --detach osixia/phpldapadmin\ndn for admin cn=admin,dc=example,dc=org pw: admin\nhttps://localhost:6443\n# Replace with valid license\nexport RSC_LICENSE=XXXX-XXXX-XXXX-XXXX-XXXX-XXXX-XXXX\n\n# Run without persistent data and using default configuration\ndocker run -it --privileged \\\n    --name rsc \\\n    --volume $PWD/rstudio-connect.gcfg:/etc/rstudio-connect/rstudio-connect.gcfg \\\n    -p 3939:3939 \\\n    -e RSC_LICENSE=$RSC_LICENSE \\\n    --network ldap-net \\\n    rstudio/rstudio-connect:latest\n\n\n12.3.3.4 Single vs Double Bind\nThere are two different ways to establish a connection between your server and the LDAP server. The first method is called Single Bind. In a single bind authentication, the user credentials are used both to authenticate to the LDAP server, and to query the server.\nIn a Double Bind configuration, there is a separate administrative service account, used to authenticate to the LDAP server. Once authentication is complete, then the user is queried in the system.\nSingle bind configurations are often more limited than double bind ones. For example, in most cases you’ll only be able to see the single user as well as the groups they’re a part of. This can limit application functionality in some cases. On the other hand, there need be no master key maintained on your server, and some admins may prefer it for security reasons.\nWe can see this really concretely. In the example above, you used a double bind by supplying admin credentials to LDAP. Let’s say instead, you just provide a single user’s credentials. In that case, I don’t get anything back if I just do a general search.\nldapsearch -x -H ldap://localhost:6389 -b dc=example,dc=org -D \"cn=joe,dc=engineering,dc=example,dc=org\" -w joe                                       \n# extended LDIF\n#\n# LDAPv3\n# base <dc=example,dc=org> with scope subtree\n# filter: (objectclass=*)\n# requesting: ALL\n#\n\n# search result\nsearch: 2\nresult: 32 No such object\n\n# numResponses: 1\nBut just searching for information about Joe does return his own information.\nldapsearch -x -H ldap://localhost:6389 -b cn=joe,dc=engineering,dc=example,dc=org -D \"cn=joe,dc=engineering,dc=example,dc=org\" -w joe                    32 ✘\n# extended LDIF\n#\n# LDAPv3\n# base <cn=joe,dc=engineering,dc=example,dc=org> with scope subtree\n# filter: (objectclass=*)\n# requesting: ALL\n#\n\n# joe, engineering.example.org\ndn: cn=joe,dc=engineering,dc=example,dc=org\ncn: joe\ngidNumber: 500\ngivenName: Joe\nhomeDirectory: /home/joe\nloginShell: /bin/sh\nmail: joe@example.org\nobjectClass: inetOrgPerson\nobjectClass: posixAccount\nobjectClass: top\nsn: Golly\nuid: test\\joe\nuidNumber: 1000\nuserPassword:: e01ENX1qL01raWZrdk0wRm1sTDZQM0MxTUlnPT0=\n\n# search result\nsearch: 2\nresult: 0 Success\n\n# numResponses: 2\n# numEntries: 1\n\n\n\n12.3.4 Kerberos Tickets\nKerberos is a relatively old ticket-based auth technology. In Kerberos, encrypted tickets are passed around between servers. Because these tickets live entirely on servers under the control of the organization, they are generally quite secure.\nThough Kerberos is freely available, it was widely adopted along with Active Directory, and it’s used almost exclusively in places that are running a lot of Microsoft products. A frequent use of Kerberos tickets is to establish database connections.\nBecause the tickets are passed around from server to server, Kerberos can be used to create a true SSO experience for users.\n\n12.3.4.1 How Kerberos Works\nAll of Kerberos works by sending information to and from the central Kerberos Domain Controller (KDC). In Kerberos, authentication and authorization are handled independently.\nWhen a Kerberos session is initialized, the service sends the users credentials off to the KDC and requests something called the Ticket Granting Ticket (TGT) from the KDC. TGTs have a set expiration period. When they expire, the client has to request an updated TGT. This is one reason why Kerberos is considered quite secure - even if someone managed to steal a TGT, they’d only be able to use it for a little while before it went stale and could be revoked.\nWhen the user wants to actually do something, they send the TGT back to the KDC again and get a session key (sometimes referred to as a service ticket) that allows access to the service, usually with a specified expiration period.\n\n\n\n12.3.4.2 Try out Kerberos\n#TODO\n\n\n\n12.3.5 SAML\nThese days Security Assertion Markup Language (SAML) is probably the most common system that provides true SSO – including single login and centrally-managed permissions. SAML does this by passing around XML tokens.4\nThe way this generally works is that a user attempts to login to a Service Provider (SP). The SP redirects the user to an Identity Provider (IdP), which checks either for a preexisting token in the users browser, or verifies the users credentials. The IdP checks for the user’s authorization to access the SP in question, and sends an authorization token back to the SP.\n\nRelative to LDAP/AD, which is from the early 1990s, SAML is a new kid on the block. SAML 1.0 was introduced in 2002, and SAML 2.0, which is the current standard, came out in 2005. Many large enterprises are switching their systems over to use SAML or have already done so.\nOne superpower of SAML IdPs is that many of them can federate identity management to other systems. So, it’s pretty common for large enterprises to maintain their user base in one or more LDAP/AD system, but actually use a SAML IdP to do authentication and authorization. In fact, this is what Azure Active Directory (AAD), which is Microsoft Azure’s hosted authentication offering does. It is possible to use LDAP/AD with AAD, but most organizations use it with SAML.\nOne of the nice things about SAML is that credentials are never shared directly with the SP. This is one of the ways in which SAML is fundamentally more secure than LDAP/AD – the users credentials are only ever shared with the IdP.\nThere are two different ways logins can occur – starting from the SP, or starting from the IdP.\nIn SAML, the XML tokens that are passed back and forth are called assertions.\n\n12.3.5.1 Try SAML\nWe’re going to use a simple SAML IdP to try out SAML a bit. This container only supports a single SP. Any IdP that might be used in an enterprise environment is going to support many SPs simultaneously.\nLet’s go through the environment variables we’re providing to this docker run command. We’re providing three different arguments:\n\nThe SP_ENTITY_ID is the URL of the\nSP_ASSERTION_CONSUMER_SERVICE is the URL of the SP that is prepared to receive the authorized responses coming back from the SAML IdP.\nSP_SINGLE_LOGOUT_SERVICE is the URL where the SP will receive a logout command once someone has been logged out at the IdP level. Many SPs do not implement single logout.\n\ndocker run --name=saml_idp \\\n-p 8080:8080 \\\n-p 8443:8443 \\\n-e SIMPLESAMLPHP_SP_ENTITY_ID=http://app.example.com \\\n-e SIMPLESAMLPHP_SP_ASSERTION_CONSUMER_SERVICE=http://localhost/simplesaml/module.php/saml/sp/saml2-acs.php/test-sp \\\n-e SIMPLESAMLPHP_SP_SINGLE_LOGOUT_SERVICE=http://localhost/simplesaml/module.php/saml/sp/saml2-logout.php/test-sp \\\n-d kristophjunge/test-saml-idp:1.15\nhttp://localhost:8080/simplesaml\nadmin/secret\n\n\n\n12.3.6 OIDC/OAuth2.0\nOIDC/OAuth is slightly newer than SAML, created in 2007 by engineers at Google and Twitter. OAuth 2.0 – the current standard was released in 2012. If you’re being pedantic, OAuth is a authorization protocol, and OpenID Connect (OIDC) is an authorization protocol that uses OAuth. In most cases, people will just call it OAuth.\n\n#TODO: this picture is bad\nIn an enterprise context, OAuth/OIDC is conceptually very similar to SAML – but instead of passing around XML tokens, it’s based on JSON Web Tokens (JWT, usually pronounced “jot”).\n\nThe fact that JSON is much more human-readable than XML is one of the big advantages of OIDC/OAuth compared to SAML.\n\n#TODO: try it out\n\n12.3.6.1 OAuth/OIDC vs SAML\nFrom a practical perspective, the biggest difference between OAuth/OIDC and SAML is that SAML is quite strict about what SPs are allowed. Each SP needs to be registered at a specific web address that the IdP knows it’s allowed to receive requests from.\nIn contrast, OAuth/OIDC was designed to be used to delegate authentication and authorization to different kinds of services that might be widely available on the internet. If you’ve ever allowed a website to Login with Apple/Google/Facebook/Github, that has been an application of OAuth/OIDC.\nBecause the set of allowable SPs is fixed under SAML, it’s more common in enterprise settings. Some admins consider SAML more secure for that reason as well.\nIn some situations, SAML is used for authentication and OAuth is used for access to other services. Most commonly in the data science world, this can come up when a user logs into a service like RStudio Server and is then authorized to a database using an OAuth JWT.\nResources: https://www.okta.com/identity-101/saml-vs-oauth/ https://www.okta.com/identity-101/whats-the-difference-between-oauth-openid-connect-and-saml/ https://phoenixnap.com/blog/kerberos-authentication https://www.dnsstuff.com/rbac-vs-abac-access-control"
  },
  {
    "objectID": "chapters/sec3/offline.html",
    "href": "chapters/sec3/offline.html",
    "title": "13  Offline",
    "section": "",
    "text": "Some organizations require that servers not be connected to the internet (airgapped/offline)\nThis makes things hard.\nUnderstanding whether it’s inbound connections that are disallowed or both outbound + inbound is important.\nDifficulties that tend to arise:\n\nDownloading software updates\nGetting R/Python packages\nSoftware licensing (often reach to license servers)"
  },
  {
    "objectID": "chapters/sec3/scaling.html#types-of-scaling",
    "href": "chapters/sec3/scaling.html#types-of-scaling",
    "title": "14  Scaling",
    "section": "14.1 Types of Scaling",
    "text": "14.1 Types of Scaling\n\nVertical Scaling – just make the server bigger\nHorizontal – add more parallel servers\n\nSometimes called load-balancing\n\nReliability - or high-availability (HA)\n\nMake sure the service doesn’t go down (or less often)\nEliminate single-points-of-failure\nReally hard to do all the way – spectrum of completeness + difficulty\n\nHealth Checks/Heartbeats – periodic checkins to ensure server/service healthy"
  },
  {
    "objectID": "chapters/sec3/scaling.html#load-balancing-configurations",
    "href": "chapters/sec3/scaling.html#load-balancing-configurations",
    "title": "14  Scaling",
    "section": "14.2 Load-Balancing configurations",
    "text": "14.2 Load-Balancing configurations\n[Graphic: network diagram of lb-config]\n\nActive/Active - all servers accept traffic\n\nSingle vs multiple masters\n\nActive/Passive (fallover/failover) - secondary server, remains inert until 2nd one fails\nDisaster Recovery - backup data to another disk – somewhat slower resumption of service"
  },
  {
    "objectID": "chapters/sec3/scaling.html#adding-servers-in-real-time",
    "href": "chapters/sec3/scaling.html#adding-servers-in-real-time",
    "title": "14  Scaling",
    "section": "14.3 Adding servers in real time",
    "text": "14.3 Adding servers in real time\n\nIf you find yourself under heavy load, you’ll want to add more servers\nIf you’re using infrastructure as code tooling, this is easy\nIt’s also easy if using some sort of docker orchestration, as the underlying hosts are all the same from the perspective of docker/K8S\n\nSpannning Multiplee AZs"
  },
  {
    "objectID": "chapters/append/env-req.html",
    "href": "chapters/append/env-req.html",
    "title": "Appendix A — Determining What You Need",
    "section": "",
    "text": "Determining Requirements for your platform Platform requirements combination of Work being done on the platform Data Science needs (i.e. horsepower) Existing state of org’s data Type of work data scientists might do Batch Jobs ETL Reports Data Generation Real-Time Interactive Models APIs Apps Data Science + IT Needs Languages (R, Python, SQL) Sharing – internal, external Online or offline? Data Security – row based access? Centralized workbench vs individual workstations Existing Assets BI/Analytics Platforms Data Connections Checklist of above"
  },
  {
    "objectID": "chapters/append/terminal.html",
    "href": "chapters/append/terminal.html",
    "title": "Appendix B — Getting Started with Terminal",
    "section": "",
    "text": "Terminal in Mac Iterm2 bash, zsh, fish\nwindows WSL Powershell PuTTY – no longer needed, but still an option Do need to enable SSH client\ntmux vim/nano"
  },
  {
    "objectID": "chapters/append/linux-cmd.html#miscellaneous-symbols",
    "href": "chapters/append/linux-cmd.html#miscellaneous-symbols",
    "title": "Appendix C — Useful Shell Commands",
    "section": "C.1 Miscellaneous Symbols",
    "text": "C.1 Miscellaneous Symbols\n\n\n\n\n\nSymbol\n\n\n\n\nWhat it is\n\n\n\n\nHelpful options\n\n\n\n\nExample\n\n\n\n\n\n\n\n\n/\n\n\n\n\nsystem root\n\n\n\n\n\n\n\n\n\n\n~\n\n\n\n\nyour home directory\n\n\n\n\n\n\necho ~\n\n\n/ home/alex.gold\n\n\n\n\n\n\n.\n\n\n\n\ncurrent working directory\n\n\n\n\n\n\n\n\n\n\nman\n\n\n\n\nmanual\n\n\n\n\n\n\n\n\n\n\n|\n\n\n\n\nthe pipe\n\n\n\n\n\n\n\n\n\n\necho\n\n\n\n\n\n\n\n\n\n\n\n\n$\n\n\n\n\n\n\n\n\n\n\n\n\nsudo\n\n\n\n\n\n\n\n\n\n\n\n\nsu"
  },
  {
    "objectID": "chapters/append/linux-cmd.html#moving-yourself-and-your-files",
    "href": "chapters/append/linux-cmd.html#moving-yourself-and-your-files",
    "title": "Appendix C — Useful Shell Commands",
    "section": "C.2 Moving yourself and your files",
    "text": "C.2 Moving yourself and your files\n\n\n\n\n\nC ommand\n\n\n\n\nWhat it does\n\n\n\n\nHelpful options\n\n\n\n\nExample\n\n\n\n\n\n\n\n\npwd\n\n\n\n\nprint working directory\n\n\n\n\n\n\n$ pwd\n\n\n/U sers/alex.gold/\n\n\n\n\n\n\ncd\n\n\n\n\nchange directory\n\n\n\n\n\n\n$ cd ~/Documents\n\n\n\n\n\n\nls\n\n\n\n\nlist\n\n\n\n\n-l - format as list\n\n\n-a - all include hidden files\n\n\n\n\n$ ls .\n\n\n$ ls -la\n\n\n\n\n\n\nrm\n\n\n\n\nremove delete permanently!\n\n\n\n\n-r - recursively a directory and included files\n\n\n-f - force - don’t ask for each file\n\n\n\n\n$ rm old_doc\n\n\nr m -rf old_docs/\n\n\nBE VERY CAREFUL WITH -rf\n\n\n\n\n\n\ncp\n\n\n\n\ncopy\n\n\n\n\n\n\n\n\n\n\nmv\n\n\n\n\nmove\n\n\n\n\n\n\n\n\n\n\nchmod"
  },
  {
    "objectID": "chapters/append/linux-cmd.html#checking-out-files",
    "href": "chapters/append/linux-cmd.html#checking-out-files",
    "title": "Appendix C — Useful Shell Commands",
    "section": "C.3 Checking out Files",
    "text": "C.3 Checking out Files\nOften useful in server contexts for reading log files.\n\n\n\nC ommand\nWhat it does\nHelpful options\nExample\n\n\n\n\ncat\n\n\n\n\n\nless\n\n\n\n\n\ntail\n\n-f\n\n\n\ngrep\n\n\n\n\n\ntar"
  },
  {
    "objectID": "chapters/append/linux-cmd.html#checking-out-server-activity",
    "href": "chapters/append/linux-cmd.html#checking-out-server-activity",
    "title": "Appendix C — Useful Shell Commands",
    "section": "C.4 Checking out Server Activity",
    "text": "C.4 Checking out Server Activity\n\n\n\nC ommand\nWhat it does\nHelpful options\nExample\n\n\n\n\ndf\n\n-h\n\n\n\ntop\n\n\n\n\n\nps\n\n\n\n\n\nlsof"
  },
  {
    "objectID": "chapters/append/linux-cmd.html#checking-out-networking",
    "href": "chapters/append/linux-cmd.html#checking-out-networking",
    "title": "Appendix C — Useful Shell Commands",
    "section": "C.5 Checking out Networking",
    "text": "C.5 Checking out Networking\n\n\n\nC ommand\nWhat it does\nHelpful options\nExample\n\n\n\n\nping\n\n\n\n\n\nne tstat\n\n\n\n\n\ncurl"
  },
  {
    "objectID": "chapters/append/linux-cmd.html#user-management",
    "href": "chapters/append/linux-cmd.html#user-management",
    "title": "Appendix C — Useful Shell Commands",
    "section": "C.6 User Management",
    "text": "C.6 User Management\n\n\n\nC ommand\nWhat it does\nHelpful options\nExample\n\n\n\n\nw hoami\n\n\n\n\n\np asswd\n\n\n\n\n\nus eradd"
  }
]