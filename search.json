[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DevOps for Data Science",
    "section": "",
    "text": "Welcome!\nThis is the website for the book DevOps for Data Science, currently in draft form.\nIn this book, you’ll learn about DevOps conventions, tools, and practices that can be useful to you as a data scientist. You’ll also learn how to work better with the IT/Admin team at your organization, and even how to do a little server administration of your own if you’re pressed into service.\nThis website is (and always will be) free to use, and is licensed under the Creative Commons Attribution-NonCommercial-NoDerivs 3.0 license.\nIf you’d like a physical copy of the book, they will be available once it’s finished!"
  },
  {
    "objectID": "index.html#software-information-and-conventions",
    "href": "index.html#software-information-and-conventions",
    "title": "DevOps for Data Science",
    "section": "Software information and conventions",
    "text": "Software information and conventions\nI used the knitr package [@xie2015] and the quarto package [@quarto] to compile my book.\nPackage names are in bold text (e.g., rmarkdown), and inline code and filenames are formatted in a typewriter font (e.g., knitr::knit('foo.Rmd')). Function names are followed by parentheses (e.g., bookdown::render_book())."
  },
  {
    "objectID": "index.html#about-the-author",
    "href": "index.html#about-the-author",
    "title": "DevOps for Data Science",
    "section": "About the Author",
    "text": "About the Author\nAlex Gold is the Director of Solutions Engineering at Posit, formerly RStudio.\nThe Solutions Engineering team works with Posit’s customers to help them deploy, configure, and use Posit’s professional software and open source tooling in R and Python.\nIn his free time, he enjoys landscaping, handstands, and Tai Chi.\nHe occasionally blogs about data, management, and leadership at alexkgold.space."
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "DevOps for Data Science",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nA lot of people are helping me write this book.\nThis book is published to the web using GitHub Actions from rOpenSci."
  },
  {
    "objectID": "index.html#color-palette",
    "href": "index.html#color-palette",
    "title": "DevOps for Data Science",
    "section": "Color palette",
    "text": "Color palette\nTea Green: #CAFFDO\nSteel Blue: #3E7CB1\nKombu Green: #273c2c\nBright Maroon: #B33951\nSandy Brown: #FCAA67"
  },
  {
    "objectID": "chapters/intro.html#devops-for-agile-software",
    "href": "chapters/intro.html#devops-for-agile-software",
    "title": "Introduction",
    "section": "DevOps for Agile Software",
    "text": "DevOps for Agile Software\nDevOps is a set of cultural norms, practices, and supporting tooling to help make the process of developing and deploying software smoother and lower risk.\nIf you feel like that definition is pretty vague and unhelpful, you’re right.\nLike Agile software development, to which it is closely related, DevOps is a squishy concept. That’s partially because DevOps isn’t just one thing – it’s the application of some principles and process ideas to whatever context you’re actually working in. That malleability is one of the great strengths of DevOps, but can also make it difficult to pin down.\nThis squishiness is furthered by the ecosystem of companies enabling DevOps. There are dozens and dozens of companies proselytizing their own particular flavor of DevOps – one that (shocker) reflects the capabilities of whatever product they’re selling.\nBut underneath the industry hype and the marketing jargon, there are some extremely valuable lessons to take from the field.\nTo understand better, let’s go back to the birth DevOps.\nAs the story goes, the history of software development before the 1990s involved a waterfall development processes. In these processes, software developers would work with clients and customers to fully define requirements for software, plan the whole thing out, and deliver final software months or years later.\nWhen the application was complete, it was hurled over the metaphorical wall from Development to Operations. IT Administrators in the Ops department would figure out the hardware and networking requirements, get it running, and keep it up.\nThroughout the 1990s, software developers observed that delivering software in small units, quickly collecting feedback, and iterating was a more effective model.\nIn 2001, the Manifesto for Agile Software Development was published, giving a name to this philosophy of software development. Agile development basically ate the world and basically all software is developed using some form of Agile these days. These days, Agile work styles have extended far beyond software into other domains as well.\nThere are dozens of Agile software development frameworks you might’ve heard of like Scrum, Kanban, Extreme Programming, and many, many more. One commonality of these frameworks were really focused on software development. What happened once the software was written?\nThe old pattern clearly wouldn’t work. If you were doing new deployments multiple times a week – or even a day – you needed a complementary process to get that software deployed and into production.\nDevOps arose as this discipline – a way for Dev and Ops to better collaborate on the process that would take software from development intor production. It took a little while for the field to be formalized, and the term DevOps came into common usage around 2010."
  },
  {
    "objectID": "chapters/intro.html#processes-and-people",
    "href": "chapters/intro.html#processes-and-people",
    "title": "Introduction",
    "section": "Processes and People",
    "text": "Processes and People\nThroughout this book, DevOps refers to the knowledge, practices, and tools that make it easier, safer, and faster to put work into production. So, if you’re a software developer (and as a data scientist, you are) you need to be thinking about DevOps.\nMost organizations also have a set of people and roles who have the permission and responsibility for managing the servers and software at your organization. Their titles vary widely by organization – they might be named Information Technology (IT), SysAdmin, Site Reliability Engineering (SRE), or DevOps.1\nFor the sake of simplicity, I’m going to use the term IT/Admin to refer to these people and teams throughout this book.\nFundamentally, DevOps is about creating good patterns for people to collaborate on developing and deploying software and those patterns vary by organization. That means that DevOps can and should be adapted to your organization.\nAs a data scientist, you are the Dev, and so a huge part of making DevOps work for you is finding some IT/Admin counterparts with whom you can develop a successful collaboration. In some cases that will be easier than others. Here are three patterns that are almost always red flags – mostly because they make it hard to develop relationships that can sustain the kind of collaboration DevOps necessitates.\n\nAt some large organizations, IT/Admin functions are split into small atomic units like security, databases, networking, storage, procurement, cloud, and more. This is useful for keeping the scope-of-work manageable for the people in that group – and often results in super deep technical expertise – but also means that it can be slow to get anything done because you’ll need to bring people together from disparate teams to actually get anything done.\nSome organizations have chosen to outsource their IT/Admin functions. While the individuals in those outsourced teams are often quite competent, building relationships can be difficult. Outsourced IT/Admin teams are often in India, so it can be hard to find meeting times with American and European teams. Additionally, turnover on projects and systems tends to be high at outsourced IT/Admin organizations. That means that institutional knowledge tends to be thin and relationships can’t be relied on long term.\nSome organizations, especially small or new ones, there isn’t an IT/Admin function. At others, the IT/Admins are preoccupied with other tasks and don’t have the capacity to help the data science team. This isn’t a tragedy, but it probably means you’re going to have to become the IT/Admin if you want to get anything done.\n\nWhether your organization has an IT/Admin setup that facilitates DevOps best practices or not, hopefully this book can help you take the first steps towards making your path to production smoother and simpler."
  },
  {
    "objectID": "chapters/intro.html#about-this-book",
    "href": "chapters/intro.html#about-this-book",
    "title": "Introduction",
    "section": "About this book",
    "text": "About this book\nOver the course of engaging with so many organizations, I’ve seen which patterns work to grease the path to production for data scientists and which ones tend to impede it.\nMany of those patterns are inspired by traditional DevOps, but they’re not identical because of differences between pure software development and data science (more on that in ?sec-1-intro).\nThe first goal of this book is to equip you with mental models you can apply in your own work and your own organization to create data science projects that are easier to take to production.\nThe second goal of this book is to give you some background on relevant IT/Admin knowledge.\nIT Administration is an older field than DevOps or data science, and it’s full of arcane language and technologies. This book will equip you with the vocabulary to talk to the IT/Admins at your organization and some basic skills of how to do IT/Admin tasks yourself.\nThe book is divided up into four sections.\nSection 1 is about data science best practices that can make it easy to take your R and Python work into production.\nSection 2 is an introduction to the tools IT/Admins use in their work like the command line, bash, and Docker.\nSection 3 is a walkthrough of basic concepts in IT Administration that will get you to the point of being able to host and manage your own small server.\nSection 4 is about how all of what you learned in Section 3 changes when you go to enterprise scale.\n\nComprehension Questions\nEach chapter in this book includes comprehension questions. As you get to the end of the chapter, take a moment to consider these questions. If you feel comfortable answering them, you’ve probably understood the content of the chapter pretty well.\nAlternatively, feel free to jump ahead to them as you’re reading the chapter. If you can already answer them all, you can probably skip that chapter’s content.\n\n\n\n\n\n\nMental Models + Mental Maps\n\n\n\nThroughout the book, I’ll talk a lot about building a mental model of different concepts.\nA mental map is way to represent mental models.\nIn a mental map, you draw each of the nouns as nodes and connect them with arrows that are labelled to explain the relationship.\nMental maps are a great way to test your mental models, so I’ll suggest them as comprehension questions in many chapters.\nHere’s an example about this book:\n\n\n\n\ngraph LR\n    A[I] --&gt;|Wrote| B[DO4DS]\n    C[You] --&gt; |Are Reading| B\n    B --&gt; |Includes| D[Exercises]\n    D --&gt; |Some Are| E[Mind Maps]\n\n\n\n\n\nNote how every node is a noun, and the edges (labels on the arrows) are verbs. It’s pretty simple! But writing down the relationships between entities like this is a great check on understanding.\n\n\n\n\nLabs\nMany chapters also contain labs. The idea of these labs is to give you hands-on experience with the concepts at hand.\nThese labs all tie together. If you follow the labs in this book, you’ll build up a reasonably complete data science platform that includes a place for you to work, a way to store data, and a deployment environment.\nPalmer Penguins is a public dataset meant for demonstrating data exploration and visualization. We’re going to pretend we care deeply about the relationship between penguin bill length and mass. We’re going to build up an entire data science environment dedicated to exploring that relationship.\nThe front end of this environment is going to be a website that contains an app that allows you to get predictions from a machine learning model of a penguin’s mass based on bill length and other features. We’re also going to include pages dedicated to exploratory data analysis and model building on the website.\nOn the backend, we’re going to build a data science workbench on an AWS EC2 instance where we can do this work. It will include RStudio Server and JupyterHub for working. It will additionally host the machine learning model as an API and the Shiny app that appears on the website.\nThe whole thing will get auto-deployed from a git repo using GitHub Actions.\nFrom an architectural perspective, it’ll look something like this:\n\n\n\n\n\ngraph LR\n    subgraph Palmer Penguins Site\n        A[\"EDA \n        R Quarto\"] \n        B[\"Model Build \n        Jupyter\"] \n        C[\"Model Explorer \n        PyShiny\"]\n    end\n    subgraph Git Repo\n        D[Repo] --&gt; |GHA| A\n        D --&gt; |GHA| B\n    end\n    subgraph AWS\n        subgraph EC2\n            F[Shiny]\n            G(RStudio Server)\n            H(Jupyter Hub)\n            I[FastAPI]\n            J[DuckDB]\n            J --&gt; I\n            F --&gt; C\n        end\n        subgraph S3\n            E[Model]\n        end\n    end\n\n\n\n\n\n\nIf you’re interested in exactly which pieces get completed in each chapter, check out Appendix B.\n\n\nConventions\nThroughout the book, I will italicize terms of art the first time I introduce them as well as the names of other publications. Bolding will be reserved for emphasis.\nR, Python, and system package names will be in code font and will have braces around them {package}.\nVariables that you would replace with your own values will appear inside angled brackets like this &lt;your-variable&gt;."
  },
  {
    "objectID": "chapters/intro.html#footnotes",
    "href": "chapters/intro.html#footnotes",
    "title": "Introduction",
    "section": "",
    "text": "I think a lot of DevOps experts would argue that you’re doing DevOps wrong if you have a standalone DevOps team.↩︎"
  },
  {
    "objectID": "chapters/sec1/1-0-sec-intro.html#labs-in-this-section",
    "href": "chapters/sec1/1-0-sec-intro.html#labs-in-this-section",
    "title": "DevOps Lessons for Data Science",
    "section": "Labs in this section",
    "text": "Labs in this section\nEach chapter in this section has a lab so you can get hands-on experience implementing the best practices I propose.\nIf you complete the labs, you’ll have stood up your Palmer Penguins website to explore the relationship between penguin bill length and mass. Your website will include pages on exploratory data analysis and model building. This website will automatically build and deploy based on changes in a git repo.\nBy the end of the section, you’ll also create a Shiny app that visualizes model predictions and an API that hosts the model and provides real-time predictions to the app. We won’t get to standing up the app and API to live on your website – that’ll have to wait for Section 3.\nFor more details on exactly what you’ll do in each chapter, see Appendix B."
  },
  {
    "objectID": "chapters/sec1/1-0-sec-intro.html#footnotes",
    "href": "chapters/sec1/1-0-sec-intro.html#footnotes",
    "title": "DevOps Lessons for Data Science",
    "section": "",
    "text": "If you enjoy this introduction, I strongly recommend The Phoenix Project by Gene Kim, Kevin Behr, and George Spafford. It’s a novel about implementing DevOps principles. A good friend described it as, “a trashy romance novel about DevOps”. It’s a very fun read.↩︎"
  },
  {
    "objectID": "chapters/sec1/1-1-env-as-code.html#environments-have-layers",
    "href": "chapters/sec1/1-1-env-as-code.html#environments-have-layers",
    "title": "1  Environments as Code",
    "section": "1.1 Environments have layers",
    "text": "1.1 Environments have layers\nData science environments have three distinct layers that build on each other. Once you understand the layers of an environment, you can think more clearly about what your actual reproducibility needs are, and which environmental layers you need to target putting into code.\nLayers of a Data Science Environments\n\n\n\n\n\n\n\nLayer\nContents\n\n\n\n\nPackages\nR + Python Packages\n\n\nSystem\nR + Python Language Versions\nOther System Libraries\nOperating System\n\n\nHardware\nVirtual Hardware\nPhysical Hardware\n\n\n\nFundamentally, the hardware and software layers should be the responsibility of an IT/Admin. It may be the case that you’re responsible for them as well, but then you’re just fulfilling that role.\nBut as a data scientist, you can and should be responsible for the package layer, and getting this layer right is where the biggest reproducibility bang for your buck lies."
  },
  {
    "objectID": "chapters/sec1/1-1-env-as-code.html#what-is-a-package-environment",
    "href": "chapters/sec1/1-1-env-as-code.html#what-is-a-package-environment",
    "title": "1  Environments as Code",
    "section": "1.2 What is a package environment",
    "text": "1.2 What is a package environment\nIt’s helpful to start by building a mental model of what a package environment is before we get into how to use them well.\nLet’s start with an intro to the different states packages can be in.\n\nIn a repository. You’re probably used to installing packages from a packages-specific repository like PyPI, Conda, CRAN, or BioConductor. These repositories are like the grocery store – the food is packaged up and ready to go, but inert. There’s also lots of variety – repositories hold current and archival versions of tons of different packages.\nIn a library. Once you install the packages you need with install.packages() or pip install or conda install, they’re in your library – the data science equivalent of a pantry. That library is specific to a certain environment. Importantly, libraries can hold – at most – one version of any given package. Libraries can be specific to the project, user, or shared across the system.\nLoaded. Loading a package with a library or import command is like taking the food out of the pantry and putting it on the counter so you can actually cook with it.\n\nIn general, the library is the package environment that’s the most relevant because it’s the part you, as a data scientist, have control over.\nLet’s say you work on one project for a while, installing packages from the repository into your library. Now let’s say you come back after a year of working on other projects or try to share your project with someone else.\nIt’s probable that future you or your colleague you won’t have the right versions and your code will break.\nWhat would’ve been better is if you’d had an environments as code strategy that created a portable environment for each project on your system.\nA successful package Environment as Code setup has two key attributes:\n\nYour package environment is isolated and cannot be disrupted by other activities on the same machine.\nYour package environment can easily be captured and transported elsewhere.\n\nBoth R and Python have great environments as code utilities that make it easy to do both these things.\n\n\n\n\n\n\nR vs Python\n\n\n\nOn a high level, the workflows for these tasks are similar between R and Python. However, it’s worth noting that Python is used all the time by your system, where R is basically only ever installed for data science purposes.\nThat means that most tutorials on using Python start with installing a standalone data science version of Python using a virtual environment or Conda, so you may already be used to using a virtual environment.\nHopefully this chapter will help fill out your mental model of what’s going on.\n\n\nIn R, {renv} is the standard on this front. In Python, there are many different options. In the context of production data science, I recommend {virtualenv}/{venv} and related tools."
  },
  {
    "objectID": "chapters/sec1/1-1-env-as-code.html#using-a-virtual-environment",
    "href": "chapters/sec1/1-1-env-as-code.html#using-a-virtual-environment",
    "title": "1  Environments as Code",
    "section": "1.3 Using a virtual environment",
    "text": "1.3 Using a virtual environment\nUsing a virtual environment tool to create an environment as code is a three-step process. This section will teach you about the process of creating and using a virtual environment – there’s a cheatsheet on the commands for both R and Python at the end of the chapter.\nStep 1: Create standalone package libraries\nEach project should have it’s own {renv}/{venv} library. When you start your project, it should be in a standalone directory that includes everything the project needs – including a virtual environment.\nThis is called a project-oriented workflow. You can do it in either R or Python. The What They Forgot to Teach You About R course (materials available online at rstats.wtf) is a great intro to a project-oriented workflow whether you work in R or Python. The tooling will be somewhat different in Python, but the idea is the same.\n\n\n\n\n\n\nNote\n\n\n\nSome projects include multiple content items – like a project that like an app project backed up by an API and an ETL script. My recommendation is to create one virtual environment for each piece of content.\n\n\nThen, every time you work on the project, you activate the virtual environment and install/activate packages in there.\nOne worry that can arise is that it could take up a lot of space to reinstall these packages over and over again for every project on your system. Don’t worry – {renv} and {venv} are a little cleverer than this. If you’re using the same package multiple times, it does some work behind the scenes using symbolic links to make sure that the package is actually only installed once.\nStep 2: Document environment state\nAs you work inside your virtual environment, you’ll want to document the state of the package library. Both {renv} and {venv} have standard file formats for documenting the packages that are installed in the environment as well as the versions.\nIn {renv}, the file is called a lockfile and it’s a requirements.txt in {venv}.\nSince all this work is occurring in a standalone package environment, you don’t have to worry about what’ll happen if you come back after a break. You’ll still have those same packages to use.\nStep 3: Collaborate or deploy\nWhen you go to share your project, you don’t want to share your actual package libraries. The actual package libraries can be large, so putting just a short lockfile or requirements file into git is definitely preferred. Additionally, package installs are specific to the operating system and the language version you’re using.\nSo if you’re working on a Mac with a particular set of R and/or Python versions and you collaborate or deploy to a Windows or Linux machine, you can’t share the actual package files. Those machines will need to install the required set of packages for themselves.\nSo you just share your lockfile or requirements.txt and your collaborator or deployment process consults that file and installs all the required packages from there."
  },
  {
    "objectID": "chapters/sec1/1-1-env-as-code.html#whats-happening-under-the-hood",
    "href": "chapters/sec1/1-1-env-as-code.html#whats-happening-under-the-hood",
    "title": "1  Environments as Code",
    "section": "1.4 What’s happening under the hood",
    "text": "1.4 What’s happening under the hood\nNow that you understand the basic process of using a virtual environment tool, it might be helpful to understand what’s actually going on.\nWhen you install a package, it installs into a package cache. This is just a directory full of installed packages. When you install a package, it’s installed into that directory. When you activate a library with library or import, it searches that directory and loads the library into the active session.\nIn R, the package cache(s) to be consulted are contained in .libPaths(). In Python it’s sys.path.\nThe key to virtual environments is that they monkey around with the package cache list, so both installation and loading happens from a cache that’s specific to the virtual environment.\nHere’s an example. If I run .libPaths() before and after activating an {renv} environment, the first entry from the .libPaths() call changes from a user-level library /Users/alexkgold to a project-level library /Users/alexkgold/Documents/do4ds/.\n&gt; .libPaths()\n[1] \"/Users/alexkgold/Library/R/x86_64/4.2/library\"                 \n[2] \"/Library/Frameworks/R.framework/Versions/4.2/Resources/library\"\nrenv::activate()\n* Project '~/Documents/do4ds/docker/docker/plumber' loaded. [renv 0.15.5]\n.libPaths()\n[1] \"/Users/alexkgold/Documents/do4ds/docker/docker/plumber/renv/library/R-4.2/x86_64-apple-darwin17.0\"\n[2] \"/Library/Frameworks/R.framework/Versions/4.2/Resources/library\"  \nSimilarly, in Python it looks like this. Note that the “after” version replaces the last line of the sys.path with a project-level library:\n&gt; python3 -m site                                       \nsys.path = [\n    '/Users/alexkgold/Documents/python-examples/dash-app',\n    '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python38.zip',\n    '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8',\n    '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/lib-dynload',\n    '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/site-packages',\n]\nUSER_BASE: '/Users/alexkgold/Library/Python/3.8' (doesn't exist)\nUSER_SITE: '/Users/alexkgold/Library/Python/3.8/lib/python/site-packages' (doesn't exist)\nENABLE_USER_SITE: True\n\n&gt; source .venv/bin/activate                       \n(.venv)\n\n&gt; python3 -m site\nsys.path = [\n    '/Users/alexkgold/Documents/python-examples/dash-app',\n    '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python38.zip',\n    '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8',\n    '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/lib-dynload',\n    '/Users/alexkgold/Documents/python-examples/dash-app/.venv/lib/python3.8/site-packages',\n]\nUSER_BASE: '/Users/alexkgold/Library/Python/3.8' (doesn't exist)\nUSER_SITE: '/Users/alexkgold/Library/Python/3.8/lib/python/site-packages' (doesn't exist)\nENABLE_USER_SITE: False\n(.venv)\nIf you’re on a shared server, you may want to share a package cache across users. This generally isn’t necessary, but can save some space on the server. Both {renv} and venv include settings to allow you to relocate the package cache to a shared location on the server. You’ll need to make sure that all the relevant users have read and write privileges to this location.\n\n\n\n\n\n\nWhy I’m not talking about Conda\n\n\n\nMany data scientists love Conda for managing their Python environments.\nConda is allows you to create a data science environment on your local laptop. It’s especially useful when your machine is locked down and you don’t have root access because it does all of its installation in user space.\nThat’s super useful for working on your laptop, but it’s not a great fit for a production environment. Conda smashes together the language version, the package management, and (sometimes) the system library management. This has the benefit of being conceptually simple and easy-to-use. But I’ve often seen it go awry in production environments, and I generally recommend people use a tool that’s just for package management, like {venv}, as opposed to an all-in-one tool like Conda."
  },
  {
    "objectID": "chapters/sec1/1-1-env-as-code.html#reproducing-the-rest-of-the-stack",
    "href": "chapters/sec1/1-1-env-as-code.html#reproducing-the-rest-of-the-stack",
    "title": "1  Environments as Code",
    "section": "1.5 Reproducing the rest of the stack",
    "text": "1.5 Reproducing the rest of the stack\nAs a data scientist, your responsibility in a production environment ends at responsibly creating and using virtual environments for your packages. That said, there may not be an IT/Admin to take care of the rest for your team.\nLet’s talk a little about how to reproduce the rest of the stack.\nManaging R and Python versions has gotten easier over the years. There are great tools like {rig} in R and {pyenv} in Python that makes it easy to manage the versions of R and Python that are available on a system.\nMany R and Python libraries are wrappers for system libraries, often written in C++ for speed. For example, many popular packages for geospatial analysis are just thin language wrappers that call out to the system libraries. In this case, it might be important to be able to maintain a particular version of the underlying system library to ensure that your code runs at all in the future.\nThere are many tools you can use to record and reproduce the R and Python versions you’re using, the system libraries, and the operating system itself.\nThese days, the clear leader of the pack on this front is Docker. Since its introduction in 2013, it has become an increasingly popular way to create, maintain, and use standalone environments – and for good reason!\n\n\n\n\n\nIn most cases, the only requirement to run any Docker container is having Docker installed. If you put your project in a Docker container and it runs today, you can be reasonably confident that the container itself can run almost anywhere else, irrespective of what else is running on that machine.2\nIn Chapter 9, you’ll learn the basics of how to use Docker as a tool for reproducing a data science project and moving it around."
  },
  {
    "objectID": "chapters/sec1/1-1-env-as-code.html#comprehension-questions",
    "href": "chapters/sec1/1-1-env-as-code.html#comprehension-questions",
    "title": "1  Environments as Code",
    "section": "1.6 Comprehension Questions",
    "text": "1.6 Comprehension Questions\n\nWhy does difficulty increase as the level of required reproducibility increase for a data science project. In your day-to-day work, what’s the hardest reproducibility challenge?\nDraw a mental map of the relationships between the 7 levels of the reproducibility stack. Pay particular attention to why the higher layers depend on the lower ones.\nWhat are the two key attributes of environments as code? Why do you need both of them? Are there cases where you might only care about one?\nDraw a mental map of the relationships between the following: package repository, package library, package, project-level-library, .libPaths() (R) or sys.path(python), lockfile\nWhy is it a bad idea to share package libraries? What’s the best way to collaborate with a colleague using an environment as code? What are the commands you’ll run in R or Python to save a package environment and restore it later?"
  },
  {
    "objectID": "chapters/sec1/1-1-env-as-code.html#lab1",
    "href": "chapters/sec1/1-1-env-as-code.html#lab1",
    "title": "1  Environments as Code",
    "section": "1.7 Lab 1: Create a website with virtual environments",
    "text": "1.7 Lab 1: Create a website with virtual environments\nIn this lab, we’re going to start working on our penguin explorer website. We’re going to create a simple website using Quarto, which is an open source scientific and technical publishing system that makes it easy to render R and Python code into beautiful documents, websites, reports, and presentations.\nIn this lab, we’re going to create pages for a simple exploratory data analysis and model building from the Palmer Penguins dataset. In order to get to practice with both R and Python, I’m going to do the EDA page in R and the modeling in Python.\nYou can follow the instructions on the Quarto site to start a new Quarto project in your editor of choice. You can check it out locally with quarto preview.\nAs we add each of the pages below, don’t forget to add them to your _quarto.yml so quarto knows to render them.\n\n1.7.1 EDA in R\nLet’s add a simple R-language EDA of the Palmer Penguins data set to our website by adding a file called eda.qmd in the root directory of your project.\nBefore you start adding code, create and activate an {renv} environment with renv::init().\nNow, go ahead and do your analysis. Here’s the contents of my eda.qmd.\n\n\neda.qmd\n\n---\ntitle: \"Penguins EDA\"\nformat:\n  html:\n    code-fold: true\n---\n\n## Penguin Size and Mass by Sex and Species\n\n```{r}\nlibrary(palmerpenguins)\nlibrary(dplyr)\nlibrary(ggplot2)\n\ndf &lt;- palmerpenguins::penguins\n```\n\n```{r}\ndf %&gt;%\n  group_by(species, sex) %&gt;%\n  summarise(\n    across(\n      where(is.numeric), \n      \\(x) mean(x, na.rm = TRUE)\n      )\n    ) %&gt;%\n  knitr::kable()\n```\n\n## Penguin Size vs Mass by Species\n\n```{r}\ndf %&gt;%\n  ggplot(aes(x = bill_length_mm, y = body_mass_g, color = species)) +\n  geom_point() + \n  geom_smooth(method = \"lm\")\n```\n\nFeel free to copy this Quarto doc right into your website or to write your own.\nOnce you’ve finished writing your EDA script and checked that it renders nicely into the website, save the doc, and create your lockfile with renv::snapshot().\n\n\n1.7.2 Modeling in Python\nNow let’s build a {scikit-learn} model for predicting penguin weight based on bill length in a Python notebook by adding a model.qmd to the root of our project.\nAgain, you’ll want to create your virtual environment and activate it before you start pip install-ing packages into the environment. Check the cheatsheet if you need help with the specific commands.\nHere’s what’s in my model.qmd, but you should feel free to include whatever you want.\n\n\nmodel.qmd\n\n---\ntitle: \"Model\"\nformat:\n  html:\n    code-fold: true\n---\n\n```{python}\nfrom palmerpenguins import penguins\nfrom pandas import get_dummies\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import preprocessing\n```\n\n## Get Data\n\n```{python}\ndf = penguins.load_penguins().dropna()\n\ndf.head(3)\n```\n\n## Define Model and Fit\n\n```{python}\nX = get_dummies(df[['bill_length_mm', 'species', 'sex']], drop_first = True)\ny = df['body_mass_g']\n\nmodel = LinearRegression().fit(X, y)\n```\n\n## Get some information\n\n```{python}\nprint(f\"R^2 {model.score(X,y)}\")\nprint(f\"Intercept {model.intercept_}\")\nprint(f\"Columns {X.columns}\")\nprint(f\"Coefficients {model.coef_}\")\n```\n\nOnce you’re happy with how the page is working, capture your dependencies in a requirements.txt using pip freeze &gt; requirements.txt on the command line."
  },
  {
    "objectID": "chapters/sec1/1-1-env-as-code.html#cheat-renv-venv",
    "href": "chapters/sec1/1-1-env-as-code.html#cheat-renv-venv",
    "title": "1  Environments as Code",
    "section": "1.8 Cheatsheet: Environments as Code",
    "text": "1.8 Cheatsheet: Environments as Code\n\n1.8.1 Checking library + repository status\n\n\n\n\n\n\n\n\nStep\nR Command\nPython Command\n\n\n\n\nCheck whether library in sync with lockfile.\nre nv::status()\nNone\n\n\n\n\n\n1.8.2 Creating and Using a Standalone Project Library\nMake sure you’re in a standalone project library.\n\n\n\n\n\n\n\n\nStep\nR Command\nPython Command\n\n\n\n\nMake sure you’ve got {r  env}/{venv}.\nin s t a l l .packages(\"renv\")\nIncluded w/ Python 3.5+\n\n\nCreate a standalone library.\nrenv::init()\np y t hon -m venv &lt;dir&gt;\nConvention: u s e .venv for &lt;dir&gt;\n\n\nActivate project library.\nrenv::activate()\nHappens automatically if using projects.\nsou r c e &lt; d ir&gt; /bin/activate\n\n\nInstall packages as normal.\nins t a l l . packages(\"&lt;pkg&gt;\")\npyth o n - m pip install &lt;pkg&gt;\n\n\nSnapshot package state.\nrenv::snapshot()\npip fr e e z e &gt;  requirements.txt\n\n\nExit project environment.\nLeave R project.\ndeactivate\n\n\n\n\n\n1.8.3 Collaborating on someone else’s project\nStart by downloading the project into a directory on your machine.\n\n\n\n\n\n\n\n\nStep\nR Command\nPython Command\n\n\n\n\nMove into project directory.\nse t w d (\"&lt;project-dir&gt;\")\nOr just open R project in RStudio.\ncd &lt;project-dir&gt;\n\n\nCreate project environment.\nrenv::init()\np y t hon -m venv &lt;dir&gt;\nRecommend: u s e .venv for &lt;dir&gt;\n\n\nEnter project environment.\nHappens automatically or renv::activate()\ns o u rce &lt; d ir&gt; /bin/activate\n\n\nRestore packages.\nHappens automatically or renv::restore()\np ip i n s t all - r  requirements.txt"
  },
  {
    "objectID": "chapters/sec1/1-1-env-as-code.html#footnotes",
    "href": "chapters/sec1/1-1-env-as-code.html#footnotes",
    "title": "1  Environments as Code",
    "section": "",
    "text": "Credit for this analogy goes to my former colleague Sean Lopp – a fount of excellent analogies and overall wonderful co-worker.↩︎\nUsually. Users of Apple’s M1 and M2 chips have run into many issues running containers due to differences in hardware architecture.↩︎"
  },
  {
    "objectID": "chapters/sec1/1-2-proj-arch.html#choose-the-right-type-of-presentation-layer",
    "href": "chapters/sec1/1-2-proj-arch.html#choose-the-right-type-of-presentation-layer",
    "title": "2  Data Project Architecture Guidelines",
    "section": "2.1 Choose the right type of presentation layer",
    "text": "2.1 Choose the right type of presentation layer\nThe presentation layer is the actual thing that will be consumed by your users. A lot of the data flows for your project will be dictated by your presentation layer, so you need to start by figuring out the details of your presentation layer.\nBasically all data science projects fall into one of four categories.\nThe first category is a job. A job matters because it changes something in another system. It might move data around, build a model, or produce plots, graphs, or numbers to be used in a Microsoft Office report.\nFrequently, jobs are written in a SQL-based pipelining tool (dbt has been quickly rising in popularity) or in a .R or .py script.1\nThe second type of data science software is an interactive app. These apps are created in frameworks like Shiny (R or Python), Dash (Python), or Streamlit (Python). In contrast to general purpose web apps, which are for all sorts of purposes, data science web apps are usually used to give non-coders a way to explore data sets and see data insights.\nThe third type is a report. Reports are code you’re turning into an output you care about – like a paper, book, presentation, or website. Reports are the result of rendering an R Markdown doc, Quarto doc, or Jupyter Notebook for people to consume on their computer, in print, or in a presentation. These docs may be completely static (this book is a Quarto doc) or they may have some interactive elements.\n\n\n\n\n\n\nNote\n\n\n\nExactly how much interactivity turns a report into an app is completely subjective. I generally think the distinction is whether there’s a running R or Python process in the background, but it’s not a particularly sharp line.\n\n\nThe fourth type is an API (application programming interface) for machine-to-machine communication. In the general purpose software world, APIs are the backbone of how two distinct pieces of software communicate. In the data science world, APIs are most often used to provide data feeds and on-demand predictions from machine learning models.\nChoosing the right type of presentation layer will make it much easier to design the rest of your project. Here are some guidelines on how to choose a presentation layer.\nIf the results of your software are for machine-to-machine use, you’re thinking about a job or API. It’s a job if it should run in a batched way (i.e. you write a data file or results into a database) and it’s an an API if you want results as queried in real time.\nIf your project is for humans to consume, you’re thinking about creating an app or report, depending on whether you need a live Python or R process on the back-end.\nHere’s a little flow chart for how I think about which of the four things you should build.\n\n\n\n\nflowchart TD\n    A{For human\\n consumption?}\n    B{Static?} \n    C{Lots of data\\n on backend?}\n    D{Should run\\batched?}\n\n    E[\"Report\\n(Static)\"]\n    F[App] \n    G[\"Report\\n(Interactive)\"]\n    H[API]\n    J[Job]\n\n\n    A --&gt;|Yes| B\n    A --&gt;|No| D\n    B --&gt;|Yes| E\n    B --&gt;|No| C\n    C --&gt;|Yes| F\n    C --&gt;|No| G\n    D --&gt;|Yes| H\n    D --&gt;|No| J"
  },
  {
    "objectID": "chapters/sec1/1-2-proj-arch.html#do-less-in-the-presentation-layer",
    "href": "chapters/sec1/1-2-proj-arch.html#do-less-in-the-presentation-layer",
    "title": "2  Data Project Architecture Guidelines",
    "section": "2.2 Do less in the presentation layer",
    "text": "2.2 Do less in the presentation layer\nAs a general rule, data scientists don’t do a great job separating out their presentation layers. It’s not uncommon for me to see apps or reports that are thousands of lines of code, with button definitions, UI bits, and user interaction definitions mixed in among the actual work of the app.\nWith presentation and processing layers that are smushed together, it’s really hard to read your code later or to test or log what’s happening inside your app.\nThe best way to separate the presentation layer is to check if you’ve got anything in your presentation layer that does anything beyond\n\nshowing things to the user\ncollecting interactions from the user\n\nCreating the things that are shown to the user or doing anything with the interactions shouldn’t be in the presentation layer. These should be deferred to the processing layer.\nOnce you’ve identified those things, they should be extracted into functions that are documented and tested – preferably in a package – and use those functions put into standalone scripts.\n\n\n\n\n\n\nTip\n\n\n\nMoving things out of the presentation layer is especially important if you’re writing a Shiny app. You really want to use the presentation layer to do reactive things and move all non-reactive interactions into the processing layer."
  },
  {
    "objectID": "chapters/sec1/1-2-proj-arch.html#aim-for-small-data-in-the-presentation-layer",
    "href": "chapters/sec1/1-2-proj-arch.html#aim-for-small-data-in-the-presentation-layer",
    "title": "2  Data Project Architecture Guidelines",
    "section": "2.3 Aim for small data in the presentation layer",
    "text": "2.3 Aim for small data in the presentation layer\nEverything is easy when your data is small because you can simply load it into your Python or R session as your code starts and never think about it again.\n“Real engineers” may scoff at this pattern, but don’t let their criticism dissuade you. If your data size is small and your project performance is good enough, just read in all of your data and operate on it live. Don’t over-complicate things. These days, this pattern often works well into the range of millions of rows.\nIt may be the case that your data isn’t small – but not all large data is created equal.\nTruly big data can’t fit into the memory on your computer all at once. Data that is actually big is pretty rare for most data science purposes.\nIt’s much more common to encounter medium data. You can load it into memory so it’s not actually big, but it’s big enough that loading it all makes your project’s performance too slow.\nDealing with medium or big data requires being somewhat clever and adopting a design pattern for big data. But being clever is hard.\nSo before you go being clever, it’s worth slowing down and asking yourself a few questions that might let you treat your data as small.\n\n2.3.1 Can I add pre-calculation or use a different project type?\nIf your data is truly big, it’s big. You could always get beefier hardware, but there are limits. But if your data is medium-sized, the thing keeping it from being small isn’t some esoteric hardware issue, its performance.\nAn app requires high performance. Someone is staring at their screen through a 90 second wait is going to think your project stinks.\nBut if you can pre-calculate a lookup table of values – or turn your app into a report that gets re-rendered on a schedule you can turn turn medium or even truly big data into a small data set in the presentation layer.\nThe degree to which you can do this depends a lot on the requirements of your presentation layer.\nTalking to your users and figuring out what cuts of the data they really care about can help you determine whether pre-calculation is feasible or whether you really need to load all the data into the presentation layer.\n\n\n2.3.2 Can I reduce data granularity?\nIf you can pre-calculate results and you’re still hitting performance issues, it’s always worth asking if your data can get smaller.\nLet’s think about a specific project to make this a little clearer.\nSuppose you work for a large retailer and are responsible for creating a dashboard of weekly sales. Your input data is a dataset of every item sold at every store going back for years. Clearly this isn’t naturally small data.\nAs you’re thinking about how to make the presentation layer data smaller, it’s worth keeping in mind that each additional dimension you allow users to cut the data multiplies the amount of data you need in the presentation layer.\nFor example, weekly sales at the department level, requires a lookup table as big as \\(\\text{number of weeks} * \\text{number of stores} * \\text{number of departments}\\). Even with a lot of stores and a lot of departments, you’re probably still squarely in the small data category.\nBut if you have to switch to a daily view, you multiply the amount of data you need by 7. If you break it out across 12 products, your data has to get 12 times bigger. And if you do both, it gets 84 times bigger. It’s not long before you’re back to a big data problem.\nTalking with your users about the tradeoffs between app performance and the number of data dimensions they need can identify opportunities to exclude dimensions and reduce your data size."
  },
  {
    "objectID": "chapters/sec1/1-2-proj-arch.html#big-data-patterns",
    "href": "chapters/sec1/1-2-proj-arch.html#big-data-patterns",
    "title": "2  Data Project Architecture Guidelines",
    "section": "2.4 Adopt a pattern to make big data small",
    "text": "2.4 Adopt a pattern to make big data small\nLet’s say you’ve made your presentation layer as small as possible or you’re trying to do your pre-calculation step to go from big data to small data. You need to figure out how to make your large data smaller.\nThe key insight is that you don’t want to pull all of the data into your Python or R session. Instead, you want to pull in only some of the data.\nHere are a few patterns to consider to make your data smaller. This isn’t an exhaustive list and each of these patterns will only work for some projects, but many can adopt one or more of these patterns.\n\n2.4.1 Be lazy with data pulls\nUp until now, we’ve been assuming that your project pulls in all of the data up front in an eager data pattern. This is often a good first cut at writing an app, as it’s much simpler than doing anything else.\nIf that won’t work for your project, you can try being lazy with your data pulls. In a lazy data pattern, you pull in only the data that’s needed when it’s needed.\nIf your project doesn’t always need all the data – especially if the data it needs depends on what the user does inside a session, it might be worthwhile to pull only exactly the data you need once the user interactions clarify what you need.\n\n\n2.4.2 Sample the data\nFor many tasks, especially machine learning ones, it may be adequate to work on only a sample of the data. In some cases like classification of highly imbalanced classes, it may actually work better to work on a sample of the data rather than the whole data set.\nSampling tends to work well when you’re trying to compute statistical attributes of your datasets. Computing averages or rates and creating machine learning models works just fine on samples of your data. Just be careful to be unbiased with your sampling and consider sampling stratification to make sure one weird sample doesn’t mess with your results.\nBut sampling doesn’t work well on counting tasks – it’s hard to count when you don’t have all the data!\n\n\n2.4.3 Chunk and pull\nIn some cases, there may be natural groups in your data. For example, in our retail dashboard example, it may be the case that we want to compute something by time frame or store or product. In this case, you could pull just that chunk of the data, compute what you need and move on to the next one.\nChunking works well for all kinds of tasks including building machine learning models and creating plots as long as the groups are cleanly separable. When they are, this is an example of an embarrassingly parallel task, which you can easily parallelize in Python or R.\nIf you don’t have distinct chunks in your data, it’s pretty hard to chunk the data.\n\n\n2.4.4 Push work to the data source\nIn most cases, actually transmitting the data from the data source to your project is the most costly step in terms of time. So basically anything you can do before you pull the data out should be done before you pull the data out.\nLet’s say you really have to provide a very high degree of granularity in your weekly sales dashboard. You can at least do any computations them in the data source and just pull the results back, as opposed to loading all the data and doing the computations in Python or R. More on how to do this in Chapter 3.\nThis tends to work quite well when you’re creating simple summary statistics and when your database is reasonably fast. If your data source is slow, or if you’re doing complicated machine learning tasks, you may not be able to push that work off to the data source."
  },
  {
    "objectID": "chapters/sec1/1-2-proj-arch.html#store-intermediate-artifacts-in-the-right-format",
    "href": "chapters/sec1/1-2-proj-arch.html#store-intermediate-artifacts-in-the-right-format",
    "title": "2  Data Project Architecture Guidelines",
    "section": "2.5 Store intermediate artifacts in the right format",
    "text": "2.5 Store intermediate artifacts in the right format\nAs you start breaking your processing layer into the different pieces, you’ll find that you have intermediate artifacts to pass the data from one stage to the next.\nIf all you’re producing is rectangular data frames (or vectors) and you have write access to a database, that’s what you should use.\nBut very often you don’t have write access to a database or you’ve got other sorts of artifacts that you need to save between steps and can’t go into a database, like machine learning models or rendered plots. In that case, you’ll need to choose how to store your data.\nFlat files are data files that can be moved around just like any other file on your computer. You can put them on your computer, and share them through tools like dropbox, google drive, scp, or more.\nThe most common is a comma separated value (csv) file, which is just a literal text file of the values in your data with commas as separators.2 You could open it in a text editor and read it if you wanted to.\nThe advantage of csvs is that they’re completely ubiquitous. Basically every programming language has some way to read in a csv file and work with it.\nOn the downside, csvs are completely uncompressed. That makes them quite large relative to other sorts of files and slow to read and write. Additionally, because csvs aren’t language-specific, complicated data types may not be preserved when saving to csv. For example, dates are often mangled going into a csv file and back.\nThey also can only hold rectangular data, so if you’re trying to save a machine learning model, a csv doesn’t make any sense.\nBoth R and Python have language-specific file types – pickle in Python and rds in R. These are nice because they include some amount of compression and preserve data types when you save a data frame. They also can hold non-rectangular data, which can be great if you want to save a machine learning model.\nIf you don’t have a database but are storing rectangular data, you should strongly consider using DuckDB. Its an in-memory database that’s great for analytics use cases. In contrast to a standard database that runs its own live process, there’s no overhead for setting up DuckDB. You just run it against flat files on disk (usually Parquet files), which you can move around like any other. And unlike a csv, pickle, or rds file, a DuckDB is query-able, so you only load the data you need into memory.\nIt’s hard to stress how cool DuckDB is. Data sets that were big just a few years ago are now medium or even small."
  },
  {
    "objectID": "chapters/sec1/1-2-proj-arch.html#choose-where-based-on-update-frequency",
    "href": "chapters/sec1/1-2-proj-arch.html#choose-where-based-on-update-frequency",
    "title": "2  Data Project Architecture Guidelines",
    "section": "2.6 Choose where based on update frequency",
    "text": "2.6 Choose where based on update frequency\nLet’s say you’ve done your data pre-calculation and have a data set you’re using for the presentation layer. You have to figure out where to keep it.\nWhere you store your data should be dictated by how often the data is updated.\nThe simplest answer is to put it in the presentation bundle, which is the code and assets that make up your presentation layer. For example, let’s say you’re building a simple Dash app, app.py.\nYou could create a project structure like this:\nmy-project/\n├─ app.py\n├─ data/\n│  ├─ my_data.csv\n│  ├─ my_model.pkl\nThis works well only if your data will be updated at the same cadence as the app or report itself. If your project is an annual report that will be rewritten when you update the data, this can work just great.\nBut if your data updates more frequently than your project code, you really want to put the data outside the project bundle.\nThere are a few ways you can do this. The most basic way is just to put the data on a location in your file system that isn’t inside the app bundle.\nBut when it comes to deployment, data on the file system can be complicated. If you’re writing your app and deploying it on the same server, then you can access the same directory. If not, you’ll need to worry about how to make sure that directory is also accessible on the server where you’re deploying your project.\nIf you’re not going to store the flat file on the filesystem and you’re in the cloud, the most common option for where it can go is in blob storage. Blob storage allows you to store and recall things by name.3 Each of the major cloud providers has blob storage – AWS’s has s3 (short for simple storage service), Azure has Azure Blob Store, and Google has Google Storage.\nThe nice thing about blob storage is that it can be accessed from anywhere that has access to the cloud. You can also control access using standard cloud identity management tooling, so you could control who has access using individual credentials or could just say that any request for a blob coming from a particular server would be valid.\nThere are packages in both R and Python for interacting with AWS that are very commonly used for getting access to s3 – {boto3} in Python, and {paws} in R.\nThere’s also the popular {pins} package in both R and Python that basically wraps using blob storage into neater code. It can use a variety of storage backends, including cloud blob storage, networked or cloud drives like Dropbox, Microsoft365 sites, and Posit Connect.\nIf you’re still early in your project lifecycle, a google sheet can be a great way to save and recall a flat file. I wouldn’t recommend a google sheet as a permanent home for data, but it can be a good intermediate step while you’re still figuring out what the right answer is for your pipeline.\nThe primary weakness of a google sheet – that it’s editable by someone who logs in – can also be an asset if that’s something you need."
  },
  {
    "objectID": "chapters/sec1/1-2-proj-arch.html#consider-auth-to-data-up-front",
    "href": "chapters/sec1/1-2-proj-arch.html#consider-auth-to-data-up-front",
    "title": "2  Data Project Architecture Guidelines",
    "section": "2.7 Consider auth to data up front",
    "text": "2.7 Consider auth to data up front\nIf everyone who views your project has the same permissions to see the data, life is easy. You can just allow the project access to the data and check for authorization to view the project.\nBut if you need to provide different data access to different users, you’re much more constrained. First off, you probably need to use an app rather than a report so that you can respond to which user is accessing the app.\nThen you have to figure out how you’re actually going to change data access based on who’s viewing the app.\nSometimes this can be accomplished in the app itself. Many app frameworks pass the username or user groups into the session, and you can write code that changes app behavior based on the user. For example, you can gate access to certain tabs or features of your app based on the user.\nSometimes you’ll actually have to pass database credentials along to the database. If this is the case for you, you’ll need to figure out how to establish the user’s database credentials, how to make sure those credentials stay only in the user’s session, and how those credentials get passed along to the database. This is most commonly done with technologies like Kerberos or OAuth and require coordination with an IT/Admin. More on this topic in Chapter 18."
  },
  {
    "objectID": "chapters/sec1/1-2-proj-arch.html#create-an-api-if-you-need-it",
    "href": "chapters/sec1/1-2-proj-arch.html#create-an-api-if-you-need-it",
    "title": "2  Data Project Architecture Guidelines",
    "section": "2.8 Create an API if you need it",
    "text": "2.8 Create an API if you need it\nIn the case of a general purpose three-layer app, it is almost always the case that the middle tier will be an application programming interface (API). In a data science app, separating processing logic into functions is often sufficient. But if you’ve got a long-running bit of business logic, like training an ML model, it’s often helpful to separate it into an API.\n\n\n\n\n\n\nNote\n\n\n\nYou may have heard the term REST API or REST-ful.\nREST is a set of architectural standards for how to build an API. An API that conforms to those standards is called REST-ful or a REST API.\nIf you’re using standard methods for constructing an API like R’s {plumber} package or {FastAPI} in Python, they’re going to be REST-ful – or at least close enough for standard usage.\n\n\nYou can basically think of an API as a “function as a service”. That is, an API is just one or more functions, but instead of being called within the same process that your app is running or your report is processing, it will run in a completely separate process.\nFor example, let’s say you’ve got an app that allows users to feed in input data and then generate a model based on that data. If you generate the model inside the app, the user will have the experience of pressing the button to generate the model and having the app seize up on them while they’re waiting. Moreover, other users of the app will find themselves affected by this behavior.\nIf, instead, the button in the app ships the long-running process to a separate API, it gives you the ability to think about scaling out the presentation layer separate from the business layer.\nLuckily, if you’ve written functions for your app, turning them into an API is trivial as packages like {fastAPI} and {plumber} let you turn a function into an API with just the addition of some specially-formatted comments."
  },
  {
    "objectID": "chapters/sec1/1-2-proj-arch.html#write-a-data-flow-chart",
    "href": "chapters/sec1/1-2-proj-arch.html#write-a-data-flow-chart",
    "title": "2  Data Project Architecture Guidelines",
    "section": "2.9 Write a data flow chart",
    "text": "2.9 Write a data flow chart\nOnce you’ve figured out the project architecture you need, it can be helpful to write a data flow chart.\nA data flow chart maps the different project components you’ve got into the three parts of the project and documents all the intermediate artifacts you’re creating along the way.\nOnce you’ve mapped your project, figuring out where the data should live and in what format will be much simpler.\nFor example, here’s a very simple data flow chart for the labs in this book. You may want to annotate your data flow charts with other attributes like data types, update frequencies, and where data objects live.\n\n\n\n\nflowchart LR\n    A[Palmer Penguins \\nData Package]\n    B[Model Creation Job] \n    C[Model Serving API]\n    D[Model Explorerer App]\n    E[EDA Report]\n    F[Model Creation Report]\n\n    subgraph Data\n        A\n    end\n\n    subgraph Processing\n        B --&gt;|Model| C\n    end\n\n    subgraph Presentation\n        D\n        E\n        F\n    end\n\n    A --&gt; B\n    B --&gt; F\n    A --&gt; E\n    C --&gt; D"
  },
  {
    "objectID": "chapters/sec1/1-2-proj-arch.html#comprehension-questions",
    "href": "chapters/sec1/1-2-proj-arch.html#comprehension-questions",
    "title": "2  Data Project Architecture Guidelines",
    "section": "2.10 Comprehension Questions",
    "text": "2.10 Comprehension Questions\n\nWhat are the layers of a three-layer application architecture? What libraries could you use to implement a three-layer architecture in R or Python?\nWhat are some questions you should explore to reduce the data requirements for your project?\nWhat are some patterns you can use to make big data smaller?\nWhere can you put intermediate artifacts in a data science project?\nWhat does it mean to “take data out of the bundle”?"
  },
  {
    "objectID": "chapters/sec1/1-2-proj-arch.html#lab2",
    "href": "chapters/sec1/1-2-proj-arch.html#lab2",
    "title": "2  Data Project Architecture Guidelines",
    "section": "2.11 Lab 2: Build the processing layer",
    "text": "2.11 Lab 2: Build the processing layer\nIn the last chapter, we did some EDA of the Palmer Penguins data set and also built an ML model. In this lab, we’re going to take that work we did and turn it into the actual presentation layer for our project.\n\n2.11.1 Step 1: Write the model outside the bundle using {vetiver}\nWhen we originally wrote our model.qmd script in Chapter 1, we didn’t save the model at all.\nIt seems likely that our model will get updated more frequently than our app, so we don’t want to store it in the app bundle. Later on, I’ll show you how to store it in the cloud. For now, I’m just going to store it in a directory on my computer.\nSince\nTo do it, I’m going to use the {vetiver} package, which is an R and Python package to version, deploy, and monitor a machine learning model.\nWe can take our existing model, turn it into a {vetiver} model, and save it to the /data/model folder with\n\n\nmodel.qmd\n\n\n```{python}\nfrom vetiver import VetiverModel\nv = VetiverModel(model, model_name='penguin_model', prototype_data=X)\n```\n\n## Save to Board\n\nIf /data/model doesn’t exist on your machine, you can create it, or use a directory that does exist.\nWhatever path you use, I’d recommend using an absolute file path, rather than a relative one.\n\n\n2.11.2 Step 2: Create an API to serve model predictions\nI’m going to say that I need real-time predictions from my model in this case, so I’ll serve the model from an API.\nAs the point of this lab is to focus on the architecture, I’m just going to use the auto-generation capabilities of {vetiver}. If you’re interested in getting better at writing APIs in general, I encourage you to consult the documentation for {plumber} or {fastAPI}.\nIf you’ve closed your modeling code, you can get your model back from your pin with:\n\nb = pins.board_folder('data/model', allow_pickle_read=True)\nv = VetiverModel.from_pin(b, 'penguin_model')\n\nThen you can auto-generate a {fastAPI} from this model with\n\napp = VetiverAPI(v, check_prototype=True)\n\nYou can run this in your Python session with app.run(port = 8080). You can then access run your model API by navigating to http://localhost:8080 in your browser.\nYou can play around with the front end there, including trying the provided examples."
  },
  {
    "objectID": "chapters/sec1/1-2-proj-arch.html#footnotes",
    "href": "chapters/sec1/1-2-proj-arch.html#footnotes",
    "title": "2  Data Project Architecture Guidelines",
    "section": "",
    "text": "Though I’ll argue in Chapter 4 that you should always use a literate programming tool like Quarto, R Markdown, or Jupyter Notebook.↩︎\nThere are other delimitors you can use. Tab separated value files (tsv) are something you’ll see occasionally.↩︎\nThe term blob is great to describe the thing you’re saving in blob storage, but it’s actually an abbreviation for binary large object. I think that’s very clever.↩︎"
  },
  {
    "objectID": "chapters/sec1/1-3-data-access.html#accessing-and-using-databases",
    "href": "chapters/sec1/1-3-data-access.html#accessing-and-using-databases",
    "title": "3  Using databases and data APIs",
    "section": "3.1 Accessing and using databases",
    "text": "3.1 Accessing and using databases\nDatabases are defined by their query-able interface, usually through structured query language (SQL).\n\n\n\n\n\n\nNote\n\n\n\nThere are many, many different kinds of databases, and choosing the right one for your project is beyond the scope of this book. One recommendation: open source PostgreSQL (Postgres) is a great place to start for most general-purpose data science tasks.\n\n\nRegardless of which database you’re using, you’ll open a connection by creating a connection object at the outset of your code. You’ll then use this object to send SQL queries – either literal ones you’ve written, or the output of a package that generates SQL code, like {sqlalchemy} in Python and {dplyr} in R.\nFor example, in Python you might write the following to connect to a Postgres database:\n\nimport psychopg2\n\ncon = psycopg2.connect()\n\nIn R, it might look like this:\n\ncon &lt;- DBI::dbConnect(RPostgres::postgres())\n\nIn order to develop a mental model for working with databases, let’s reverse engineer this example code.\nPython or R both have standard connection APIs that define operations like connecting and disconnecting, sending queries, and retrieving results.\nIn Python, packages for individual databases like {psychopg2} directly implement the API, which is why the example above calls the connect() method of the {psychopg2} package.\nIn R, the API is split into two parts. The {DBI} package (short for database interface) implements the actual connections. It works with a database driver package, whtich is the first argument to DBI::dbConnect(). Packages that implement the {DBI} interface are called DBI-compliant.\n\n\n\n\n\n\nNote\n\n\n\nThere are Python packages that don’t implement the connections API, and there are non DBI-compliant database connector packages in R. These packages may work for you, but I’d recommend sticking with the standard route.\n\n\nIn a lot of cases, there will be a Python or R package that directly implements your database driver. For example, when you’re connecting to a Postgres database, there are Postgres-specific connectors – {psychopg2} in Python and {RPostgres} in R. For Spark, you’ve got {pyspark} and {sparklyr}.\nIf there’s a package specific to your database, it’s probably faster and may provide additional database-specific functionality than other options.\nIf there isn’t a database-specific package that directly implements the driver, you’ll need to use a generic system driver in concert with a Python or R package that can interface with system drivers.\nIn that case, the example code above might look like in Python\n\nimport pyodbc\n\ncon = pyodbc.connect(\"DSN=MY_DSN\")\n\nIn R, it might look like this:\n\ncon &lt;- DBI::dbConnect(odbc::odbc(), dsn = \"MY_DSN\")\n\nWhile performance sometimes isn’t as good for system drivers, the tradeoff is that IT/Admins can pre-configure details of the connection in a data source name (DSN). If one is pre configured for you, you don’t have to remember – or even learn – the database name, host, and port, even username and password if they’re shared. All you need in your code is the DSN name, which is MY_DSN in the example above.\nSystem drivers come in two main varieties Java Database Connectivity (JDBC) and Open Database Connectivity (ODBC).\nIn Python, {pyodbc} is the main package for using ODBC connections and {JayDeBeApi} for connecting using JDBC. In R, {odbc} is the best package for using system ODBC connections and {RJDBC} is the standard way to use JDBC.\n\n\n\n\n\n\nTip\n\n\n\nIf you’re using R and have the choice between JDBC and ODBC, I strongly recommend ODBC. JDBC requires an extra hop through Java and the {rJava} package, which is painful to configure.1"
  },
  {
    "objectID": "chapters/sec1/1-3-data-access.html#connecting-to-apis",
    "href": "chapters/sec1/1-3-data-access.html#connecting-to-apis",
    "title": "3  Using databases and data APIs",
    "section": "3.2 Connecting to APIs",
    "text": "3.2 Connecting to APIs\nSome data sources come in the form of an API (application programming interface).\nIn the data science world, APIs are most often used to provide data feeds and on-demand predictions from machine learning models.\nIt’s common to have Python or R packages that wrap APIs, so you just write Python or R code without needing to think about the API underneath. The usage of these patterns often looks similar to databases – you create and use a connection object that stores the connection details. If your API has a package like this, you should just use it.\nIf you’re consuming a private API at your organization, a helper package probably doesn’t exist or you may have to write it yourself.\n\n\n\n\n\n\nNote\n\n\n\nThere’s increasingly good tooling to auto-generate packages based on API documentation, so you may never have to write an API wrapper package by hand. It’s still helpful to understand how it works.\n\n\nIf find yourself having to call an API directly, you can use the {requests} package in Python or {httr2} in R.\nThese packages idiomatic R and Python ways to call APIs. It’s worth understanding that they’re purely syntactic sugar. There’s nothing special about calling an API from inside Python or R versus using the curl command on the command line and you can go back and forth as you please.\n\n3.2.1 What’s in an API?\nAPIs are the standard way for two computer systems to communicate with each other. It’s an extremely general term that describes the definition of machine-to-machine communication.\nThe APIs used by data scientists are usually http-based REST-ful APIs. What exactly that means isn’t really important, but the rest of this section just addresses the subset of the wild world of APIs you’re likely to encounter as a data scientist.\nhttp operates on a request-response model. So when you use an API, you send a request to the API and it sends a response back.\nThe best way to learn about a new API is to read the documentation, which will include a lot of details about how to use it. Let’s go through some of the most salient ones.\n\n\n3.2.2 API Endpoints and Paths\nEach request to an API is directed to a specific endpoint. An API can have many endpoints, each of which you can think of like a function in a package. Each endpoint lives on a path, which is where you find that particular endpoint.\nFor example, if you did the lab in Chapter 2 and used {vetiver} to create an API for serving the penguin mass model, you found your API at http://localhost:8080. By default, you went to the root path at / and found the API documentation there.\nAs you scrolled the documentation, there were two endpoints – /ping and /predict. Those paths are relative to the root, so you could access /ping at http://localhost:8080/ping.\n\n\n3.2.3 HTTP verbs\nWhen you make a request over HTTP, you are asking a server to do something. The http verb, also known as the request method, describes the type of operation you’re asking for. Each endpoint has one or more verbs that it knows how to use.\nIf you look at the penguin mass API, you’ll see that /ping is a GET endpoint and /predict is a POST. This isn’t coincidence. I’d approximate that 95% of the API endpoints you’ll use as a data scientist are GET and POST, which respectively fetch information from the server and provide information to the server.\nTo round out the basic http verbs you might see, PUT and PATCH change or update something and DELETE (you guessed it) deletes something. There are more esoteric ones you’ll probably never see.\n\n\n3.2.4 Request parameters and bodies\nLike a function in a package, each endpoint accepts specific arguments in a required format. Again, like a function, some arguments may be optional and some may be required.\nFor GET requests, the arguments are specified via query parameters that end up embedded in the URL after a ?. So if you ever see a URL in your browser that looks like ?first_name=alex&last_name=gold, those are query parameters.\nFor POST, PUT, and PATCH requests, arguments are provided in a body, which is usually formatted as JSON.2 Both {httr2} and {requests} have built-in functionality for converting standard Python and R data types to their JSON equivalents, but it can sometimes take some experimentation to figure out exactly how to match the argument format. Experimenting with conversions using {json} in Python and {jsonlite} in R can be very useful.\n\n\n3.2.5 (Auth) Headers\nYou will need to figure out how to authenticate to the API. The most common forms of authentication are a username and password combination, an API key, or an OAuth token.\nAPI keys and OAuth tokens are often associated with particular scopes. Scopes are permissions to do particular things. For example, an API key might be scoped to have GET access to a given endpoint, but not POST access.\nRegardless of your authentication type, it will be provided in a header to your API call. Your API documentation will tell you how to provide your username and password, API key, or token to the API in a header. Both {requests} and {httr2} provide easy helpers for adding authentication headers and also general ways to set headers if you need to.\nAside from authentication, headers are also used for a variety of different metadata like the type of machine that is sending the request and cookies that are set. You’ll rarely interact directly with these.\n\n\n3.2.6 Request Status Codes\nThe first thing you’ll consult when you get a result back is the status code. Status codes indicate what happened with your request to the server. You always hope to see 200 codes, which indicate a successful response.\nThere are also a two common types of error codes. 4xx codes indicate that there’s a problem with your request and the API couldn’t understand what you were asking. 5xx codes indicate that your request was fine, but some sort of error happened in processing your request.\nThere’s a cheatsheet below with some other codes and what they mean.\n\n\n3.2.7 Response Bodies\nThen there’s the actual contents of the response in the body. You’ll need to turn the body into a Python or R object you can work with.\nMost often, bodies are in JSON and you’ll decode them with {json} or {jsonlite}. Usually JSON is the default and you may be given the option to specify something else if you’ve got a preference.\n\n\n3.2.8 Common API patterns\nHere are a couple of common API patterns that are good to be familiar with:\n\nPagination – many data-feed APIs implement pagination. A paginated API returns only a certain number of results at a time to keep data sizes modest. You’ll need to figure out how to get all the pages back when you make a request.\nJob APIs – HTTP is designed for relatively quick request-response cycles. So if your API kicks off a long-running job, it’s rare to wait until the job is done to get a response. Instead, the API immediately returns an acknowledgement and a job-id which you can use to poll a job-status endpoint to check how things are going and eventually find your result.\nMultiple Verbs – a single endpoint often accepts multiple verbs – for example a GET and a POST at the same endpoint for getting and setting the data that endpoint stores."
  },
  {
    "objectID": "chapters/sec1/1-3-data-access.html#env-vars",
    "href": "chapters/sec1/1-3-data-access.html#env-vars",
    "title": "3  Using databases and data APIs",
    "section": "3.3 Environment variables to secure data connections",
    "text": "3.3 Environment variables to secure data connections\nWhen you take an app to production, authenticating to your data source while keeping your secrets secure is crucial.\nThe single most important thing you can do to secure your credentials is to avoid ever putting credentials in your code. Your username and password or API key should never appear in your code.\nThe simplest way to provide credentials without the values appearing in your code is with an environment variable. Environment variables are set before your code starts – sometimes from completely outside Python or R.\n\n3.3.1 Getting environment variables\nThe power of using an environment variable is that you reference them by name. Just sharing that there’s an environment variable called API_KEY doesn’t reveal anything secret, so if your code just includes environment variables, it’s completely safe to share with others.\n\n\n\n\n\n\nNote\n\n\n\nIt is convention to make environment variable names in all caps with words separated by underscores. The values are always simple character values, though these can be cast to some other type inside R or Python.\n\n\nIn Python, you can read environment variables from the os.environ dictionary or by using os.getenv(\"&lt;VAR_NAME&gt;\"). In R, you can get environment variables with Sys.getenv(\"&lt;VAR_NAME&gt;\").\nIt’s common to provide environment variables directly to functions as arguments, though you can also put the values in normal Python or R variables and use them from there.\n\n\n3.3.2 Setting environment variables\nThe most common way to set environment variables in a development environment is to load secrets from a plain text file. In Python, environment variables are usually set by reading a .env file into your Python session. The {python-dotenv} package is a good choice for doing this.\nR automatically reads the .Renviron file as environment variables and also sources the .Rprofile file, where you can set environment variables with Sys.setenv(). I personally prefer putting everything in .Rprofile for simplicity – but that’s not a universal opinion.\nSome organizations don’t ever want credentials files in plain text. After all, if someone stole a plain text secrets file, there’s nothing to stop them from using them.\nThere are packages in both R and Python called {keyring} that allow you to use the system keyring to securely store environment variables and recall them at runtime.\nSetting environment variables in production is a little harder.\nJust moving your secrets from your code into a different file you push to prod is still bad. And using {keyring} in a production environment is quite cumbersome.\nYour production environment may provide environment management tools. For example, GitHub Actions and Posit Connect both provide you the ability to set secrets that aren’t visible to the users, but are accessible to the code at runtime in an environment variable.\nIncreasingly, organizations are using token-based authorization schemes that just exchange one cryptographically secure token for another, never relying on credentials at all. The tradeoff for the enhanced security is that they can be difficult to implement, likely requiring coordination with an IT/Admin to use technologies like Kerberos or OAuth. There’s more on how to do that in Chapter 18."
  },
  {
    "objectID": "chapters/sec1/1-3-data-access.html#data-connection-packages",
    "href": "chapters/sec1/1-3-data-access.html#data-connection-packages",
    "title": "3  Using databases and data APIs",
    "section": "3.4 Data Connection Packages",
    "text": "3.4 Data Connection Packages\nIt’s very common for organizations to write their own data connector packages in Python or R that include all of the shared connection details so users don’t have to remember them. If everyone has their own credentials, it’s also nice if those packages set standard names for the environment variables so they can be more easily set in production.\nWhether you’re using R or Python, the function in your package should return the database connection object for people to use.\nHere’s an example of what that might look like if you were using a Postgres database from R:\n\n#' Return a database connection\n#'\n#' @param user username, character, defaults to value of DB_USER\n#' @param pw password, character, defaults to value of DB_PW\n#' @param ... other arguments passed to \n#' @param driver driver, defaults to RPostgres::Postgres\n#'\n#' @return DBI connection\n#' @export\n#'\n#' @examples\n#' my_db_con()\nmy_db_con &lt;- function(\n    user = Sys.getenv(\"DB_USER\"), \n    pw = Sys.getenv(\"DB_PW\"), \n    ..., \n    driver = RPostgres::Postgres()\n) {\n  DBI::dbConnect(\n    driver = driver,\n    dbname = 'my-db-name', \n    host = 'my-db.example.com', \n    port = 5432, \n    user = user,\n    password = pw, \n    ...\n  )\n}\n\nNote that the function signature defines default environment variables that will be consulted. If those environment variables are set ahead of time by the user, this code will just work."
  },
  {
    "objectID": "chapters/sec1/1-3-data-access.html#comprehension-questions",
    "href": "chapters/sec1/1-3-data-access.html#comprehension-questions",
    "title": "3  Using databases and data APIs",
    "section": "3.5 Comprehension Questions",
    "text": "3.5 Comprehension Questions\n\nDraw two mental map for connecting to a database. One usinga database driver in a Python or R package vs an ODBC or JDBC driver. You should (at a minimum) include the nodes database package, DBI (R only), driver, system driver, ODBC, JDBC, and database.\nDraw a mental map for using an API from R or Python. You should (at a minimum) include nodes for {requests}/{httr2}, request, http verb/request method, headers, query parameters, body, json, response, and response code.\nHow can environment variables be used to keep secrets secure in your code?"
  },
  {
    "objectID": "chapters/sec1/1-3-data-access.html#lab-3-use-a-database-and-an-api",
    "href": "chapters/sec1/1-3-data-access.html#lab-3-use-a-database-and-an-api",
    "title": "3  Using databases and data APIs",
    "section": "3.6 Lab 3: Use a database and an API",
    "text": "3.6 Lab 3: Use a database and an API\nIn this lab, we’re going to build out both the data layer and the presentation layer for our penguin mass model exploration. We’re going to create an app to explore the model, which will look like this: \nLet’s start by moving the data into a real data layer.\n\n3.6.1 Step 1: Put the data in DuckDB\nLet’s start by moving the data into a DuckDB database and use it from there for the modeling and EDA scripts.\nTo start, let’s load the data.\nHere’s what that looks like in R:\ncon &lt;- DBI::dbConnect(duckdb::duckdb(), dbdir = \"my-db.duckdb\")\nDBI::dbWriteTable(con, \"penguins\", palmerpenguins::penguins)\nDBI::dbDisconnect(con)\nOr equivalently, in Python:\nimport duckdb\nfrom palmerpenguins import penguins\n\ncon = duckdb.connect('my-db.duckdb')\ndf = penguins.load_penguins()\ncon.execute('CREATE TABLE penguins AS SELECT * FROM df')\ncon.close()\nNow that the data is loaded, let’s adjust our scripts to use the database.\nIn R, we are just going to replace our data loading with connecting to the database. Leaving out all the parts that don’t change, it looks like\n\n\neda.qmd\n\n\ncon &lt;- DBI::dbConnect(\n  duckdb::duckdb(), \n  dbdir = \"my-db.duckdb\"\n  )\ndf &lt;- dplyr::tbl(con, \"penguins\")\n\nWe also need to call to DBI::dbDisconnect(con) at the end of the script.\nBecause we wrote our data processing code in {dplyr}, we actually don’t have to change anything. Under the hood, {dplyr} can switch seamlessly to a database backend, which is really cool.\n\n\neda.qmd\n\ndf %&gt;%\n  group_by(species, sex) %&gt;%\n  summarise(\n    across(\n        ends_with(\"mm\") | ends_with(\"g\"),\n      \\(x) mean(x, na.rm = TRUE)\n      )\n    ) %&gt;%\n  dplyr::collect() %&gt;%\n  knitr::kable()\n\nIt’s not necessary, but I’ve added a call to dplyr::collect in line 31. It will be implied if I don’t put it there manually, but it helps make obvious that all the work before there has been pushed off to the database. Only the result of this code is coming back to the R process. Obviously it doesn’t matter for this small dataset, but this would be a huge benefit if the dataset were much larger.\nIn Python, we’re just going to load the entire dataset into memory for modeling, so the line loading the dataset changes to\n\n\nmodel.qmd\n\ncon = duckdb.connect('my-db.duckdb')\ndf = con.execute(\"SELECT * FROM penguins\").fetchdf().dropna()\ncon.close()\n\n\nNow let’s switch to figuring out the connection we’ll need to our processing layer in the presentation layer.\n\n\n3.6.2 Step 2: Call the model API from code\nBefore you start, make sure the API is running on your machine from the last lab.\n\n\n\n\n\n\nNote\n\n\n\nI’m assuming it’s running on port 8080 in this lab. If you’ve put it somewhere else, change the 8080 in the code below to match the port on your machine.\n\n\nIf you want to call the model in code, you can use any http request library. In R you should use httr2 and in Python you should use requests.\nHere’s what it looks like to call the API in Python\n\nimport requests\n\nreq_data = {\n  \"bill_length_mm\": 0,\n  \"species_Chinstrap\": False,\n  \"species_Gentoo\": False,\n  \"sex_male\": False\n}\nreq = requests.post('http://127.0.0.1:8080/predict', json = req_data)\nres = req.json().get('predict')[0]\n\nor equivalently in R\n\nreq &lt;- httr2::request(\"http://127.0.0.1:8080/predict\") |&gt;\n  httr2::req_body_json(\n    list(\n      \"bill_length_mm\" = 0,\n      \"species_Chinstrap\" = FALSE,\n      \"species_Gentoo\" = FALSE,\n      \"sex_male\" = FALSE\n    )\n  ) |&gt;\n  httr2::req_perform()\nres &lt;- httr2::resp_body_json(r)$predict[[1]]\n\nNote that there’s no translation necessary to send the request. The {requests} and{httr2} packages automatically know what to do with the Python dictionary and the R list.\nGetting the result back takes a little more work to find the right spot in the JSON returned. This is quite common.\n\n\n\n\n\n\nNote\n\n\n\nThe {vetiver} package also includes the ability to auto-query a {vetiver} API. I’m not using it here to expose the details of calling an API.\n\n\nNow, let’s take this API-calling code and build the presentation layer around it.\n\n\n3.6.3 Step 3: Build a shiny app\nWe’re going to use the {shiny} package, which is an R and Python package for creating interactive web apps using just Python code. If you don’t know much about {shiny}, you can choose to just blindly follow the examples here, or you could spend some time with the Mastering Shiny book to learn to use it yourself.\nEither way, an app that looks like the picture above would look like this in Python\n\n\napp.py\n\nfrom shiny import App, render, ui, reactive\nimport requests\n\napi_url = 'http://127.0.0.1:8080/predict'\n\napp_ui = ui.page_fluid(\n    ui.panel_title(\"Penguin Mass Predictor\"), \n    ui.layout_sidebar(\n        ui.panel_sidebar(\n            [ui.input_slider(\"bill_length\", \"Bill Length (mm)\", 30, 60, 45, step = 0.1),\n            ui.input_select(\"sex\", \"Sex\", [\"Male\", \"Female\"]),\n            ui.input_select(\"species\", \"Species\", [\"Adelie\", \"Chinstrap\", \"Gentoo\"]),\n            ui.input_action_button(\"predict\", \"Predict\")]\n        ),\n        ui.panel_main(\n            ui.h2(\"Penguin Parameters\"),\n            ui.output_text_verbatim(\"vals_out\"),\n            ui.h2(\"Predicted Penguin Mass (g)\"), \n            ui.output_text(\"pred_out\")\n        )\n    )   \n)\n\ndef server(input, output, session):\n    @reactive.Calc\n    def vals():\n        d = {\n            \"bill_length_mm\" : input.bill_length(),\n            \"sex_Male\" : input.sex() == \"Male\",\n            \"species_Gentoo\" : input.species() == \"Gentoo\", \n            \"species_Chinstrap\" : input.species() == \"Chinstrap\"\n\n        }\n        return d\n    \n    @reactive.Calc\n    @reactive.event(input.predict)\n    def pred():\n        r = requests.post(api_url, json = vals())\n        return r.json().get('predict')[0]\n\n    @output\n    @render.text\n    def vals_out():\n        return f\"{vals()}\"\n\n    @output\n    @render.text\n    def pred_out():\n        return f\"{round(pred())}\"\n\napp = App(app_ui, server)\n\nAnd like this in R\n\n\napp.R\n\nlibrary(shiny)\n\napi_url &lt;- \"http://127.0.0.1:8080/predict\"\n\nui &lt;- fluidPage(\n  titlePanel(\"Penguin Mass Predictor\"),\n\n  # Model input values\n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\n        \"bill_length\",\n        \"Bill Length (mm)\",\n        min = 30,\n        max = 60,\n        value = 45,\n        step = 0.1\n      ),\n      selectInput(\n        \"sex\",\n        \"Sex\",\n        c(\"Male\", \"Female\")\n      ),\n      selectInput(\n        \"species\",\n        \"Species\",\n        c(\"Adelie\", \"Chinstrap\", \"Gentoo\")\n      ),\n      # Get model predictions\n      actionButton(\n        \"predict\",\n        \"Predict\"\n      )\n    ),\n\n    mainPanel(\n      h2(\"Penguin Parameters\"),\n      verbatimTextOutput(\"vals\"),\n      h2(\"Predicted Penguin Mass (g)\"),\n      textOutput(\"pred\")\n    )\n  )\n)\n\nserver &lt;- function(input, output) {\n  # Input params\n  vals &lt;- reactive(\n    list(\n      bill_length_mm = input$bill_length,\n      species_Chinstrap = input$species == \"Chinstrap\",\n      species_Gentoo = input$species == \"Gentoo\",\n      sex_male = input$sex == \"Male\"\n    )\n  )\n\n  # Fetch prediction from API\n  pred &lt;- eventReactive(\n    input$predict,\n    httr2::request(api_url) |&gt;\n      httr2::req_body_json(vals()) |&gt;\n      httr2::req_perform() |&gt;\n      httr2::resp_body_json(),\n    ignoreInit = TRUE\n  )\n\n  # Render to UI\n  output$pred &lt;- renderText(pred()$predict[[1]])\n  output$vals &lt;- renderPrint(vals())\n}\n\n# Run the application\nshinyApp(ui = ui, server = server)\n\nOver the next few chapters, we’re going to implement more architectural best practices for the app, and in [Chapter @env-as-code] we’ll actually go to deployment."
  },
  {
    "objectID": "chapters/sec1/1-3-data-access.html#http-code-cheatsheet",
    "href": "chapters/sec1/1-3-data-access.html#http-code-cheatsheet",
    "title": "3  Using databases and data APIs",
    "section": "3.7 HTTP Code Cheatsheet",
    "text": "3.7 HTTP Code Cheatsheet\nAs you work more with http traffic, you’ll learn some of the common codes. Here’s a cheatsheet for some of the most frequent you’ll see.\n\n\n\n\n\n\n\nCode\nMeaning\n\n\n\n\n200\nEveryone’s favorite, a successful response.\n\n\n3xx\nYour query was redirected somewhere else, usually ok.\n\n\n4xx\nErrors with the request\n\n\n400\nBad request. This isn’t a request the server can understand.\n\n\n401 and 403\nUnauthorized or forbidden. Required authentication hasn’t been provided.\n\n\n404\nNot found. There isn’t any content to access here.\n\n\n5xx\nErrors with the server once your request got there.\n\n\n500\nGeneric server-side error. Your request was received, but there was an error processing it.\n\n\n504\nGateway timeout. This means that a proxy or gateway between you and the server you’re trying to access timed out before it got a response from the server."
  },
  {
    "objectID": "chapters/sec1/1-3-data-access.html#footnotes",
    "href": "chapters/sec1/1-3-data-access.html#footnotes",
    "title": "3  Using databases and data APIs",
    "section": "",
    "text": "I have heard that some write operations may be faster with a JDBC driver than an ODBC one. I would argue that if you’re doing enough writing to a database that speed matters, you probably should be using database-specific data loading tools, not just writing from R or Python.↩︎\nIn a lot of cases, people use POST for things that look like GETs to my eyes. The reason is request bodies. GET endpoints only recently started allowing bodies – and it’s still discouraged. In the {vetiver} API example, I think of fetching results from an ML model as a GET-type operation, but it uses a POST because it also uses a body in the query.↩︎"
  },
  {
    "objectID": "chapters/sec1/1-4-monitor-log.html#observing-correctness",
    "href": "chapters/sec1/1-4-monitor-log.html#observing-correctness",
    "title": "4  Logging and Monitoring",
    "section": "4.1 Observing Correctness",
    "text": "4.1 Observing Correctness\nObservability of general purpose software is primarily concerned with the operational qualities of the software. A software engineer wants to know whether their software is using too much RAM or CPU, whether its fast enough, and whether its crashed.\nFor a general purpose software engineer, an uncaught exception that makes the software crash is about as bad as it gets.\nBut for a data scientist there’s something even scarier – an issue that doesn’t result in code failure but yields incorrect answers. Data joins usually complete even if the merge quality is really bad. Model APIs will return a prediction even if the prediction is very, very bad.\nIt’s hard to check the actual correctness of the numbers and figures work work returns because you’re (basically by definition) doing something novel. So you’re basically left putting process metrics in place that can help reveal a problem before it surfaces.\nOne important tool you have in your toolbox is correctly architecting your project. Jobs are generally much easier to check for correctness than presentation layers. By moving as much processing as possible out of the presentation layer and into the data and processing layers, you can make it easier to observe.\nMoreover, you’re already very familiar with tools for literate programming like Jupyter Notebooks, R Markdown Documents, and Quarto Documents.\nOne of my spicier opinions is that all jobs should be in a literate programming format. These tools, when used well, intersperse code, commentary, and output, which is one of the best ways of observing the correctness of a job.\nOn a job, there are three particular things I always monitor.\nThe first is the quality of data joins. Based on the number of rows (or unique ids), you know how many rows should be in the data set after a join. Checking that the joined data matches expectations can reveal many data quality issues just waiting to ruin your day.\nThe second checking is cross-tabulations before and after recoding a categorical variable. Making sure the recode logic does what you think and that the values coming in aren’t changing over time is always worth the effort.\nThe last is goodness-of-fit metrics of an ML model in production. There are many, many frameworks and products for monitoring model quality and model drift once your model is in production. I don’t have strong opinions on these other than that you need to use one if you’ve got a model that’s producing results you hope to rely on."
  },
  {
    "objectID": "chapters/sec1/1-4-monitor-log.html#observing-operations",
    "href": "chapters/sec1/1-4-monitor-log.html#observing-operations",
    "title": "4  Logging and Monitoring",
    "section": "4.2 Observing Operations",
    "text": "4.2 Observing Operations\nNow let’s turn to the issue of observing the operational qualities of your code. The operational qualities of your project are things like the system resources its consuming, the number of users, and user interactions just before an error occurred.\nThe first step to making your app or API observable is to add logging. You may be used to just adding print statements throughout your code. And, honestly, this is far better than nothing. But purpose-built tooling for logging includes ways to apply consistent formats, emit logs in useful ways, and provide visibility into severity of issues.\nThere are great logging packages in both Python and R. Python’s logging package is standard. There is not a standard logging package in R, but I recommend {log4r}.\nThese packages – and basically every other logging package – work very similarly. At the outset of your code, you’ll create and parameterize a log session that persists as long as the Python or R session. When your code does something you want to love, you’ll you’ll use the log session to write log statements. When the log statement runs, it creates a log entry.\nFor example, here’s what logging for an app starting up might look like in Python\n\n\n\napp.py\n\nimport logging\n\n# Configure the log object\nlogging.basicConfig(\n    format='%(asctime)s - %(message)s',\n    level=logging.INFO\n)\n\n# Log app start\nlogging.info(\"App Started\")\n\n\nAnd here’s what that looks like using {log4r}\n\n\n\napp.R\n\n# Configure the log object\nlog &lt;- log4r::logger()\n\n# Log app start\nlog4r::info(log, \"App Started\")\n\n\nWhen the R or Python interpreter hits either of these lines, it will create a log entry that looks something like this:\n2022-11-18 21:57:50 INFO App Started\nLike all log entries, this entry has three components:\n\nThe log metadata is data what the logging library automatically includes on every entry. It is configured when you initialize logging. In the example above, the only metadata is the timestamp. Log metadata can include additional information, like which server you’re running on.\nThe second component is the log level. The log level indicates the severity of the event you’re logging. In the example above the log level was INFO.\nThe last component is the log data, which provides details on the event you want to log – App Started in this case.\n\n\n4.2.1 Understanding log levels\nThe log level indicates how serious the logged event is. Most logging libraries have 5-7 log levels. As you’re writing statements into your code, you’ll have to think carefully about the appropriate logging level for a given event.\nBoth the Python {logging} library and {log4r} have five levels from least to most scary:\n\nDebug: what the code was doing in detail that will only make sense to someone who really knows the code. For example, you might include which function ran and with what arguments in a debug log.\nInfo: something normal happened in the app. Info statements record things like starting and stopping, successfully making database and other connections, and runtime configuration options.\nWarn/Warning: an unexpected application issue that isn’t fatal. For example, you might include having to retry doing something or noticing that resource usage is high. If something were to go wrong later, these might be helpful breadcrumbs to look at.\nError: an issue that will make an operation not work, but that won’t bring down your app. An example might be a user submitting invalid input and the app recovering.\nCritical: an error so big that the app itself shuts down. This is the SOS your app sends as it shuts down. For example, if your app cannot run without a connection to an outside service, you might log an inability to connect as a Critical error.\n\nWhen you initialize your logging session, you’ll set your log level, which is the least critical level you want to see in the session. In development, you probably want to log everything down to the debug level, while that probably isn’t ideal in prod.\n\n\n4.2.2 Configuring log formats and log handling\nWhen you initialize your logging session, you’ll choose where logs will be written and in what format. You’ll configure the format with a formatter or layout and where it goes with a handler or an appender.\nFor most logging frameworks, the default is to emit logs to the console in plain text.\nFor example, a plain text log of an app starting might put this on your console\n2022-11-18 21:57:50 INFO App Started\nYou’ll decide the format of your log based on how you’re planning to consume them.\nPlain text logs is a great choice if humans are going to be directly reading them. If you’re shipping your logs off to an aggregation service, you might prefer to have structured logs.\nThe most common structured logging format is JSON, though YAML and XML are often options. If you used JSON logging, the same record might be emitted as\n{\n  \"time\": \"2022-11-18 21:57:50\",\n  \"level\": \"INFO\", \n  \"data\": \"App Started\"\n}\nWhere your logs go should be determined by where your code is running.\nIn development, printing logs for the console makes it easy to iterate quickly.\nIn production, the most common way to handle logs is to append them to a file. It makes them easy for humans to access and many tools for aggregating and consuming logs are comfortable watching a file and aggregating lines as they are written.\nIf you are emitting logs to file, you may also want to consider how long those logs stay around.\nLog rotation is the process of periodically creating new log files, storing old logs for a set retention period, and deleting files outside that period. A common log rotation pattern is to have a log file that lasts for 24 hours, is retained for 30 days, and is then deleted.\nThe Python {logging} library does log rotation itself. {log4r} does not, but there is a Linux library called logrotate that you can use in concert with {log4r}.1\nIf you’re running in a Docker container you don’t want to write to a file on disk. As you’ll learn more about in Chapter 9, anything that lives inside a Docker container is ephemeral. This is obviously bad if you’re writing a log that might contain clues for why a Docker container was unexpectedly killed.\nIn that case, it’s common practice for a service running in a container to emit logs inside the container and then have some sort of more permanent service collecting the logs outside. This is usually accomplished by sending normal operating logs to go to stdout (usually pronounced standard out) and failures to go to stderr (standard error).\nIt’s also possible you want to do something else completely custom with your logs. This is most common for critical or error logs. For example, you may want to send an email, slack, or text message immediately if your system emits a high-level log message.\nIt’s also very common to have different format and location settings in development vs in production. The most common way to enable different logging configurations in different environments is with config files and environment variables. More on how to use these tools in Chapter 1.\n\n\n4.2.3 Working with Metrics\nThe most common place to see metrics in a data science context is when deploying and monitoring ML models in production. Additionally, monitoring ETL data quality is ripe for more monitoring.\nIf you are going to configure metrics emission or consumption, most modern metrics stacks are built around the open source tools Prometheus and Grafana.\nPrometheus is an open source monitoring tool that makes it easy to store metrics data, query that data, and alert based on it. Grafana is an open source dashboarding tool that sits on top of Prometheus to do visualization of the metrics. They are usually used together to do monitoring and visualization of metrics.\nYou can run Prometheus and Grafana yourself, but Grafana Labs provides a generous free tier for their SaaS service. This is great because you can just set up their service and point your app to it.\nBecause the Prometheus/Grafana stack started out in the DevOps world, they are most optimized to do monitoring of a whole server or fleet of servers – but it’s not hard to use them to monitor things you might care about like data quality, API response times, or other things.\nIf you want to register metrics from your API or app with Prometheus, there is an official Prometheus client in Python and the {openmetrics} package in R makes it easy to register metrics from a Plumber API or Shiny app.\nThere’s a great Get Started with Grafana and Prometheus doc on the Grafana Labs website if you want to actually try it out."
  },
  {
    "objectID": "chapters/sec1/1-4-monitor-log.html#comprehension-questions",
    "href": "chapters/sec1/1-4-monitor-log.html#comprehension-questions",
    "title": "4  Logging and Monitoring",
    "section": "4.3 Comprehension Questions",
    "text": "4.3 Comprehension Questions\n\nWhat is the difference between monitoring and logging? What are the two halves of the monitoring and logging process?\nIn general, logging is good, but what are some things you should be careful not to log?\nAt what level would you log each of the following events:\n\nSomeone clicks on a particular tab in your Shiny app.\nSomeone puts an invalid entry into a text entry box.\nAn http call your app makes to an external API fails.\nThe numeric values that are going into your computational function."
  },
  {
    "objectID": "chapters/sec1/1-4-monitor-log.html#lab-4-an-app-with-logging",
    "href": "chapters/sec1/1-4-monitor-log.html#lab-4-an-app-with-logging",
    "title": "4  Logging and Monitoring",
    "section": "4.4 Lab 4: An App with Logging",
    "text": "4.4 Lab 4: An App with Logging\nLet’s go back to the prediction generator app from the last lab and add a little logging. This is quite easy in both R and Python. In both, we just declare that we’re using the logger and then we put logging statements into our code.\nI decided to log when the app starts, just before and after each request, and an error logger if an HTTP error code comes back from the API.\nWith the logging now added, here’s what the app looks like in R:\n\n\napp.R\n\nlibrary(shiny)\n\napi_url &lt;- \"http://127.0.0.1:8080/predict\"\nlog &lt;- log4r::logger()\n\nui &lt;- fluidPage(\n  titlePanel(\"Penguin Mass Predictor\"),\n\n  # Model input values\n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\n        \"bill_length\",\n        \"Bill Length (mm)\",\n        min = 30,\n        max = 60,\n        value = 45,\n        step = 0.1\n      ),\n      selectInput(\n        \"sex\",\n        \"Sex\",\n        c(\"Male\", \"Female\")\n      ),\n      selectInput(\n        \"species\",\n        \"Species\",\n        c(\"Adelie\", \"Chinstrap\", \"Gentoo\")\n      ),\n      # Get model predictions\n      actionButton(\n        \"predict\",\n        \"Predict\"\n      )\n    ),\n\n    mainPanel(\n      h2(\"Penguin Parameters\"),\n      verbatimTextOutput(\"vals\"),\n      h2(\"Predicted Penguin Mass (g)\"),\n      textOutput(\"pred\")\n    )\n  )\n)\n\nserver &lt;- function(input, output) {\n  log4r::info(log, \"App Started\")\n  # Input params\n  vals &lt;- reactive(\n    list(\n      bill_length_mm = input$bill_length,\n      species_Chinstrap = input$species == \"Chinstrap\",\n      species_Gentoo = input$species == \"Gentoo\",\n      sex_male = input$sex == \"Male\"\n    )\n  )\n\n  # Fetch prediction from API\n  pred &lt;- eventReactive(\n    input$predict,\n    {\n      log4r::info(log, \"Prediction Requested\")\n      r &lt;- httr2::request(api_url) |&gt;\n        httr2::req_body_json(vals()) |&gt;\n        httr2::req_perform()\n      log4r::info(log, \"Prediction Returned\")\n\n      if (httr2::resp_is_error(r)) {\n        log4r::error(log, paste(\"HTTP Error\"))\n      }\n\n      httr2::resp_body_json(r)\n    },\n    ignoreInit = TRUE\n  )\n\n  # Render to UI\n  output$pred &lt;- renderText(pred()$predict[[1]])\n  output$vals &lt;- renderPrint(vals())\n}\n\n# Run the application\nshinyApp(ui = ui, server = server)\n\nAnd in Python:\n\n\napp.py\n\nfrom shiny import App, render, ui, reactive\nimport requests\nimport logging\n\napi_url = 'http://127.0.0.1:8080/predict'\nlogging.basicConfig(\n    format='%(asctime)s - %(message)s',\n    level=logging.INFO\n)\n\napp_ui = ui.page_fluid(\n    ui.panel_title(\"Penguin Mass Predictor\"), \n    ui.layout_sidebar(\n        ui.panel_sidebar(\n            [ui.input_slider(\"bill_length\", \"Bill Length (mm)\", 30, 60, 45, step = 0.1),\n            ui.input_select(\"sex\", \"Sex\", [\"Male\", \"Female\"]),\n            ui.input_select(\"species\", \"Species\", [\"Adelie\", \"Chinstrap\", \"Gentoo\"]),\n            ui.input_action_button(\"predict\", \"Predict\")]\n        ),\n        ui.panel_main(\n            ui.h2(\"Penguin Parameters\"),\n            ui.output_text_verbatim(\"vals_out\"),\n            ui.h2(\"Predicted Penguin Mass (g)\"), \n            ui.output_text(\"pred_out\")\n        )\n    )   \n)\n\ndef server(input, output, session):\n    logging.info(\"App start\")\n\n    @reactive.Calc\n    def vals():\n        d = {\n            \"bill_length_mm\" : input.bill_length(),\n            \"sex_Male\" : input.sex() == \"Male\",\n            \"species_Gentoo\" : input.species() == \"Gentoo\", \n            \"species_Chinstrap\" : input.species() == \"Chinstrap\"\n\n        }\n        return d\n    \n    @reactive.Calc\n    @reactive.event(input.predict)\n    def pred():\n        logging.info(\"Request Made\")\n        r = requests.post(api_url, json = vals())\n        logging.info(\"Request Returned\")\n\n        if r.status_code != 200:\n            logging.error(\"HTTP error returned\")\n\n        return r.json().get('predict')[0]\n\n    @output\n    @render.text\n    def vals_out():\n        return f\"{vals()}\"\n\n    @output\n    @render.text\n    def pred_out():\n        return f\"{round(pred())}\"\n\napp = App(app_ui, server)\n\nNow, if you load up this app locally, you can see the logs of what’s happening stream in as you’re pressing buttons in the app.\nYou can feel free to log whatever you think is helpful – for example, it’d probably be more useful to get the actual error contents if an HTTP error comes back."
  },
  {
    "objectID": "chapters/sec1/1-4-monitor-log.html#footnotes",
    "href": "chapters/sec1/1-4-monitor-log.html#footnotes",
    "title": "4  Logging and Monitoring",
    "section": "",
    "text": "There are two common naming patterns with rotating log files.\nThe first is to have dated log filenames that look like my-log-20221118.log.\nThe other pattern is to keep one file that’s current and have the older ones numbered. So today’s log would be my-log.log, yesterday’s would be my-log.log.1 , the day before my-log.log.2, etc. This second pattern works particularly well if you’re using logrotate with log4r, because then log4r doesn’t need to know anything about the log rotation. It’s just always writing to my-log.log.↩︎"
  },
  {
    "objectID": "chapters/sec1/1-5-deployments.html#separate-the-prod-environment",
    "href": "chapters/sec1/1-5-deployments.html#separate-the-prod-environment",
    "title": "5  Deployments and code promotion",
    "section": "5.1 Separate the prod environment",
    "text": "5.1 Separate the prod environment\nThe bedrock of a good CI/CD process is a production environment that’s actually separate from non-production environments.\nCI/CD is all about easily promoting code into production, but if the boundaries of production are a mushy mess, it’s all too easily to accidentally mess up code that’s in production.\nIn order to structure a smooth pathway to production, software environments are separated into three – dev, test, and prod. Dev is the development environment where new work is produced, test is where the code is tested for performance, usability, and feature completeness, and prod is the production environment. Depending on your organization you might have just a dev and prod or you might have more environments between dev and prod.\n\nThe number and configuration of lower environments will vary according to your organization and its needs. But like Tolstoy said about happy families, all prod environments are alike.\nSome criteria that all good prod environments meet:\n\nThe environment is created using code. For data science, that managing R and Python packages using environments as code tooling, as discussed in Chapter 1.\nChanges happen via a promotion process. The process combines human approvals that the code is ready for production and automations to run tests and do the actual deployment.\nChanges only happen via the promotion process. This means no manual changes to the environment or the code.\n\nRules 1 and 2 are pretty straightforward to follow. But the first time something breaks in your prod environment, you will be sorely tempted to violate rule 3. Don’t do it.\nIf you want to run a data science project that becomes critical to your organization, keeping a pristine prod environment that you can rely on is critical. Re-create the issue in a lower environment to figure out what’s wrong and push changes through your promotion process.\n\n5.1.1 Dev and test environments\nThese guidelines for a prod environment look almost identical to guidelines for general purpose software engineering. It’s in the composition of lower environments that the needs of data scientists diverge from general purpose software engineers.\nAs a data scientist, dev means working in a lab environment like RStudio, Spyder, VSCode, or PyCharm and experimenting with the data. You’re slicing the data this way or that to see if anything meaningful emerges, creating plots to see if they are the right way to show off a finding, and checking whether certain features improve model performance. All this means that it’s basically impossible to do work without real data.\n“Duh”, you say, “Of course it’s silly to do data science on fake data.”\nThis may be obvious to you, but doing dev data science on real data is a very common source of friction with IT/Admins.\nThat’s because this need is unique to data scientists. For general purpose software engineering, a lower environment needs data that is formatted like the real data, but the actual content doesn’t matter.\nFor example, if you’re building an online store, you need dev and test environments where the API calls from the sales system are in the same format as the real data – but you don’t actually care if it’s real data. In fact, you probably want to create some odd-looking cases for testing purposes.\nOne way to help allay these concerns is to create a data science sandbox. A great data science sandbox provides:\n\nRead-only access to real data for experimentation.\nPlaces to write mock data to test out things you’ll write for real in prod.\nBroad access to R and Python packages to experiment with before things go to prod.\n\nWorking with your IT/Admin team to get these things isn’t always easy. One thing to point out is that creating this environment actually makes things more secure. It gives you a place to do development without any fear that you might actually damage production data or services."
  },
  {
    "objectID": "chapters/sec1/1-5-deployments.html#version-control-implements-code-promotion",
    "href": "chapters/sec1/1-5-deployments.html#version-control-implements-code-promotion",
    "title": "5  Deployments and code promotion",
    "section": "5.2 Version control implements code promotion",
    "text": "5.2 Version control implements code promotion\nYou need a way to actually operationalize your code promotion process. If your process says that your code needs testing and review before it’s pushed to prod, you need a place to actually do that. Version control is the tool to make your code promotion process real.\nVersion control is software that allows you to keep the prod version of your code safe, gives contributors their own copy to work on, and hosts tools to manage merging changes back together. These days, git is the industry standard for version control.\nGit is a system for tracking changes in computer files in a project called a repository. Git is open source and freely available. There are a number of different companies that host git repositories. Many of them allow you to host public – and some private - repositories for free and have enterprise products that your organization may pay for. GitHub is by far the most popular git host, but GitLab, Bitbucket, and Azure DevOps are also common.\nThis is not a book on git. If you’re not already comfortable with using local and remote repositories, branching, and merging, the rest of this chapter is going to be completely useless. I recommend you take a break from this book and spend some time learning git.\n\n\n\n\n\n\nHints on Learning Git\n\n\n\nPeople who say git is easy to learn are either lying or have forgotten. I am sorry our industry has standardized on a tool with such terrible ergonomics, but it’s really worth it to learn.\nWhether you’re an R or Python user, I’d recommend starting with a resource designed to teach git to a data science user. My recommendation is to check out HappyGitWithR by Jenny Bryan.\nIf you’re a Python user, some of the specific tooling suggestions won’t apply, but the general principles will be exactly the same.\n\n\nIf you understand git and just need a reminder of some common commands, there is a cheatsheet at the end of the chapter.\nThe precise contours of your code promotion process – and therefore your git policies – are up to you and your organization’s needs. Do you need multiple rounds of review? Can anyone promote something to prod, or just certain people? Is automated testing required?\nYou should make these decisions as part of designing your code promotion process, which you can then enshrine in the configuration of your project’s git repository.\nOne important decision you’ll make is on how to configure the branches of your git repository. Here’s how I’d suggest you do it for production data science projects:\n\nMaintain two long running branches – main is the prod version of your project, and test is a long-running pre-prod version.\nCode can only be promoted to main via a merge from test. Direct pushes to main are not allowed.\nNew functionality is developed in short-lived feature branches that are merged into test when you think they’re ready to go. Once sufficient approvals are granted, the feature branch changes in test are merged into main.\n\nThis framework helps maintain a reliable prod version on the main branch, while also leaving sufficient flexibility to accomplish basically any set of approvals and testing you might want.\nHere’s an example of how this might work. Let’s say you were working on a dashboard and were trying to add a new plot.\nYou would create a new feature branch, perhaps called new_plot to work on the plot. When you were happy with how it looked you would merge the feature branch to test. Depending on your organization’s process, you might be able to merge to test yourself or you might require approval.\nIf your testing turned up a bug, you’d fix the bug in the feature branch, merge the bug fix into test, re-test, and merge to main once you were satisfied.\nHere’s what the git graph for that sequence of events might look like:\n\nOne of the tenets of a good CI/CD practice is that changes are merged frequently and incrementally into production.\nA good rule of thumb is that you want your merges to be the smallest meaningful change that can be incorporated into main in a standalone way.\nCreating feature branches for every word of text you might change is clearly too small. Completely rewriting the dashboard in one merge request is also probably too big.\nThere’s no hard and fast rules here. Knowing the appropriate scope for a single merge is an art – one that can take years to develop. Your best resource here is more senior team members who’ve already figured it out."
  },
  {
    "objectID": "chapters/sec1/1-5-deployments.html#cicd-automates-git-operations",
    "href": "chapters/sec1/1-5-deployments.html#cicd-automates-git-operations",
    "title": "5  Deployments and code promotion",
    "section": "5.3 CI/CD automates git operations",
    "text": "5.3 CI/CD automates git operations\nThe role of git is to make your code promotion process happen. Git allows you to configure requirements for whatever approvals and testing you might need. Your CI/CD tool sits on top of that so that all this merging and branching actually does something.1\nTo be more precise a CI/CD pipeline for a project watches the git repository and does something when certain triggers are met, like a merge to a particular branch or a pull request opening.\nThe most common CI/CD operations are pre-merge checks like spell checking, code linting, and automated testing and post-merge deployments.\nThere are a variety of different CI/CD tools available. Because of the tight linkage between GitHub repos and CI/CD, CI/CD pipelines built right into git providers are very popular.\nGitHub Actions (GHA) was released a few years ago and is eating the world of CI/CD. Depending on your organization and the age of your CI/CD pipeline, you might also see Jenkins, Travis, Azure DevOps, or GitLab.\nIf you’re curious how exactly this works, you’ll get your hands dirty in the lab at the end of the chapter.\n\n5.3.1 Configuring per-environment behavior\nAs you promote an app from dev to test and prod, you probably want behavior to look different across the environments. For example, you might want to switch data sources from a dev database to a prod one, or switch a read-only app into write mode, or use a different level of logging.\nThe easiest way to create per-environment behavior is to write code that behaves differently based on on the value of an environment variable and to set that environment variable in each environment.\nMy recommendation is to use a config file to store the values you want for your environment variables for each environment. My preference is to use YAML to store configuration, but there are different ways it can be done.\n\n\n\n\n\n\nNote\n\n\n\nOnly non-secret configuration settings should go in a config file. Secrets should always be configured using secrets management settings in the tooling you’re using so they don’t appear in plain text.\n\n\nFor example, you could write a project that knows whether to write or not based on the value of the config’s write and which database using the config’s db-path. Then you could use the YAML below to specify which environments write and which ones use which database:\n{yaml filename=\"config.yml\"} dev:   write: false   db-path: dev-db test   write: true prod:   write: true   db-path: prod-db\nYou would set a relevant environment variable so your code pulls the dev configuration in dev, test in test, and prod in prod.\nIn Python there are many different ways to set and read in your a per-environment configuration. If you want to use YAML like in the example above, you could save it as config.yml and use the {yaml} package to read it in as a dictionary, and choose which part of the dictionary to at the start of your script.\nIn R, the {config} package is the standard way to load an environmental configuration from a YAML file. The config::get() function uses the value of the R_CONFIG_ACTIVE environment variable to choose which configuration to use. That means that switching from the dev to the prod version of the app is as easy as making sure you’ve got the correct environment variable set on your system."
  },
  {
    "objectID": "chapters/sec1/1-5-deployments.html#comprehension-questions",
    "href": "chapters/sec1/1-5-deployments.html#comprehension-questions",
    "title": "5  Deployments and code promotion",
    "section": "5.4 Comprehension Questions",
    "text": "5.4 Comprehension Questions\n\nWrite down a mental map of the relationship between the three environments for data science?\nWhy is git so important to a good code promotion strategy? Can you have a code promotion strategy without git?\nWhat is the relationship between git and CI/CD? What’s the benefit of using git and CI/CD together?\nWrite out a mental map of the relationship of the following terms: git, GitHub, CI/CD, GitHub Actions, Version Control"
  },
  {
    "objectID": "chapters/sec1/1-5-deployments.html#lab5",
    "href": "chapters/sec1/1-5-deployments.html#lab5",
    "title": "5  Deployments and code promotion",
    "section": "5.5 Lab 5: Host a website with automatic updates",
    "text": "5.5 Lab 5: Host a website with automatic updates\nIn labs 1 through 4, you’ve created a Quarto website for the penguin model. You’ve got sections on EDA and model building. But it’s still just on your computer.\nIn this lab, we’re going to actually deploy that website to a public site on GitHub and and set up GitHub Actions as CI/CD so the EDA and modeling steps re-render every time we make changes.\nBefore we get into the meat of the lab, there are a few things you have to do on your own. If you don’t know how to do these things, there are plenty of great tutorials online.\n\nCreate an empty public git repo on GitHub.\nConfigure the repo as the remote for your Quarto project directory.\n\nOnce you’ve got the GitHub repo connected to your project, you need to set up the Quarto project to publish via GitHub Actions. There are great directions on how to get that configured on the Quarto website.\nFollowing those instructions will accomplish three things for you:\n\nGenerate a _publish.yml, which is a Quarto-specific file for configuring publishing locations.\nConfigure GitHub Pages to serve your website off a long-running standalone branch called gh-pages.\nGenerate a GitHub Actions workflow file, which will live at .github/workflows/publish.yml.\n\nHere’s the basic GitHub Actions file (or close to it) that the process will auto-generate for you.\n\n\n.github/workflows/publish.yml\n\non:\n  workflow_dispatch:\n  push:\n    branches: main\n\nname: Quarto Publish\n\njobs:\n  build-deploy:\n    runs-on: ubuntu-latest\n    permissions:\n      contents: write\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v2\n\n      - name: Set up Quarto\n        uses: quarto-dev/quarto-actions/setup@v2\n\n      - name: Render and Publish\n        uses: quarto-dev/quarto-actions/publish@v2\n        with:\n          target: gh-pages\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n\nOne of the reasons GitHub Actions has gotten so popular is that the actions defined in a very human-readable YAML file and it’s very likely you can read and understand this without much editorializing. But let’s still go through it in some detail.\nThis particular syntax is unique to GitHub Actions, but the idea is universal to all CI/CD systems – you define a trigger and a job to do when it’s triggered.\nIn GitHub Actions, the on section defines when the workflow occurs. In this case, we’ve configured the workflow only to trigger on a push to the main branch.2 Another common case would be to trigger on a pull request to main or another branch.\nThe jobs section defines what happens.\nWhen your action starts up, it’s in a completely standalone environment. This is actually a great thing – if you can easily specify how to start from zero and get your code running in GitHub actions, you can bet it’ll do the same in prod.\nThe runs-on field specifies exactly where we start, which in this case is the latest version of the Ubuntu and not much else.\nOnce that environment is up, each step in jobs runs sequentially.\nThe most common way to define a step is with uses, which calls a preexisting GitHub Actions step that someone else has written. In some cases, you’ll want to specify variable values using with or environment variables with env.\nTake a close look at how this action uses the GITHUB_TOKEN. That’s an environment secret that’s auto-provisioned for an action. By using it as a variable here, it’s easy to see what happens, but the value is still totally secret.\nNow, if you try to run this, it probably won’t work.\nThat’s because the CI/CD process occurs in a completely isolated environment. This auto-generated action doesn’t including setting up versions of R and Python or the packages to run our EDA and modeling scripts. We have to get that configured before this action will work.\n\n\n\n\n\n\nNote\n\n\n\nIf you read the Quarto documentation, they recommend freezing your computations. Freezing is very useful if you want to render your R or Python code only once and just update the text of your document. You wouldn’t need to set up R or Python in CI/CD and the document would render faster.\nThat said, freezing isn’t an option if you intend the R or Python code to re-run because it’s a job you care about.\nBecause the main point here is to learn about getting environments as code working in CI/CD you should not freeze your environment.\n\n\nFirst, add the commands to install R, {renv}, and the packages for your content to the GitHub Actions workflow.\n\n\n.github/workflows/publish.yml\n\n      - name: Install R\n        uses: r-lib/actions/setup-r@v2\n        with:\n          r-version: '4.2.0'\n          use-public-rspm: true\n\n      - name: Setup renv and install packages\n        uses: r-lib/actions/setup-renv@v2\n        with:\n          cache-version: 1\n        env:\n          RENV_CONFIG_REPOS_OVERRIDE: https://packagemanager.rstudio.com/all/latest\n\n\n\n\n\n\n\nNote\n\n\n\nIf you’re having slow package installs in CI/CD for R, I’d strongly recommend using a repos override like in the example above.\nThe issue is that CRAN doesn’t serve binary packages for Linux, which means really slow installs. You’ve got to direct {renv} to install from Public Posit Package Manager, which does have Linux binaries.\n\n\nYou’ll also need to add a workflow to GitHub Actions to install Python and the necessary Python packages from the requirements.txt.\n\n\n.github/workflows/publish.yml\n\n      - name: Install Python and Dependencies\n        uses: actions/setup-python@v4\n        with:\n          python-version: '3.10'\n          cache: 'pip'\n      - run: pip install jupyter\n      - run: pip install -r requirements.txt\n\nNote that in this case, we run the Python environment restore commands with run rather than uses. Where uses takes an existing GitHub Action and runs it, run just runs the shell command natively.\nOnce you’ve made those changes, try pushing or merging your project to main. If you click on the Actions tab on GitHub you’ll be able to see the Action running.\nIn all honesty, it will probably fail the first time or five. You will almost never get your Actions correct on the first try. Just breathe deeply and know we’ve all been there. You’ll figure it out.\nOnce it finishes, you should be able to see your change reflected on your website.\nOnce it’s up, your website will be available at https://&lt;username&gt;.github.io/&lt;repo-name&gt;."
  },
  {
    "objectID": "chapters/sec1/1-5-deployments.html#cheat-git",
    "href": "chapters/sec1/1-5-deployments.html#cheat-git",
    "title": "5  Deployments and code promotion",
    "section": "5.6 Cheatsheet: Git",
    "text": "5.6 Cheatsheet: Git\n\n\n\n\n\n\n\nCommand\nWhat it Does\n\n\n\n\ngit clone &lt;remote&gt;\nClone a remote repo – make sure you’re using SSH URL.\n\n\ngit add &lt;files/dir&gt;\nAdd files/dir to staging area.\n\n\ngit commit -m &lt;message&gt;\nCommit your staging area.\n\n\ngit push origin &lt;branch&gt;\nPush to a remote.\n\n\ngit pull origin &lt;branch&gt;\nPull from a remote.\n\n\ngit checkout &lt;branch name&gt;\nCheckout a branch.\n\n\ngit checkout -b &lt;branch name&gt;\nCreate and checkout a branch.\n\n\ngit branch -d &lt;branch name&gt;\nDelete a branch."
  },
  {
    "objectID": "chapters/sec1/1-5-deployments.html#footnotes",
    "href": "chapters/sec1/1-5-deployments.html#footnotes",
    "title": "5  Deployments and code promotion",
    "section": "",
    "text": "Strictly speaking, this is not true. There are a lot of different ways to kick off CI/CD jobs. But the right way to do it is to base it on git operations.↩︎\nA completed merge counts as a push.↩︎"
  },
  {
    "objectID": "chapters/sec2/2-0-sec-intro.html#labs-in-this-section",
    "href": "chapters/sec2/2-0-sec-intro.html#labs-in-this-section",
    "title": "IT/Admin Tools",
    "section": "Labs in this Section",
    "text": "Labs in this Section\nThere’s only one lab in this section. Most of the chapters in this section are just introductions to tools you’ll need later on, so they don’t have labs.\nChapter 9 on using Docker does include a lab. You’ll take the API and app you created in the last section and learn how to Docker-ize them.\nFor more details on what you’ll do in each chapter, see Appendix B."
  },
  {
    "objectID": "chapters/sec2/2-1-terminal.html#what-makes-up-the-command-line",
    "href": "chapters/sec2/2-1-terminal.html#what-makes-up-the-command-line",
    "title": "6  The Terminal",
    "section": "6.1 What makes up the command line?",
    "text": "6.1 What makes up the command line?\nIt is possible to spend a lotof time customizing your terminal to be exactly what you like. Some might argue it wouldn’t be the best use of your time to do so.\nSuch people are no fun, and having a terminal that’s super customized to what you like is great. Plus you get to feel like a real hacker.\nOne of the confusing things about customizing your command line is understanding what program you’re actually interacting with and where it’s customized. So here’s a little intro.\nThere are three programs that sit on top of each other when you interact with the command line – the terminal, the shell, and the operating system.\nThe terminal is the visual program where you’ll type in commands. The terminal program you use will dictate the colors and themes available for the window, how tabs and panes work, and the keyboard shortcuts you’ll use to manage them.\nThe shell is the program you’re interacting with as you’re typing in commands. It’s what matches the words you type to actual commands or programs on your system. Depending on which shell you choose, you’ll get different options for autocompletion, options for plugins for things like git, and coloring and theming of the actual text in your terminal.\nThere is some overlap of things you can customize via the terminal vs the shell, so mix and match to your heart’s content.\nLastly, the operating system is what actually runs the commands you’re typing in. So the set of commands available to you will differ by whether you’re using Windows or Mac or Linux.\n\n\n\n\ngraph LR\n    A[A Human] --&gt; |Types| B[Commands]\n    A --&gt; |Opens| E\n    E[Terminal] --&gt; |Opens| C\n    C[Shell] --&gt; |Dispatches| B\n    D[Operating System] --&gt; |Defines the set of| B\n    D[Operating System] --&gt; |Runs| B\n\n\n\n\n\nIn the next few sections of this chapter, we’ll get into how to set up your terminal and shell so that it looks and behaves exactly the way you want.\n\n\n\n\n\n\nNote\n\n\n\nI haven’t used a Windows machine in many years. I’ve collected some recommendations here, but I can’t personally vouch for them the way I can my Mac recommendations."
  },
  {
    "objectID": "chapters/sec2/2-1-terminal.html#choose-your-terminal",
    "href": "chapters/sec2/2-1-terminal.html#choose-your-terminal",
    "title": "6  The Terminal",
    "section": "6.2 Choose your terminal",
    "text": "6.2 Choose your terminal\n\nMacOSWindows\n\n\nIf you’re using a Mac, you can use the built-in terminal app, conveniently called Terminal. It’s fine.\nIf you’re going to be using your terminal more than occasionally, I’d recommend downloading and switching to the the free iTerm2, which adds a bunch of niceties like better theming and multiple tabs.\n\n\nIf you’re using Windows, there are a variety of alternative terminals you can try, but the built-in terminal is the favorite of many users. Experiment if you like, but feel free to stick with the default."
  },
  {
    "objectID": "chapters/sec2/2-1-terminal.html#choosing-and-configuring-your-shell",
    "href": "chapters/sec2/2-1-terminal.html#choosing-and-configuring-your-shell",
    "title": "6  The Terminal",
    "section": "6.3 Choosing and configuring your shell",
    "text": "6.3 Choosing and configuring your shell\n\nMacOSWindows\n\n\nThe default shell for MacOS (and Linux) is called bash. It’s pretty great shell. There’s nothing to really replace bash, but there are bash alternatives that extend bash in various ways.\nThe most popular bash alternatives include zsh, Ksh, and Fish. If you don’t already have a favorite, I recommend zsh.1\nIt has a few advantages over bash out of the box, like better autocompletion. It also has a huge ecosystem of themes and plugins that can make your shell way prettier and more functional. There are plugins that do everything from displaying your git status on the command line to controlling your Spotify playlist.\nThere are two popular plugin managers for zsh – OhMyZsh and Prezto. I prefer and recommend Prezto, but the choice is really up to you.\nI’m not going to go through the steps of installing these tools – there are numerous online walkthroughs and guides that you can google.\nBut it is a little confusing to know what to customize where, so here’s the high level overview if you’ve installed iTerm2, zsh, and prezto. You’ll customize the look of the window and the tab behavior in the iTerm2 preferences and customize the text theme and plugins via prezto. You can mostly skip any customization of zsh in the .zshrc since you’ll be doing that in Prezto.\n\n\nWindows comes with two shells built in, the Command shell (cmd) and the PowerShell.\nThe command shell is older and has been superseded by PowerShell. If you’re just getting started, you absolutely should just work with PowerShell. If you’ve been using Command shell on a Windows machine for a long time, most Command shell command work in PowerShell, so it may be worth switching over.\nOnce you’ve installed PowerShell, many people like customizing it with Oh My Posh."
  },
  {
    "objectID": "chapters/sec2/2-1-terminal.html#comprehension-questions",
    "href": "chapters/sec2/2-1-terminal.html#comprehension-questions",
    "title": "6  The Terminal",
    "section": "6.4 Comprehension Questions",
    "text": "6.4 Comprehension Questions\n\nDraw a mental map that includes the following: terminal, shell, operating system, my laptop"
  },
  {
    "objectID": "chapters/sec2/2-1-terminal.html#footnotes",
    "href": "chapters/sec2/2-1-terminal.html#footnotes",
    "title": "6  The Terminal",
    "section": "",
    "text": "zsh is pronounced by just speaking the letters aloud. Some people say it zeesh, but they’re not writing this book, are they?↩︎"
  },
  {
    "objectID": "chapters/sec2/2-2-ssh.html#how-ssh-works",
    "href": "chapters/sec2/2-2-ssh.html#how-ssh-works",
    "title": "7  Connecting Securely with SSH",
    "section": "7.1 How SSH works",
    "text": "7.1 How SSH works\nSSH allows you to directly access the command line on a remote host from anywhere that can connect to it over a network. It is the main way to administer a server.\nSSH works via the exchange of cryptographic keys. You will create an SSH key, which comes in two parts – the public key and the private key.\n\n\n\n\n\n\nNote\n\n\n\nI believe the terms public key and private key are a little bit of a misnomer. The analogy to the real world is a little clearer by thinking of the private key as the key and the public key as the lock.\nYou’re the only one who has the key, but you can hand copies of the lock around so they can always verify that your key is the real one.\n\n\nAs the name might suggest, you keep the private key secret. The best practice is to never move it once it has been created. You can give the public key out to anywhere you might need to access using SSH. Popular targets include remote servers you’ll need to SSH into as well as remote git hosts, like GitHub.\nSSH works on the basis of public key cryptography, which is really cool. It also defies common sense a little bit – it is a little strange that you create this two-part thing and it’s absolutely fine to hand one half around but really bad if you mix them up.\nA short digression about the mathematics of public key cryptography may help clarify.\nPublic key cryptography relies on mathematical operations that are easy in one direction, but really hard to reverse. This means that if I’ve got the public key, it’s really hard to reverse-engineer the private key, but really easy to check that the private key is right if I’m given it up front.\nAn example of an operation like this is multiplying prime numbers together. Having a public key is just like being told a number – say \\(91\\). Even if you know it’s the product of two primes, it’ll probably take you a few moments to figure out the right primes are \\(7\\) and \\(13\\).\nBut if you already have \\(91\\) and I tell you that the right primes are \\(7\\) and \\(13\\), it’s super quick to check that those are indeed the right ones.\nThe biggest difference between multiplying \\(7 * 13 = 91\\) and modern encryption algorithms is the size of the number. Public key cryptography doesn’t use small numbers like 91. It uses numbers with 91 or 9,191 digits.\nModern encryption methods also use substantially more convoluted mathematical operations than simple multiplication – but the idea is completely the same, and prime numbers are equally important.\nThe point is that SSH public keys are very big numbers, so while someone could try to reverse-engineer the private keys by brute force, it’d take more time than we have left before the heat death of the universe at current computing speeds.\nThis is why you can give your public key to a server or service that you might not fully control. Someone who has your public key can verify that your private key is the one that fits that public key – but it’s basically impossible to reverse engineer the private key with the public key in hand.\nHowever, it is totally possible to compromise the security of an SSH connection by being sloppy with your private keys. So while SSH is cyptographically super secure, the whole system is only as secure as you. Always keep your private keys securely in the place where they were created and share only the public keys."
  },
  {
    "objectID": "chapters/sec2/2-2-ssh.html#practical-ssh-usage",
    "href": "chapters/sec2/2-2-ssh.html#practical-ssh-usage",
    "title": "7  Connecting Securely with SSH",
    "section": "7.2 Practical SSH usage",
    "text": "7.2 Practical SSH usage\nBefore SSH will work, your keypair needs to be created and the public key needs to be shared. There are tons of guides online to creating an SSH key for your operating system – google one when you need it.\n\n\n\n\n\n\nDebugging SSH\n\n\n\nSSH has one of my favorite debugging modes.\nIf something’s not working when you try to connect, just add a -v to your command for verbose mode. If that’s not enough information, add another v for -vv, and even another!\nEvery v you add (up to 3) will make the output more verbose.\n\n\nThe way you register a keypair as valid on a server you control is by creating a user on that server and adding the public key to the end of the .ssh/authorized_keys file inside the user’s their home directory. More on server users and home directories in Chapter 11.\nIf you’re the server admin, you’ll have your users create their SSH keys, share the public keys with you, and you’ll put them into the right place on the server.\nIf you need to SSH from the server to another server or to a service that uses SSH, like GitHub, you’ll create another SSH key on the server and use that public key on the far end of the connection.\nIf you follow standard instructions for creating a key, it will use the default name, probably id_ed25519.1 I’d recommend sticking with the default name if you’ve only got one. This is because the ssh command will just use the keys you’ve created if they have the default name.\nIf you don’t want to use the default name for some reason, you can specify a particular key with the -i flag.\nIf you’re using SSH a lot on the same servers, I’d recommend setting up an SSH config file. You can include usernames and addresses in a config file so instead of typing ssh alexkgold@do4ds-lab.shop I can just type ssh lab.\nA google search should return good instructions for setting up your SSH config when you get there."
  },
  {
    "objectID": "chapters/sec2/2-2-ssh.html#comprehension-questions",
    "href": "chapters/sec2/2-2-ssh.html#comprehension-questions",
    "title": "7  Connecting Securely with SSH",
    "section": "7.3 Comprehension Questions",
    "text": "7.3 Comprehension Questions\n\nUnder what circumstances should you move or share your SSH private key?\nWhat is it about SSH public keys that makes them safe to share?"
  },
  {
    "objectID": "chapters/sec2/2-2-ssh.html#cheat-ssh",
    "href": "chapters/sec2/2-2-ssh.html#cheat-ssh",
    "title": "7  Connecting Securely with SSH",
    "section": "7.4 Cheatsheet: ssh",
    "text": "7.4 Cheatsheet: ssh\nssh &lt;user&gt;@&lt;host&gt;\n\n\n\n\n\n\n\n\nFlag\nWhat it does\nNotes\n\n\n\n\n-v\nVerbose, good for debugging\nAdd more vs (up to 3) for more info\n\n\n-i\nIdentity file\nPick which private key to use"
  },
  {
    "objectID": "chapters/sec2/2-2-ssh.html#footnotes",
    "href": "chapters/sec2/2-2-ssh.html#footnotes",
    "title": "7  Connecting Securely with SSH",
    "section": "",
    "text": "The pattern is id_&lt;encryption type&gt;. ed25519 is the standard SSH key encryption type as of this writing.↩︎"
  },
  {
    "objectID": "chapters/sec2/2-3-cmd-line.html#the-structure-of-bash-commands",
    "href": "chapters/sec2/2-3-cmd-line.html#the-structure-of-bash-commands",
    "title": "8  Getting comfortable on the Command Line",
    "section": "8.1 The structure of bash commands",
    "text": "8.1 The structure of bash commands\nbash and its derivatives provide small programs that each do one small thing well called a command.\nA command is the program you want to run, usually an abbreviation of the word for what you want to do. For example, the command to list the contents of a directory is ls.\nArguments tell the command what to run on. They come after the command with a space in between. For example, if I want to run ls on the directory /home/alex, I can run ls /home/alex on the command line.\nMany commands have default arguments. For example, ls runs by default on the directory I’m currently in. So if I’m in /home/alex, running ls and running ls /home/alex would return the same thing.\nOptions or flags modify how the command operates. Flags are denoted by having one or more dashes before them. For example, the ls command, which lists files, has the optional flag -l, which indicates that the files should be displayed as a list.\nFlags always come in between the command and any arguments to the command. So, for example, if I want to get the files in /home/alex as a list, I can run ls -l /home/alex or navigate to /home/alex and run ls -l.\nSome flags themselves have arguments. So, for example, if you’re using the -l flag on ls, you can also use the -D flag to format the datetime when the file was last updated.\nSo, for example, running ls -l -D %Y-%m-%dT%H:%M:%S /home/alex will list all the files in /home/alex with the date-time of the last update formatted in ISO 8601 format (which is always the correct format for dates).\nIt’s nice that this structure is standard. You always know that a bash command will be formatted as &lt;command&gt; &lt;flags + flag args&gt; &lt;command args&gt;. The downside is that having the main argument come all the way at the end, after all the flags, can make it really hard to mentally parse commands if you don’t know them super well.\nBecause there can be so many arguments, bash commands can get long. Sometimes you’ll see bash commands split them over multiple lines. You can tell bash you want it to keep reading after a line break by ending the line with a space and a \\. It’s often nice to include one flag or argument per line.\nFor example, here’s that ls command more nicely formatted:\n&gt; ls -l \\\n  -D %Y-%m-%dT%H:%M:%S \\\n  /home/alex\nThis is at least a little easier to parse. There is also help available!\nAll of the flags and arguments for commands can be found in the program’s man page (short for manual). You can access the man page for any command with man &lt;command&gt;. You can scroll the man page with arrow keys and exit with q.\nIf you ever can’t figure out how to quit, ctrl + c will generally quit from any command line situation on Linux, Mac, and Windows."
  },
  {
    "objectID": "chapters/sec2/2-3-cmd-line.html#linux-directories-and-files",
    "href": "chapters/sec2/2-3-cmd-line.html#linux-directories-and-files",
    "title": "8  Getting comfortable on the Command Line",
    "section": "8.2 Linux directories and files",
    "text": "8.2 Linux directories and files\nIn Linux, directories define where you can be. A directory is just a container for files and other directories.\nIn Linux, the entire file system is a tree (or perhaps an upside-down tree). The root directory, / is the base of the tree and a / in between two directories means that it’s a sub-directory. So the directory /home/alex is the alex directory, which is contained in home, which is in the root directory /.\nIn Linux, every directory is a sub-directory of / or a sub-directory of a sub-directory of / or…you get the picture. The sequence of directories that defines a location is called a file path.\nEvery Linux command happens at a particular file path – called the working directory. In some cases the commands you’re allowed to run or what they do will vary a lot based on where you are when they run.1\nFile paths can be either absolute – specified relative to the root – or relative to the working directory. Absolute file paths always start with / so they’re easy to identify.\nDepending on what you’re doing, either absolute or relative paths make more sense. In general, absolute file paths make more sense when you want to access the same resource regardless of where the command is run, and relative file paths make more sense when you want to access a resource specific to where you run it.\nAt any time, you can get the full path to your working directory with the pwd command, which is an abbreviation for print working directory. When you’re writing out a file path, the current working directory is at ..\nGoing back to the ls command, you can now see that the default argument to ls is .. You can test this for yourself by comparing the output of ls and ls .. They should be identical.\nAside from / and ., there are two other special directories.\n.. is the parent of the directory you’re in, so you can move to the parent of your current directory using cd .. and to it’s parent with cd ../...\n~ is the home directory of your user (assuming it has one). We’ll get more into what that means in a bit.\n\n\n\n\n\n\nHow does / compare to C:?\n\n\n\nIf you’re a Windows person, you might think this is analogous to C:. You’re not wrong, but the analogy is imprecise.\nIn Linux, everything is a sub-directory of /, irrespective of the configuration of physical or virtual drives that houses the storage. Frequently, people will put extra drives on their server – a process called mounting – and house them at /mnt (short for…you guessed it).\nThat’s different from Windows. In Windows you can have multiple roots, one for each physical or logical disk you’ve got. That’s why your machine may have a D: drive, or if you have network shares, those will often be on M: or N: or P:.\n\n\nAlong with being able to inspect directories, it’s useful to be able to change your working directory with the cd command, short for change directory."
  },
  {
    "objectID": "chapters/sec2/2-3-cmd-line.html#reading-text-files",
    "href": "chapters/sec2/2-3-cmd-line.html#reading-text-files",
    "title": "8  Getting comfortable on the Command Line",
    "section": "8.3 Reading text files",
    "text": "8.3 Reading text files\nBeing comfortable opening and navigating around text files is an essential IT/Admin skill.\ncat is the command to print a file, starting at the beginning.\nSometimes you’ve got a really big file and you want to see just part. less starts at the top with the ability to scroll down. head prints the first few lines and quits. It is especially useful to peer at the beginning of a plain text data file (like csv) as it prints the first few rows and exits – so you can preview the beginning of a very large data file very quickly.\ntail skips right to the end of a file. Log files usually are written so the newest part is last – so much so that “tailing a log file” is a synonym for looking at it. In some cases, you’ll want to tail a file as the process is still running and writing information to the log. You can get a live view of the end of the file using the -f flag (for follow).\nSometimes you want to search around inside a text file. You’re probably familiar with the power of regular expressions (regex) to search for specific character sequences in text strings. The Linux command to do regex searches is grep, which returns results that match the regex pattern you specify.\nThe true power of grep is unlocked in combination with the pipe. The Linux pipe operator – | – takes the output of the previous command and sends it into the next one.\n\n\n\n\n\n\nHaven’t I seen the pipe before?\n\n\n\nThe pipe should feel extremely familiar to R users.\nThe {magrittr} pipe, %&gt;%, has become extremely popular as part of the tidyverse since its introduction in 2013. A base R pipe, |&gt;, was released as a part of R 4.1 in 2021.\nThe original pipe in {magrittr} took inspiration from both the Unix pipe and the pipe operator in the F# programming langauge.\n\n\nA combination I do all the time is to pipe the output of ls into grep when searching for a file inside a directory. So if I was searching for a file whose name contained the word data, that might look something like ls ~/projects/my-project | grep data."
  },
  {
    "objectID": "chapters/sec2/2-3-cmd-line.html#copying-moving-and-removing-files-and-directories",
    "href": "chapters/sec2/2-3-cmd-line.html#copying-moving-and-removing-files-and-directories",
    "title": "8  Getting comfortable on the Command Line",
    "section": "8.4 Copying, moving and removing files and directories",
    "text": "8.4 Copying, moving and removing files and directories\nYou can copy a file from one place to another using the cp command. cp leaves behind the old file and adds the new one at the specified location. You can move a file with the mv command, which does not leave the old file behind.\nIf you want to remove a file entirely, you can use the rm command. There is also a version to remove a directory, rmdir.\n\n\n\n\n\n\nWarning\n\n\n\nBe very careful with the rm command.\nUnlike on your desktop there’s no recycle bin! Things that are deleted are instantly deleted forever.\n\n\nIf you want to make a directory, mkdir makes a directory at the specified filepath. mkdir will only work if it’s creating the entire file path specified, so the -p flag can be handy to create only the parts of the path that don’t exist.\nSometimes it’s useful to operate on every file inside a directory. You can get every file that matches a pattern with the wildcard, *. You can also do partial matching with the wildcard to get all the files that match part of a pattern.\nFor example, let’s say I have a /data directory and I want to put a copy of only the .csv files inside into a new sub-directory. I could do the following:\n&gt; mkdir -p /data/data-copy\n&gt; cp /data/*.csv /data/data-copy"
  },
  {
    "objectID": "chapters/sec2/2-3-cmd-line.html#moving-things-to-and-from-the-server",
    "href": "chapters/sec2/2-3-cmd-line.html#moving-things-to-and-from-the-server",
    "title": "8  Getting comfortable on the Command Line",
    "section": "8.5 Moving things to and from the server",
    "text": "8.5 Moving things to and from the server\nIt’s very common to have a file on your server you want to move to your desktop or vice versa.\nIt’s generally easier to move a single file rather than a whole bunch. The tar command turns a set of files or whole directory into a single archive file, usually with the file suffix .tar.gz. Creating an archive also does some compression when it creates the archive file. The amount depends on the content.\nThe tar command is used to both create and unpack (extract) archive files and telling it which one requires the use of several flags. I never remember them – this is a command I google every time I use it. The flags you’ll use most often are in the cheat sheet below.\nOnce you’ve created an archive file, you’ve got to move it. The scp command is the way to do this. scp – short for secure copy – is basically a combo of SSH and copy.2 scp is particularly nice because it uses the syntax you’re used to from using cp.\nSince scp establishes an SSH connection, you need to make the request to somewhere that is accepting SSH connections. Hopefully your server is accepting SSH connections and your laptop is not.\nYou’ll almost certainly have the experience at some point of being on your server and wanting to scp something to or from your laptop. You need to do the scp command from a regular terminal on your laptop, not one that’s already SSH-ed into your server."
  },
  {
    "objectID": "chapters/sec2/2-3-cmd-line.html#writing-files-on-the-command-line",
    "href": "chapters/sec2/2-3-cmd-line.html#writing-files-on-the-command-line",
    "title": "8  Getting comfortable on the Command Line",
    "section": "8.6 Writing files on the command line",
    "text": "8.6 Writing files on the command line\nThere will be many situations where writing into a text file will be handy while administering your server – for example, when changing config files. When you’re on the command line, you’ll use a command line tool for writing into those files – meaning you’ll only have your keyboard to navigate, save, and exit.\nThere are times when you want to make files or directories with nothing in them – the touch command makes a blank file at the specified file path.\nYou also may want to take some text and make it into a file. You can do this with the &gt; command. &gt;&gt; does the same thing, but appends it to the end of the file. This works similarly to the pipe, |, where the output of the left-hand side is passed as the input to a file on the right-hand side.\nA common reason you might want to do this is to add something to the end of your .gitignore. You can’t just type a word on the command line and have it treated like a string – so you may need the echo command to have something you type treated as a string.\nFor example, if you want to add your .Rprofile file to your .gitignore, you could do that with echo .Rprofile &gt;&gt; .gitignore.\n\n8.6.1 Command line text editors\nThere are two command line text editors you’ll probably encounter – both extremely powerful text editing tools: nano and vi/vim.3\nYou can open a file in either by typing nano &lt;filename&gt; or vi &lt;filename&gt;. Unfortunately for many newbie Linux Admins it’s extremely easy to get stuck inside a file with no hint of how to get out!\nIn nano there will be helpful prompts along the bottom to tell you how to interact with the file, so you’ll see once you’re ready to go, you can exit with ^x. But what is ^? On most keyboards, you can insert the caret character, ^, by pressing Shift + 6. But that’s not what this is.\nIn this case, the ^ caret is short for Ctrl on Windows and for Command (⌘) on Mac. Phew!\nWhere nano gives you helpful – if obscure – hints, vim leaves you all on your own. It doesn’t even tell you you’re inside vim!\nThis is where many people get stuck and end up having to just exit and start a new terminal session. It’s not the end of the world if you do, but knowing a few vim commands can help you avoid that fate.\nOne of the most confusing things about vim is that you can’t edit the file when you first enter. That’s because vim keybindings were (1) developed before keyboards uniformly had arrow keys and (2) are designed to minimize how much your hands need to move.\nIf you feel like taking the time, learning vim keybindings can make navigating and editing text (code) files easier. Plus it just feels really cool. I recommend spending some time trying. In this section, I’m just going to help you get the minimum amount of vim you need to be safe.\nWhen you enter, you’re in normal mode, which is for navigating through the file. Typing things on your keyboard won’t type into the document, but will do other things.\nPressing the i key activates insert mode. For those of us who are comfortable in a word processor like Word or Google Docs, insert mode will feel very natural. You can type and words will appear and you can navigate with the arrow keys.\nOnce you’re done writing, you can go back to normal mode by pressing the escape key. In addition to navigating the file, normal mode allows you to do file operations like saving and quitting.\nFile operations are prefixed with a colon :. The two most common commands you’ll use are save (write) and quit. You can combine these together, so you can save and quit in one command using :wq.\nSometimes you may want to exit without saving. If you’ve made changes and try to exit with :q, you’ll find yourself in an endless loop of warnings that your changes won’t be saved. You can tell vim you mean it with the exclamation mark and exit using :q!."
  },
  {
    "objectID": "chapters/sec2/2-3-cmd-line.html#comprehension-questions",
    "href": "chapters/sec2/2-3-cmd-line.html#comprehension-questions",
    "title": "8  Getting comfortable on the Command Line",
    "section": "8.7 Comprehension Questions",
    "text": "8.7 Comprehension Questions\n\nIf you don’t know the real commands for them, make up what you think the bash commands might be to do the following. So if you think you’d create a command called cmd with a flag -p and an argument arg, you’d write cmd -p &lt;what p does&gt; &lt;arg&gt;. In the next chapter you’ll get to see how close you got to the real thing:\n\nChange Directories, the only argument is where to go\nMaking a Directory, with an optional flag to make parents as you go. The only argument is the directory to make.\nRemove files, with flags to do so recursively and to force it without checking in first. The only argument is the file or directory to remove."
  },
  {
    "objectID": "chapters/sec2/2-3-cmd-line.html#cheat-cli",
    "href": "chapters/sec2/2-3-cmd-line.html#cheat-cli",
    "title": "8  Getting comfortable on the Command Line",
    "section": "8.8 Cheatsheet: Command Line",
    "text": "8.8 Cheatsheet: Command Line\n\n8.8.1 General Command Line\n\n\n\n\n\n\n\nSymbol\nWhat it is\n\n\n\n\nman\nmanual\n\n\nq\nQuit man pages (and many other situations)\n\n\n\\\nContinue command on new line\n\n\n\n\n\n8.8.2 Linux Navigation\n\n\n\n\n\n\n\n\nCommand\nWhat it does/is\nHelpful options\n\n\n\n\n/\nsystem root\n\n\n\n.\ncurrent working directory\n\n\n\nls &lt;dir&gt;\nlist objects in a directory\n-l - format as list\n-a - all (include hidden files)\n\n\npwd\nprint working directory\n\n\n\ncd &lt;dir&gt;\nchange directory\n\n\n\n~\nhome directory of the current user\n\n\n\n\n\n\n8.8.3 Reading Text Files\n\n\n\n\n\n\n\n\nCommand\nWhat it does\nNotes + Helpful options\n\n\n\n\ncat &lt;file&gt;\nPrints a file.\n\n\n\nless &lt;file&gt;\nPrints a file, but just a little.\nCan be very helpful to look at a few rows of csv.\nLazily reads lines, so can be much faster than cat for big files.\n\n\ntail &lt;file&gt;\nLook at the end of a file.\nUseful for logs, where the newest part is last.\nThe -f flag is useful for a live view.\n\n\nhead &lt;file&gt;\nLook at the beginning of a file.\nDefaults to 10 lines, can specify a different number with -n &lt;n&gt;.\n\n\ngrep &lt;expression&gt;\nSearch a file using regex.\nWriting regex can be a pain. I suggest testing expressions on regex101.com.\nOften useful in combination with the pipe.\n\n\n|\nthe pipe\n\n\n\n\n\n\n8.8.4 Manipulating Files\n\n\n\n\n\n\n\n\nCommand\nWhat it does/is\nNotes + Helpful Options\n\n\n\n\nrm &lt;path&gt;\nremove\n-r - recursively remove everything below a file path\n-f - force - don’t ask for each file\nBe very careful, it’s permanent\n\n\nc p &lt;from&gt; &lt;to&gt;\ncopy\n\n\n\nm v &lt;from&gt; &lt;to&gt;\nmove\n\n\n\n*\nwildcard\n\n\n\nmkdir/ rmdir\nmake/remove directory\n-p - create any parts of path that don’t exist\n\n\n\n\n\n8.8.5 Move things to/from server\n\n\n\n\n\n\n\n\nCommand\nWhat it does\nNotes + Helpful options\n\n\n\n\ntar\nco mpress/decompress file/directory\nAlmost always used with flags.\nCreate is usually\ntar -c zf &lt;archive name&gt; &lt;file(s)&gt;\nExtract is usually\ntar -xfv  &lt;archive name&gt;\n\n\nscp\nCopy across ssh\nCan use most ssh flags (like -i and -v)\n\n\n\n\n\n8.8.6 Write files from the command line\n\n\n\n\n\n\n\n\nCommand\nWhat it does\nNotes\n\n\n\n\ntouch\nCreates file if doesn’t already exist.\nUpdates timestamp to current time if it does exist\n\n\n&gt;\nOverwrite file contents\nCreates a new file if it doesn’t exist\n\n\n&gt;&gt;\nConcatenate to end of file\nCreates a new file if it doesn’t exist\n\n\n\n\n\n8.8.7 Command Line Text Editors (Vim + Nano)\n\n\n\n\n\n\n\n\nVim Command\nWhat it does\nNotes + Helpful options\n\n\n\n\n^\nPrefix for file command in nano editor.\nIts the ⌘ or Ctrl key, not the caret symbol.\n\n\ni\nEnter insert mode in vim\n\n\n\nescape\nEnter normal mode in vim.\n\n\n\n:w\nWrite the current file in vim (from normal mode)\nCan be combined to save and quit in one, :wq\n\n\n:q\nQuit vim (from normal mode)\n:q! quit without saving"
  },
  {
    "objectID": "chapters/sec2/2-3-cmd-line.html#footnotes",
    "href": "chapters/sec2/2-3-cmd-line.html#footnotes",
    "title": "8  Getting comfortable on the Command Line",
    "section": "",
    "text": "It’s interesting to note that this is also true on your computer - when you open a program, a particular user is running a program on your computer and opening a GUI window for you to interact with. The point-and-clicking obfuscates this, but it’s still true.↩︎\nIt’s worth noting that scp is now considered “insecure and outdated”. The ways it is insecure are rather obscure and not terribly relevant for most people. But if you’re moving a lot of data, you may want something faster. If so, I’d recommend more modern options like sftp and rsync. I probably wouldn’t bother if you’re only occasionally scp-ing small files to or from your server.↩︎\nvi is the original fullscreen text editor for Linux. vim is its successor (vim stands for vi improved). For our purposes, they’re completely interchangeable.↩︎"
  },
  {
    "objectID": "chapters/sec2/2-4-docker.html#container-lifecycle",
    "href": "chapters/sec2/2-4-docker.html#container-lifecycle",
    "title": "9  Demystifying Docker",
    "section": "9.1 Container Lifecycle",
    "text": "9.1 Container Lifecycle\nLet’s spend just a minute digging into the the lifecycle of a Docker container.\nThis image explains the different states a Docker container can be in, and the commands you’ll need to move them around.\nTODO: change Docker Container to Docker Container Instance, add 3 vertical stages: build, move, run\n\nThe most pure distillation of a Docker Container is the Docker Image. An image is an immutable snapshot of a container, suspended in time. It is totally possible to have multiple similar images, but images are immutable by definition. Any changes would make it a different image.\nThe most common place to store images is in a Docker Registry, but they can also be moved around like regular files.\nDocker Registries are quite similar to git repositories. Some organizations run their own registries, most often using offerings from cloud providers.1 The Docker company also runs Docker Hub, which allows public hosting and private hosting of images in free and paid tiers.\nImages have names that consist of an id and a tag. If you’re using the public Docker Hub registry, container ids take the form &lt;user&gt;/&lt;name&gt;, so my penguin model container is alexkgold/penguin-model.\nTags are used to specify versions – sometimes of the container itself, but also sometimes of the software it contains. If you don’t specify a tag, it defaults to latest. For example, the rocker/r-ver container, which is a container pre-built with a version of R in it uses tags for the version of R.\nBefore the Docker Image exists, it has to get built somehow. It is possible, but not recommended, to interactively build a Docker Container. Instead, containers are usually built from a Dockerfile. Dockerfiles are just code, so they’re often stored in a git repository and it’s common to build and store them with CI/CD.\nA Docker Container Instance is the Docker Container running somewhere at a given point in time. It’s the thing you’ll actually interact with as a running service.\n\n\n\n\n\n\nNote\n\n\n\nI’ve included docker pull on the graphic for completeness, but you’ll almost never run it. docker run auto-pulls the container(s) it needs.\n\n\nYou can control Docker Containers from the Docker Desktop app. But if you’re going to be using Docker on a server, you’ll mostly be interacting via the CLI. There’s a cheatsheet for Docker CLI commands at the end of this chapter.\n\n9.1.1 Flags for docker run\nYou run a container by using the docker run command on the command line.\nAt its most basic, docker run &lt;image name&gt; will give you a running container. However, Docker commands usually use a lot of command line flags – so many that it’s easy to miss what the command actually is.\nOne of the things to consider about running a Docker container is how restricted its access is. By default, the container can’t access anything from the host where it’s running. This is a great security feature, but if you want to make data or code available from your host inside the container – or you want to run the container to host an app or API, you’ll need to include those permissions in your docker run command.\nOne of the important things to include in your mental model is that the container has its own networking and file system. For example, the -v flag makes a volume (directory) from outside the container available inside.\nSo let’s say you’ve got some data in your ~/Documents/my-project/data directory. You might just want to make it available to the container at /data or /mnt/data. That’s easy to do with the -v command: -v ~/Documents/my-project/data:/data.\nTODO: slashes are wrong."
  },
  {
    "objectID": "chapters/sec2/2-4-docker.html#building-from-dockerfiles",
    "href": "chapters/sec2/2-4-docker.html#building-from-dockerfiles",
    "title": "9  Demystifying Docker",
    "section": "9.2 Building from Dockerfiles",
    "text": "9.2 Building from Dockerfiles\nA Dockerfile is a set of instructions that you use to build a Docker Image. Dockerfiles have a particular syntax, but it’s quite readable and if you know how to accomplish something on a running machine, you shouldn’t have too much trouble building a Dockerfile to do the same.\nThe basic notion to have in mind is building up layers – each one being a usable container.\nYour first layer is always the container you’re starting from – sometimes a barebones operating system-only container like ubuntu:latest and sometimes a much richer starting point like rocker/shiny or python/3.10.\nThen you’ll take action on top of that layer, installing some software, copying in files or directories, or telling the container what to do when it starts.\n\n\n\n\n\n\nNote\n\n\n\nOne of the nicest conveniences of building Docker Containers is that Docker understands your container’s layers and will only rebuild changed layers and subsequent ones.\nFor example, let’s say you have a Dockerfile with 3 layers – a starting container, copy in some files, and setting the run time command. If you change the run time command in your Dockerfile and rebuild, the build process will start from the container after everything’s been copied in. This drastically speeds up the container development process.\n\n\nAs you design your Dockerfile, you’ll have to think about what should get put into the container at build time. Your container gets frozen in time during the build, so that’s when you want to put everything the container needs into the container. Moreover, the building script has pretty much free-regin over your system, so that’s when you’ll do things like setting up the versions of R and Python, copying in your project code, and installing system requirements.\n\n9.2.1 Dockerfile Commands\nThe Dockerfile documentation provides many different commands you can use during build, but here are the handful that are enough to build most images you might need.\n\nFROM – specify the base image.\nRUN – run any command as if you were sitting at the command line inside the container. Just remember, if you’re starting from a very basic container, you may need to make a command available before you can run it (like wget in my container below).\nCOPY – copy a file from the host filesystem into the container.\nCMD - Specifies what command to run on the container’s shell when it runs. This would be the same command you’d use to run your data science project from the command line.2\n\nOnce you’ve created your Dockerfile, you build it into an image using docker build -t &lt;image name&gt; &lt;build directory&gt;. If you don’t provide a tag, the default tag is latest.\nYou can then push that to DockerHub or another registry using docker push &lt;image name&gt;."
  },
  {
    "objectID": "chapters/sec2/2-4-docker.html#comprehension-questions",
    "href": "chapters/sec2/2-4-docker.html#comprehension-questions",
    "title": "9  Demystifying Docker",
    "section": "9.3 Comprehension Questions",
    "text": "9.3 Comprehension Questions\n\nWhat does using a Docker container for a data science project make easier? What does it make harder?\nDraw a mental map of the relationship between the following: Dockerfile, Docker Image, Docker Registry, Docker Container\nWhen would you want to use each of the following flags for docker run? When wouldn’t you?\n\n-p, --name, -d, --rm, -v\n\nWhat are the most important Dockerfile commands?"
  },
  {
    "objectID": "chapters/sec2/2-4-docker.html#lab-putting-an-api-in-a-container",
    "href": "chapters/sec2/2-4-docker.html#lab-putting-an-api-in-a-container",
    "title": "9  Demystifying Docker",
    "section": "9.4 Lab: Putting an API in a Container",
    "text": "9.4 Lab: Putting an API in a Container\nLet’s put our Penguin model prediction API from Chapter 2 into a container.\nAgain, Vetiver has some nice tooling to make this very easy to do. You should follow the instructions on the {vetiver} website for how to generate your Dockerfile.\nOne thing to note about this container – it follows best practices for how to put data (in this case the machine-learning model) into the container. That means the model isn’t built into the container. Instead, the container knows how to fetch the model.\nI’m using this code to generate my Dockerfile\nfrom pins import board_folder\nfrom vetiver import prepare_docker\n\nboard = board_folder(\"/data/model\", allow_pickle_read=True)\nprepare_docker(board, \"penguin_model\", \"docker\")\nOnce you’ve generated your Dockerfile, take a look at it. Here’s the one for my model:\n\n\nDockerfile\n\n# # Generated by the vetiver package; edit with care\n# start with python base image\nFROM python:3.9\n\n# create directory in container for vetiver files\nWORKDIR /vetiver\n\n# copy  and install requirements\nCOPY vetiver_requirements.txt /vetiver/requirements.txt\n\n#\nRUN pip install --no-cache-dir --upgrade -r /vetiver/requirements.txt\n\n# copy app file\nCOPY app.py /vetiver/app/app.py\n\n# expose port\nEXPOSE 8080\n\n# run vetiver API\nCMD [\"uvicorn\", \"app.app:api\", \"--host\", \"0.0.0.0\", \"--port\", \"8080\"]\n\nThis auto-generated Dockerfile is very nicely commented, so its easy to follow.\nNow build the container using docker build -t penguin-model ..\nOk, now let’s run the container using\ndocker run --rm -d \\\n  -p 8080:8080 \\\n  --name penguin-model \\\n  penguin-model-local\nIf you go to http://localhost:8080 you’ll find that…it doesn’t work? Why? If you run the container attached (remove the -d from the run command) you’ll get some feedback that might be helpful.\nIn line 15 of the Dockerfile, we copied the app.py in to the container. Let’s take a look at that file to see if we can find any hints.\n\n\napp.py\n\nfrom vetiver import VetiverModel\nimport vetiver\nimport pins\n\n\nb = pins.board_folder('./model', allow_pickle_read=True)\nv = VetiverModel.from_pin(b, 'penguin_model', version = '20230422T102952Z-cb1f9')\n\nvetiver_api = vetiver.VetiverAPI(v)\napi = vetiver_api.app\n\nLook at that (very long) line 6. The API is connecting to a local directory to pull the model. Is your spidey sense tingling? Something about container filesystem vs host filesystem?\nThat’s right – we put our model at /data/model on our host machine. But the API inside the container is looking for /data/model inside the container, which doesn’t exist!\nThis is a case where we need to mount a volume into the container like so\ndocker run --rm -d \\\n  -p 8080:8080 \\\n  --name penguin-model \\\n  -v /data/model:/data/model \\\n  penguin-model-local\nAnd NOW you should be able to get your model up in no time.\nIt’s worth noting that it’s somewhat rare that you want to put the volume at the same location inside the container as it is outside. But because of the {vetiver} tooling, it expects it to be at the same place.\nNow, thinking back to Chapter 3, we’re going to run into some trouble when when and if we move the API off of our laptop and onto a server. We’ll address that in Chapter 10.\n\n9.4.1 Lab Extension\nRight now, logs from the API just stay inside the app. But that means that the logs go away when the container does. That’s obviously bad if the container dies because something goes wrong. How might you make sure that the container’s logs get written somewhere more permanent?"
  },
  {
    "objectID": "chapters/sec2/2-4-docker.html#cheat-docker",
    "href": "chapters/sec2/2-4-docker.html#cheat-docker",
    "title": "9  Demystifying Docker",
    "section": "9.5 Cheatsheet: Docker",
    "text": "9.5 Cheatsheet: Docker\n\n9.5.1 Docker CLI Commands\n\n\n\n\n\n\n\n\n\nStage\nCommand\nWhat it does\nNotes and helpful options\n\n\n\n\nBuild\ndocker build &lt;directory&gt;\nBuilds a directory containing a Dockerfile into an image.\n-t &lt;name:tag&gt; provide a name to the container. tag is optional, defaults to latest.\n\n\nMove\ndocker push &lt;image&gt;\nPush a container to a registry.\n\n\n\nRun\ndocker run &lt;image&gt;\nRun a container.\nSee section below.\n\n\nRun\ndocker stop &lt;container&gt;\nStop a running container.\ndocker kill can be used if stop fails.\n\n\nRun\ndocker ps\nList containers.\nUseful to get running container id to do things to it.\n\n\nRun\ndocker exec &lt;container&gt; &lt;command&gt;\nRun a command inside a running container.\nBasically always used to open a shell in a container with docker exec -it &lt;container&gt; /bin/bash\n\n\nRun\ndocker logs &lt;container&gt;\n\n\n\n\n\n\n\n9.5.2 Flags for docker run\n\nReminder: Order for -p and -v is &lt;host&gt;:&lt;container&gt;\n\n\n\n\n\n\n\nFlag\nEffect\nNotes\n\n\n\n\n--name &lt;name&gt;\nGive a name to the running container.\nOptional. Will be auto-assigned if not provided. Then you’ll need to discover name with docker ps.\n\n\n--rm\nRemove container when its stopped.\nVery useful when playing around because it cleans things up. Almost never used in prod, because you usually want to be able to inspect logs from old containers.\n\n\n-d\nDetach container (don’t block the terminal).\nAlmost always used in production. Sometimes useful to stay attached when troubleshooting.\n\n\n-p &lt;port&gt;:&lt;port&gt;\nPublish port from inside container to outside.\nNeeded if you want to access anything inside the container with HTTP.\n\n\n-v &lt;dir&gt;:&lt;dir&gt;\nMount volume into the container.\nMake a directory (volume) on the host available to the container for reading and writing.\n\n\n\n\n\n9.5.3 Dockerfile Commands\nThese are the commands that go in a Dockerfile when you’re building it.\n\n\n\n\n\n\n\n\nCommand\nPurpose\nExample\n\n\n\n\nFROM\nIndicate base container\nFROM rocker/r-ver:4.1.0\n\n\nRUN\nRun a command when building\nRUN apt-get update\n\n\nCOPY\nCopy from build directory into the container\nCOPY . /app/\n\n\nCMD\nSpecify the command to run when the container starts\nCMD quarto render ."
  },
  {
    "objectID": "chapters/sec2/2-4-docker.html#footnotes",
    "href": "chapters/sec2/2-4-docker.html#footnotes",
    "title": "9  Demystifying Docker",
    "section": "",
    "text": "The big three container registries are AWS Elastic Container Registry (ECR), Azure Container Registry, and Google Container Registry.↩︎\nYou may also see ENTRYPOINT, which sets the command CMD runs against. Usually the default /bin/sh -c to run CMD in the shell will be the right choice.↩︎"
  },
  {
    "objectID": "chapters/sec3/3-0-sec-intro.html#labs-in-this-section",
    "href": "chapters/sec3/3-0-sec-intro.html#labs-in-this-section",
    "title": "IT/Admin for Data Science",
    "section": "Labs in this Section",
    "text": "Labs in this Section\nEach chapter in this section has a lab. This section is about building out the backend around the website we created in the first section of the book.\nIn this section you’ll stand up an EC2 instance to serve as a workbench, add tools to work in R or Python, and host your Dockerized model and Shiny app there. We’ll also add the Shiny app to the website you created and make sure all the plumbing gets hooked up.\nFor more details on what you’ll do in each chapter, see Appendix B."
  },
  {
    "objectID": "chapters/sec3/3-1-cloud.html#layers-of-cloud-services",
    "href": "chapters/sec3/3-1-cloud.html#layers-of-cloud-services",
    "title": "10  Getting Started with the Cloud",
    "section": "10.1 Layers of cloud services",
    "text": "10.1 Layers of cloud services\nWhile the basic premise of the cloud is rent a server, there are layers and layers of services on top of that basic premise. Depending on your organization’s needs, it may make sense to rent a very basic service, or a higher level one.\nLet’s talk about cakes to help make the levels clear. This year, I’m planning to bake and decorate a beautiful cake for my friend’s birthday. It’s going to be a round layer cake with white frosting, and it’s going to say “Happy Birthday!” in teal with giant lavender frosting rosettes.\nNow that I’ve decided what I’m making, I have a few different options for how I get there. If I’m a real DIY kind of person, I could buy all the ingredients from the grocery store and make everything from scratch. Or I could buy a cake mix – reducing the likelihood I’ll buy the wrong ingredients or end up with unnecessary leftovers. Or maybe I don’t want to bake at all – I could just buy a premade cake already covered in white frosting and just do the writing and the rosettes.\nThe choice of how much to DIY my friend’s birthday cake and how much to get from a bakery is very much akin to your choices when buying server capacity from the cloud.\nTODO: Image of getting cake w/ IaaS, PaaS, SaaS\nIn the US, a huge fraction of server workloads run on servers rented from one of the “big three clouds” – Amazon Web Services (AWS), Microsoft Azure, and Google Cloud Platform (GCP). There are also numerous smaller cloud providers, many of which target more specific use cases.\nIt’s totally possible to just rent the raw ingredients from these cloud providers – they’ll happily rent you the most basic cloud services like servers, networking, and storage and let you put them together yourself. This kind of cloud service – the equivalent of baking from scratch – is called Infrastructure as a Service (IaaS, pronounced eye-az).\nBut then I’m responsible for managing all of the reproducibility stack – I have to make sure the servers are up to date with new operating systems and security patches, while also ensuring there are modern versions of R and Python on the system, and making sure there are RStudio and Jupyter Notebooks present.\nBut just as you might not want to have to buy eggs, milk, and sugar just to end up with a pretty cake, IT/Admin organizations are increasingly taking advantage of the cloud equivalent of cake mixes and premade blank sheet cakes.\nIf I want to offload more of the work, I can think about a Platform as a Service (PaaS – pronounced pass) solution. This would give me the ability to somehow define an environment and send it somewhere to run without worrying about the underlying servers. For example, I might want to build a docker container with Python and Jupyter Notebook and host it somewhere that autoscales for the number of users and the amount of other resources needed.\nThere are a bunch of different entities you can run in this way, and services to match each. For example, if you want to run a general app, you can use something like AWS’s Elastic Beanstalk to run a user-facing app that dynamically scales (TODO: Azure/GCP equivalents), if you want to run a container, you might use XXXX, and if you want to get a Kubernetes cluster as a service, you might use XXXX. If you want to just run a function, there’s always AWS Lambda.\n\n\n\n\n\n\nCaution\n\n\n\nI am not recommending you run a data science workbench on a service like Elastic Beanstalk. This actually tends not to work very well, for reasons we’ll get into in a bit.\n\n\nIf I want to go one level more abstracted, I might want to go with a Software as a Service (SaaS - pronounced sass) solution. This would be something like one of the cloud providers hosted machine learning environment where you can train, deploy, and monitor machine learning models. AWS has SageMaker, Azure has Azure Machine Learning, and GCP has Vertex AI. And there are organizations that host popular data science tools like Posit.cloud and Saturn Cloud that you can use.\nSometimes you’ll hear people describe PaaS or SaaS solutions called “going serverless”.\nThe first thing to understand about serverless computing is that there is no such thing as serverless computing.\nEvery bit of cloud computation runs on a server - the question is whether you have to deal with the server or if you just deal with a preconfigured service.\nIn this book, we’ll be working exclusively with IaaS services – taking the most basic services and building up a working data science environment. Once you understand how these pieces fit together, you’ll be in a much better place if it turns out your organization wants to leverage more abstracted versions of those services."
  },
  {
    "objectID": "chapters/sec3/3-1-cloud.html#common-services-for-data-science-usage",
    "href": "chapters/sec3/3-1-cloud.html#common-services-for-data-science-usage",
    "title": "10  Getting Started with the Cloud",
    "section": "10.2 Common services for data science usage",
    "text": "10.2 Common services for data science usage\nIf you’re not familiar with cloud provider terminology, it can be very hard to tell what service you might need from a cloud provider, and they don’t really help the matter. It’s very common (especially in AWS) to have many different services that fulfill similar needs, and it can be really hard to concretely tell the difference.\nMaking the issue even more difficult, many companies go out of their way to make their services sound grand and important and don’t just say, “this is a ___ you can rent”.\nIt’s helpful to keep in mind that at the very bottom, all you’re doing is renting servers and storage and managing networking and permissions for the servers and storage. Every other service is just a combination of server, storage, networking, and permissions that comes with software pre-installed or configured to make your life easier.4\nWe’re going to talk through some of the basic services that are offered by each of the three major cloud platforms.\n\n\n\n\n\n\nCloud service naming\n\n\n\nAzure and GCP tend to name their offerings pretty literally, while AWS chooses cutesy names that have, at best, a tangential relationship to the task at hand. I’m going to try to name the services for each of the purposes I’m talking about, but it’s worth noting that feature sets aren’t exactly parallel across cloud providers.\nThis makes AWS names a little harder to learn, but much easier to recall once you’ve learned them. In this section – contrary to standard practice – I’m going to use the common abbreviated names for AWS services and put the full name in parentheses as these are just trivia.\n\n\nRemember, at the bottom of all cloud services are servers and each of cloud service provider has a service that is “just rent me a server”. AWS has EC2 (Elastic Cloud Compute) and Azure has Azure VMs, and Google has Google Compute Engine.\nAlong with servers, there are two main kinds of storage you’ll rent. The first is file storage, where you’ll store things in a file directory like on your laptop. These are AWS’s EBS (Elastic Block Store), Azure Managed Disk, and Google Persistent Disk.\nThe major cloud providers all also have blob (Binary Large Object) storage. Blob storage allows you to store individual objects somewhere and recall them to any other machine that has access to the blob store. The major blob stores are AWS Simple Storage Service (S3), Azure Blob Storage, and Google Cloud Storage.\nThere are also two important networking services for each of the clouds – a way to make a private network and a way to do DNS routing. If you don’t know what these mean, there will be a lot more detail in Chapter 12. For now, it’s enough to know that private networking is done in AWS’s VPC (Virtual Private Cloud), Azure’s Virtual Network, and Google’s Virtual Private Cloud. DNS is done in AWS’s Route 53, Azure DNS, and Google Cloud DNS.\nOnce you’ve got all this stuff up and running, you need to make sure that permissions are set in the right way. AWS has IAM (Identity and Access Management), Azure has Azure Active Directory, and Google has Identity Access Management.\nNow, there are a variety of things you might want to do past these basic tasks of server, storage, networking, and access management. Here are a few more services you’ll likely hear about over time.\n\n\n\n\n\n\n\n\n\nService\nAWS\nAzure\nGCP\n\n\n\n\nKubernetes cluster\nEKS (Elastic Kubernetes Service)\nAKS (Azure Kubernetes Service)\nGKE (Google Kubernetes Engine)\n\n\nRun a function as an API\nLambda\nAzure Functions\nGoogle Cloud Functions\n\n\nDatabase\nRDS/Redshift5\nAzure Database\nGoogle Cloud SQL\n\n\nML Platform\nSageMaker\nAzure ML\nVertex AI"
  },
  {
    "objectID": "chapters/sec3/3-1-cloud.html#comprehension-questions",
    "href": "chapters/sec3/3-1-cloud.html#comprehension-questions",
    "title": "10  Getting Started with the Cloud",
    "section": "10.3 Comprehension Questions",
    "text": "10.3 Comprehension Questions\n\nWhat is the difference between PaaS, IaaS, and SaaS? What’s an example of each that you’re familiar with?\nWhat are the names for AWS’s services for: renting a server, file system storage, blob storage"
  },
  {
    "objectID": "chapters/sec3/3-1-cloud.html#lab-intro-to-aws",
    "href": "chapters/sec3/3-1-cloud.html#lab-intro-to-aws",
    "title": "10  Getting Started with the Cloud",
    "section": "10.4 Lab: Intro to AWS",
    "text": "10.4 Lab: Intro to AWS\nWelcome to the lab!\nThe point of these exercises is to get you hands on with running servers and get you practicing the things you’re learning in the rest of the book.\nIf you walk through the labs sequentially, you’ll end up with a working data science workbench. It won’t suffice for any enterprise-level requirements, but it’ll be secure enough for a hobby project or even a small team.\nThese days most people are using servers from a cloud provider. We’re going to use AWS, as they’re by far the biggest cloud provider and the one you’re most likely to run into in the real world.\nAt a high level, you can do all of this with any cloud provider. Feel free to try if you prefer Azure or GCP, but the details on how to get started and service names will be different.\nIn the lab for this chapter, we’re going to get you up and running with an AWS account and show you how to manage, start, and stop EC2 instances in AWS.\nThe server we’ll stand up will be from AWS’s free tier – so there will be no cost involved as long as you haven’t used up all your AWS free tier credits before now.\nFor AWS things, I’m going to tell you what to do…but not the exact buttons, as AWS frequently changes the interface and this will almost certainly be out-of-date by the time you read it. The high level steps won’t change though.\nThroughout the lab, we’re going to go step by step through getting things done. If you know how to do any of the steps, I’d recommend you at least skim the step for any important details.\n\n10.4.1 Stand up an EC2 instance\n\n10.4.1.1 Step 1: Login to the AWS Console\nWe’re going to start by logging into AWS.\n\n\n\n\n\n\nNote\n\n\n\nAn AWS account is separate from an Amazon account for ordering stuff online and watching movies. If you’ve never created an account for AWS in particular, you’ll have to make an account.\n\n\nStart by signing in at https://aws.amazon.com. You most likely haven’t created an IAM account and should just login as root.\nOnce you’ve logged in, you’ll be confronted by the AWS console. There are a ton of things here – feel free to poke around if you want – come back to continue when you’re ready.\n\n\n10.4.1.2 Step 2: Stand up an instance\nAWS’s most basic “rent a server by the hour” service is called EC2 (Elastic Cloud Compute).\nThere are five things we have to configure in the process of launching the server. I’ll give you advice on each of them. If I don’t mention it here, feel free to stick with defaults.\n\n10.4.1.2.1 Name + Tags\nThe first choice is instance name and tags. They aren’t required, but I’d recommend you name the server something like do4ds-lab so you can remember what this server is later.\n\n\n10.4.1.2.2 Image\nNext, you’ll have to choose the image. An image is a snapshot of a system and serves as the starting point for your server. Images can range from just a bare operating system to a running RStudio instance that’s paused in between two computations.\nAll clouds have the concept of images. AWS calls their’s Amazon Machine Images or AMIs. Some images are free and some are paid.\nSince we’re going to work on configuring the server from the ground up, we’re going to choose an AMI that’s just the operating system. For the lab, let’s use Ubuntu Server LTS 22.04 image. It should say free tier eligible.\n\n\n\n\n\n\nNote\n\n\n\nIf you want to use a different operating system, that’s fine. For the purpose of being able to follow along with this lab, I’d recommend using Ubuntu 22.04.\nIf you happen to be reading this after 2026 or so, there will be newer operating systems. A newer version of Ubuntu should work very similarly for the purposes of this lab.\n\n\n\n\n10.4.1.2.3 Instance Type\nYou’ll have to choose an Instance Type. In AWS, there are two components to instance type – the family and the size. The family is the category of server that you’re using. In AWS, families are denoted by letters and numbers, so there are T2s and T3s, C4s, R5s, and many more.\nWithin each family, there are different sizes. The sizes availble vary by the family, but generally range from nano to multiples of xlarge like 24xlarge.\nFor now, I’d recommend you get the largest server that is free tier eligible, which is a t2.micro with 1 CPU and 1 Gb of memory as of this writing.\n\n\n\n\n\n\nServer sizing for the lab\n\n\n\nA t2.micro with 1 CPU and 1 Gb of memory is a very small server. For example, your laptop probably has at least 8 CPUs and 8 Gb of memory.\nIf all you’re doing is walking through the lab, it should be sufficient, but if you actually want to do any data science work, you’ll need a substantially larger server.\nIf you want to pick a server for actual data science work, there’s lots of advice on how to choose in Chaper 15.\n\n\n\n\n10.4.1.2.4 Keypair\nNext, you’ll need to make sure you have a keypair. A keypair is a special kind of SSH key that’s only valid for a particular server.\nIf you aren’t sure what that means, I’d recommend you check out Chapter 8.\nEven if you have a keypair you already use, I’d suggest creating a new one and naming it do4ds-lab-key. Download the pem version of your key.\nI’ll use that name throughout the lab, so you can just copy commands straight from this book if you use the same name.\nI’d recommend creating a directory for this lab, perhaps something like do4ds-lab and putting your keypair there. If you’re not going to do that, just make sure you keep track of where you downloaded it.\n\n\n10.4.1.2.5 Storage\nFor storage, you should bump it up to something more than 8 Gb. As of this writing, you can get 30 Gb under the free tier. I’d recommend at least 16 Gb, but feel free to go bigger if you wish.\n\n\n\n10.4.1.3 Step 3: Start your Server\nIf you followed these instructions, the summary on the right side should look something like this:\n\n\n\n\n\nClick Launch Instance. AWS is now creating a virtual server just for you.\nIf you go back to the EC2 page and click on Instances you can see your instance as it comes up. You may need to remove the filter for State: Running since it’ll take a few moments to be Running.\n\n\n10.4.1.4 Step 4: Grab the address of your server\nIf you click on the actual instance ID in blue, you can see all the details of your server.\nWait for a few moments for the instance state to transition to Running.\nOnce the instance is up, you’ll be able to see the instance ID and public IP addresses, which were auto-assigned.\nCopy the Public IPv4 DNS address , which starts with ec2- and ends with amazonaws.com. That little icon on the left of the address copies it.\n\n\n\n\n\n\nSet a Server Address Variable\n\n\n\nIn the rest of the labs in this book, I’m going to write the commands using the bash variable SERVER_ADDRESS. That means that if you create that variable, you’ll be able to just copy the commands out of the book.\nFor example, as I write this, my server has the address ec2-54-159-134-39.compute-1.amazonaws.com. So would set my server address variable with SERVER_ADDRESS=ec2-54-159-134-39.compute-1.amazonaws.com.\nIf you’re used to R or Python, where it’s best practice to put spaces around =, notice that assigning variables in bash requires no spaces around =.\n\n\n\n\n10.4.1.5 Step 5: Stop the Server\nWhenever you’re stopping for the day, you may want to suspend your server so you’re not paying for it overnight or using up your free tier hours. You can suspend an instance in the state it’s in so it can be restarted later.\nIf you’re storing a lot of data with your instance, it may not be free – but it is quite cheap. In the free tier, a suspended instance should be free for some time.\nWhenever you want to suspend your instance, go to the EC2 page for your server. Under the Instance State drop down in the upper right, choose Stop Instance.\nAfter a couple minutes the instance will stop. Before you come back to the next lab, you’ll need to start the instance back up so it’s ready to go.\n\n\n\n\n\n\nNote\n\n\n\nIf you stop your server, the server address will change and you’ll have to use the new one. In Chapter 13, we’ll get into getting a stable IP address and URL for the server.\n\n\nIf you want to completely delete the instance at any point, you can choose to Terminate Instance from that same Instance State dropdown.\n\n\n\n10.4.2 Step 6: Put the penguins data and model in S3\nAs discussed in Chapter 3, we’re going to want to put our data and our model somewhere other than on our local machine.\nWhile this data doesn’t get updated, you might want to be able to update the data and the model periodically.\nA bucket is a great place to store non-tabular data like a model. AWS’s bucket storage is called S3. You’ll probably find yourself using it a lot. It can also store flat files to be used by DuckDB.\nFor the model, {vetiver} interfaces with another package called {pins} which makes it easy to store data in a variety of backends, including an s3 bucket.\nFirst, you’ll need to create an S3 bucket in the AWS console. Give it a name that’s descriptive and memorable – it will also have to be unique. I named mine do4ds-lab. You’ll have to choose the AWS region where your bucket is located – take note of this.\nNow, let’s change our code to write to the S3 bucket we created. Vetiver makes this very easy. All you have to do to move the model into S3 is to change the line that creates the bucket in your code so instead of using a board_temp, it uses board_s3.\n\n\nmodel.qmd\n\nfrom vetiver import VetiverModel\nv = VetiverModel(model, model_name='penguin_model', prototype_data=X)\n```\n\n\nThat line will now become\n\n\nmodel.qmd\n\nfrom vetiver import VetiverAPI\napp = VetiverAPI(v, check_prototype = True)\n```\n\nNow, if you try this line in your code, it won’t work. Why? Because of authentication. The bucket we’re using isn’t public and AWS needs to know that we’re actually allowed to access it.\nIn this case, we’re going to use environment variables to give access to the S3 bucket. If you play around a little, you’ll discover that {pins} does authentication via the s3fs package, which can do authentication in several different ways. We’re going to use environment variables because they work nicely locally and from GitHub Actions.\nThe 3 environment variables are AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, and AWS_REGION. You can get the access key and secret access key from the AWS console.\nOnce you’ve done that, create a file in the directory where your project is called .env. In that file you’ll put the access keys, along with the AWS region where you put your bucket.\nIt’ll look something like this6\n\n\n.env\n\nAWS_ACCESS_KEY_ID=AJAASJKD88ALLKAIS8A\nAWS_SECRET_ACCESS_KEY=0JAJSduasdjkASDLISjkasd8AD78\nAWS_REGION=us-east-1\n\nNow, add a block to the code that loads in the environment variables.\n\n\nmodel.qmd\n\n\nmodel_board = board_folder(\"/data/model\", allow_pickle_read = True)\nvetiver_pin_write(model_board, v)\n\nIf these variables are in the environment, the call to board_s3 should just work locally.\nWe have one more step to make this work so that the model can also be written from GitHub actions. GitHub Actions needs to know how to access those environment variables too.\nSo in the Render and Publish step, we’re going to declare those variables as well as environment variables.\nOnce you’re done, that section of the publish.yml should look something like this.\n\n\n.github/workflows/publish.yml\n\non:\n  workflow_dispatch:\n  push:\n    branches: main\n\nname: Quarto Publish\n\njobs:\n  build-deploy:\n    runs-on: ubuntu-latest\n    permissions:\n      contents: write\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v2\n\n      - name: Set up Quarto\n        uses: quarto-dev/quarto-actions/setup@v2\n\n      - name: Install R\n        uses: r-lib/actions/setup-r@v2\n        with:\n          r-version: '4.2.0'\n          use-public-rspm: true\n\n      - name: Setup renv and install packages\n        uses: r-lib/actions/setup-renv@v2\n        with:\n          cache-version: 1\n        env:\n          RENV_CONFIG_REPOS_OVERRIDE: https://packagemanager.rstudio.com/all/latest\n\n      - name: Install Python and Dependencies\n        uses: actions/setup-python@v4\n        with:\n          python-version: '3.10'\n          cache: 'pip'\n      - run: pip install jupyter\n      - run: pip install -r requirements.txt\n\n      - name: Render and Publish\n        uses: quarto-dev/quarto-actions/publish@v2\n        with:\n          target: gh-pages\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}\n          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n          AWS_REGION: us-east-1\n\nNow, unlike the GITHUB_TOKEN secret, which GitHub Actions automatically provides to itself, we’ll have to provide these secrets to the GitHub interface.As of this writing, you can provide them under the repository settings, secrets and variables, actions.\nThe other thing you’ll need to do is to update the API to use the S3 pin. Luckily, this is very easy. Just update the script you used to build your Dockerfile so it pulls from the pin in the S3 bucket rather than the local folder.\nNow, the script to build the Dockerfile looks like this:\n---\ntitle: \"Prepare Dockerfile\"\nformat:\n  html:\n    code-fold: true\n---\n\n## Load Environment\n\n```{python}\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nfrom pins import board_s3\nfrom vetiver import vetiver_prepare_docker\n\nboard = board_s3(\"do4ds-lab\", allow_pickle_read=True)\nvetiver_prepare_docker(board, \"penguin_model\")\n```\nThere are a variety of ways to put the data into S3 as well. I would recommend doing this if you are going to be working with data that will update. In fact, DuckDB allows you to directly interface with parquet files on S3."
  },
  {
    "objectID": "chapters/sec3/3-1-cloud.html#footnotes",
    "href": "chapters/sec3/3-1-cloud.html#footnotes",
    "title": "10  Getting Started with the Cloud",
    "section": "",
    "text": "https://www.visualcapitalist.com/aws-powering-the-internet-and-amazons-profits/↩︎\nhttps://a16z.com/2021/05/27/cost-of-cloud-paradox-market-cap-cloud-lifecycle-scale-growth-repatriation-optimization/↩︎\nBy temporary workloads, I mean for example the need for a very large server to run a particular machine learning model. This is not the autoscaling failure case, because you’re not expecting this server to turn itself on and off. Instead, you’d stand it up – run it for a few hours, days, or weeks while you’re using it, and then manually turn it off. This is super easy with a cloud server and very difficult in an on-prem world.↩︎\nThere are also some wild services that do specific things, like let you rent you satellite ground station infrastructure or do Internet of Things (IoT) workloads. Those services are really cool, but so far outside the scope of this book that I’m fine with talking like they don’t exist.↩︎\nThere are many others. These are just the most popular I’ve seen for data science use cases.↩︎\nNot my real access key or secret access key. Never share them.↩︎"
  },
  {
    "objectID": "chapters/sec3/3-2-linux-admin.html#linux-is-an-operating-system-with-a-long-history",
    "href": "chapters/sec3/3-2-linux-admin.html#linux-is-an-operating-system-with-a-long-history",
    "title": "11  Basic Linux SysAdmin",
    "section": "11.1 Linux is an operating system with a long history",
    "text": "11.1 Linux is an operating system with a long history\nA computer’s operating system (OS) defines how applications – like Microsoft Word, RStudio, and Minecraft – interact with the underlying hardware to actually do computation. OSes define how files are stored and accessed, how applications are installed and can connect to networks, and more.\nThese days, basically all computers run on one of a few different operating systems – Windows, MacOS, or Linux for laptops and desktops; Windows or Linux for servers, Android (a flavor of Linux) or iOS for phones and tablets, and Linux for other kinds of embedded systems (like ATMs and the chips in your car).\nWhen you stand up a server, you’re going to be choosing from one of a few versions of Linux. If you’re unfamiliar with Linux, the number of choices can seem overwhelming, so here’s a quick primer on the history of operating systems. Hopefully it’ll help it all make sense.\nBefore the early 1970s, the market for computer hardware and software looked nothing like it does now. Computers released in that era had extremely tight linking between hardware and software. There were no standard interfaces between hardware and software, so each hardware manufacturer also had to release the software to use with their machine.\nIn the early 1970s, Bell Labs released Unix – the first operating system.\nOnce there was an operating system, the computer market started looking a lot more familiar to 2020s eyes. Hardware manufacturers would build machines that ran Unix and software companies could write applications that ran on Unix. The fact that those applications would run on any Unix machine was a game-changer.\nIn the 1980s, programmers wanted to be able to work with Unix themselves, but didn’t necessarily want to pay Bell Labs for Unix, so they started writing Unix-like operating systems. Unix-like OSes or Unix clones behaved just like Unix, but didn’t actually include any code from Unix itself.1\nIn 1991, Linus Torvalds – then a 21 year-old Finnish grad student – released Linux, an open source Unix clone via a amusingly nonchalant newsgroup posting.2\nSince then, Linux has seen tremendous adoption. A large majority of the world’s servers run on Linux.3 Along with most of the world’s servers, almost all of the world’s embedded computers – in ATMs, cars and planes, TVs, and most other gadgets and gizmos – run on Linux. If you have an Android phone or a Chromebook – that’s Linux. Basically all of the world’s supercomputers use Linux.\nAs you might imagine, running Linux in so many different places has necessitated the creation of many different kinds of Linux. For example, a full-featured Linux server is going to require a very different operating system than the barebones operating system running on an ATM with extremely modest computational power.\nThese different versions are called distributions (distros for short) of Linux. They have a variety of technical attributes and also different licensing models.\nSome versions of Linux, like Ubuntu, are completely open source. Others, like Red Hat Enterprise Linux (RHEL), are paid. Most paid Linux OSes have closely-related free and open source versions – like CentOS and Fedora for RHEL.4\nMany organizations have a standard Linux distro they use – most often RHEL/CentOS or Ubuntu. Increasingly, organizations deploying in AWS are using Amazon Linux, which is independently maintained by Amazon but was originally a RHEL derivative. There are also some organizations that use SUSE (pronunced soo-suh), which has both open source and enterprise versions."
  },
  {
    "objectID": "chapters/sec3/3-2-linux-admin.html#a-tiny-intro-to-linux-administration",
    "href": "chapters/sec3/3-2-linux-admin.html#a-tiny-intro-to-linux-administration",
    "title": "11  Basic Linux SysAdmin",
    "section": "11.2 A tiny intro to Linux administration",
    "text": "11.2 A tiny intro to Linux administration\nWe’ll get into how to administer a Linux server just below, but before we get there, let’s introduce what you’ll be doing as a Linux server admin. There are three main things you’ll manage as a Linux server admin:\n\nSystem resources Each server has a certain amount of resources available. In particular, you’ve got CPU, RAM, and storage. Keeping track of how much you’ve got of these things and how they’re being used – and making sure no one is gobbling up all the resources – is an important part of system administration.\nNetworking Your server is only valuable if you and others can connect to it, so managing how your server can connect to the environment around it is an important part of Linux administration.\nPermissions Servers generally exist to allow a number of people to access the same machine. Creating users and groups – and managing what they’re allowed to do – is a huge part of server administration.\nApplications Generally you want to do something with your server, so being able to interact with applications that are running, debug issues, and fix things that aren’t going well is an essential Linux admin skill.\n\nWhen you log into a Linux server, you’ll be interacting exclusively via the command line, so all of the commands in this chapter are going to be terminal commands. If you haven’t yet figured out how to open the terminal on your laptop (and gotten it themed and customized so it’s perfect), I’d advise going back to Chapter 8 to get it all configured.5\n\n\n\n\n\n\nWindows, Mac, and Linux\n\n\n\nMacOS is based on BSD, a Unix clone, so any terminal commands you’ve used before on Mac will be very similar to Linux commands.\nWindows, on the other hand, is basically the only popular operating system that isn’t a Unix clone. Over time, the Windows command line has gotten more Unix-like, so the differences aren’t as big as they used to be, but there will be some differences in the exact commands that work on Windows vs Linux.\nThe most obvious difference is the types of slashes used in file paths. Unix-like systems use forward slashes / to denote file hierarchies, while Windows historically uses back slashes \\. These days Windows deals just fine with either."
  },
  {
    "objectID": "chapters/sec3/3-2-linux-admin.html#managing-who-can-do-what",
    "href": "chapters/sec3/3-2-linux-admin.html#managing-who-can-do-what",
    "title": "11  Basic Linux SysAdmin",
    "section": "11.3 Managing who can do what",
    "text": "11.3 Managing who can do what\nWhenever you’re doing something in Linux, you’re doing that thing as a particular user.\nOn any Unix-like system, you can check your active user at any time with the whoami command. For example, here’s what it looks like on my MacBook.\n❯ whoami                                                       \nalexkgold\nwhoami returns the username of my user.\nUsernames have to be unique on the system – but they’re not the true identifier for a Linux user. A user is uniquely (and permanently) identified by their user id (uid). All other attributes (including username, password, home directory, groups, and more) are malleable, but uid is forever.\nMany of the users on a Linux server correspond to actual humans, but there are more users than that. Most programs that run on a Linux server run as a service account that represent the set of permissions allowed to that program.\nFor example, installing RStudio Server will create a user with username rstudio-server. Then, when rstudio-server goes to do something – start an R session for example – it will do so as rstudio-server.\n\n\n\n\n\n\nA few details on UIDs\n\n\n\nuids are just numbers from 0 to over 2,000,000,000. uids are assigned by the system at the time the user is created. You should probably keep uids below 2,000,000 or so if you are ever assigning uids manually – some programs can’t deal with uids any bigger.\n10,000 is the the lowest uid that’s available for use by a user account. Everything below that is reserved for predefined system accounts or application accounts.\n\n\nIn addition to users, Linux has a notion of groups. A group is a collection of users. Each user has exactly one primary group and can be a member of secondary groups.6 By default, each user’s primary group is the same as their username.\nLike a user has a uid a group has a gid. User gids start at 100.\nYou can see a user’s username, uid, groups, and gid with the id command.\n ❯ id                                                                \nuid=501(alexkgold) gid=20(staff) groups=20(staff),12(everyone),61(localaccounts),79(_appserverusr),80(admin),81(_appserveradm),98(_lpadmin),701(com.apple.sharepoint.group.1),33(_appstore),100(_lpoperator),204(_developer),250(_analyticsusers),395(com.apple.access_ftp),398(com.apple.access_screensharing),400(com.apple.access_remote_ae)\nOn my laptop, I’m a member of a number of different groups.\nThere’s one extra special user – called the admin, root, sudo, or super user. They get the ultra-cool uid 0. That user has permission to do anything on the system. You almost never want to actually log in as the root user. Instead, you make users and add them to the admin or sudo group so that they have the ability to temporarily assume those admin powers.\nThe easiest way to make users is with the useradd command. Once you have a user, you may need to change the password, which you can do at any time with the passwd command. Both useradd and passwd start interactive prompts, so you don’t need to do much more than run those commands.\n\n\n\n\n\n\n\nCommand\nWhat it does\n\n\n\n\nsu &lt;username&gt;\nChange to be a different user.\n\n\nwhoami\nGet username of current user.\n\n\nid\nGet full user + group info on current user.\n\n\npasswd\nChange password.\n\n\nuseradd\nAdd a new user.\n\n\n\n\n11.3.1 File Permissions\nEvery object in Linux is just a file. Every log – file. Every picture – file. Every program – file. Every system setting – file.\nSo determining whether a user can take a particular action is really a question of whether they have the right permissions on a particular file.\n\n\n\n\n\n\nNote\n\n\n\nThe question of who’s allowed to do what – authorization – is an extremely deep one. There’s a chapter all about authorization, how it differs from authentication, and the different ways your IT/Admins might want to manage it later in the book.\nThis is just going to be a high-level overview of basic Linux authorization.\n\n\nThere are three permissions you can have: read, write, and execute. Read means you’re allowed to see the contents of a file, write means you can save a changed version of a file, and execute means you’re allowed to run the file as a program.\nThe execute permission really only makes sense for some kinds of files - what would it mean to execute a csv file? But Linux doesn’t care – you can assign any combination of these three permissions for any file.\nHow are these permissions assigned? Every file has an owner and an owning group.\nSo you can think of permissions in Linux as being assigned in a 3x3 grid. The owner, the owning group, and everyone else can have permissions to read, write, or execute the file.\nTODO: change to graphic\n\n\n\n\nOwner\nGroup\nEveryone Else\n\n\n\n\nRead\n✅/❌\n✅/❌\n✅/❌\n\n\nWrite\n✅/❌\n✅/❌\n✅/❌\n\n\nExecute\n✅/❌\n✅/❌\n✅/❌\n\n\n\nTo understand better, let’s look at the permissions on an actual file.\nRunning ls -l on a directory gives you the list of files in that directory, along with their permissions. The first few columns of the list give you the full set of file permissions – though they can be a little tricky to read.\nSo, for example, here’s a few lines of the output of running ls -l on a python project I have.\n❯ ls -l                                                           \n-rw-r--r--  1 alexkgold  staff     28 Oct 30 11:05 config.py\n-rw-r--r--  1 alexkgold  staff   2330 May  8  2017 credentials.json\n-rw-r--r--  1 alexkgold  staff   1083 May  8  2017 main.py\ndrwxr-xr-x 33 alexkgold  staff   1056 May 24 13:08 tests\nThis readout has the file permissions (a series of ten characters), followed by a number7, then the file’s owner, and the file’s group. Let’s learn how to read these.\nThe file’s owner and group are the easiest to understand. In this case, I alexkgold own all the files, and the group of all the files is staff.\nThe ten-character file permissions are relative to that user and group.\nThe first character indicates the type of file: - for normal and d for a directory.\nThe next nine characters are indicators for the three permissions – r for read, w for write, and x for execute (or - for in place of any of those for not) – first for the user, then for the group, then for any other user on the system.\nSo, for example, my config.py file with permissions of rw-r-r-- indicates the user (alexkgold) can read and write the file, and everyone else – including in the file’s group staff – has read-only permission.\nIn the course of administering a server, you will probably need to change a file’s permissions. You can do so using the chmod command.\nFor chmod, permissions are indicated with only three numbers – one for the user, one for the group, and one for everyone else. The way this works is pretty clever – you just sum up the permissions as follows: 4 for read, 2 for write, and 1 for execute. You can check for yourself, but any set of permissions can be uniquely identified by a number between 1 and 7.8\nSo chmod 765 &lt;filename&gt; would give the user full permissions (4 + 2 + 1), read and write (4 + 2) to the group, and read and execute (4 + 1) to everyone else. This would be a strange set of permissions to give a file, but it’s a perfectly valid chmod command.\n\n\n\n\n\n\nNote\n\n\n\nIf you spend any time administering a Linux server, you almost certainly will at some point find yourself running into a problem and applying chmod 777 out of frustration to rule out a permissions issue.\nI can’t in good faith tell you not to do this – we’ve all been there. But if it’s something important, be sure you change it back once you’re finished figuring out what’s going on.\n\n\nIn some cases you might actually want to change the owner or group of a file. You can change users or groups with either names or ids. You can do so using the chown command. If you’re changing the group, the group name gets prefixed with a colon.\nIn some cases, you might not be the correct user to take a particular action. You might not want to change the file permissions, but instead to change who you are. In that case, you can switch users with the su command.\nSome actions are also reserved for the admin user. For example, let’s take a look at this configuration file:\n ❯ ls -l /etc/config/my-config                      \n-rw-r--r--  1 root  system  4954 Dec  2 06:37 config.conf\nAs you can see, all users can read this file to check the configuration settings. But this file is owned by root, and only the owner has write permissions. So I could run cat config.conf to see it. Or I could go into it with vim config.conf, but I’d find myself stuck if I wanted to make changes.\nSo if I want to change this configuration file, I’d need to temporarily assume my root powers to make changes. Instead of switching to be the root user, I would run sudo vim config.conf and open the file for editing with root permissions.\n\n\n\n\n\n\n\n\nCommand\nWhat it does\nHelpful options + notes\n\n\n\n\nc h m o d &lt; permissions&gt; &lt;file&gt;\nModifies permissions on a file.\nNumber indicates permissions for user, group, others: add 4 for read, 2 for write, 1 for execute, 0 for nothing.\n\n\nc h o w n &lt;user/group&gt; &lt;file&gt;\nChange the owner of a file.\nCan be used for user or group, e.g. :my-group.\n\n\nsu &lt;username&gt;\nChange active user.\n\n\n\nsudo &lt;command&gt;\nAdopt super user permissions for the following command."
  },
  {
    "objectID": "chapters/sec3/3-2-linux-admin.html#installing-stuff",
    "href": "chapters/sec3/3-2-linux-admin.html#installing-stuff",
    "title": "11  Basic Linux SysAdmin",
    "section": "11.4 Installing Stuff",
    "text": "11.4 Installing Stuff\nThere are several different ways to install programs for Linux, and you’ll see a few of them throughout this book.\nJust as CRAN and PyPI are repositories for R and Python packages, Linux distros also have their own repositories. For Ubuntu, the apt command is used for accessing and installing .deb files from the Ubuntu repositories. For CentOS and RedHat, the yum command is used for installing .rpm files.\n\n\n\n\n\n\nNote\n\n\n\nThe examples below are all for Ubuntu, since that’s what we use in the lab for this book. Conceptually, using yum is very similar, though the exact commands differ somewhat.\n\n\nWhen you’re installing packages in Ubuntu, you’ll often see commands prefixed with apt-get update && apt-get upgrade y. This command makes your machine update the list of available packages it knows about on the server and upgrade everything to the latest version.\nPackages are installed with apt-get install &lt;package&gt;. Depending on which user you are, you may need to prefix the command with sudo.\nYou can also install packages that aren’t from the central package repository. Doing that will generally involve downloading a file directly from a URL – usually with wget and then installing it from the file you’ve downloaded – often with the gdebi command.\n\n\n\n\n\n\n\nCommand\nWhat it does\n\n\n\n\na p t - g e t  update  && apt-get upgrade y\nFetch and install upgrades to system packages\n\n\napt-get install &lt;package&gt;\nInstall a system package.\n\n\nwget\nDownload a file from a URL.\n\n\ngdebi\nInstall local .deb file.\n\n\n\n\n11.4.1 Managing R and Python\nAs the admin of a data science server, managing R and Python installations is a central task you’ll have to do, as well as installing software on top of that like RStudio Server, JupyterHub, and/or Shiny Server. While R and Python are both open source data science languages, the way you’ll have to administer them are pretty different.\nPeople pretty much only ever install R to do data science. In contrast, Python is one of the world’s most popular programming languages for general purpose computing. Contrary to what you might think, this actually makes configuring Python harder than configuring R.\nThe reason is that your system comes with a version of Python installed, but we don’t want to use that version. Given the centrality of that version to normal server operations, we want to leave it alone. It turns out that installing one or more other versions of Python and then ignoring the system version of Python isn’t totally trivial to do. Then we’re going to want to create a standalone virtual environment that’s just for running JupyterHub so it doesn’t get messed up later.\nRStudio and R are system libraries. So when RStudio runs, it calls and owns the R process that you’ll use inside RStudio Server. In contrast, JupyterHub and JupyterLab are Python programs, so we install them inside a Python installation.\nTODO: diagram of relationships of system python, DS python, Jupyter Python."
  },
  {
    "objectID": "chapters/sec3/3-2-linux-admin.html#debugging-and-troubleshooting",
    "href": "chapters/sec3/3-2-linux-admin.html#debugging-and-troubleshooting",
    "title": "11  Basic Linux SysAdmin",
    "section": "11.5 Debugging and troubleshooting",
    "text": "11.5 Debugging and troubleshooting\nThere are three main resources you’ll need to manage as the server admin – CPU, RAM, and storage space. There’s more on all three of these and how to make sure you’ve got enough in Chapter 15.\nFor now, we’re just going to go over how to check how much you’ve got, how much you’re using, and getting rid of stuff that’s misbehaving.\n\n11.5.1 Storage\nA common culprit for weird server behavior is running out of storage space. There are two handy commands for monitoring the amount of storage you’ve got – du and df. These commands are almost always used with the -h flag to put file sizes in human-readable formats.\ndu, short for disk usage, gives you the size of individual files inside a directory. This can be helpful for finding your largest files or directories if you think you might need to clean up things. It’s particularly useful in combination with the sort command.\nFor example, here’s the result of running du on the chapters directory where the text files for this book live.\n ❯ du -h chapters | sort -h                                      \n 44K    chapters/sec2/images-servers\n124K    chapters/sec3/images-scaling\n156K    chapters/sec2/images\n428K    chapters/sec2/images-traffic\n656K    chapters/sec1/images-code-promotion\n664K    chapters/sec1/images-docker\n1.9M    chapters/sec1/images-repro\n3.4M    chapters/sec1\n3.9M    chapters/sec3/images-auth\n4.1M    chapters/sec3\n4.5M    chapters/sec2/images-networking\n5.3M    chapters/sec2\n 13M    chapters\nSo if I were thinking about cleaning up this directory, I could see that my images-networking directory in sec2 is the biggest single bottom-level directory. If you find yourself needing to find big files on your Linux server, it’s worth spending some time with the help pages for du. There are lots of really useful options.\ndu is useful for identifying large files and directories on a server. df, for disk free, is useful for diagnosing issues that might be a problem for a directory. If you’re struggling to write into a directory – perhaps getting out of space errors, df can help you diagnose.\ndf answers the question – given a file or directory, what device is it mounted on and how full is that device?\nSo here’s the result of running the df command on that same chapters directory.\n ❯ df -h chapters                                                    \nFilesystem     Size   Used  Avail Capacity iused      ifree %iused  Mounted on\n/dev/disk3s5  926Gi  163Gi  750Gi    18% 1205880 7863468480    0%   /System/Volumes/Data\nSo you can see that the chapters folder lives on a disk called /dev/disk3s5 that’s a little less than 1Tb and is 18% full – no problem. On a server this can be really useful to know, because it’s quite easy to switch a disk out for a bigger one in the same spot.\n\n\n\n\n\n\n\n\nC ommand\nWhat it does\nH elpful o ptions\n\n\n\n\ndu\nCheck size of files.\nMost likely to be used d u  -h &lt;d  i r &gt;  | s o rt -h [^3 -2-l in u x - a d m in-9]\nAlso useful to c ombine with head.\n\n\ndf\nCheck s torage space on d evice.\n-h\n\n\n\n\n\n11.5.2 Monitoring processes\nEvery program your computer runs is a process. For example, when you type python on the command line to open a REPL, that’s a process. Running more complicated programs usually involves more than one process.\nFor example, running RStudio involves (at minimum) one process for the IDE itself and one for the R session that it uses in the background. The relationships between these different processes is mostly hidden from you – the end user.\nAs a server admin, finding runaway processes, killing them, and figuring out how to prevent the them from happening again is a pretty common task. Runaway processes usually misbehave by using up the entire CPU, filling up the entire machine’s RAM.\nLike users and groups have ids, each process has a numeric process id (pid). Each process also has an owner – this can be either a service account or a real user. If you’ve got a rogue process, the pattern is to try to find the process and make note of its pid. Then you can immediately end the process by pid with the kill command.\nSo, how do you find a troublesome process?\nThe top command is a good first stop. top shows the top CPU-consuming processes in real time. Here’s the top output from my machine as I write this sentence.\nPID    COMMAND      %CPU TIME     #TH    #WQ  #PORT MEM    PURG   CMPRS PGRP\n0      kernel_task  16.1 03:56:53 530/10 0    0     2272K  0B     0B    0\n16329  WindowServer 16.0 01:53:20 23     6    3717  941M-  16M+   124M  16329\n24484  iTerm2       11.3 00:38.20 5      2    266-  71M-   128K   18M-  24484\n29519  top          9.7  00:04.30 1/1    0    36    9729K  0B     0B    29519\n16795  Magnet       3.1  00:39.16 3      1    206   82M    0B     39M   16795\n16934  Arc          1.8  18:18.49 45     6    938   310M   144K   61M   16934\n16456  Messages     1.7  06:58.27 4      1    603   138M   2752K  63M   16456\n1      launchd      1.7  13:41.03 4/1    3/1  3394+ 29M    0B     6080K 1\n573    diagnosticd  1.4  04:31.97 3      2    49    2417K  0B     816K  573\n16459  zoom.us      1.3  66:38.37 30     3    2148  214M   384K   125M  16459\n16575  UniversalCon 1.3  01:15.89 2      1    131   12M    0B     2704K 16575\nIn most instances, the first three columns are the most useful. You’ve got the name of the command and how much CPU they’re using. Right now, nothing is using very much CPU. If I were to find something concerning – perhaps an R process that is using 500% of CPU – I would want to take notice of its pid to kill it with kill.\n\n\n\n\n\n\nSo much CPU?\n\n\n\nFor top (and most other commands), CPU is expressed as a percent of single core availability. So, on a modern machine (with multiple cores), it’s very common to see CPU totals well over 100%. Seeing a single process using over 100% of CPU is rarer.\n\n\nThe top command takes over your whole terminal. You can exit with Ctrl + c.\nAnother useful command for finding runaway processes is ps aux.9 It lists all processes currently running on the system, along with how much CPU and RAM they’re using. You can sort the output with the --sort flag and specify sorting by cpu with --sort -%cpu or by memory with --sort -%mem.\nBecause ps aux returns every running process on the system, you’ll probably want to pipe the output into head.\nAnother useful way to use ps aux is in combination with grep. If you pretty much know what the problem is – often this might be a runaway R or Python process – ps aux | grep &lt;name&gt; can be super useful to get the pid.\nFor example, here are the RStudio processes currently running on my system.\n &gt; ps aux | grep \"RStudio\\|USER\"                                                                                      [10:21:18]\nUSER               PID  %CPU %MEM      VSZ    RSS   TT  STAT STARTED      TIME COMMAND\nalexkgold        23583   0.9  1.7 37513368 564880   ??  S    Sat09AM  17:15.27 /Applications/RStudio.app/Contents/MacOS/RStudio\nalexkgold        23605   0.5  0.4 36134976 150828   ??  S    Sat09AM   1:58.16 /Applications/RStudio.app/Contents/MacOS/rsession --config-file none --program-mode desktop \n\n\n\n\n\n\nTip\n\n\n\nThe grep command above looks a little weird because I used a little trick. I wanted to keep the header in the output, so the regex I used matches both the header line (USER) and the thing I actually care about (RStudio).\n\n\n\n\n\n\n\n\n\n\nCommand\nWhat it does\nHelpful options\n\n\n\n\ntop\nSee what’s running on the system.\n\n\n\nps aux\nSee all system processes.\nConsider using --sort and pipe into head or grep\n\n\nkill\nKill a system process.\n-9 to force kill immediately\n\n\n\n\n\n11.5.3 Managing networking\nNetworking is a complicated topic, which we’ll approach with great detail in Chapter 12. For now, it’s important to be able to see what’s running on your server that is accessible from the outside world on a particular port.\nThe main command to help you see what ports are being used and by what services is the netstat command. netstat returns the services that are running and their associated ports. netstat is generally most useful with the -tlp flags to show programs that are listening and the programs associated.\nTODO: get netstat example\n\n\n\n\n\n\n\n\nCommand\nWhat it does\nHelpful options\n\n\n\n\nnetstat\nSee ports and services using them.\nUsually used with -tlp\n\n\n\nSometimes you know you’ve got a service running on your machine, but you just can’t seem to get the networking working. It can be useful to access the service directly without having to deal with networking. You can do this with port forwarding, also called tunneling.\nSSH port forwarding allows you to take the output of a port on a remote server, route it through SSH, and display it as if it were on a local port. For example, let’s say I’ve got RStudio Server running on my server. Maybe I don’t have networking set up yet, or I just can’t get it working. If I’ve got SSH to my server working properly, I can double check that the service is working as I expect and the issue really is somewhere in the network.\nI find that the syntax for port forwarding completely defies my memory and I have to google it every time I use it. For the kind of port forwarding you’ll use most often in debugging, you’ll use the -L flag.\nssh -L &lt;local port&gt;:&lt;remote ip&gt;:&lt;remote port&gt; &lt;ssh hostname&gt;\nWhen you’re doing ssh forwarding, local is the place you’re ssh-ed into (aka your server) and the remote is another location – usually your laptop.\nSince the “remote” is my laptop, I almost always want to use localhost as the remote IP, and I usually want to use the same port remotely and locally – unless the local service is on a reserved port.\nSo let’s say I’ve got RStudio Server running on my server at my-ds-workbench.com on port 3939. Then I could run ssh -L 3939:localhost:3939 my-user@my-ds-workbench.com. With this command, I can bypass networking and just access whatever is at port 3939 on my server (hopefully RStudio Workbench!) by just going to localhost:3939 in my laptop’s browser.\n\n\n11.5.4 Understanding PATHs\nLet’s say you want to open R on your command line. Once you’ve got everything properly configured, you can just type R and have it open right up.\n ❯ R                                                       \n\nR version 4.2.0 (2022-04-22) -- \"Vigorous Calisthenics\"\nCopyright (C) 2022 The R Foundation for Statistical Computing\nPlatform: x86_64-apple-darwin17.0 (64-bit)\n\nR is free software and comes with ABSOLUTELY NO WARRANTY.\nYou are welcome to redistribute it under certain conditions.\nType 'license()' or 'licence()' for distribution details.\n\n  Natural language support but running in an English locale\n\nR is a collaborative project with many contributors.\nType 'contributors()' for more information and\n'citation()' on how to cite R or R packages in publications.\n\nType 'demo()' for some demos, 'help()' for on-line help, or\n'help.start()' for an HTML browser interface to help.\nType 'q()' to quit R.\n\n&gt;\nBut how does the operating system know what you mean when you type R? If you’ve been reading carefully, you’ve realized that running the command means opening a particular runnable file, and R isn’t a file path on my system.\nYou can actually just type the complete filename of a runnable binary into your command line. For example, on my MacBook, my version of R is at /usr/bin/local/R, so I could open an R session by typing that full path. Sometimes it can be handy to be precise about exactly which executable you’re opening (I’m looking at you, multiple versions of Python), so you may want to use full paths to executables.\nIf you ever want to check which actual executable is being used by a command, you can use the which command. For example, on my system this is the result of which R.\n ❯ which R                                                    \n/usr/local/bin/R\nMost of the time you don’t want to have to bother with full paths for executables. You want to just type R on the command line and have R open. Moreover, there are cases where functionality relies on another executable being able to find R and run it – think of running RStudio Server, which starts a version of R under the hood.\nThe operating system knows how to find the actual runnable programs on your system via something called the path. When you type R into the command line, it searches along the path to find a version of R it can run.\nYou can check your path at any time by echoing the PATH environment variable with echo $PATH. On my MacBook, this is what the path looks like.\n ❯ echo $PATH                                                      \n/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/System/Cryptexes/App/usr/bin:/usr/bin:/bin:/usr/sbin:/sbin\nLater, when we get into running versions of programs that aren’t the system versions, we may have to append locations to the path so that we can run them easily."
  },
  {
    "objectID": "chapters/sec3/3-2-linux-admin.html#comprehension-questions",
    "href": "chapters/sec3/3-2-linux-admin.html#comprehension-questions",
    "title": "11  Basic Linux SysAdmin",
    "section": "11.6 Comprehension Questions",
    "text": "11.6 Comprehension Questions\n\nCreate a mind map of the following terms: Operating System, Windows, MacOS, Unix, Linux, Distro, Ubuntu\nWhen you initially SSH-ed into your server using ubuntu@$SERVER_ADDRESS, what user were you and what directory did you enter? What about when you used test_user@$SERVER_ADDRESS?\nWhat are the 3x3 options for Linux file permissions? How are they indicated in an ls -l command?\nHow would you do the following?\n\nFind and kill the process IDs for all running rstudio-server processes.\nFigure out which port JupyterHub is running on.\nCreate a file called secrets.txt, open it with vim, write something in, close and save it, and make it so that only you can read it."
  },
  {
    "objectID": "chapters/sec3/3-2-linux-admin.html#lab-a-working-data-science-workbench",
    "href": "chapters/sec3/3-2-linux-admin.html#lab-a-working-data-science-workbench",
    "title": "11  Basic Linux SysAdmin",
    "section": "11.7 Lab: A working Data Science Workbench",
    "text": "11.7 Lab: A working Data Science Workbench\nIn the last chapter’s lab, we set up a bare Ubuntu server in AWS. In this chapter, we’re going to make that server into a data science workbench. By the end of this chapter, you’ll have a functional data science workbench - though it won’t yet be accessible to the outside world.\n\n11.7.0.1 Step 1: Log on with the .pem key\nThe .pem key you downloaded when you set up the server is the skeleton key – it will automatically let you in with complete admin privileges. We’re going to use the .pem key to get started on the server, but be extremely careful with the power of the .pem key.\nBecause the keypair is so powerful, AWS requires that you restrict the access pretty severely. If you try to use the keypair without first changing the permissions, you’ll be unable to, and you’ll get a warning that looks something like this:\n\n\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\n\n\\@ WARNING: UNPROTECTED PRIVATE KEY FILE! \\@\n\n\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\n\nPermissions 0644 for 'do4ds-lab-key.pem' are too open.\n\nIt is required that your private key files are NOT accessible by others.\n\nThis private key will be ignored.\n\nLoad key \"do4ds-lab-key.pem\": bad permissions\n\nubuntu\\@ec2-54-159-134-39.compute-1.amazonaws.com: Permission denied (publickey).\nBefore we can use it to open the server, we’ll need to change the permissions so that only the user can use it. We’re going to give the key 600 permissions. If you recall from above, that means that the user can read and write the file and no one else can access.\nOn my machine, changing the key goes like this:\n\n\nTerminal\n\ncd ~/Documents/do4ds-lab\nchmod 600 do4ds-lab-key.pem\n\nIn your terminal type the following\n\n\nTerminal\n\nssh -i do4ds-lab-key.pem \\\n  ubuntu@$SERVER_ADDRESS\n\nType yes when prompted, and you’re now logged in to your server!\n\n\n\n\n\n\nSSH on Windows\n\n\n\nFor a long time, Windows didn’t come with a built in SSH client, so you had to use PuTTY to do SSH from a Windows machine. Microsoft brought a native SSH client to Windows 10 in 2015, and it has been enabled by default since 2018.\nIf you run into any trouble using SSH commands on Windows, double check that you’ve enabled the OpenSSH Client.\n\n\n\n\n11.7.1 Step 2: Create a non-root user\nThe first thing we’re going to do is create a user so that you can login without running as root all the time. In general, if you’ve got a multitenant server, you’re going to want users for each actual human who’s accessing the system.\nI’m going to use the username test-user. If you want to be able to copy/paste commands from the online book, I’d advise doing the same. If you were creating users based on real humans, I’d advise using their names.\nLet’s create a user using the adduser command. This will walk us through a set of prompts to create a new user with a home directory and a password. Feel free to add any information you want – or to leave it blank – when prompted.\n\n\nTerminal\n\nsudo adduser test-user\n\nWe want this new user to be able to adopt root privileges. Remember that the way that is determined is whether the user is part of the sudo group. Here’s the command you’ll need to make it so.\n\n\nTerminal\n\nsudo usermod -aG sudo test-user\n\n\n\n11.7.2 Step 3: Add an SSH Key for your user\nThe first thing you should do is check if you have an SSH key on your laptop. You’ll need to open a different terminal window or tab than the one that’s SSH-ed into your server and try ls ~/.ssh.\nIf you’ve got one already, I’d recommend just using that key so you don’t have to mess around with multiple keys. If you don’t have one, google how to create an SSH key and do it on your laptop. If you’re not really sure what an SSH key is, I’d recommend re-reading Chapter 7.\nOnce you’ve got an SSH key, we’re going to add it to the authorized_users for our user on the server.\nFirst, you need to get your public key to the server. I recommend using scp.10\nYou’ll need to check that your SERVER_ADDRESS variable is still set using echo $SERVER_ADDRESS. If it’s not set, you’ll have to re-set it to be the public IPv4 from your server.\nFor me, the command looks like this\n\n\nTerminal\n\nscp -i ~/Documents/do4ds-lab/do4ds-lab-key.pem ~/.ssh/id_ed25519.pub ubuntu@$SERVER_ADDRESS:/home/ubuntu \n\nYou may need to change the file paths to either your server’s pem key or your SSH key if they’re not in the same locations as mine.\nNow our public key is on the server, but it’s in the ubuntu user’s home directory. We’re going to add it as an authorized key for the test-user so that we can SSH in as that user.\nLet’s go back to the tab that’s open to the SSH connection to our server.\nHere are the commands to do so:\n\n\nTerminal\n\nsu test-user #change user\ncd ~ # move to home directory\nmkdir -p .ssh # create .ssh directory\nsudo mv /home/ubuntu/id_ed25519.pub ./.ssh # move key into .ssh \nsudo chown test-user:test-user ~/.ssh/id_ed25519.pub # take ownership of key\nchmod 700 .ssh # Lock directory from other users\ncat .ssh/id_ed25519.pub &gt; .ssh/authorized_keys #add public key to end of authorized_keys file\nchmod 600 .ssh/authorized_keys #set permissions\n\nNow we’re all set up with SSH, and you can log in as a normal user from your laptop just using ssh test-user@$SERVER_ADDRESS.\nIf you want to set up an SSH config for this server, I’d advise waiting until we’ve got a permanent URL for it in the next chapter.\nNow that we’re all set up, you should store the pem key somewhere safe and never use it to log in again.\nWhen you ever want to exit SSH and get back to your machine, you can just type exit.\n\n\n\n\n\n\nTip\n\n\n\nIf you run into trouble assuming sudo with your new user, try exiting SSH and coming back. Sometimes these changes aren’t picked up until you restart the shell.\n\n\n\n\n11.7.3 Step 4: Install R\nEverything until now has been generic server administration. Now let’s get into some data-science-specific work – setting up R and Python.\nThere are a number of ways to install R on your server including installing it from source, from the system repository, or using R-specific tooling.\nYou can use the system repository version of R, but then you just get whatever version of R happens to be current when you run sudo apt-get install R. My preferred option is to use rig, which is an R-specific installation manager.\n\n\n\n\n\n\nNote\n\n\n\nAs of this writing, rig only supports Ubuntu. If you want to install on a different Linux distro, you will have to install R a different way.\n\n\nThere are good instructions on downloading rig and using it to install R on the rlib/rig GitHub repo. Use those instructions to install the current R release on your AWS server.\nOnce you’ve installed R on your server, you can check that it’s running by just typing R into the command line. If that works, you’re good to move on to the next step.\n\n\n11.7.4 Step 5: Install Python\nIt’s very likely that the version of Python on your system is old. Generally we’re going to want to install a newer Python for doing data science work, so let’s start there. As of this writing, Python 3.10 is a relatively new version of Python, so we’ll install that one.\nLet’s start by actually installing Python 3.10 on our system. We can do that with apt.\n\n\nTerminal\n\nsudo apt-get install python3.10-venv\n\nOnce you’ve installed Python, you can check that you’ve got the right version by running\n\n\nTerminal\n\npython3 --version\n\n\n\n11.7.5 Step 5: Installing RStudio Server\nOnce R is installed, let’s download and install RStudio Server.\nI’m not going to reproduce the commands here because the RStudio Server version numbers change frequently and you probably want the most recent one.\nYou can find the exact commands on the Posit website. Make sure to pick the version that matches your operating system. Since you’ve already installed R, you can skip down to the “Install RStudio Server” step.\nOnce you’ve installed, you can check the status with sudo systemctl status rstudio-server. If there’s a line that says Active: active (running), you’re good to go!\n\n\n11.7.6 Step 6: Installing JupyerHub + JupyterLab\nBecause python is so crucial to the operation of your system are python programs, we’re going to want to create a standalone virtual environment for running JupyterHub.\nHere are the commands to create and activate a jupyterhub virtual environment\n\n\nTerminal\n\nsudo python3 -m venv /opt/jupyterhub\nsource /opt/jupyterhub/bin/activate\n\nNow we’re going to actually get JupyterHub up and running inside the virtual environment we just created. JupyterHub produces docs that you can use to get up and running very quickly. If you have to stop for any reason, make sure to come back, assume sudo, and start the JupyterHub virtual environment we created.\nHere were the installation steps that worked for me:\n\n\nTerminal\n\nsudo su\napt install npm nodejs\nnpm install -g configurable-http-proxy\npython3 -m pip install jupyterhub jupyterlab notebook\n\nln -s /opt/jupyterhub/bin/jupyterhub-singleuser /usr/local/bin/jupyterhub-singleuser # symlink in singleuser server, necessary because we're using virtual environment\n\njupyterhub\n\nIf all went well, you’ll now have JupyterHub up and running on port 8000!\n\n11.7.6.1 Running JupyterHub as a service\nAs I mentioned above, JupyterHub is a Python process, not a system process. This is ok, but it means that we’ve got to remember the command to start it if we have to restart it, and that it won’t auto restart if it were to fail for any reason.\nA program that runs in the background on a machine, starting automatically, and controlled by systemctl is called a daemon. Since we want JupyterHub to be a daemon, we’re got to add it as a system daemon, which isn’t hard.\nWe don’t need it right now, but it’ll be easier to manage JupyterHub later on from a config file that’s in /etc/jupyterhub.\nLet’s create a default config file and move it into the right place using\n\n\nTerminal\n\nsudo su\nsource /opt/jupyterhub/bin/activate\njupyterhub --generate-config\nmkdir -p /etc/jupyterhub\nmv jupyterhub_config.py /etc/jupyterhub\n\nNow we’ve got to daemon-ize JupterHub. There are two steps – create a file describing the service for the server’s daemon, and then start the service.\nTo start with, end the existing JupyterHub process. If you’ve still got that terminal open, you can do so with ctrl + c. If not, you can use your ps aux and grep skills to find and kill the JupyterHub processes.\nOn Ubuntu, adding a daemon file uses a tool called systemd and is really straightforward.\nFirst, add the following to /etc/systemd/system/jupyterhub.service. If you’re reading this book in hard copy, you can go to the online version to copy/paste or get this file on the book’s Git repo at TODO.\n\n\n/etc/systemd/system/jupyterhub.service\n\n[Unit]\nDescription=Jupyterhub\nAfter=syslog.target network.target\n\n[Service]\nUser=root\nEnvironment=\"PATH=/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/opt/jupyterhub/bin\"\nExecStart=/opt/jupyterhub/bin/jupyterhub -f /etc/jupyterhub/jupyterhub_config.py\n\n[Install]\nWantedBy=multi-user.target\n\nHopefully this file is pretty easy to parse. Two things to notice – the Environment line adds /opt/jupyterhub/bin to the path – that’s where our virtual environment is.\nSecond, the ExecStart line is the startup command and includes our -f /etc/jupyterhub/jupyterhub_config.py – this is the command to start JupyterHub with the config we created a few seconds ago.\nNow we just need to reload the daemon tool so it picks up the new service it has available and start JupyterHub!\n\n\nTerminal\n\nsystemctl daemon-reload\nsystemctl start jupyterhub\n\nYou should now be able to see that JupyterHub is running using systemctl status jupyterhub and can see it again by tunneling to it.\nTo set JupyterHub to automatically restart when the server restarts, run systemctl enable jupyterhub.\n\n\n\n11.7.7 Step 7: Running a Plumber API in a Container\nIn addition to running a development workbench on your server, you might want to run a data science project. If you’re running a Shiny app, Shiny Server is easy to configure along the lines of RStudio Server.\nHowever, if you put an API in a container like we did in Chapter 9, you might want to deploy that somewhere on your server as a running container.\nThe first step is to install docker on your system with sudo apt-get install docker.io. You can check that you can run docker with docker ps. You may need to adopt sudo privileges to do so.\nOnce we have docker installed, getting the API running is almost trivially easy using the command we used back in Chapter 9 to run our container.\n\n\nTerminal\n\nsudo docker run --rm -d \\\n  -p 8080:8080 \\\n  --name penguin-model \\\n  alexkgold/penguin-model\n\nThe one change you might note is that I’ve changed the port on the server to be 8080, since we already have JupyterHub running on 8000.\nOnce it’s up, you can check that it’s running with docker ps.\nThis is why people love docker – it’s wildly easy to get something simple running. But getting Docker hardened for production takes a bit more work.\nThe first thing to notice is that there’s no auth on our API. Anyone can hit this API as much as they want if they have the URL. Needless to say, this is not a security best practice unless you intend to host a public service.\nThe other issue is that we haven’t daemonized the API. If we restart the server or the container dies for any reason, it won’t auto-restart.\nIt’s not generally a best practice to daemon-ize a docker container by just putting the run command into systemd. Instead, you should use a container management system that is designed specifically to manage running containers, like Docker Compose or Kubernetes. Getting deeper into those systems is beyond the scope of this book.\n\n\n11.7.8 Step 8: Putting up the Shiny app\nInstall Shiny Server using the instructions from the Admin Guide. Note that you don’t need to install R, as we’ve already done that, and you don’t need to install {shiny}, as we’re going to use {renv}.\nTODO\nEdit /etc/shiny-server/shiny-server.conf to run the right app – may have to edit user.\n\n\n/etc/shiny-server/shiny-server.conf\n\n# Instruct Shiny Server to run applications as the user \"shiny\"\nrun_as test-user;\n\n# Define a server that listens on port 3838\nserver {\n  listen 3838;\n\n  # Define a location at the base URL\n  location / {\n\n    # Host the directory of Shiny Apps stored in this directory\n    site_dir /home/test-user/do4ds-lab/app;\n\n    # Log all Shiny output to files in this directory\n    log_dir /var/log/shiny-server;\n\n    # When a user visits the base URL rather than a particular application,\n    # an index of the applications available in this directory will be shown.\n    directory_index off;\n  }\n}\n\nStart Shiny Server\n\n\n11.7.9 Step 8: Check it all out\nJust knowing that all of our services are running isn’t nearly as fun as actually trying them out.\nWe don’t have a stable public URL for the server yet, so we can’t just access it from our browser. This is a perfect use case for an SSH tunnel.\nTODO: picture of server w/ RStudio on 8787, JupyterHub on 8000, and Palmer Penguin container on 8080.\nIf you recall, the command for an SSH tunnel from a remote server to localhost is to do ssh -L &lt;remote port&gt;:localhost:&lt;local port&gt; &lt;user&gt;@&lt;server&gt;.\nWe’ve got three services running on our server, RStudio Server at 8787, JupyterHub on 8000, and our Plumber API on 8080. You can try each of them out by subbing those in for the remote port and putting them at a local port. I’d recommend just using the same one.\nFor example, by running ssh -L 8787:localhost:8787 test-user@$SERVER_ADDRESS, I can visit RStudio Server in my browser at localhost:8787 and login with the username test-user and password I set on the server."
  },
  {
    "objectID": "chapters/sec3/3-2-linux-admin.html#footnotes",
    "href": "chapters/sec3/3-2-linux-admin.html#footnotes",
    "title": "11  Basic Linux SysAdmin",
    "section": "",
    "text": "Or at least they weren’t supposed to. There’s an interesting history of lawsuits around the BSD operating system including Unix code. BSD is a Unix clone that was the predecessor of MacOS.↩︎\nCheck out the History of Linux Wikipedia article for the posting.\nPedants will scream that the original release of Linux was just the operating system kernel, not a full operating system like Unix. Duly noted, now go away.↩︎\nThe remainder are almost entirely Windows servers. There are a few other Unix-like systems that you might encounter, like Oracle Solaris. There is no MacOS server. There is a product called Mac Server, but it’s just a program for managing Mac desktops and iOS devices.↩︎\nCentOS (short for Community ENTerprise Operating System) is an open source operating system maintained by Red Hat. The relationship between RHEL and CentOS is changing. The details are somewhat complicated, but most people expect less adoption of CentOS in enterprise settings going forward.↩︎\nMy goal here is to be useful not precise, so I’m intermingling bash commands and Linux system commands because they’re useful. If you know the difference and are pedantic enough to care, this list isn’t for you anyway.↩︎\nDepending on your version of Linux, there may be a limit of 16 groups per user.↩︎\nYou can generally ignore the number, which is the number of links to the file.↩︎\nClever eyes may realize that this is just the base-10 representation of a three-digit binary number.↩︎\nThis is another one where you’ll almost never use ps without aux.↩︎\nAlternatively, you could open a file of the right name and just copy/paste the contents of your public key in there. Either way works.↩︎"
  },
  {
    "objectID": "chapters/sec3/3-3-networking.html#tcpip-and-the-mail",
    "href": "chapters/sec3/3-3-networking.html#tcpip-and-the-mail",
    "title": "12  Intro to Computer Networks",
    "section": "12.1 TCP/IP and the mail",
    "text": "12.1 TCP/IP and the mail\nLet’s start by imagining you have a penpal, who lives in an apartment building across the country.1\nLet’s say you’ve got some juicy gossip to share with your penpal. Because this gossip is so juicy, you’re not going to put it on a postcard. You’d write your letter, put it inside an envelope, address the letter to the right spot, and put it in the mail.\nYou trust the postal service to take your letter, deliver it to the correct address, and then your friend will be able to read your letter.\nThe process of actualy getting data from one computer to another is governed by the TCP/IP protocol and is called packet switching.2\nWhen a computer has data to send to another computer, it takes that information and packages it up into a bundle called a packet.3\nThe data in the packet is like the contents of your letter.\nJust as the postal service defines permissible envelope sizes and address formats, the TCP/IP protocol defines what a packet has to look like from the outside, including what constitutes a valid address.\nA uniform resource locator (URL) is the way to address a specific resource on a network.\nA full URL includes 4 pieces of information: \\[\\overbrace{\\text{https://}}^\\text{protocol}\\overbrace{\\text{example.com}}^\\text{address}\\overbrace{\\text{:443}}^\\text{port}\\overbrace{\\text{/}}^\\text{resource}\\]\nThe protocol starts the URL. It is separated from the rest of the URL by ://.\nEach of the rest of the pieces of the URL is needed to get to a specific resource and has a real-world analog.\nThe address is like the street address of your penpal’s building. It specifies the host where your data should go.4\nA host is any entity on a network that can receive requests and send responses. So a server is a common type of host on a network, but so is a router, your laptop, and a printer.\nThe port is like your friend’s apartment. It specifies which service on the server to address.\nLastly, the resource dictates what resource you want on the server. It’s like the name on the address of the letter, indicating that you’re writing to your penpal, not their mom or sister.\nThis full URL may look a little strange to you. You’re probably used to just putting a standalone domain like \\(google.com\\) into your browser when you want to go to a website. That’s because https, port 443, and / are all defaults, so modern browsers don’t make you specify them.\nBut what if you make it explicit? Try going to https://google.com:443/. What do you get?"
  },
  {
    "objectID": "chapters/sec3/3-3-networking.html#ports-get-you-to-the-right-service",
    "href": "chapters/sec3/3-3-networking.html#ports-get-you-to-the-right-service",
    "title": "12  Intro to Computer Networks",
    "section": "12.2 Ports get you to the right service",
    "text": "12.2 Ports get you to the right service\nA port is a location on a server where a network connection is possible. It’s like the apartment number for the mail. Each port has a unique number 1 to just over 65,000. By default, the overwhelming majority of the ports on the server are closed for security reasons.\nYour computer automatically opens ports to make outgoing connections, but if you want to allow someone to make inbound connections – like to access RStudio or a Shiny app on a server – you need to manually configure and open a port.\nAny program that is running on a server and that you intend to be accessible from the outside is called a service. For example, we set up RStudio, JupyterHub, and a Plumber API as services in the lab in Chapter 11. Each service lives on a unique port on the server.\nSince each service running on a server needs a unique port, it’s common to choose a somewhat random relatively high-numbered port. That makes sure it’s unlikely that the port will conflict with another service running on the server.\nFor example RStudio Server runs on port 8787 by default. According to JJ Allaire, there’s no special meaning to this port. It was chosen because it was easy to remember and not 8888, which some other popular projects had taken.\nThere’s a cheatsheet of commonly used ports at the end of the chapter."
  },
  {
    "objectID": "chapters/sec3/3-3-networking.html#assigning-addresses",
    "href": "chapters/sec3/3-3-networking.html#assigning-addresses",
    "title": "12  Intro to Computer Networks",
    "section": "12.3 Assigning addresses",
    "text": "12.3 Assigning addresses\nThe proper address of a server on a network is defined by the Internet Protocol (IP) and is called an IP Address. IP addresses are mapped to human-friendly domains like \\(google.com\\) with the Domain Name Service (DNS).\n\n\n\n\n\n\nNote\n\n\n\nIn this chapter, we’re going to set DNS aside and talk exclusively about IP Addresses.\nChapter 13 is all about DNS.\n\n\nEvery host on the internet has a public IP address.\nA public IP address is an IP address that is valid across the entire internet. That means that each public IP address is unique across the entire internet. You can think of a public IP address like the public street address of a building.\nBut many hosts are not publicly accessible on the internet. Many are housed in private networks. A private network is one where the hosts aren’t directly accessible to the public. You can think of a host in a private network like a building inside a gated community. You may still be able to get there from the public, but you can’t just walk up to the building from the street. Instead you need to come in through the specific gates that have been permitted and approach only on the roads that are allowed.\nTODO: Image of public IPs like street address, private like cul-de-sac\nThere are many different kinds of private networks. Some are small and enforced by connection to a physical endpoint, like the private network your WiFi router controls that houses your laptop, phone, TV, Xbox, and anything else you allow to connect to your router. In other cases, the network is a software network. Many organizations have virtual private networks (VPNs). In this case, you connect to the network via software. There may be resources you can only connect to inside your VPN and there also might be limitations about what you can go out and get.\nThere are a few different reasons for this public/private network split. The first is security. If you’ve got a public IP address, anyone on the internet can come knock on your virtual front door. That’s actually not so bad. What’s more problematic is that they can go all the way around the building looking for an unlocked side door. Putting your host inside a private network is a way to ensure that people can only approach the host through on the pathways you intend.\nThe second reason is convenience.\nPrivate networks provide a nice layer of abstraction for network addresses.\nYou probably have a variety of network-enabled devices in your home, from your laptop and phone to your TV, Xbox, washing machine, and smart locks. But from the outside, your house has only one public IP address – the address of the router in your home. That means that your router has to keep track of all the devices you’ve got, but they don’t need to register with any sort of public service just to be able to use the internet.\nAs we’ll get into in Chapter 13, keeping track of IP Addresses is best left to machines. If you’re managing a complex private network, it’s really nice to give hostnames to individual hosts. A nice feature of a private hostname compared to a public address is that you don’t have to worry if the hostname is unique across the entire internet – it just has to be unique inside your private network.\n\n12.3.1 Firewalls, allow-lists, and other security settings\nOne of the most basic ways to keep a server safe is to not allow traffic from the outside to come in. Generally that means that in addition to keeping the server ports themselves closed, you’ll also have a firewall up that defaults to all ports being closed.\nIn AWS, the basic level of protection for your server is called the security group. If you remember, we used a default security group in launching your server. When you want to go add more services to your server, you’ll have to open additional ports both on the server and in the security group.\nIn addition to keeping particular ports closed, you can also set your server to only allow incoming traffic from certain IP addresses. This is generally a very coarse way to do security, and rather fragile. For example, you could configure your server to only accept incoming requests from your office’s IP address, but what if someone needs to access the server from home or the office’s IP address is reassigned?\nOne thing that is not a security setting is just using a port that’s hard to guess. For example, you might think, “Well, if I were to put SSH on port 2943 instead of 22, that would be safer because it’s harder to guess!” I guess so, but it’s really just an illusion of security. There are ways to make your server safer. Choosing esoteric port numbers really isn’t it."
  },
  {
    "objectID": "chapters/sec3/3-3-networking.html#how-packets-are-routed",
    "href": "chapters/sec3/3-3-networking.html#how-packets-are-routed",
    "title": "12  Intro to Computer Networks",
    "section": "12.4 How packets are routed",
    "text": "12.4 How packets are routed\nThe way packets travel from one computer to another is called routing. A router is a hardware or software device that route packets.\nYou can think of routers and public networks as existing in trees. Each router knows about the IP addresses downstream of it and also the single upstream default address.5\nTODO: diagram of routers in trees\nFor example, the router in your house just keeps track of the actual devices that are attached to it. So if you were to print something from your laptop, the data would just go to your router and then to your printer.\nOn the other hand, when you look at a picture on Instagram, that traffic has to go over the public network. The default address for your home’s router is probably one owned by your internet service provider (ISP) for your neighborhood. And that router’s default address is probably also owned by your ISP, but for a broader network.\nSo your packet will get passed upstream to a sufficiently general network and then back downstream to the actual address you’re trying to reach.\nMeanwhile, your computer just waits for a response. Once the server has a response to send, it comes back using the same technique. Obviously a huge difference between sending a letter to a penpal and using a computer network is the speed. Where sending a physical letter takes a minimum of days, sending and receiving packets over a network is so fast that the delay is imperceptible when you’re playing a multiplayer video game or collaborating on a document online."
  },
  {
    "objectID": "chapters/sec3/3-3-networking.html#how-to-recognize-an-ip-address",
    "href": "chapters/sec3/3-3-networking.html#how-to-recognize-an-ip-address",
    "title": "12  Intro to Computer Networks",
    "section": "12.5 How to recognize an IP address",
    "text": "12.5 How to recognize an IP address\nYou’ve probably seen IPv4 addresses many times. They’re four blocks of 8-bit fields (numbers between 0 and 255) with dots in between, so they look something like 65.77.154.233.\nIf you do the math, you’ll realize there are “only” about 4 billion of these. While we can stretch those 4 billion IP addresses to many more devices since most devices only have private IPs, we are indeed running out of public IPv4 addresses.\nThe good news is that smart people started planning for this a while ago. In the last few years, adoption of the new IPv6 standard has started. IPv6 addresses are eight blocks of hexadecimal (0-9 + a-f) digits separated by colons, with certain rules that allow them to be shortened, so 4b01:0db8:85a3:0000:0000:8a2e:0370:7334 or 3da4:66a::1 are both examples of valid IPv6 addresses.\nIPv6 will coexist with IPv4 for a few decades and we’ll eventually switch entirely to IPv6. There’s no worry about running out of IPv6 addresses any time soon, because the total quantity of IPv6 addresses is a number with 39 zeroes.\n\n12.5.1 Reserved IP Addresses\nMost IPv4 addresses are freely available to be assigned, but there are a few that you’ll see in particular contexts and it’s useful to know what they are.\nThe first IP address you’ll see a lot is 127.0.0.1, also known as localhost or loopback. This is the way a machine refers to itself.\nFor example, if you open a Shiny app in RStudio Desktop, the app will pop up in a little window along with a notice that says\nListening on http://127.0.0.1:6311\nThat http://127.0.0.1 is indicating that your computer is serving the Shiny app to itself on the localhost address.\nThere are also a few blocks of addresses that are reserved for use on private networks, so they’re never assigned in public.\n\n\n\n\n\n\n\nCode\nMeaning\n\n\n\n\n127.0.0.1\nlocalhost or loopback – the machine that originated the request\n\n\n192.168.x.x\n172.16.x.x.x\n10.x.x.x\nProtected address blocks used for private IP addresses.\n\n\n\nYou don’t really need to remember these, but it’s very likely you’ve seen an address like 192.168.0.1 or 192.168.1.1 if you’ve ever tried to configure a router or modem for your home wifi.\nNow you know why."
  },
  {
    "objectID": "chapters/sec3/3-3-networking.html#basic-network-troubleshooting",
    "href": "chapters/sec3/3-3-networking.html#basic-network-troubleshooting",
    "title": "12  Intro to Computer Networks",
    "section": "12.6 Basic network troubleshooting",
    "text": "12.6 Basic network troubleshooting\nNetworking can be difficult to manage because there are so many layers where it can go awry. Let’s say you’ve configured a service on your server, but you just can’t seem to access it.\nThe ping command can be useful for checking whether your server is reachable on the network. For example, here’s what happens when I ping the domain where this book sits.\n&gt; ping -o do4ds.com                                                                        \nPING do4ds.com (185.199.110.153): 56 data bytes\n64 bytes from 185.199.110.153: icmp_seq=0 ttl=57 time=13.766 ms\n\n--- do4ds.com ping statistics ---\n1 packets transmitted, 1 packets received, 0.0% packet loss\nround-trip min/avg/max/stddev = 13.766/13.766/13.766/0.000 ms\nThis looks great – it sent 1 packet to the server and got one back. That’s exactly what I want. Seeing an unreachable host or packet loss would be an indication that my networking probably isn’t configured correctly somewhere between me and the server. I generally like to use ping with the -o option for sending just one packet – as opposed to continuously trying.\nIf ping fails, it means that my server isn’t reachable. The things I’d want to check is that I have the URL for the server correct, that DNS is configured correctly (see Chapter 13), that I’ve correctly configured any firewalls to have the right ports open (Security Groups in AWS), and that any intermediate networking devices are properly configured (see more on proxies in Chapter 17).\nIf ping succeeds but I still can’t access the server, curl is good to check. curl actually attempts to fetch the website at a particular URL. It’s often useful to use curl with the -I option so it just returns a simple status report, not the full contents of what it finds there.\nFor example, here’s what I get when I curl CRAN from my machine.\n &gt; curl -I https://cran.r-project.org/                                                         \n \nHTTP/1.1 200 OK\nDate: Sun, 15 Jan 2023 15:34:19 GMT\nServer: Apache\nLast-Modified: Mon, 14 Nov 2022 17:33:06 GMT\nETag: \"35a-5ed71a1e393e7\"\nAccept-Ranges: bytes\nContent-Length: 858\nVary: Accept-Encoding\nContent-Type: text/html\nThe important thing here is that first line. The server is returning a 200 HTTP status code, which means all is well. For more on HTTP status codes and how to interpret them, see Chapter 2.\nIf ping succeeds, but curl does not, it means that the server is up at the expected IP address, but the service is not accessible. At that point, you might check whether the right ports are accessible – it’s possible to (for example) have port 443 or 80 accessible on your server, but not the port you actually need for your service. You also might check on the server itself that the service is running and that it is running on the port you think it is.\nIf you’re running inside a container, you should check that you’ve properly configured the port inside container to be forwarded to the outside."
  },
  {
    "objectID": "chapters/sec3/3-3-networking.html#comprehension-questions",
    "href": "chapters/sec3/3-3-networking.html#comprehension-questions",
    "title": "12  Intro to Computer Networks",
    "section": "12.7 Comprehension Questions",
    "text": "12.7 Comprehension Questions\n\nWhat are the 4 components of a URL? What’s the significance of each?\nWhat are the two things a router keeps track of? How does it use each of them?\nAre there any inherent differences between public and private IP addresses?\nWhat is the difference between an IP address and a port?\nLet’s say you’ve got a server at 54.33.115.12. Draw a mind map of what happens when you try to SSH into the server. Your explanation should include the terms: IP Address, port, 22, default address, router, sever."
  },
  {
    "objectID": "chapters/sec3/3-3-networking.html#lab-making-it-accessible-in-one-place",
    "href": "chapters/sec3/3-3-networking.html#lab-making-it-accessible-in-one-place",
    "title": "12  Intro to Computer Networks",
    "section": "12.8 Lab: Making it accessible in one place",
    "text": "12.8 Lab: Making it accessible in one place\nRight now, the only way to get to the various services on our server is only possible via an SSH tunnel. That’s fine for you as you’re working with it – but doesn’t work well if you want to share with other folks.\nNow, you could just open up the different ports each service is on and have people access them there – RStudio on 8787, JupyterHub on 8000, and the API on 8080. But that’s not really ideal. That would mean your users would have to remember and use those ports.\n\n\n\n\n\n\nTip\n\n\n\nIf you do want to try it to prove to yourself that this “works”, go to your server’s Security Group settings and add custom TCP rules allowing access to ports 8787, 8000, and 8080 from anywhere. If you visit $SERVER_ADDRESS:8787 you should get RStudio, similarly with JupyterHub at $SERVER_ADDRESS:8000, and the API at $SERVER_ADDRESS:8080.\nOk, now close those ports back up so we can do this the right way.\n\n\nSo instead, we want all the traffic to come in one front door and for users to use convenient subpaths to reach the services. The tool to accomplish this kind of rerouting is called a proxy.\nIn our case, we’re just going to run a software proxy on our server that reroutes traffic to different ports on our server. We’re going to use Nginx – a very popular open source proxy.\nTODO: Image of proxy\nProxies are an advanced networking topic. Most enterprise networks make extensive uses of proxies. For the data science workbench you’re configuring, you may also need to use a proxy because some open source tooling doesn’t support configuring SSL/HTTPS and don’t permit authentication.\nLet’s get it configured.\n\n12.8.1 Step 1: Configure Nginx\nThe first thing we’re going to do is to configure Nginx on our server. Configuring Nginx is pretty straightforward – you install Nginx, put the configuration file into place, and restart the service to pick up the changes. The hard part is figuring out the right configuration. I’ve tested these steps, and they should work for you the first time. But if they don’t, you’re about to learn about the pain of proxy debugging.\nHere are the steps to configure your proxy on your server:\n\nSSH into your server.\nInstall Nginx with sudo apt install nginx.\nSave a backup of the default nginx.conf, cp /etc/nginx/nginx.conf /etc/nginx/nginx-backup.conf.6\nEdit the Nginx configuration with sudo vim /etc/nginx/nginx.conf and replace it with:\n\n\n\n/etc/nginx/nginx.conf\n\nuser www-data;\nworker_processes auto;\npid /run/nginx.pid;\ninclude /etc/nginx/modules-enabled/*.conf;\n\nevents {\n\tworker_connections 768;\n\t# multi_accept on;\n}\n\nhttp {\n\n map $http_upgrade $connection_upgrade {\n    \t\tdefault upgrade;\n    \t\t''      close;\n  }\n\n  server {\n    listen 80;\n\n    location /rstudio/ {\n      # Needed only for a custom path prefix of /rstudio\n      rewrite ^/rstudio/(.*)$ /$1 break;\n\n      # Use http here when ssl-enabled=0 is set in rserver.conf\n      proxy_pass http://localhost:8787;\n\n      proxy_http_version 1.1;\n      proxy_set_header Upgrade $http_upgrade;\n      proxy_set_header Connection $connection_upgrade;\n      proxy_read_timeout 20d;\n\n      # Not needed if www-root-path is set in rserver.conf\n      proxy_set_header X-RStudio-Root-Path /rstudio;\n\n      # Optionally, use an explicit hostname and omit the port if using 80/443\n      proxy_set_header Host $host:$server_port;\n    }\n\n    location /jupyter/ {\n      # NOTE important to also set bind url of jupyterhub to /jupyter in its config\n      proxy_pass http://127.0.0.1:8000;\n\n      proxy_redirect   off;\n      proxy_set_header X-Real-IP $remote_addr;\n      proxy_set_header Host $host;\n      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n      proxy_set_header X-Forwarded-Proto $scheme;\n\n      # websocket headers\n      proxy_set_header Upgrade $http_upgrade;\n      proxy_set_header Connection $connection_upgrade;\n    }\n\n    location /penguins/ {\n      proxy_pass http://127.0.0.1:8555/;\n      proxy_set_header Host $host;\n    }\n  }\n}\n\n\n\nTest that your configuration is valid sudo nginx -t.\nStart Nginx with sudo systemctl start nginx. If you see nothing all is well.\n\nIf you need to change anything, update the config and then restart with sudo systemctl restart nginx.\n\n\n12.8.2 Step 2: Open port 80\nNow, if you try to go to your server, your browser will spin for a while and nothing will happen. That’s because the AWS security group still only allows SSH access on port 22. We need to add a rule that will allow HTTP access on port 80.\nOn the AWS console page for your instance, find the Security section and click into the security group for your instance. You want to add a new inbound HTTP rule that allows access on port 80 from anywhere. Make sure not to get rid of the rule that allows SSH access on 22. You still need that one too.\nOnce you do this, you should be able to visit your server address and get the default Nginx landing page.\n\n\n12.8.3 Step 3: Configure RStudio Server and JupyterHub to be on a subpath\nComplex web apps like RStudio and JupyterHub frequently reroute people back to themselves. In general, they assume that they’re on the root path /. That’s not true in this case, so we’ve got to let them know about the subpath where they’re actually located.\nConfiguring RStudio Server is already done. The X-RStudio-Root-Path line in the Nginx configuration adds a header to each request coming through the proxy that tells RStudio Server that it’s on the /rstudio path.\nJupyter needs an explicit update to its own configuration to let it know that it’s on a subpath. Luckily it’s a very simple change. You can edit the Jupyter configuration with\n\n\nTerminal\n\nsudo vim /etc/jupyterhub/jupyterhub_config.py\n\nFind the line that reads # c.JupyterHub.bind_url = 'http://:8000'.\n\n\n\n\n\n\nTip\n\n\n\nYou can search in vim from normal mode with / &lt;thing you're searching for&gt;. Go to the next hit with n.\n\n\nDelete the # to uncomment the line and add the subpath on the end. If you’re using the /jupyter subpath and the default 8000 port, that line will read c.JupyterHub.bind_url = 'http://:8000/jupyter'.\nJupyterHub should pick up the new config when it’s restarted with\n\n\nTerminal\n\nsudo systemctl restart jupyterhub\n\n\n\n12.8.4 Step 4: Try it out!\nNow we should have each service configured on a subpath. RStudio Server at /rstudio, JupyterHub at /jupyter, and our machine learning API at /penguins. For example, with my server at ec2-54-159-134-39.compute-1.amazonaws.com, I can get to RStudio Server at http://ec2-54-159-134-39.compute-1.amazonaws.com/rstudio.\n\n\n\n\n\n\nNote\n\n\n\nAs of this writing, the machine learning API serves itself off of /__docs__/, so you actually won’t be able to access anything interesting at /penguins. Instead, you’ll find the API at /penguins/__docs__/.\nThis should be fixed before final publication of this book.\n\n\nNote that right now, this server is on HTTP, which is not a best practice. In fact, it’s such a bad practice that your browser will probably autocorrect the url to start with https and it won’t work. You’ll have to manually correct it to http. Don’t leave it like this for long – make sure to make sure to configure https in Chapter 14 before doing anything real on this server.\n\n\n12.8.5 Lab Extensions\nIf you’ve gone to the bare URL for your server, you’ve probably noticed that it’s just the default Nginx landing page, which is not very attractive.\nYou might want to create a landing page with links to the subpath by serving a static html page off of /. Or maybe you want one of the services at / and the others at a different subpath.\nIf you want to change the subpaths, the location lines in the nginx.conf define the subpaths where the services will be available. By changing those locations, you can change the paths where the services live or you could add another that serves a static web page.\nYou also could add a different service at a different path. Note that the proxy_path lines define the port where the service is running on the server. Depending on the service you’re configuring, there may be other configuration you’ll have to do, but that will vary on a service-by-service basis.\n\n\n12.8.6 Cheatsheet: Special Ports\nAll ports below 1024 are reserved for common server tasks, so you can’t assign services to low-numbered ports.\nThere are also three common ports that will come up over and over. These are handy because if you’re using the relevant service, you don’t have to indicate if it’s using the default port.\n\n\n\nProtocol\nDefault Port\n\n\n\n\nHTTP\n80\n\n\nHTTPS\n443\n\n\nSSH\n22"
  },
  {
    "objectID": "chapters/sec3/3-3-networking.html#footnotes",
    "href": "chapters/sec3/3-3-networking.html#footnotes",
    "title": "12  Intro to Computer Networks",
    "section": "",
    "text": "For this analogy to work, everyone lives in apartment buildings rather than standalone houses.↩︎\nTCP and IP are actually two separate protocols at different layers of the protocol stack. But they’re so closely linked that we can talk about them as one.↩︎\nOne of the biggest ways the mail is not like packet switching is that your message gets chopped up among lots of different packets, which are routed independently, and are reassembled when they get where they’re going. Works well for computers, not so well for real-world mail. It’s also pretty much an irrelevant detail, since this whole process works quite invisibly.↩︎\nOften it’s not a single server at the address – but it behaves like one. It could instead be a proxy in front of a cluster of servers or even more complicated routing. All that matters is that it behaves like a single server from the outside.↩︎\nThere are actually a few different types of addresses used to do this. IP addresses are used for identifying network resources and the MAC address used for physical hardware. Your router is also responsible for assigning IP addresses to devices as they join the network via the dynamic host configuration protocol (DHCP). I’m glossing over all these details as they’re immaterial to the understanding important for this chapter.↩︎\nThis is generally a good practice before you start messing with config files. Bad configuration is usually preferable to a service that can’t start at all because you’ve messed up the config so badly. It happens.↩︎"
  },
  {
    "objectID": "chapters/sec3/3-4-dns.html#how-dns-lookups-work",
    "href": "chapters/sec3/3-4-dns.html#how-dns-lookups-work",
    "title": "13  DNS allows for human-readable addresses",
    "section": "13.1 How DNS lookups work",
    "text": "13.1 How DNS lookups work\nIn Chapter 12, we discussed how packets gets routed to successively higher-level routers until there’s one that knows where the packet is going and sends it back down. It turns out that this process essentially happens twice. The first time is to get the IP address for the domain in a process called DNS resolution. The second time is when it actually sends the information to that IP address.\nYou can see the basic structure of a DNS lookup using the terminal command nslookup. Here’s the DNS routing for \\(google.com\\):\n ❯ nslookup google.com                                              \nServer:     192.168.86.1\nAddress:    192.168.86.1#53\n\nNon-authoritative answer:\nName:   google.com\nAddress: 142.251.163.113\nName:   google.com\nAddress: 142.251.163.100\nName:   google.com\nAddress: 142.251.163.138\nName:   google.com\nAddress: 142.251.163.101\nName:   google.com\nAddress: 142.251.163.102\nName:   google.com\nAddress: 142.251.163.139\nThese addresses do change periodically, so they may no longer be valid by the time you read this. But if you want to go do an nslookup yourself, you can visit one of those IP addresses and see that it takes you right to the familiar \\(google.com\\).1\nDNS resolution is actually quite a complex process because every computer in the world needs to be able to resolve a DNS entry to the same IP address in a timely fashion. The simplest form of DNS lookup would be easy – there would just be one nameserver with the top-level domains and then one server for each top-level domain.\nThere are two problems with this theoretical DNS lookup – it wouldn’t be very resilient because of a lack of redundancy and it would be slow. In order to speed things up, the DNS system is highly decentralized.\nAlong with decentralization, DNS resolution relies on a lot of cacheing. When your computer or an intermediate DNS server looks up an IP address for you, it caches it. This is because its likely that if you’ve looked up a domain once, you’re going to do it again soon.\nThis is great if you are using the internet and don’t want to wait for DNS lookups, but when you’re changing the domains on servers you control, there are thousands of public DNS servers that a request could get routed to, and many of them may have outdated cache entries. DNS changes can take up to 24 hours to propagate.\nThat means that if you make a change and it’s not working, you have no idea whether you made a mistake or it just hasn’t propagated yet. It’s very annoying.\nSometimes, using a private browsing window will cause DNS cache refreshes, but not always."
  },
  {
    "objectID": "chapters/sec3/3-4-dns.html#how-dns-is-configured",
    "href": "chapters/sec3/3-4-dns.html#how-dns-is-configured",
    "title": "13  DNS allows for human-readable addresses",
    "section": "13.2 How DNS is configured",
    "text": "13.2 How DNS is configured\nFrom the perspective of someone trying to set up their own website, there’s only one DNS server that matters to you personally – the DNS server for your domain name registrar.\nDomain name registrars are the companies that actually own domains. You can buy or rent one from them in order to have a domain on the internet.\nYour first stop would be a domain name registrar where you’d find an available domain you like and pull out your credit card.\nCosts for domain names vary widely. Buying a meaningless domain in a less popular top-level domain, say ladskfmlsdf.me can cost as little as $3 per year. On the other hand, buying a .com domain that’s a real word or phrase can be a few thousand dollars. There are articles every few years about some major company accidentally allowing their domain name to lapse and ransoming it back for stupid amounts of money.\nConfiguration of DNS is done by way of records, of which there are a menagerie of types you can configure. Luckily, most simple configurations only need CNAME and A records.\nHere’s an imaginary DNS record table for the domain example.com:\n\n\n\nPath/Host\nType\nTarget\n\n\n\n\n@\nA\n143.122.8.32\n\n\nwww\nCNAME\nexample.com\n\n\n*\nA\n143.122.8.33\n\n\n\nLet’s go through how to read this table.\nSince we’re configuring example.com, the paths/hosts in this table are relative to example.com.\nIn the first row we’re configuring an A record to go to the target IP address. A records (or their IPv6 cousin AAAA records) map a domain to an actual IP address. The path @ is a special symbol meaning exact match. So by this configuration, any traffic to example.com will be passed straight through to the specified IP address.\nThe second row deals with traffic to the www subdomain. CNAME records alias sub-domains. They’re most frequently used to map sub-domains to the main domain. Since this is a CNAME record for example.com, this record indicates that traffic to www.example.com should be treated exactly like traffic to example.com. Some domain providers do automatic redirection of www traffic, and so this row may not be necessary in some configurations.\nThe last record uses the wildcard symbol * to send all subdomain traffic that’s not already spoken for – say blog.example.com or info.example.com directly to the IP address specified. In this case, I’m sending all of those subdomains to a different IP address, maybe a 404 (not found) page – or maybe I’m serving all the subdomains off a different server.\nSo what happens is that your query goes through several layers of public DNS servers to get to the DNS entry for your domain name registrar. In many cases, you’ll directly configure your domain name registrar to point to your website or server – but you also can configure the domain name registrar to point at another set of DNS servers you actually control with an NS record.\nIf you’re setting up your own server, this probably isn’t the case, but some large enterprises do run their own private DNS servers."
  },
  {
    "objectID": "chapters/sec3/3-4-dns.html#comprehension-questions",
    "href": "chapters/sec3/3-4-dns.html#comprehension-questions",
    "title": "13  DNS allows for human-readable addresses",
    "section": "13.3 Comprehension Questions",
    "text": "13.3 Comprehension Questions\nTODO"
  },
  {
    "objectID": "chapters/sec3/3-4-dns.html#lab-configuring-dns-for-your-server",
    "href": "chapters/sec3/3-4-dns.html#lab-configuring-dns-for-your-server",
    "title": "13  DNS allows for human-readable addresses",
    "section": "13.4 Lab: Configuring DNS for your server",
    "text": "13.4 Lab: Configuring DNS for your server\nIn the last lab, we configured the server so that all of the services were served off of one single port that redirected to various subpaths.\nNow we want to get a real URL for the server so that you don’t have to remember some random ec2- URL for your server. In this lab, we’ll configure DNS records for our server so it’s available at a real URL.\n\n13.4.1 Step 1: Allocate Elastic IP\nEC2 instances get assigned a public IP when they are started. That’s great – it makes it easy to do the kind of work we’ve done. But if you’ve stopped or restarted your server, you’ve you’ve probably noticed that the public IP address changed!\nThis is no good for configuring a long-lived DNS record that consistently routes to your server. So we’re going to use AWS’s Elastic IP service, which gives you a stable public IP address you can keep and move around from one instance to another as you wish.\nElastic IP payment is a little weird. You don’t get charged for Elastic IPs as long as you’re using them. So if your server is up and using an elastic IP, it’s all free. If you take your server down and want to reserve the Elastic IP so it’s the same when you bring it back up, you’ll pay. It’s very cheap for one IP (12 cents per day as of this writing), so don’t worry about it.\nBasically AWS doesn’t want you hoarding Elastic IPs. Great – just make sure to give back the elastic IP if you take down your instance permanently.\n\n\n\n\n\n\nNote\n\n\n\nThe labs in this book are ordered to promote learning.\nIf you configure a server later for real, I’d recommend setting up an Elastic IP as soon as you bring the server up so you have a permanent IP address right off the bat.\n\n\nTo actually do it, find Elastic IP in the AWS console and allocate an address. Once it has been allocated, click through to the elastic IP and associate it with the default private IP address for your instance.\nNote that once you make this change, your server will no longer be available at its old IP address, so you’ll have to SSH in at the new one. If you have SSH terminals open when you make the change, they will break.\n\n\n13.4.2 Step 2: Buy a domain\nYou can buy a domain from any of the many domain purchasing services on the web. Now, it won’t be free, but many domains are very cheap.\nThe easiest place to buy a domain is via AWS’s Route53 service. You can get domains on Route53 for as little as $9 per year – but there are even cheaper services. For example, I was able to get the domain \\(do4ds-lab.shop\\) for $1.98 for a year on namecheap.com, which has long been my choice for purchasing domains because it is indeed, cheap.\n\n\n13.4.3 Step 3: Configure DNS\nOnce you’ve got your domain, you have to configure your DNS. You’ll have to create 2 A records – one each for the @ host and the * host pointing to your IP and one for the CNAME at the www with the value being your bare domain.\nExactly how you configure this will depend on the domain name provider you choose.\nIn NameCheap, my Advanced DNS configuration looks like this:\n\n\n\n13.4.4 Step 4: Wait an annoyingly long time\nNow you just have to be patient. Unfortunately DNS takes time to propagate. After a few minutes (or hours?), your server should be reachable at your domain.\nIf it’s not (yet) reachable, try seeing if an incognito browser works. If it doesn’t, wait some more. When you run out of patience, try reconfiguring everything and check if it works now.\nNote that we still haven’t configured SSL, so you’ll need to make sure to correct your browser away from the https URL.\n\n\n13.4.5 Step 5: Add the Shiny app to your site\nNow that the Shiny app is at a stable URL, let’s put it on our site so people can look at our penguin size prediction model.\nWe’re going to use something called an iFrame, which lets you embed one website inside another. Luckily, it’s really easy to create an iFrame in a Quarto site.\n\n\n\n\n\n\nNote\n\n\n\nOnce you change your website to go over HTTPS in the next section, you’ll have to adjust the iFrame URL as well.\n\n\nIn Quarto, you can just add an html block into your index.qmd. In my case, that block just says\n\n\nindex.qmd\n\n&lt;iframe width=\"780\" height=\"500\" src=\"https://do4ds-lab.shop/penguins/\" title=\"Penguin Model Explorer\"&gt;&lt;/iframe&gt;"
  },
  {
    "objectID": "chapters/sec3/3-4-dns.html#footnotes",
    "href": "chapters/sec3/3-4-dns.html#footnotes",
    "title": "13  DNS allows for human-readable addresses",
    "section": "",
    "text": "If you try to do an nslookup on some websites – like \\(do4ds.com\\), you’ll notice the IP address doesn’t actually take you to this book’s site. That’s because this book isn’t actually hosted directly on a server. It’s instead hosted on GitHub pages – so the IP addresses nslookup finds are the Google Pages servers, which themselves know how to resolve \\(do4ds.com\\).↩︎"
  },
  {
    "objectID": "chapters/sec3/3-5-ssl.html#the-problems-https-solves",
    "href": "chapters/sec3/3-5-ssl.html#the-problems-https-solves",
    "title": "14  You should use SSL/HTTPS",
    "section": "14.1 The problems https solves",
    "text": "14.1 The problems https solves\nFor a long time, most web traffic was just plain http traffic.\nIf you recall, there used to be a lot of warnings around that you shouldn’t just connect to and use the WiFi at your neighborhood Starbucks. That was (mostly) because of the risks of using plain http traffic.\nOver the last 10 years or so, almost all web traffic has migrated to https, particularly since 2015 when Google Chrome began the process of marking any site using plain http as insecure. These days it’s actually pretty safe to use any random WiFi network you want – because of https.\nThe biggest risk with those public WiFi networks was that someone could compromise the router. If the router was compromised, there were two things a bad actor could do.\nhttp has no way to verify that the website you think you’re interacting with is actually that website. So a bad actor could mess with the router’s DNS resolution, and make it so that \\(bankofamerica.com\\) went to a lookalike website to capture your banking login. That’s called a man-in-the-middle attack.\nhttp traffic is also completely unencrypted. So that packet you’re sending would be just as readable by someone who installed spyware on the router that’s passing it along as by the website you intended to send it to. This is called a packet sniffing attack.\nBoth of these risks are defanged when you’re using https rather than plain http.\n\n\n\n\n\n\nWhat doesn’t https do?\n\n\n\nUsing SSL/TLS helps a lot with security – but it’s worth noting that it ends at the domain. Using https validates that you’ve gotten to the real example.com if that’s the website you think you’re visiting. It doesn’t in any way validate who owns example.com, whether they’re reputable, or whether you should trust them.\nThere are higher levels of SSL certification that do validate that, for example, the company that owns google.com is actually the company Google.\n\n\nHere’s the process of connecting to a website that uses https. Before your browser starts exchanging any real data with the website, the website sends over its signature, which is generated with its private certificate.\nYour computer checks this signature with a trusted certificate authority (CA) who has a copy of the site’s public certificate. Once your browser has verified that the signature is valid, you know that you’re actually communicating with \\(bankofamerica.com\\) and you’re not victim to a man-in-the-middle attack.\nOnce the verification process is done, your browser then establishes an encryption routine with the website on the other end. Only then does it start sending real data – now encrypted so that only the website on the other end can read the contents.\n\n\n\n\ngraph TD\n    A[Browser] --&gt;|Request| B[SSL Certificate]\n    B --&gt; |Sent| A\n    C[Certificate Authority] --&gt; |Verifies| B\n    A --&gt; |Consults| C"
  },
  {
    "objectID": "chapters/sec3/3-5-ssl.html#configuring-https",
    "href": "chapters/sec3/3-5-ssl.html#configuring-https",
    "title": "14  You should use SSL/HTTPS",
    "section": "14.2 Configuring https",
    "text": "14.2 Configuring https\nThere are three steps to configuring https on a website you control – getting an SSL certificate, putting the certificate on the server, and making sure the server only accepts https traffic.\nIt used to be that getting an SSL certificate was somewhat of a pain. While it could cost less than $10 per year to get a basic SSL certificate for particular subdomains, getting a wildcard certificate that covered all the subdomains of a root domain could be quite pricey.\nThe main alternative at the time was to use a self-signed cert. With a self-signed cert, you would create a certificate yourself and add the certificate directly to any machine that needed to access that site. This is a pain. You need to share the certificate with everyone and get them to install it on their machine. Certificates do expire and so this would have to be periodically repeated. The alternative is just to ignore the scary warning that you might be the victim of a man-in-the-middle attack – and some software doesn’t even allow that.\nLuckily there’s now another option. With the rise of https everywhere, there’s now an organization called letsencrypt. It’s a free CA that issues basic SSL/TLS certificates for free. They even have some nice tooling that makes it super easy to create and configure your certificate right on your server.\nWhen you’re talking about communications that run entirely inside private networks, you have a choice to make. Large organizations often run their own private CA that verifies all the certificates inside your private network.\nThis is a bit of a hassle, since you’ve got to maintain the CA and configure every host inside the network to trust your private CA. Many organizations decide they’re fine with plain http communication within private networks.\nEither way, once you’ve configured your server with https, you generally want to shut down access via plain http, usually by redirecting http traffic, which comes in on port 80 to come in over https via port 443."
  },
  {
    "objectID": "chapters/sec3/3-5-ssl.html#comprehension-questions",
    "href": "chapters/sec3/3-5-ssl.html#comprehension-questions",
    "title": "14  You should use SSL/HTTPS",
    "section": "14.3 Comprehension Questions",
    "text": "14.3 Comprehension Questions\n\nWhat are the two risks of using plain http and how does https mitigate that?\nWrite down a mental map of how SSL secures your web traffic. Include the following: public certificate, private certificate, certificate authority, encrypted traffic, port 80, port 443"
  },
  {
    "objectID": "chapters/sec3/3-5-ssl.html#lab-configure-ssl",
    "href": "chapters/sec3/3-5-ssl.html#lab-configure-ssl",
    "title": "14  You should use SSL/HTTPS",
    "section": "14.4 Lab: Configure SSL",
    "text": "14.4 Lab: Configure SSL\nWe’re going to use letsencrypt’s certbot utility to automatically generate an SSL certificate, share it with the CA, install it on the server, and even update your nginx configuration.\nFor anyone who’s never dealt with self-signing certificates in the past, let me tell you, the fact that it does this all automatically is magical!\n\n14.4.1 Step 1: Update Nginx configuration\nIn our nginx configuration, we’ll need to add a line certbot will use to know which site to generate the certificate for.\nSomewhere inside the nginx configuration’s server block, add\n\n\n/etc/nginx/nginx.conf\n\nserver_name do4ds-lab.shop www.do4ds-lab.shop;\n\nsubstituting in your domain for mine. Make sure to include both the bare domain and the www subdomain.\n\n\n14.4.2 Step 2: Certbot does its thing\nIf you google “configure nginx with letsencrypt”, there’s a great article by someone at Nginx walking you through the process.\nAs of this writing, that was as simple as running these lines on the server\n\n\nTerminal\n\nsudo apt-get install certbot python3-certbot-nginx \nsudo systemctl restart nginx \nsudo certbot --nginx -d do4ds-lab.shop -d www.do4ds-lab.shop\n\nBefore you move along, I’d recommend you take a moment and inspect the /etc/nginx/nginx.conf file to look at what certbot added.\nYou’ll notice two things. You’ll notice that the listen 80 is gone from inside the server block. We’re no longer listening for http traffic.\nInstead there’s a listen 443 – the default port for https, and a bunch of stuff that tells Nginx where to find the certificate on the server.\nScrolling down a little, there’s a new server block that is listening on 80. This block returns a 301, which you might recall is a redirect code (specifically for a permanent redirect) sending all http traffic to https.\n\n\n14.4.3 Step 3: Let RStudio know it’s on https\nBefore we exit and test it out, let’s do one more thing. RStudio does a bunch of sending traffic back to itself. For that reason, the /rstudio proxy location also needs to know to upgrade traffic from http to https.\nSo add the following line to the Nginx config:\n\n\n/etc/nginx/nginx.conf\n\nproxy_set_header X-Forwarded-Proto https;\n\nThis line adds a header to the traffic the proxy forwards specifying the protocol to be https. RStudio Server uses this header to know that it should send traffic back to itself using https, not http.\nOk, now try it by going to the URL your server is hosted at, and you’ll find that…it’s broken again.\nBefore you read along, think for just a moment. Why is it broken?\n\n\n14.4.4 Step 4: Configure security groups\nIf your thoughts went to something involving ports and AWS security groups, you’re right!\nBy default, our server was open to SSH traffic on port 22. Since then, we may have opened or closed port 80, 8000, 8787, and/or 8080.\nBut now that we’re exclusively sending https traffic into the proxy on 443 and letting the proxy redirect things elsewhere. So you have to go into the security group settings and change it so there are only 2 rules – one that allows SSH traffic on 22 from anywhere and one that allows https traffic on 443 from anywhere.\nIt’s up to you whether you want to leave port 80 open. If you do, it will redirect people to https on 443. If you close it entirely, people who come to port 80 will just be blocked and will eventually get a timeout. That shouldn’t be an issue as all modern browsers default to https.\n\n\n\n\n\n\nNote\n\n\n\nIf you decided to only configure one service on the server, you could have a much simpler setup. Neither RStudio Server nor Plumber support direct configuration of a SSL certificate, so you would still need a proxy – though it could be a much simpler configuration that just passes traffic at the root / on to your service at a different port.\nJupyterHub can be configured directly with an SSL certificate, so you would just configure JupyterHub to be aware of the SSL certificate and put itself on port 443.\n\n\n\n\n14.4.5 Step 5: Check it out!\nNOW you should be able to go to &lt;your-domain&gt;/rstudio and get to RStudio, &lt;your-domain&gt;/jupyter to get to JupyterHub, and &lt;your-domain&gt;/penguins to get the API! Voila!\nAnd you should still be able to SSH into your server on your domain.\nAt this point your server is fully configured. You have a server hosting three real data science services available on a domain of your choosing protected by https. You can really use this server to do real data science work.\nThat’s not to say this is fully enterprise-ready. If you’re working on a small team or at a small organization, this may be sufficient. But if you’re working at a large organization, your IT/Admin group is going to have other concerns.\nYou can learn more about what those concerns are in the last section of this book, starting with ?sec-4-intro."
  },
  {
    "objectID": "chapters/sec3/3-6-servers.html#computers-are-just-addition-factories",
    "href": "chapters/sec3/3-6-servers.html#computers-are-just-addition-factories",
    "title": "15  Choosing the right server for you",
    "section": "15.1 Computers are (just) addition factories",
    "text": "15.1 Computers are (just) addition factories\nYou’re probably aware that everything you’ve ever seen on a computer – from this book to your work in R or Python, your favorite internet cat videos, and Minecraft – it’s just 1s and 0s.\nThe theory of why this works is deep and fascinating.1 Luckily, the amount of computational theory you need to understand to be an effective data scientist can be can be summarized in three sentences:\n\nEverything on a computer is represented by a (usually very large) number.\nAt a hardware level the only thing computers do is add these numbers together.\nModern computers add very quickly and very accurately.\n\nThis means that for your day-to-day work, you can think of a computer as just a big factory for doing additions.\nEvery bit of input your computer gets is turned into an addition problem, processed, and the results are reverted back into something we interpret as meaningful.\nThe addition assembly line itself – where the work actually gets done – is referred to as compute. It’s where 2+2 gets turned into 4, and where 345619912 + 182347910 gets turned into 527967822.\nThere are a number of different kinds of compute, but they all basically work the same. The total speed is determined by two factors – the number of conveyor belts (cores) and the speed at which each belt is running (clock speed).\nWe’re going to spend some time considering the three important resources – compute, memory, and storage – you have to allocate and manage on your computer or server. We’re going to spend some time generally exploring what each one is and then get into how you should think about each specifically with regard to data science."
  },
  {
    "objectID": "chapters/sec3/3-6-servers.html#recommendation-1-fewer-faster-cpu-cores",
    "href": "chapters/sec3/3-6-servers.html#recommendation-1-fewer-faster-cpu-cores",
    "title": "15  Choosing the right server for you",
    "section": "15.2 Recommendation 1: Fewer, faster CPU cores",
    "text": "15.2 Recommendation 1: Fewer, faster CPU cores\nAll computers have a central processing unit (CPU). These days, most consumer-grade laptops have between 4 and 16 cores, and may have additional capabilities that effectively doubles that number. So most laptop CPUs can do between 4 and 32 simultaneous addition problems.\nClock speeds are measured in operations per second or hertz (hz). The cores in your laptop probably run between two and five gigahertz (GHz), which means between 2 and 5 billion operations per second when running at full speed.\nFor decades, many of the innovations in computing were coming from increases in clock speed, but raw increases in speed have fallen off a lot in the last few decades. The clock speeds of consumer-grade chips increased by approximately 10x during the 90s, by 2-3x in the 2000s, and somewhere between not at all and 1.5x in the 2010s.\nBut computers have continued getting a lot faster even as the increase in clock speeds has slowed. The increase has mostly come from increases in the number of cores, better software usage of parallelization, and innovative chip architectures for special-purpose usage.\nR and Python are single-threaded. Unless you’re using special libraries for parallel processing, you’ll end up red-lining a single CPU core while the other just look on in silence.\nTherefore for most R and Python work, single core clock speed matters more than the number of cores, and fewer, faster cores are usually preferable to many slower\nYou’re probably not used to thinking about this tradeoff from buying a laptop or phone. The reality is that modern CPUs are pretty darn good and you should just buy the one that fits your budget.\nIf you’re standing up a server, you often do have an explicit choice between more slower cores and fewer faster ones. In AWS, the instance type is what dictates the tradeoff – more on this below.\nIf you’re running a multi-user server, the number of cores you need is really hard to estimate. If you’re doing non-ML tasks like counts and dashboarding or relatively light-duty machine learning I might advise the following:\n\\[\n\\text{n cores} = \\text{1 core per user} + 1\n\\]\nThe spare core is for the server to do its own operations apart from the data science usage. On the other hand, if you’re doing heavy-duty machine learning or parallelizing jobs across the CPU, you may need more cores than that."
  },
  {
    "objectID": "chapters/sec3/3-6-servers.html#recommendation-2-get-as-much-ram-as-feasible",
    "href": "chapters/sec3/3-6-servers.html#recommendation-2-get-as-much-ram-as-feasible",
    "title": "15  Choosing the right server for you",
    "section": "15.3 Recommendation 2: Get as much RAM as feasible",
    "text": "15.3 Recommendation 2: Get as much RAM as feasible\nYour computer’s random access memory (RAM) is its short term storage. In the computer as adding factory analogy, the RAM is like the stock that’s sitting out on the factory floor ready to go right on an assembly line.\nRAM is very fast to for your computer to access, so you can read and write to it very quickly. The downside is that it’s temporary. When your computer turns off, the RAM gets wiped.2\n\n\n\n\n\n\nNote\n\n\n\nYou probably know this, but memory and storage is measured in bytes prefixed by metric prefixes. Common sizes for memory these days are in gigabytes (billion bytes) and terrabytes (trillion bytes). Some enterprise data stores run on the scales of thousands of terrabytes (pettabytes) or even thousands of pettabytes (yottabytes).\n\n\nModern consumer-grade laptops come with somewhere between 4 and 16 Gb of memory.\nIn most cases, R and Python have to load all of your data into memory. Thus, the size of the data you can use is limited to the size of your machine’s RAM. Most other limits of your machine will just result in things being slower than you’d really want, but trying to load too much data into memory will result in a session crash, and you won’t be able to do your analysis.\n\n\n\n\n\n\nNote\n\n\n\nYou can get around the in-memory limitation by using a database or libraries that facilitate on-disk operations like Apache Arrow or dask.\n\n\nBecause you’ll often be doing some sort of transformation that results in invisible data copies and your computer can’t devote all of its memory, you’ll want to leave plenty of room over your actual data size.\nIt’s easy to say that you’ll always want more RAM, but a rough rule of thumb for whether you’ve got enough is the following:\n\\[\\text{Amount of RAM} = \\text{max amount of data} * 3\\]\nIf you’re thinking about running a multi-user server, you’ll want to think about the maximum simultaneous number of users on the server, the most data each one would want to have loaded into memory and use that for your max amount of data"
  },
  {
    "objectID": "chapters/sec3/3-6-servers.html#recommendation-3-get-lots-of-storage-its-cheap",
    "href": "chapters/sec3/3-6-servers.html#recommendation-3-get-lots-of-storage-its-cheap",
    "title": "15  Choosing the right server for you",
    "section": "15.4 Recommendation 3: Get lots of storage, it’s cheap",
    "text": "15.4 Recommendation 3: Get lots of storage, it’s cheap\nRelative to the RAM that’s right next to the factory floor, your computer’s storage is like the warehouse in the next building over. It’s much, much slower to get things from storage than RAM, but it’s also permanent once its stored there.\nUp until a few years ago, hard drives were very slow. HDD drives have a bunch of magnetic disks that spin very fast (5,400 and 7,200 RPM are common speeds). Magnetized read/write heads move among the disks and save and read your data.\nWhile 7,200 RPM is very fast, there were still physical moving parts, and reading and writing data was very slow by computational standards.\nIn the last few years, solid-state drives (SSDs) have become more-or-less standard in laptops. SSDs, which are collections of flash memory chips with no moving parts, are up to 15x faster than HDDs. They also can take a wider variety of shapes and sizes, and are more reliable and durable because they have no moving parts. The main drawback is that they’re usually more expensive per byte, but prices are still quite reasonable.\nMany consumer laptops have only an SSD at this point. Some desktops and high-end laptops combine a smaller SSD with a larger HDD.\nAs for storage – get a lot – but don’t think about it too hard, because it’s cheap. Both a 1TB SSD and a 4TB HDD are around $100. Storage is cheap enough these days that it is almost always more cost efficient to buy more storage rather than making a highly-paid professional spend their time trying to figure out how to move things around.\nOne litmus test of an IT organization that is well-equipped to support data science is whether they understand this. Smart organizations know that just getting more storage is almost always worth the cost in terms of the time of admins and data scientists.\nIf you’re running a multi-user server, the amount of storage you need depends a lot on your data and your workflows. A reasonable rule of thumb is to choose\n\\[\n\\text{Amount of Storage} = \\text{data saved} + 1Gb * \\text{n users}\n\\]\nOne thing to keep in mind is that you can basically just choose storage to match your data size and add a Gigabyte per person.\nAside from actual data, the amount of space each person needs on the server is small. Code is very small and it’s rare to see R and Python packages take up more than a few dozen Mb per data scientist.\n\n\n\n\n\n\nNote\n\n\n\nIf you’re working with a professional IT admin, they may be concerned about the storage implications of having package copies for each person on their team. I’ve heard this concern a lot from IT/Admins thinking prospectively about running their server and almost never of a case where it’s actually been a problem.\nFinding a person who has more than a few hundred Mb of packages would be very strange indeed.\n\n\nSo it really comes down to how much data you expect to save on the server. In some organizations, each data scientist will save dozens of flat files of a Gb or more for each of their projects. That team would need a lot of storage. In other teams, all the data lives in a database and you basically don’t need anything beyond that 1 Gb per person.\nIf you’re operating in the cloud, this really isn’t an important choice. As you’ll see in the lab, upgrading the amount of storage you have is a really trivial operation, requiring at most a few minutes of downtime. Choose a size you guess will be adequate and add more if you need."
  },
  {
    "objectID": "chapters/sec3/3-6-servers.html#recommendation-4-get-a-gpumaybe",
    "href": "chapters/sec3/3-6-servers.html#recommendation-4-get-a-gpumaybe",
    "title": "15  Choosing the right server for you",
    "section": "15.5 Recommendation 4: Get a GPU…maybe",
    "text": "15.5 Recommendation 4: Get a GPU…maybe\nThe most common special architecture chips is the graphical processing unit (GPU). GPUs are specialized chips used for tasks like editing photo or videos, rendering video game graphics, some kinds of machine learning, and (yes) Bitcoin mining.\nLike a CPU, a GPU is just an addition factory but with a different architecture. A CPU has a few fast cores, so it can only work on a few problems simultaneously, but it does them very fast. In contrast, a GPU takes the opposite approach, with many slower cores. Where a consumer-grade CPU has 4-16 cores, mid-range GPUs have 700-4,000 cores, but each one runs between 1% and 10% the speed of a CPU core.\nFor GPU-centric tasks, the overwhelming parallelism of a GPU is more important than the speed of any individual core, and GPU computation can be dramatically faster. For the purposes of data science, many popular machine learning techniques – including neural networks, XGBoost, and other tree-based models – potentially run much much faster on GPUs relative to CPUs.\nSome machines are also adding other types of specialized chips to do machine learning – though these generally aren’t accessible for training models. For example, iPhone TODO.\nThe choice of whether you need a GPU to do your work will really depend on what you’re doing and your budget.\nOnly certain kinds of data science tasks are even amenable to GPU-backed acceleration. Many data science tasks can only be done in sequence others can be parallelized, but splitting it into a small number of CPU cores is perfectly adequate. For the most part, the things that will benefit most from GPU computing are training highly parallel machine learning models like a neural network or tree-based models.\nIf you do have one of these use cases, GPU computing can massively speed up your computation – making models trainable in hours instead of days.\nIf you are planning to use cloud resources for your computing, GPU-backed instances are quite pricey, and you’ll want to be careful about only putting those machines up when you’re using them.\nIt’s also worth noting that using a GPU won’t happen automatically. The tooling has gotten good enough that it’s usually pretty easy to set up, but your computer won’t train your XGBoost models on your GPU unless you tell it to do so.\nBecause GPUs are expensive, I generally wouldn’t bother with GPU-backed computing unless you’ve already tried without a GPU and find that it takes too long to be feasible."
  },
  {
    "objectID": "chapters/sec3/3-6-servers.html#scaling-servers",
    "href": "chapters/sec3/3-6-servers.html#scaling-servers",
    "title": "15  Choosing the right server for you",
    "section": "15.6 Scaling Servers",
    "text": "15.6 Scaling Servers\nThere are two reasons you might need to scale to a bigger server. The first reason that people think of most often is because people are running big jobs. This can happen at any scale of organization. There are data science teams of one who have use cases that necessitate terrabytes of data. There are also issues that come up as your data science team gets bigger.\nEither way, there are two basic options for how to scale your data science workbench. The first is vertical scaling, which is just a fancy way of saying get a bigger server. If your budget allows it, you shouldn’t feel shy about vertically scaling – the complexity of managing a t3.nano with 2 cores and 0.5 Gb of memory is exactly the same as a C5.24xlarge with 96 cores and 192 Gb of memory. In fact, the bigger one may well be easier to manage, since you won’t have to worry about running low on resources.\nAWS resource costs generally scale linearly within a server family. But there are limits. As of this writing, AWS’s general-use instance types max out at 96-128 cores these days. That’s probably sufficient for many workloads, but if you’ve got an RStudio Server with 50 concurrent users doing reasonably heavy compute loads, that can quickly get eaten up.\nThe nice thing about being in the cloud is that vertical scaling is almost trivially easy, as we’ll see in the lab in this chapter.\nIf your needs grow beyond the capacity of a single server, you’re starting to get into horizontal scaling territory. Once you’re thinking about horizontal scaling, you should almost certainly get an IT/Admin professional involved. See Chapter 19 for more on how to talk to them about it."
  },
  {
    "objectID": "chapters/sec3/3-6-servers.html#aws-costs",
    "href": "chapters/sec3/3-6-servers.html#aws-costs",
    "title": "15  Choosing the right server for you",
    "section": "15.7 AWS Costs",
    "text": "15.7 AWS Costs\nThat said, a modestly-sized server is still pretty cheap if you’re only putting it up for a short amount of time.\nI’m writing this on a 2021 M1 Macbook Pro with 10 CPUs and 32 Gb of memory. If you wanted that same computational power from an AWS server, it’s roughly comparable to a t3.2xlarge – with 8 CPUs and 32Gb of memory. That server costs is $0.33 an hour. So a full year running full time for an instance is nearly $3,000, but if you’re only running that instance for a little while – say, the few hours it’ll take you to complete this lab – it will probably only be a few dollars."
  },
  {
    "objectID": "chapters/sec3/3-6-servers.html#comprehension-questions",
    "href": "chapters/sec3/3-6-servers.html#comprehension-questions",
    "title": "15  Choosing the right server for you",
    "section": "15.8 Comprehension Questions",
    "text": "15.8 Comprehension Questions\n\nThink about the scenarios below – which part of your computer would you want to upgrade to solve the problem?\n\nYou try to load a big csv file into pandas in Python. It churns for a while and then crashes.\nYou go to build a new ML model on your data. You’d like to re-train the model once a day, but it turns out training this model takes 26 hours on your laptop.\nYou design an visualization Matplotlib , and create a whole bunch in a loop, you want to parallelize the operation. Right now you’re running on a t2.small with 1 CPU.\n\nDraw a mind map of the following: CPU, RAM, Storage, Operations Per Second, Parallel Operations, GPU, Machine Learning\nWhat are the architectural differences between a CPU and a GPU? Why does this make a GPU particularly good for Machine Learning?\n15.9 AWS Instance Classes for Data Science\nAWS offers a variety of different EC2 instance types. There are a few different types you’ll probably consider, here’s a quick guide to the types most commonly used for data science purposes.\nWithin each family, there are different sizes available, ranging from nano to 2xl. Instances are denoted by &lt;family&gt;.&lt;size&gt;. So, for example, when we put our instance originally on a free tier machine, we put it on a t2.micro.\nIn most cases, going up a size doubles the amount of RAM, the number of cores, and the cost.\n\n\n\n\n\n\n\nInstance Type\nNotes\n\n\n\n\nt3\nThe “standard” configuration. Relatively cheap per core/Gb RAM.\ngood b/c of instance credits, limited size\n\n\nC\nFaster CPUs\n\n\nR\nHigher ratio of RAM to CPU\n\n\nP\nGPU instances, very expensive\n\n\n\n15.10 Lab: Changing Instance Size\n\nOk, now we’re going to experience the real magic of the cloud – flexibility.\nWe’re going to upgrade the size of our server in just a minute or two.\n\n15.10.1 Step 1: Confirm current server size\nFirst, let’s confirm what we’ve got available. You can check the number of CPUs you’ve got with lscpu in a terminal. Similarly, you can check the amount of RAM with free -h. This is just so you can prove to yourself later that the instance really changed.\n\n\n15.10.2 Step 2: Change the instance type and bring it back\nNow, you can go to the instance page in the AWS console. The first step is to stop (not terminate!) the instance. This means that changing instance type does require some downtime for the instance, but it’s quite limited.\nOnce the instance has stopped, you can change the instance type under Actions &gt; Instance Settings. Then start the instance. It’ll take a few seconds to start the instance.\nAnd that’s it.\n\n\n15.10.3 Step 3: Confirm new server size\nSo, for example, I changed from a t2.micro to a t2.small. Both only have 1 CPU, so I won’t see any difference in lscpu, but running free -h before and after the switch reveals the difference in the total column:\ntest-user@ip-172-31-53-181:~$ free -h\n               total        used        free      shared  buff/cache   available\nMem:           966Mi       412Mi       215Mi       0.0Ki       338Mi       404Mi\nSwap:             0B          0B          0B\ntest-user@ip-172-31-53-181:~$ free -h\n               total        used        free      shared  buff/cache   available\nMem:           1.9Gi       225Mi       1.3Gi       0.0Ki       447Mi       1.6Gi\nSwap:             0B          0B          0B\nI got twice as much RAM!\nThere are some rules around being able to change from one instance type to another…but this is an amazing superpower if you’ve got variable workloads or a team that’s growing. The ability to scale a machine so easily is a game-changer.\nThe work we did in earlier lab chapters, for example adding the elastic IP and daemonizing JupyterHub, are the reason that we’re able to bring it back up so smoothly.\nIt’s similarly easy to resize the EBS volume attached to your server for more storage.\nThere are two caveats worth knowing for EBS:\n\nYou can only automatically adjust volume sizes up, so you’d have to manually transfer all of your data if you ever wanted to scale back down.\nIf you resize the volume, you’ll also have to adjust the Linux filesystem so it knows about the new space available. AWS has a great walk through called Extend a Linux filesystem after resizing the volume that I recommend you follow."
  },
  {
    "objectID": "chapters/sec3/3-6-servers.html#footnotes",
    "href": "chapters/sec3/3-6-servers.html#footnotes",
    "title": "15  Choosing the right server for you",
    "section": "",
    "text": "The reason why this is the case and how it works is fascinating. If you’re interested, it comes back to Alan Turing’s famous paper on computability. I recommend The Annotated Turing: A Guided Tour Through Alan Turing’s Historic Paper on Computability and the Turing Machine by Charles Petzold for a surprisingly readable walkthrough of the paper.↩︎\nYou probably don’t experience this personally. Modern computers are pretty smart about dumping RAM onto the hard disk before shutting down, and bringing it back on startup, so you usually won’t notice this happening.↩︎"
  },
  {
    "objectID": "chapters/sec4/4-0-sec-intro.html#what-enterprise-it-is-about",
    "href": "chapters/sec4/4-0-sec-intro.html#what-enterprise-it-is-about",
    "title": "Making it Enterprise-Grade",
    "section": "What enterprise IT is about",
    "text": "What enterprise IT is about\nAs a data scientist, your primary concern about your data science environment is that it’s useful. You want to be able to get all the data you want at your fingertips.\nMany data scientists in enterprises find that this desire runs headlong into requirements from their organization’s IT/Admin teams.\nThis can be extremely frustrating, so it’s helpful to understand the concerns enterprise IT/Admins have in mind. Broadly, IT/Admins care about the security and stability of the systems they control.\nGreat IT/Admin teams also care about the usefulness of the system to users (that’s you), but it’s usually a distant third. And there is sometimes a tension here. After all the only system that’s completely secure is the one that doesn’t exist at all.\nBut that’s not always the case. Often, there’s a lot to gain by partnering with the IT/Admin team at your organization. You may be primarily focused on getting stuff done minute-to-minute, but a data science platform that is insecure and allows bad actors to break in and steal data is not useful. And one where you can do what you want but end up crashing the workbench for 50 other users is ultimately self-defeating.\nBalancing security, stability, and usefulness is always about tradeoffs. Great IT/Admin organizations are constantly in conversations with other parts of the organization to figure out the right stance for your organization given the set of tradeoffs you face.\nUnfortunately, many IT/Admin organizations don’t act that way – they act as gatekeepers to the resources you need to do your job. That means you’ll have to figure out how to communicate with those teams, understand what matters to them, help them understand what matters to you, and reach acceptable organizational outcomes.\nYou probably already have a good understanding of how a data science environment can be useful – but what about secure and stable. What do they mean?\nSecurity is about making sure that the right people can interact with the systems they’re supposed to and that unauthorized people can’t.\nIT security professionals think about security in layers. And while you’ve done a good job setting your server up to comply with basic security best practices, there are no layers. That server front door is open to the internet. Literally anyone in the world can come to that authentication page for your RStudio Server or JupyterHub and start trying out passwords. That means you’re just one person choosing the password password away from a bad actor getting access to your server.\nLest you think you’re immune because you’re not an interesting target, there are plenty of bots out there randomly trying to break in to every existing IP address, not because they care about what’s inside, but because they want to co-opt your resources for their own purposes like crypto mining or virtual DDOS attacks on Turkish banks.1\nMoreover, security and IT professionals aren’t just concerned with bad actors from outside (called outsider threat) or even someone internal who decides to steal data or resources (insider threat). They are (or at least should be) also concerned with accidents and mistakes – data that is accidentally permanently deleted is bad the same way stolen data is bad.\nStability is ensuring enterprise-grade systems are around when people need them, and that they are stable during whatever load they face during the course of operating. The importance of stability tends to rise along with the scale of the team and the centrality of their operations to the functioning of your organization.\nIf you’re a team of three data scientists who sit in a room together, it probably won’t be a huge deal if someone accidentally knocks your data science workbench offline for 30 minutes because they tried to run a job that was too big. You’re probably all sitting in the same room and you can learn something from the experience.\nThat’s not the case when you get to enterprise-grade tooling. An enterprise-grade data science workbench probably supports dozens or hundreds of professionals across multiple teams. The server being down isn’t a sorta funny occurrence you can all fix together – it’s a problem that must be fixed immediately – or even better avoided altogether.\nIT/Admins think hard about how to provide resources in a way that avoids having servers go down because they’re hitting resource constraints.\nOne thing that is almost certain to be untrue in an enterprise context is that you’ll have root access as a user of the system.\nThere is no one-size-fits-all (or even most) position for security. Instead, great security teams are constantly articulating and making decisions about tradeoffs."
  },
  {
    "objectID": "chapters/sec4/4-0-sec-intro.html#labs-in-this-section",
    "href": "chapters/sec4/4-0-sec-intro.html#labs-in-this-section",
    "title": "Making it Enterprise-Grade",
    "section": "Labs in this Section",
    "text": "Labs in this Section\nThere are no labs in this section. This section is about how to interact with a professional IT/Admin group. Since it’s about work you really shouldn’t undertake yourself, there are no labs for you to DIY these things.\nIf you want to go deeper into any of the topics, there are plenty of great resources online – a google search should turn them up quickly"
  },
  {
    "objectID": "chapters/sec4/4-0-sec-intro.html#footnotes",
    "href": "chapters/sec4/4-0-sec-intro.html#footnotes",
    "title": "Making it Enterprise-Grade",
    "section": "",
    "text": "Yes, these both really happened.↩︎"
  },
  {
    "objectID": "chapters/sec4/4-1-os-ds-in-ent.html#data-science-sandboxes",
    "href": "chapters/sec4/4-1-os-ds-in-ent.html#data-science-sandboxes",
    "title": "16  Open Source Data Science in the Enterprise",
    "section": "16.1 Data Science Sandboxes",
    "text": "16.1 Data Science Sandboxes\nIn Chapter 5 on code promotion, we talked about wanting to create separate dev/test/prod environments for your data science assets. In many organizations, your good intentions to do dev/test/prod work for your data science assets are insufficient.\nIn the enterprise, it’s often the case that IT/Admins are required to ensure that your dev work can’t wreak havoc on actual production systems. If you can work with your organization’s IT/Admins to get a place to work, this is a good thing! It’s nice to have a sandbox where you can do whatever you want with no concern of breaking anything real.\nThere are three components to a data science sandbox:\n\nFree read-only access to real data\nRelatively broad access to packages\nA process to promote data science projects into production\n\nMany IT/Admins are familiar with building workbenches for other sorts of software development tasks, so they may believe they understand what you need.\n\n16.1.0.1 Read-Only Data Access\nIn software engineering, the shape of the data in the system matters a lot, but the content doesn’t matter much. If you’re building software to do inventory tracking, you’re perfectly fine testing on data that isn’t of a real inventory as long as it’s formatted the same as real inventory data. That means that a software engineering dev environment doesn’t require access to any real data.\nIT/Admins may believe that providing a standalone or isolated environment for you to work in is sufficient. You’re probably wincing reading this knowing that it’s not the case.\nAs you know, data science isn’t like that. In data science, what you’re doing in dev environments – exploring relationships between variables, visualizing, training models, looks much more different from test and prod than for software engineering. You need a place where you can work with real data, explore relationships in the data, and try out ways of modeling and visualizing the data to see what works.\nIn many cases, creating a data science sandbox with read-only access to data is a great solution. If you’re creating a tool that also writes data, you can write the data only within the dev environment.\nIf you’re able to do this, you can explore the data to your heart’s content without the IT/Admin team having to worry you’re going to mess up production data.\nDepending on your organization, they may also be worried about data exfiltration – people intentionally or accidentally removing data from the sandbox environment. In that case, you may want to run your sandbox without the ability to connect to the internet, sometimes called airgapped or offline. More on how to work in an airgapped environment is in Chapter 17.\n\n\n16.1.0.2 Package Availability\nYour organization may not have any restrictions on the packages you can install and use in your environment. If so, that’s great! You should still use environment as code tooling as we explored in Chapter 1.\nBut other organizations don’t allow free-and-open access to packages inside their environments for security or other reasons. They may have restrictive networking rules that don’t allow you to reach our to CRAN, BioConductor, PyPI, or GitHub to install packages publicly.\nThere may also be rules about having to validate packages – for security and/or correctness – before they can be used in a production context.\nThis is difficult, since the exploratory dev process often involves trying out new packages – is it best to use this modeling package or that, best to use one type of visualization or another. If you can convince your IT/Admins to give you freer access to packages in dev, that’s ideal.\nYou can work with them to figure out what the promotion process will look like. It’s easy to generate a list of the packages you’ll need to promote apps or reports into production with tools like renv or venv. Great collaborations with IT/Admin are possible if you can develop a procedure where you give them package manifests that they can compare to allow-lists and then make those packages available.\n\n\n16.1.0.3 A Promotion Process\nThe last thing you’ll need is a process to promote content out of the dev environment into test or prod. The parameters of this process will vary a lot depending on your organization.\nIn many organizations, data scientists won’t have permissions to deploy into the prod environment – only a small group of admins will have the ability to verify changes and submit them to the prod environment.\nIn this case, a code promotion strategy (see Chapter 5) co-designed with your IT/Admins is the way to go. You want to hand off a fully-baked production asset so all they need to do is put it into the right place, preferably with CI/CD tooling."
  },
  {
    "objectID": "chapters/sec4/4-1-os-ds-in-ent.html#devtestprod-for-admins",
    "href": "chapters/sec4/4-1-os-ds-in-ent.html#devtestprod-for-admins",
    "title": "16  Open Source Data Science in the Enterprise",
    "section": "16.2 Dev/Test/Prod for Admins",
    "text": "16.2 Dev/Test/Prod for Admins\nIn the chapter on environment as code tooling, we discussed how you want to create a version of your package environment in a safe place and then use code to share it with others and promote it into production.\nThe IT/Admin group at your organization probably is thinking about something similar, but at a more basic level. They want to make sure that they’re providing a great experience to you – the users of the platform.\nSo they probably want to make sure they can upgrade the servers you’re using, change to a newer operating system, or upgrade to a newer version of R or Python without interrupting your use of the servers.\nThese are the same concerns you probably have about the users of the apps and reports you’re creating. In this case, I recommend a two-dimensional grid – one for promoting the environment itself into place for IT/Admins and another for promoting the data science assets into production.\nIn order to differentiate the environments, I often call the IT/Admin development and testing area staging, and use dev/test/prod language for the data science promotion process.\nBack in section one, we learned about environments as code – using code to make sure that our data science environments are reproducible and can be re-created as needed.\nThis idea isn’t original – in fact, DevOps has it’s own set of practices and tooling around using code to manage DevOps tasks, broadly called Infrastructure As Code (IaC).\nTo get from “nothing” to a usable server state, there are (at minimum) two things you need to do – provision the infrastructure you need, and configure that infrastructure to do what you want.\nFor example, let’s say I’m standing up a server to deploy a simple shiny app. In order to get that server up, I’ll need to stand up an actual server, including configuring the security settings and networking that will allow the proper people to access the server. Then I’ll need to install a version of R on the server, the Shiny package, and a piece of hosting software like Shiny Server.\nSo, for example, you might use AWS’s CloudFormation to stand up a virtual private cloud (VPC), put an EC2 server instance inside that VPC, attach an appropriately-sized storage unit, and attach the correct networking rules. Then you might use Chef to install the correct software on the server and get your Shiny app up-and-running.\nIn infrastructure as code tooling, there generally isn’t a clear dividing line between tools that do provisioning and tools that do configuration…but most tools lean one way or the other.\nBasically any tool does provisioning will directly integrate into the APIs of the major cloud providers to make it easy to provision cloud servers. Each of the cloud providers also has their own IaC tool, but many people prefer to use other tools when given the option (to be delicate).\nIt’s worth noting that Docker by itself is not an IaC tool. A Dockerfile is a reliable way to re-create the container, but that doesn’t get you all the way to a deployed container that’s reliable. You’ll need to combine a container with a deployment framework like Docker Compose or Kubernetes, as well as a way to stand up the servers that underlie that cluster.\nIt’s also worth noting that IaC may or may not be deployed using CI/CD. It’s generally a good practice to do so, but you can have IaC tooling that’s deployed manually.\nBasically none of these tools will save you from your own bad habits, but they can give you alternatives."
  },
  {
    "objectID": "chapters/sec4/4-1-os-ds-in-ent.html#package-management-in-the-enterprise",
    "href": "chapters/sec4/4-1-os-ds-in-ent.html#package-management-in-the-enterprise",
    "title": "16  Open Source Data Science in the Enterprise",
    "section": "16.3 Package Management in the Enterprise",
    "text": "16.3 Package Management in the Enterprise\nIn the chapter on environments as code, we went in depth on how to manage a per-project package environment that moves around with that project. This is a best practice and you should do it.\nBut in some Enterprise environments, there may be further requirements around how packages get into the environment, which packages are allowed, and how to validate those packages.\n\n16.3.1 Open Source in Enterprise\nOpen Souce software is the water code-first data science swims in. R and Python are open source. Any package you’ll access on CRAN, BioConductor, or PyPI is open source.\nEven if you don’t know the details, you’re probably already a big believer in the power of Free and Open Source Software (FOSS) to help people get stuff done.\nIn the enterprise, it is sometimes the case that people are more acquainted with the risks of open source than the benefits.\nFOSS is defined by a legal license. When you put something out on the internet, you can provide no licensing at all. That means that people can do whatever they want with it, but they also have no reassurances you might not come to sue them later.\nI am not a lawyer and this is not legal advice, but hopefully this is helpful context on the legal issues around FOSS software.\nPutting a license on a piece of software makes clear what is and isn’t allowed with that piece of software.\nThe type of license you’re probably most familiar with is a copyright. A copyright gives the owner exclusivity to do certain things with whatever falls under the copyright. FOSS licenses are the opposite – they guarantee that you’re freely allowed to do what you want with the software, though there may be other obligations as well.\nThe point of FOSS is to allow people to build on each other’s work. So the free in FOSS is about freedom, not about zero cost. As a common saying goes – it means free as in free speech, not free as in free beer.\nThere are many different flavors of open-source licenses, and all of them I’m aware of (even the anti-capitalist one) allows you to charge for access. The question is what you’re allowed to do after you acquire the software.\nBasically all open source licenses guarantee four freedoms. These are the freedom to view and inspect the source code, to run the software, to modify the software, and to redistribute the software as you see fit.\nWithin those parameters, there are many different kinds of open source licenses. The Open Source Initiative lists dozens of different licenses with slightly different parameters, but they fall into two main categories – permissive and copyleft.\nPermissive licenses allow you to do basically whatever you want with the software.\nFor example, the common MIT license allows you to, “use, copy, modify, merge, publish, distribute, sublicense, and/or sell” MIT-licensed software without attribution. The most important implication of this is that you can take MIT-licenses software, incorporate it into some other piece of code, and keep that code completely closed.\nCopyleft or viral licenses require that any derivative works are released under the same license. The idea is that open source software should beget more open source software and not silently be used by big companies to make megabucks.\nThe biggest concern with copyleft licenses is that they might propagate into the private work you are doing inside your company. This concern is especially keen at organizations that themselves sell proprietary software. For example, what if a court were to rule that Apple or Google had to suddenly open source all their software?\nThis is a subtle concern and largely revolves around what it means to include another piece of software. Many organizations deem that inclusion only happens if you were to literally copy/paste open source code into your code. In this view, the things created with open source software are not themselves open source.\nThe reality is that there have been basically no legal cases on this topic and nobody knows how it would shake out if it did get to court, so some organizations err on the side of caution.\nSo one of the concerns around open source software is the mixure of licenses involved. R is released under a copyleft GPL license, Python under a permissive Python Software Foundation (PSF) license, RStudio under a copyleft AGPL, and Jupyter Notebook under a permissive modified BSD. And every single package author can choose a license for themselves.\nSay I create an app or plot in R and then share that plot with the public – is that app or plot bound by the license as well? Do I now have to release my source code to the public? Many would argue no – it uses R, but doesn’t derive from R in any way. Others have concerns that stricter interpretations of copyleft might hold up in court if it were to come to that.\nThere are disagreements on this among lawyers, and you should be sure to talk to a lawyer at your organization if you have concerns.\n\n\n16.3.2 Dealing with Package Restrictions\nChapter 1 on environments as code dealt with how to create a reproducible version of your package library for any given piece of content. But in some enterprises, you won’t be able to freely create a library. Instead, your organization may have restrictions on package installation and will need to gate the packages that are allowed into your environment.\nIn order to enact these restrictions, IT/Admins have to do two things - make sure that public repositories are not available to users of their data science platforms, and use one of these repository tools to manage the set of packages that are available inside their environment. It often takes a bit of convincing, but a good division of labor here is generally that the IT/Admins manage the repository server and what’s allowed into the environment and the individual teams manage their own project libraries.\n\n\n\n\n\n\nAmount of Space for Packages\n\n\n\nWhen admins hear about a package cache per-project, they start getting worried about storage space. I have heard this concern many times from admins who haven’t yet adopted this strategy, and almost never heard an admin say they were running out of storage space because of package storage.\nThe reality is that most R and Python packages are very small, so storing many of them is reasonably trivial.\nAlso, these package storage tools are pretty smart. They have a shared package cache across projects, so each package only gets installed once, but can be used in a variety of projects.\nIt is true that each user then has their own version of the package. Again, because packages are small, this tends to be a minor issue. It is possible to make the package cache one that is shared across users, but the (small) risk this introduces of one user affecting other users on the server is probably not worth the very small cost of provisioning enough storage that this just isn’t an issue.\n\n\nMany enterprises run some sort of package repository software. Common package repositories used for R and Python include Jfrog Artifactory, Sonatype Nexus, Anaconda Business, and Posit Package Manager.\nArtifactory and Nexus are generalized library and package management solutions for all sorts of software, while Anaconda and Posit Package Manager are more narrowly tailored for data science use cases.\nThere are two main concerns that come up in the context of managing packages for the enterprise. The first is how to manage package security vulnerabilities.\nIn this context, the question of how to do security scanning comes up. What exactly security professionals mean by scanning varies widely, and what’s possible differs a good bit from language to language.\nIt is possible to imagine a security scanner that actually reads in all of the code in a package and identifies potential security risks – like usage of insecure libraries, calls to external web services, or places where it accesses a database. The existence of tools at this level of sophistication exist roughly in proportion to how popular the language is and how much vulnerability there is.\nSo javascript, which is both extremely popular and also makes up most public websites, has reasonably well-developed software scanning. Python, which is very popular, but is only rarely on the front end of websites has fewer scanners, and R, which is far less popular has even fewer. I am unaware of any actual code scanners for R code.\nOne thing that can be done is to compare a packaged bit of software with known software vulnerabilities.\nNew vulnerabilities in software are constantly being identified. When these vulnerabilities are made known to the public, the CVE organization attempts to catalog them all. One basic form of security checking is looking for the use of libraries with known CVE records inside of packages.\nThe second thing your organization may care about is the licenses software is released under. They may want to disallow certain licenses – especially aggressive copyleft licenses – from being present in their codebases."
  },
  {
    "objectID": "chapters/sec4/4-1-os-ds-in-ent.html#comprehension-questions",
    "href": "chapters/sec4/4-1-os-ds-in-ent.html#comprehension-questions",
    "title": "16  Open Source Data Science in the Enterprise",
    "section": "16.4 Comprehension Questions",
    "text": "16.4 Comprehension Questions\n\nWhat is the purpose of creating a data science sandbox? Who benefits most from the creation of a sandbox?\nWhy is using infrastructure as code an important prerequisite for doing Dev/Test/Prod?\nWhat is the difference between permissive and copyleft open source licenses? Why are some organizations concerned about using code that includes copyleft licenses?\nWhat are the key issues to solve for open source package management in the enterprise?"
  },
  {
    "objectID": "chapters/sec4/4-2-ent-networks.html#enterprise-networking-terminology",
    "href": "chapters/sec4/4-2-ent-networks.html#enterprise-networking-terminology",
    "title": "17  Enterprise Networking",
    "section": "17.1 Enterprise Networking Terminology",
    "text": "17.1 Enterprise Networking Terminology\nHopefully the analogy of a basic server as an apartment building and an enterprise server as a castle keep behind a drawbridge makes basic sense. But let’s get into the way you’ll actually talk about this with the IT/Admins at your organization – I promise they won’t talk about castles and apartments.\nWhen you stand up a server in the cloud, it’s inside a private network. In AWS, the private network that houses the servers is called a virtual private cloud (VPC), which you probably saw somewhere in the AWS console.\nFor our workbench server, we took that private network and made it public so every server (there was just one) inside our private network also has a public IP address so it was accessible from the internet.\nIn an enterprise configuration you won’t do that. Instead, you’ll take your private network and divide it into subnets – most often two of them.\nNow you’ll take the subnets and put all the stuff you actually care about in a private subnet. Private networks generally host all of the servers that actually do things. Your data science workbench server, your databases, server for hosting shiny apps – all these things should live inside the private network. Nothing in the private subnet will be directly accessible from the public internet.\n\n\n\n\n\n\nDefining private networks and subnets\n\n\n\nPrivate networks and subnets are defined by something called a Classless Inter-Domain Routing (CIDR) block. A CIDR block is basically an IP address range, so a private network is a CIDR block and each subnet is CIDR blocks within the private network’s block.\nEach CIDR block is defined by a starting address and the size of the network. For example, the address 10.33.0.0 and the /26 CIDR defines the block of 64 addresses from 10.33.0.0 to 10.33.0.63.\nLarger CIDR numbers indicate a smaller block, so you could take the 10.33.0.0/26 CIDR and split it into the 10.33.0.0/27 block that includes 10.33.0.0 to 10.33.0.31 and 10.33.0.32/27 for 10.33.0.32 through 10.33.0.63.\nAs you’ve probably guessed, the number of IPs in each CIDR have to do with powers of two. But the rules are hard to remember and there are online calculators if you ever have to figure a block out for yourself.\n\n\nThe only things you’ll put in the public subnet – often called a demilitarized zone (DMZ) – are servers that exist solely for the purpose of relaying traffic back and forth to the servers in your private network. These servers are called proxy servers or proxies – more on them in a moment.\nThis means that the traffic actually coming to your workbench comes only from other servers you control. It’s easy to see why this is more secure.\n[TODO: image of private networks + proxies]\nIn most cases, you’ll have minimum two servers in the DMZ. You’ll usually have one or more proxies to handle the incoming HTTPS traffic that comes in from the outside world. You’ll also usually have a proxy that is just for passing SSH traffic along to hosts in the private network, often called a bastion host or jump box.\nThe other benefit of using a private network for the things you actually care about is that you can manage the IP addresses and hostnames of those servers without having to worry about getting public addresses. If you want to name one of the servers in your private subnet google.com, you can do that (I wouldn’t recommend it), because the only time that name will be used is when traffic is coming past your proxy servers and into the private network.\nThere’s a device sitting on the boundary of all networks that provide translations between private IP addresses and public ones. For your private subnet, you’ll only have an outbound one available. In AWS, it’s called a Network Address Translation (NAT) Gateway. For your private network as a whole, there’ll be another gateway that provides both inbound and outbout support, it’s called an Internet Gateway by AWS.\n\n17.1.1 What proxies do\nAs a data scientist, this may be the first time you’re encountering the term proxy, but for IT/Admins – especially ones who specialize in networking – configuring proxies is an everyday activity.\nProxies can be either in software or hardware. For example, in our workbench server, we installed the nginx software proxy on the same server as our workbench to allow people to go to any of the different services we installed on that server. In enterprise use cases, proxies are most often on standalone pieces of hardware. They may run nginx or Apache – the other popular open source option. Popular paid enterprise options include F5, Citrix, Fortinet, and Cloudflare.\nProxies can deal with traffic coming into the private network, called an inbound proxy or they can deal with traffic going out from the private network, called an outbound proxy.\n\n\n\n\n\n\nNote\n\n\n\nInbound and outbound are not industry standard terms for proxies. The terms you’ll hear from IT/Admins are forward and reverse. Proxies are discussed from the perspective of being inside the network, so forward equals outbound and reverse equals inbound.\nI find it nearly impossible to remember which is which and IT/Admins will absolutely know what you mean with the terms inbound and outbound, so I recommend you use them instead.\n\n\nProxies are usually used for redirection, port management and firewalling.\nRedirection is when the proxy accepts traffic at the public DNS record and passes (proxies) it along to the actual server. One great thing about this configuration is that only the proxy needs to know the real hostname for your server. For example, you could configure the proxy so example.com/rstudio routes to the RStudio Server that’s at my-rstudio-1 inside the private network. If you want to change it to my-rstudio-2 later on, you just change the proxy routing, which is much easier than changing the public DNS record.\nOne advantage of doing redirection is making it easy to manage ports. For example, RStudio Server runs on port 8787 by default. Generally, you don’t want people to have to remember to go to a random port to access RStudio Server so it’s standard practices to keep standard ports (80 for HTTP, 443 for HTTPS, and 22 for SSH) open on the proxy and have the proxy just redirect the traffic coming into it on 443 to 8787 on the server with RStudio Server.\n\n\n\n\n\n\nNote\n\n\n\nFor our workbench server, we did path rewriting and port management in our nginx proxy.\nIf you recall, by the time we were done, our nginx config was set to only allow HTTPS traffic on 443, redirect all HTTP traffic on 80 to HTTPS on 443, and to take traffic at /rstudio to port 8787 on the same server, /jupyter to port 8000, and /palmer to 8080.\n\n\n[TODO: image of path rewriting + load-balancing]\nProxies are additionally sometimes configured to block traffic that isn’t explicitly allowed. In a data science environment, this means that you’ll have to configure the inbound proxy with all the locations you need. If you’ve got an outbound proxy that blocks traffic, you’re in an airgapped/offline situation.\nThere are a few other things a proxy may be used for. These use cases are less common in a data science environment.\nSometimes proxies terminate SSL. Because the proxy is the last server that is accessible from the public network, many organizations don’t bother to implement SSL/HTTPS inside the private network so they don’t have to worry about managing SSL certificates inside their private network. This is getting rarer as tooling for managing SSL certificates gets better, but it’s common enough that you might start seeing HTTP addresses if you’re doing server-to-server things inside the private network.\nOccasionally proxies also do authentication. In most cases, proxies pass along any traffic that comes in to where it’s supposed to go. If there’s authentication, it’s often at the server itself.\nSometimes the proxy is actually where authentication happens, so you have to provide the credentials at the edge of the network. Once those credentials have been supplied, the proxy will let you through. Depending on the configuration, the proxy may also add some sort of token or header to your incoming traffic to let the servers inside know that your authentication is good and to pass along identification for authorization purposes.\nTODO: image of auth at proxy\nLastly, there’s a special kind of reverse proxy called a load-balancer. A load-balancer is used to scale a service across a pool of servers on the back end. We’ll get more into how this works in Chapter 19."
  },
  {
    "objectID": "chapters/sec4/4-2-ent-networks.html#what-data-science-needs-from-the-network",
    "href": "chapters/sec4/4-2-ent-networks.html#what-data-science-needs-from-the-network",
    "title": "17  Enterprise Networking",
    "section": "17.2 What data science needs from the network",
    "text": "17.2 What data science needs from the network\nAs you’ve probably grasped, enterprise networking can be complex. And your IT/Admin group knows a lot about it. What they don’t know a lot about is the interaction of networking and data science, so it’s helpful for you to be able to clearly state what you need.\n\n\n\n\n\n\nWhat ports do I need?\n\n\n\nOne of the first questions IT/Admins ask is what ports need to be open. Depending on what ports you choose for the services you’re running those ports need to be open.\nThe good news is that almost all traffic for data science purposes is standard HTTP(S) traffic, so it can happily run over 80 or 443 if there are limitations on what ports can be open.\n\n\nOne of the most common issues with data science environments in an enterprise is proxy behavior. If you’re experiencing weird behavior in your data science environment – files failing to upload or download, sessions getting cutoff strangely, or data not transferring right – asking your IT/Admin about whether there are proxies and their behavior should be suspect number one.\nWhen you’re talking to your IT/Admin about the proxies, it’s really helpful to have a good mental model of what traffic might be hitting an inbound proxy and what traffic might be hitting an outbound one.\nAs we went over in Chapter 12, network traffic always operates on a call and response model. So whether your traffic is inbound or outbound is dependent on who makes the call. Inbound means that the call is coming from a computer outside the private network directed to a server inside the private network, and outbound is the opposite.\nTODO: image inbound vs outbound connection\nSo basically, anything that originates on your laptop – including the actual session into the server is an inbound connection, while anything that originates on the server – including everything in code that runs on the server is an outbound connection.\n\n17.2.1 Issues with inbound proxies\nInbound proxies affect the connection you’re making from your personal computer to the server. There are two ways this might affect your experience doing data science on a server.\nIt’s reasonably common for organizations to have settings that limit file sizes for uploads and downloads or implementing timeouts on file uploads, downloads, and sessions. In data science contexts, files tend to be big and session lengths long.\nIf you’re trying to work in a data science context and weird things are happening with file uploads or downloads or sessions ending unexpectedly, checking on inbound proxy settings is a good first step.\nSome data science app frameworks (including Shiny and Streamlit) use a technology called Websockets for maintaining the connection between the user and the app session. Most modern proxies (including those you’ll get from a cloud provider) support Websockets, but some older on-prem proxies don’t and you may have to figure out a workaround if you can’t get Websockets enabled on your proxy.\n\n\n17.2.2 Issues with forward/outbound proxies\nAlmost all enterprise networks have inbound proxies. Outbound ones are somewhat rarer. That’s because outbound proxies limit connections made from inside the network to the outside. It’s obvious why you’d need to protect your data science environment from the entire outside world.\nMany organizations don’t feel the need to limit what external resources people can interact with from inside their firewall, but limitations on outbound access have long been common in highly regulated industries with strong requirements around data security and governance and are becoming increasingly common in many different industries. Many organizations have these proxies to reduce the risk of someone getting in and then being able to exfiltrate valuable resource.\nOrganizations who limit outbound access from their data science environment usually refer to the environment as offline or airgapped. The term airgapped indicates that there is a physical gap – air – between the internet and the environment. It is very rare for this to be the case. In most cases, airgapping is accomplished by putting in an outbound proxy that disallows (nearly) all connections.\nThe good news is that once you’re working on your data science server, you don’t need to go out much. The bad news is that you will have to go out sometimes. It’s important you work with your IT/Admin to develop a plan for how to handle when outbound connectivity is needed.\nHere are the four most common reasons you’ll need to make outbound connections from inside your data science environment.\n\nDownloading Packages Downloading a package from a public repository requires a network connection to that repository. So you’ll need outbound access when you want to install R or Python packages from CRAN, BioConductor, public Posit Package Manager, Conda, PyPI, or GitHub.\nAccessing External Data In most data science work, you’re mostly just working on data from databases or files inside your private network, so you don’t really need access to data or resources outside. On the other hand, if you’re consuming data from public APIs or scraping data from the web, that may require external connections. You also may need an external connection if you’re accessing private data that lives in an external location – for example you might have data in an AWS S3 bucket you need to access from an on-prem workbench or data in Google Sheets that you need to access from AWS.\nSystem Libraries In addition to the R and Python packages, there are also system libraries you’ll need installed, like the versions of R and Python themselves, and other packages used by the system. Generally it’ll be the IT/Admin managing and installing these, so they probably have a strategy for doing it. This comes up in the context of data science if you’re using R or Python packages that are basically just wrappers around system libraries, like the R and Python packages that use the GDAL system library for geospatial work.\nSoftware Licensing If you’re using all open source software, this probably won’t be an issue. But if you’re buying licenses to a professional product, you’ll have to figure out how to activate the software licensing. This usually involves reaching out to servers owned by the software vendor. They should have a method for activating servers that can’t reach the internet, but your IT/Admins will appreciate if you’ve done your homework on this before asking them to activate some new software.\n\nWhat if your organization doesn’t default to allowing all of these things to be available? In some cases, ameliorating these issues is as easy as talking to your IT/Admin and asking them to open the outbound proxy to the right server.\nBefore you go ahead treating your environment as truly offline/airgapped, it’s almost always worth asking if narrow exceptions can be made to a network that is offline/airgapped. The answer may surprise you. Especially if it’s just a URL or two that are protected by HTTPS – for example CRAN, PyPI, or public RStudio Package Manager, it’s generally pretty safe and many organizations are happy to allow-list a limited number of outbound addresses.\nIf not, you’ll have to have a deeper conversation with the IT/Admin.\nYour organization probably has standard practices around managing system libraries and software licenses in their environment.\nExternal data connections and package management are the areas where you’ll have to have a conversation to make them accessible.\nIT/Admins often do not understand how crucial R and Python packages are to doing data science work. It will be on you to make them understand that your offline environment is useless if you can’t come up with a plan to manage packages together.\nThe best plans for offline package operations involve the IT/Admin curating a repository of allowed packages inside the private network using a professional tool like Posit Package Manager, Jfrog Artifactory, or Sonatype Nexus and then giving data scientists free reign to install those packages as needed inside the environment.\nThis can take a lot of convincing. Good luck."
  },
  {
    "objectID": "chapters/sec4/4-2-ent-networks.html#comprehension-questions",
    "href": "chapters/sec4/4-2-ent-networks.html#comprehension-questions",
    "title": "17  Enterprise Networking",
    "section": "17.3 Comprehension Questions",
    "text": "17.3 Comprehension Questions\n\nWhat is the advantage of adopting a more complex networking setup than a server just deployed directly on the internet? Are there advantages other than security?\nDraw a mental map with the following entities: inbound traffic, outbound traffic, proxy, DMZ, private subnet, public subnet, VPC\nOur workbench server has an nginx proxy that redirects inbound traffic on a few different paths to the right port on the same server. Looking at your nginx.conf, what would have to change if you moved each of those services to different servers? Is there anything you’d have to check on the server itself?\nLet’s say you’ve got a private VPC that hosts an instance of RStudio Server, an instance of JupyterHub, and a Shiny Server that has an app deployed. Here are a few examples of traffic – are they outbound, inbound, or within the network?\n\nSomeone connecting to and starting a session on RStudio Server.\nSomeone SFTP-ing an app and packages from RStudio Server to Shiny Server.\nSomeone installing a package to the Shiny Server.\nSomeone uploading a file to JupyterHub.\nA call in a Shiny app using httr2 or requests to a public API that hosts data.\nAccessing a private corporate database from a Shiny for Python app using sqlalchemy.\n\nWhat are the most likely pain points for running a data science workbench that is fully offline/airgapped?"
  },
  {
    "objectID": "chapters/sec4/4-3-auth.html#how-itadmins-think-about-auth",
    "href": "chapters/sec4/4-3-auth.html#how-itadmins-think-about-auth",
    "title": "18  Auth in Enterprise",
    "section": "18.1 How IT/Admins think about auth",
    "text": "18.1 How IT/Admins think about auth\nLet’s start with an analogy to the real world. Imagine all the different services your company’s IT/Admin team manages as rooms in a building. Email, databases, data science workbenches, social media accounts, HR systems, and more. Each one is a room in the building.\nIn order to comply with the principle of least privilege, people are given access only to the rooms they need. Now imagine you’re the one who has to figure out how to keep it all secure.\nWhen someone tries to enter a room, you’ll need a way to know who’s asking – to ascertain and validate their identity. This process is called authentication.\nBut just knowing who they are is insufficient. Remember, not everyone gets to access every room. So you’ll also need a way to check whether they have permission to access that room. That process is called authorization.\nTogether, the whole system of authenticating and authorizing people to take actions is called auth.\nMany organizations start out simply. They just add IT systems one at a time and use their built-in auth functionality. That’d be like posting a guard at the door to each room who don’t talk to each other.\nEach guard would issue some sort of credential that can be used to verify that the person approaching is who they say they are. For computer systems, the most common type of credential is a username and password combination.\nIncreasingly, organizations are moving towards other kinds of credentials instead of or in addition to usernames and passwords, including biometrics like fingerprints and facial identification, multi-factor codes or a push notification on your phone, or ID cards of some sort.\nBut just having each guard keep their own list of who’s allowed in and hand out their own credential is kinda a mess. As someone walking around the building, you’ve gotta keep all your credentials straight. Moreover, adding new people to the different systems or removing old people is a huge pain. You’ve gotta run around and tell each guard and then the guard has to revoke their credential.\nThis is a common scenario in small organizations without much central IT/Admin expertise. They tend to just use whatever auth comes with the systems they’re adopting.\nIn the labs in section 3, this is how we configured our server. We created a standalone server with standalone credentials. And while that’s secure enough on its own, it doesn’t integrate with any central IT/Admin capabilities.\nObviously, among enterprise organizations, this pattern doesn’t fly. If you’re only a team of 20 and you have 5 systems, it’s ok to metaphorically run around to each room. But a team of 2,000 or 20,000 with 50 or 500 systems can’t be managed this way.\nSo let’s start thinking about how we could make the management of this system simpler and easier.\n\n18.1.1 Centralizing user management with LDAP/AD\nThe first thing would be to standardize on a single set of credentials. No more letting each guard issue their own. You’ll create a central security booth where you’ll manage all the credentials. Whenever someone approaches a door, the guard will radio in the credential, you’ll verify that all is well and tell the guard, “yep, that’s Heather!”\nThis makes life easier for someone using the system since they have only one set of credentials. It’s also somewhat more secure. We know now that all of the doors are using a similar level of security and if someone loses their card or gets it stolen, we just have to do one trade.\nThis is basically the situation that many companies have been in from the mid 1990s onwards and many still use.\nAn open protocol called Lightweight Directory Access Protocol (LDAP – pronounced ell-dap) and the Microsoft implementation of LDAP called Active Directory (AD) allow organizations to maintain central lists of users and the groups to which they belong.\nThey configure their systems to query their LDAP/AD servers. LDAP/AD would sends back information on the user including their username and groups and the service could use that information to authorize the user.\nThis is a huge improvement over each system having its own set of credentials, but there are still three big issues.\nFirst, we haven’t done anything to make authorization easier. You’re now centrally verifying identities, but each guard still needs to maintain their own list of who’s allowed into their room. Changing those lists still requires running around to each room, which is time-consuming and error-prone.\nSecond, just how much do you trust those guards? The guards are radioing in the credentials. There’s nothing to stop them from writing them down for themselves.\nLastly, it’s kinda a pain. Having to re-authenticate at every door is a pain. If you’re logging into a lot of different systems, you’re going to have to authenticate dozens of times a day.\nOver time, these problems have only gotten keener. A few decades ago, most companies had only a few systems and they were probably all on-premise systems, reducing both the hassle of changes and the risk of credentials actually being entered to those systems.\nThese days, even modestly-sized organizations have dozens or hundreds of IT systems and many of them are SaaS services. That means the risk of sharing credentials with those services is much higher.\nWouldn’t it be nice if there were a way to solve all these problems? A solution would allow for centralized authorization, never sharing credentials with the guards, and authenticating only once.\nEnter the world of Single-Sign On (SSO).\n\n\n18.1.2 Single Sign On (SSO)\nOk, so let’s revamp the system one more time.\nNow, when someone enters the building, they’ll stop at the central security office and provide their credentials. In exchange, they’ll get a building access pass that’s unique to them and they cannot share.\nMoreover, you’ll equip each room with a machine to swipe a building access pass. Swiping the access pass sends a request back to the central security office with the person’s name and the room number. You can check whether they’re allowed in and send back the allow signal if so and the red disallow signal if not.\nNow, the guards don’t need to know anything about the people approaching and don’t need to be trusted to do anything other than appropriately respond to the allow/disallow indicator.\nMoreover, you can now manage authorization in the central security office. No more running around to each room when you need to onboard or offboard someone or change their role.\nThis is basically how SSO works – your building access pass is a token in your computer’s web browser, and the allow/disallow decision comes from the centralized auth management system.\nNow, SSO isn’t a description of a technology – it’s a description of a user and administrator experience. People have been making SSO work for a long time, but two main standards for doing SSO have arisen in the past 15 years or so.1\nThe most common option in enterprises is Security Assertion Markup Language (SAML), which is an XML-based standard.2 The current standard, SAML 2.0 was finalized in 2005.\nThe other option is Open Identity Connect (OIDC)/OAuth2.0, which used JSON-based tokens.3 It’s slightly newer than SAML, originally created by engineers at Google and Twitter in 2007. OAuth2 – the current standard – was released in 2012.\nFor nearly all SAML or OAuth implementations, organizations use an external identity manager. The most common ones to use are Okta, OneLogin, and Azure Active Directory (AAD).4\nThese days, almost all enterprises are moving quickly towards implementing SSO using either SAML or OAuth (or both).\nThere’s a lot more detail on the technical details of how LDAP, OAuth, SAML, and more work in Appendix A.\n\n\n18.1.3 Managing permissions\nIrrespective of what kind of auth technology you’re using, you have to actually manage permissions somehow. It’s worth learning a little about the options.\nThe simplest option is just to maintain a list of who’s allowed to enter each room. This is called an Access Control List (ACL).5\nACLs are a simple kind of permissions management that make a lot of intuitive sense.\nBut, as you can imagine, if your building has hundreds of rooms, that’s a lot of different rooms to keep individual lists for. Additionally, if you have permissions changing a lot, having to change individual user permissions is a pain.\nInstead, you might want to create a role that has certain permissions and then assign people to that role. For example, maybe there’s the role of manager, who has access to certain rooms. There might be another role that’s executive who has access to different rooms. Managing permissions in this way is called Role Based Access Control (RBAC), and many organizations have a requirement to be able to implement RBAC to adopt a new system.\n\nRBAC has the advantage of allowing more flexibility in creating roles and managing them relative to ACLs. You can also see how it’d be relatively simple to hook up something like your centralized HR database to an RBAC system by mapping actual users to a set of roles that is appropriate for them.\nBut RBAC has its own drawbacks. RBAC can result in role explosion over time – if people need specialized permissions, it’s often easier to create tons and tons of special roles rather than figure out how to harmonize them into a system.\nIt also can’t accommodate certain kinds of highly specific permissions scenarios. For example, what if you have a situation where room 4242 should only allow managers except from 9-12 on Tuesdays when it should allow maintenance services? That’s not something RBAC can accommodate.\nIf you have the need for even more granular permissions than RBAC can provide, you can create rules based on the room, the person, and the environment. This highly-flexible system is called Attribute Based Access Control (ABAC). The downside of ABAC is that it can be a real pain to set up because it is so powerful.\nTODO: ABAC diagram (mapping room 4242 to person/manager to environment)\nThe most well-known ABAC system is the AWS Identity and Access Management (IAM) system. If you’ve ever been utterly befuddled by applying permissions to resources in AWS, you can thank the complexity of ABAC. That complexity is the tradeoff for the very high degree of flexibility ABAC provides."
  },
  {
    "objectID": "chapters/sec4/4-3-auth.html#data-science-auth-concerns",
    "href": "chapters/sec4/4-3-auth.html#data-science-auth-concerns",
    "title": "18  Auth in Enterprise",
    "section": "18.2 Data Science Auth Concerns",
    "text": "18.2 Data Science Auth Concerns\nThe first and simplest concern that comes up for a new data science workbench is that it needs to support whatever auth mechanism your IT/Admin group has decided your organization is going to use. Hopefully you now have some context on what those various mechanisms are.\nThese days, most data science tools you might want to use support all of the standard SSO mechanisms – though many of them reserve that functionality for paid tiers.\n\n18.2.1 Data Access\nThe second concern that comes up is how to get access to data. In our building metaphor, this would be like sending a note from one room asking for a resource in another room.\nThis kind of access is actually easier to configure in a non-SSO configuration. If your database can be accessed with a username and password, it’s the equivalent of being able to just put your request on a piece of paper and append your credentials. The guard can verify the credentials and send back the proper information.\nIt’s worth noting that this configuration is still quite common. Most organizations have implemented SSO for their external-facing services, but internal data sources are often available with username and password auth. This is mostly because relatively few databases have implemented SSO configurations. I expect this will change quickly in the next few years.\nIn an SSO configuration, getting access to data is kinda complicated. Remember, you’re sitting in a room and you’ve got your building access pass, which you can’t give to anyone, and you can’t get access to another room without swiping your pass at the door.\nThe main way this is accomplished is by giving people the ability to remotely acquire the data room access token without actually going there with the building access pass. Depending on the tool you’re using and the type of technology being used, it may be able to automatically do this exchange for you or you may have to do it manually in your code.\nThe first technology used to authenticate to data services is a Windows-based system called a Kerberos ticket. Kerberos is a relatively old – but highly secure system. Kerberos is mainly used in Windows-based environments, so it’s mostly used when accessing a Microsoft SQL Server.\nIf your organization uses Kerberos for database access, you’ll need to work with your organization’s IT/Admin group to get everything configured.\nThe second way to accomplish SSO into a data source is to pass along an JSON Web Token (JWT, pronounced jot). JWTs are the technology that underlies OAuth/OIDC. They’re accepted by relatively few databases these days, but I expect that will change in the next few years.\nFor more technical details on how this experience is implemented, and some of the technical difficulties you may run into, see the appendix section on OAuth and SAML.\nThe last way to access a data source seamlessly is via integration with cloud IAM services. This will only work if you’re trying to access a cloud resource from another cloud resource. So you could allow access from an EC2 instance to an S3 bucket, but not from your on-prem compute cluster to an S3 bucket.\nThe patterns for doing this vary a lot by cloud and by the data science tooling you’re using. Configuring a cloud instance to “just know” who’s making a request is possible, but requires some savvy with managing cloud services.\n\n\n18.2.2 Service Accounts\nThe last concern that comes up a lot is what entity is doing the access. In workbench environments, it’s common for humans to have credentials to data sources and to login as themselves. However, when those resources go to production, it’s very common to introduce service accounts.\nA service account is a non-human entity that has permissions of its own. For example, maybe you’ve got a Shiny app that users use to visualize data that’s in a database. Very often, you don’t want the app to have the same permisions as the app’s author, or to inherit the permissions of the people viewing the app.\nOne important reason you might want this is that you want to manage the permissions of the app itself even if the author were to leave the company or change roles.\nInstead, you want the app to be able to have permissions to do certain database operations. In that case, you would create a service account to give to the Shiny app that has exactly those permissions.\nThere’s no particular magic to service accounts, but you’ll need to figure out how to make your app or report run as the right user and have the correct service account credentials."
  },
  {
    "objectID": "chapters/sec4/4-3-auth.html#comprehension-questions",
    "href": "chapters/sec4/4-3-auth.html#comprehension-questions",
    "title": "18  Auth in Enterprise",
    "section": "18.3 Comprehension Questions",
    "text": "18.3 Comprehension Questions\n\nWhat is the difference between authentication and authorization?\nWhat are some different ways to manage permissions? What are the advantages and drawbacks of each?\nWhat is some advantages of token-based auth? Why are most organizations adopting it? Are there any drawbacks?\nFor each of the following, is it a username + password method or a token method? PAM, LDAP, Kerberos, SAML, ODIC/OAuth"
  },
  {
    "objectID": "chapters/sec4/4-3-auth.html#footnotes",
    "href": "chapters/sec4/4-3-auth.html#footnotes",
    "title": "18  Auth in Enterprise",
    "section": "",
    "text": "Aside from the options below, some organizations use a system called Kerberos to accomplish SSO. This is quite rare.↩︎\nXML is a markup language, much like HTML. The good thing about XML is that it’s very flexible. The bad thing is that it’s relatively hard for a human to easily read.↩︎\nTechnically, OIDC is an authentication standard and OAuth is an authorization standard. You’ll usually just hear it all referred to as OAuth.↩︎\nYes, AAD is used for SAML/OAuth, not for LDAP. It’s confusing.↩︎\nStandard Linux permissions (POSIX permissons) that were discussed in Chapter 11 are basically a special case of ACLs. ACLs allow setting individual-level permissions for any number of users and groups, as opposed to the one owner, one group, and everyone else permissions set for POSIX.\nLinux distros now have support for ACLs on top of the standard POSIX permissions.↩︎"
  },
  {
    "objectID": "chapters/sec4/4-4-ent-servers.html#managing-complexity-with-infrastructure-as-code",
    "href": "chapters/sec4/4-4-ent-servers.html#managing-complexity-with-infrastructure-as-code",
    "title": "19  Enterprise Server Management",
    "section": "19.1 Managing complexity with Infrastructure as Code",
    "text": "19.1 Managing complexity with Infrastructure as Code\nHere’s where you don’t want to be an an enterprise IT/Admin. You stand up a server for the data science team in a known good state – with versions of R and Python and the applications they need. But then, over time, more stuff gets installed. Maybe they needed some more versions of R. And that package they installed required some updated system libraries.\nIf you just ssh in and do these things — or let the data science team do it themselves, you’ll quickly lose track of what the current state of the server is. You can’t just tear down the server and stand it back up, because none of the changes you’ve made are documented. In DevOps language, the server has become stateful – and that’s a no-no.\nNow, when you’ve just got one server like this, it’s annoying, but not the end of the world. If you’re an IT/Admin organization with 40 or 400 servers it is much more troublesome. And moreover, if you have many servers that need to be configured identically – for example because they’re treated as one cluster – this is impossible. In many enterprises, this whole though experiment would just never fly because production servers can’t be changed. They’re validated from top to bottom for compliance or security reasons and then have to be completely re-validated if any changes are needed.\nIT/Admin organizations want to avoid server state drift and make sure the environment is always in a known good state. If it ever needs to be torn down or moved, it should be a painless activity to re-create the exact same environment with a minimum of fuss.\nThere’s a DevOps adage to encompass this need to manage servers as a herd, rather than individually – servers should be cattle, not pets.\nTo restart an entire server environment from nothing, there are two general steps. Provisioning is about creating and configuring the (virtualized) hardware including the servers and networking configuration. Once the servers are provisioned, they need to be configured including installing things on them and getting them running.\nIf you’re thinking of restarting an environment from scratch, you need to\n\nCreate the networking environment for servers (VPC)\nStand up the actual servers and networking facilities\nConfigure servers (create users, groups, mount storage, etc)\nConfigure networking hardware (ports, firewalls, etc)\nInstall system libraries and operating system dependencies\nInstall R and/or Python\nInstall application layer (RStudio Server, JupyterLab, Shiny Server, etc)\nConfigure application layer (write config files)\nConfigure application layer networking (SSL, DNS)\nStart application\n\nAs an R or Python-loving data scientist, you know that the best way to create this kind of setup is with code. Much like you can use packages like renv and venv to document the state of your R or Python environment and restore it somewhere else (see Chapter 1), IT/Admin professionals use Infrastructure as Code (IaC) tooling to stand up and manage their servers.\nIt’s not always the case that organizations need to put all of this in code. For example, while servers should be re-create-able in code, it’s quite rare that you need to re-create a VPC, so it may well be the case that step remains manual.\nEither way, some names you’ll hear a lot if you’re looking into IaC are Terraform, Ansible, CloudFormation (AWS’s IaC tool), Chef, Puppet, and Pulumi. The other option, which we’ll discuss in more detail below, is using Docker in Kubernetes.\n\n\n\n\n\n\nNo Code IaC\n\n\n\nThere are also ways to manage enterprise servers that don’t involve IaC. These usually involve writing extensive “run-books” for how to configure servers or point-and-click graphical interfaces for server administration. If your spidey sense is tingling that this probably isn’t nearly as good, you’re right. Enterprise IT/Admin organizations that don’t use IaC tooling is definitely a red flag.\n\n\nWe’re not going to get into any more detail on how these IaC tools work – there are books on all of them you should feel free to read if you’re interested. But now you’ll understand what you’re hearing when the IT/Admin team says they need to write a Terraform script or a Chef recipe to be able to stand up a particular service.\n\n19.1.1 Dev/Test/Prod for IT/Admins\nLike you want a Dev/Test/Prod setup for your data science projects as discussed in Chapter 5, IT/Admins usually use a Dev/Test/Prod setup for themselves. They want an environment to test changes before promoting them for testing and eventually into production. In ideal states, even changes in Dev are only attempted with IaC tools to make sure it’s easy to move forward once the changes are final.\nIn enterprises, upgrades and migrations are major to-dos. Planning for a “lift-and-shift” to the cloud is a multi-year affair in many organizations. Even just upgrading operating systems can be a major undertaking.\nFor enterprises, I recommend a two-dimensional Dev/Test/Prod setup where the IT/Admin group make changes to the platform in a staging environment.1 Data scientists never get access to the Staging environments and do all their work in the IT/Admin prod environment.\n\nLooking back to Chapter 5, it should be obvious that the best way to move servers and applications from staging to prod is using IaC and CI/CD to make sure that changes to code always make it into production at the right time."
  },
  {
    "objectID": "chapters/sec4/4-4-ent-servers.html#enterprise-scale-and-stability",
    "href": "chapters/sec4/4-4-ent-servers.html#enterprise-scale-and-stability",
    "title": "19  Enterprise Server Management",
    "section": "19.2 Enterprise scale and stability",
    "text": "19.2 Enterprise scale and stability\nNeeding to scale data science environments for large jobs is a common need across large and small organizations. But the enterprise need to support many data scientists often requires a different kind of approach to scaling.\nIn Chapter 15, we discussed how just making a single server bigger with vertical scaling can take you a long way. But there are limits. For example, I was recently working with a large customer of Posit’s who needed to support up to 2,000 data scientists in their environment.\nAt that scale, there is no server that is nearly large enough. The way to accomplish this kind of scale isto spread the workload out across many servers with horizontal scaling.\nAdditionally, when you’re operating at that scale, the stability of the system gets really important. When you’ve got 2,000 data scientists using your platform, each hour of lost time costs more than $100,000.2\nFor some organizations, this necessitates the creation of a disaster recovery plan. This may mean that there are frequent snapshots of the state of the server (often nightly) so the IT/Admins can just roll back to a previous known good state in the event of a failure. Sometimes it also means that there is actually a copy of the server waiting on standby to be activated at all times.\nOther times, there are stiffer requirements such that nodes in the cluster could fail and the users wouldn’t be meaningfully affected. This requirement for limited cluster downtime is often called high availability. High availability is not a description of a particular technology or technical approach – though it is sometimes treated as such. High availability is a description of a desired outcome for a cluster and different organizations have different definitions.\n\n\n\n\n\n\nMeasuring Uptime\n\n\n\nDowntime limits are often measured in nines of uptime. Nines of uptime refers to the proportion of the time that the service is guaranteed to be online out of a year. So a one-nine service is guaranteed to be up 90% of the time, allowing for 36 days of downtime a year. A five-nine service is up for 99.999% of the time, allowing for only about 5 1/4 minutes of downtime a year.\n\n\nHorizontal scaling and high availability are different of requirements with different implications. Depending on your organization, you may have horizontal scaling requirements or high availability requirements or both.\nOne option, where you have horizontal scaling requirements, but not high availability is to just put different users or team on their own server. The upside to this is that the team gets ownership of the server. The downside is fragmentation and that the team has to manage your own servers. In organizations that allow this kind of fragmentation, each team is usually on their own to support those servers and you’re basically back in non-enterprise land.\nIn enterprises, this usually isn’t the way things get done. Most enterprises want to run one centralized service that everyone in the company – or at least across a large group – come to.\nIf you’re trying to run a large centralized service, it’s going to be run as a load-balanced cluster. In this context, you don’t want to think about individual servers. Instead, you want to manage the cluster as a whole and make it really easy to add individual servers, called nodes.\nIn order to go from a single server to a cluster, you add two requirements – there needs to be one front door to all the servers, called a load-balancer – and any state users care about needs to move from the individual server to a shared location all of the nodes can access. Usually that state is stored in a database or network attached storage (NAS), which is just a filesystem that lives on its own server.\n\nIf you are a solo data scientist reading this – please do not try to run your own data science cluster. When you undertake load balancing, you’ve taken on a distributed systems problem and those are inherently difficult. When done for scaling reasons, it is almost always worthwhile to exhaust the part of vertical scaling where costs grow linearly with compute before undertaking horizontal scaling.\nIf your organization has high availability requirements, it’s worth considering that just adding more nodes may not be sufficient. As the requirements for high availability get steeper, the engineering cost to make sure the service really is that resilient rise exponentially. In addition to considering what to do if a node goes offline, you also have to have backup for the load-balancer and the database/NAS, as well as any other part of the system.\nIn fact, it’s totally possible to make your system less stable by just doing horizontal scaling in one spot without thinking through the implications.\n\n19.2.1 Load-balancing requirements\nLoad-balancers are just a special kind of proxy that routes sessions among the nodes in the cluster. A simple load-balancer just rotates sending traffic to the different nodes using a round-robin configuration.\nAt a minimum, your load-balancer has to know what nodes are accepting traffic. This is accomplished by configuring a health check/heartbeat endpoint for your product. A health check is a feature of an application. The load-balancer periodically pings the health check. If it gets a response, it knows it can send traffic to that node. If no response comes back, it treats that node as unhealthy and doesn’t send traffic there.\nIt is sometimes necessary to do more complicated load-balancing that pays attention to how loaded different nodes are. Additionally, some products feature internal load-balancers, so it really doesn’t matter what your load-balancer does.\nOne other feature that may come up is sticky sessions or sticky cookies. For stateful applications – like Shiny apps – you want to get back to the same node in the cluster so you can resume a previous session. In most load-balancers, this is a simple option you can just turn on.\nThere are a few different ways to configure load balancing for servers. The first is called active/active. This just means that all of the servers are online all of the time. So if I have two RStudio Server instances, they’re both accepting traffic all the time.\nIn active/passive configurations, you have two or more servers, with one set accepting traffing all the time, and the other set remaining inert until or unless the first set goes offline. This is sometimes called a blue/green or red/black configuration.\nPeople often really like this configuration if they have high requirements for uptime, and want to be able to do upgrades to the system behind the scenes and then just cut the traffic over at some point without an interruption in service. It is a nice idea. It is often very hard to pull off.\nFor an application, there are basically two ways it can store data – in a database or in files on the filesystem. Now, this isn’t data you need to access, but it’s needed for the internal workings of the application. For example, it may need to save log information or counts of the number of active users.\nWhen you’re in a single-server configuration, all this can just go somewhere on the server with the application. Very often the database is just a sqlite file that sits in a file on the filesystem.\nBut when you go to to a load-balanced cluster, all of the nodes need symmetric access to read and write. You need an actual database (like postgres) for the application to use. Additionally, anything on the filesystem now needs to move off of an individual server onto a networked system that all the nodes can access.\n\n\n19.2.2 Accommodating different kinds of workloads\nWhen you’re doing horizontal-scaling, people often want to accomodate a variety of workloads. For example, they might want to be able to incorporate several different sizes of jobs that might require a few different sized nodes. Or maybe they want to run a mixture of GPU-backed and non-GPU workloads in the cluster and want to reserve GPU nodes for the jobs that need it.\nIn general, it’s not trivial to set up a single cluster that can support different kinds of workloads, and it’s often easier to set up, for example, a standalone GPU cluster.\nThese days, people are often reaching for Kubernetes for clusters that need to support varies workloads. While it’s true that Kubernetes can theoretically support different kinds of nodes within one cluster, this often isn’t trivial to configure.\nOften, a better option for data science workloads is to use use a high-performance computing (HPC) framework. HPC is particularly appropriate when you need very large machines. In Kubernetes, it is not possible to schedule pods larger than the size of the actual nodes in the cluster. In contrast, most HPC frameworks allow you to combine an arbitrary number of nodes into what acts like a single machine with thousands or tens of thousands of nodes.\nFor example, Slurm is an HPC framework that support multiple queues for different sets of machines. AWS has a service called ParallelCluster that allows users to easily set up a Slurm cluster – and with no additional cost relative to the cost of the underlying AWS hardware.\n\n\n19.2.3 Adding Autoscaling\nSometimes people also undertake horizontal scaling to be able to do autoscaling. The idea here is that the organization could maintain a small amount of “always-on” capacity and scale out other capacity as needed to maintain costs. This is possible – but it requires nontrivial engineering work.\nIn particular, autoscaling a data science workbench down is quite hard. The main reason for this is that many autoscaling routines assume you can easily move someone’s job from one node to another just keeping track of the long-term state. This is a bad assumption for a data science workbench and autoscaling a data science workbench downwards is a difficult challenge.\nLet’s think about a relatively stateless workload. For example, let’s imagine a search engine – every time you put in a search, it spins up a job, does your search, and then spins down.\nIf you come back in a minute or two, it can just spin up another job. It only needs to remember your last query, but that’s a pretty simple bit to state to pass around. If you’re on a different node, no big deal!\nThat’s not the case with a data science workbench. Many autoscaling frameworks these days assume that applications are mostly stateless. In a stateless applications, every interaction is standalone – there’s very little path dependence. The line between statefulness and statelessness are pretty blurry, but working inside a development environment like RStudio or a Jupyter Notebook is about as stateful as it gets. You have long-term state like the files you need and your preferences and short-term state like the recent commands you’ve typed and the packages loaded into your R and Python environment."
  },
  {
    "objectID": "chapters/sec4/4-4-ent-servers.html#dockerkubernetes-in-enterprise",
    "href": "chapters/sec4/4-4-ent-servers.html#dockerkubernetes-in-enterprise",
    "title": "19  Enterprise Server Management",
    "section": "19.3 Docker/Kubernetes in Enterprise",
    "text": "19.3 Docker/Kubernetes in Enterprise\nOriginally created at Google and released in 2014, Kubernetes is the way to run production services out of Docker containers.3 It is an open source project. If you’re confused by the name, apparently Kubernetes is an ancient Greek word for “helmsman”.\nMany organizations are moving towards using Docker and Kubernetes for their server-based infrastructure. Kubernetes solves all three of the key enterprise IT/Admin challenges with running servers because you manage a single cluster, you can scale rather effortlessly, and app-level administration is extremely simple.\nThe elegance of using Kubernetes is that it completely separates provisioning and configuration challenges. The main unit of analysis in Kubernetes is a pod, which is the term for a Docker container that is running in Kubernetes.\nThe elegance of Kubernetes is that you create a cluster of a certain number of nodes and separately request a certian number of pods with a certain amount of horsepower. Kubernetes takes care of scheduling the pods on the nodes.\nTODO: Graphic of Kubernetes\nThis is amazing, because unlike running services on a regular VM, you just make sure you’ve got enough horsepower in the cluster and then all the app-level requirements go in the container. Then when you declare how many pods you want, you don’t have to worry about what’s going on with each of the nodes in the cluster because it is just running Kubernetes and Docker.\nThis really is extremely powerful – it’s pretty easy to tell Kubernetes, “I want one instance of my load balancer container connected to three instances of my Workbench container with the same storage volume connected to all three.”\nIn practice, unless you’re part of a very sophisticated IT organization, you’ll almost certainly use Kubernetes via one of the cloud providers’ Kubernetes clusters as a service. AWS’s is called Elastic Kubernetes Service (EKS).4\nOne really nice thing about using these Kubernetes clusters as a service is that adding more compute power to your cluster is generally as easy as a few button clicks. On the other hand, that also makes it dangerous from a cost perspective.\nIt is possible to define a Kubernetes cluster “on the fly” and deploy things to a cluster in an ad hoc way. But then you’re back out of IaC world. You can use any standard IaC tooling to stand up a Kubernetes cluster in any of the three major clouds. Once you’ve got the cluster up, Helm is the standard tool for defining what’s running in the cluster.\nThere’s a reason Kubernetes is taking the world by storm – it makes managing complicated enterprise workloads way easier. For example, let’s say you want to stand up a new service in a Kubernetes cluster using IaC. I listed 10 steps above for doing this in a standard server-based configuration. For Kubernetes, there are only three steps:\n\nProvision the cluster itself using IaC or add nodes to an existing cluster (usually trivially easy)\nGet the Docker containers to run in the cluster (often provided by vendors)\nDeclare what container configuration you want in the environment using Helm\n\nOne really nice thing about this is that it’s really clear where the state lies. The applications themselves are installed in the Docker containers and because the containers themselves are ephemeral, state has to be mounted in from somewhere external.\nRelative to more stateful server-based configuration, Kubernetes-based configurations rest a lot more heavily on environment variables, so a lot of configuring application setup is the same as configuring the environment variables in the running container.\nIn the extreme version, some enterprises are going to “we just run a single kubernetes cluster for our entire organzation”. In my opinion, this will someday be a great pattern, but it’s still a little immature. In particular, Kubernetes kinda makes the underlying assumption that all of the resources for the cluster are shared across the cluster. There are ways to use namespaces and other things – but not all Kubernetes resources can be split into namespaces, and managing a Kubernetes cluster for multiple use cases or groups isn’t at all trivial.\nIt’s easy to look at the power of Kubernetes and think it will make everything easy. This is not the case. While the high-level outlines of Kubernetes are super appealing, it is still a complex tool and a data science workbench is a particularly difficult fit for Kubernetes.\nIn many organizations, adopting Kubernetes is synonymous with trying to do autoscaling. As previously discussed, autoscaling a data science workbench is a particularly difficult task – and Kubernetes is particularly friendly to stateless workloads. Autoscaling in Kubernetes with a data science workbench really requires a highly competent Kubernetes admin.\nNetworking in Kubernetes can also be quite complicated. For anything that lives fully in Kubernetes, like your Workbench nodes and load-balancer, it’s quite simple. But getting things into and out of the Kubernetes cluster – like the filesystem you probably need to mount in and accessing databases is a real challenge.\nAll this to say – these are solvable problems for an experienced Kubernetes admin. But they will probably need some guidance around specific requirements for a data science workbench."
  },
  {
    "objectID": "chapters/sec4/4-4-ent-servers.html#comprehension-questions",
    "href": "chapters/sec4/4-4-ent-servers.html#comprehension-questions",
    "title": "19  Enterprise Server Management",
    "section": "19.4 Comprehension Questions",
    "text": "19.4 Comprehension Questions\n\nWhat is the difference between horizontal and vertical scaling? For each of the following examples, which one would be more appropriate?\n\nYou’re the only person using your data science workbench and run out of RAM because you’re working with very large data sets in memory.\nYour company doubles the size of the team that will be working in your data science workbench. Each person will be working with reasonably small data, but there’s going to be a lot more of them.\nYou have a big modeling project that’s too large for your existing machine. The modeling you’re doing is highly parallelizable.\n\nWhat is the role of the load balancer in horizontal scaling? When do you really need a load balancer and when can you go without?\nWhat are the biggest strengths of Kubernetes as a scaling tool? What are some drawbacks?"
  },
  {
    "objectID": "chapters/sec4/4-4-ent-servers.html#footnotes",
    "href": "chapters/sec4/4-4-ent-servers.html#footnotes",
    "title": "19  Enterprise Server Management",
    "section": "",
    "text": "I call it staging because I call the data science environments Dev/Test/Prod. You’ll have to work out language with your IT/Admin group.↩︎\nA low-end estimate of $100,000 fully-loaded cost for one data scientist FTE and 2,000 hours is $50 per hour.↩︎\nIf you are pedantic, there are other tools for deploying Docker containers like Docker Swarm and Kubernetes is not limited to Docker containers. But those are corner cases.↩︎\nIf you aren’t using EKS, Azures AKS, or Google’s GKE, the main other competitor is Oracle’s OpenShift, which some organizations have running in on-prem Kubernetes clusters.↩︎"
  },
  {
    "objectID": "chapters/append/auth.html#user-database",
    "href": "chapters/append/auth.html#user-database",
    "title": "Appendix A — Auth Technologies",
    "section": "A.1 User Database",
    "text": "A.1 User Database\nMany pieces of software come with integrated authentication. When you use those systems, the product stores encrypted username and password pairs in a database.\nThese setups are often really easy from an admin perspective – you just set up individual users on the server.\nHowever, the flip side is that users have one more username and password to remember, which is annoying for them. Moreover, if you have more than a few users, or the system is one of more than a few, it’s hard to manage users on a lot of different systems. It can be a real pain to create accounts on a ton of different systems when a new person joins the organization, or to remove their permissions one-by-one when they leave.\nFor this reason, most IT/Admin organizations strongly prefer using some sort of centralized identity store."
  },
  {
    "objectID": "chapters/append/auth.html#linux-accounts",
    "href": "chapters/append/auth.html#linux-accounts",
    "title": "Appendix A — Auth Technologies",
    "section": "A.2 Linux Accounts",
    "text": "A.2 Linux Accounts\nMany pieces of software – especially data science workbenches – are able to look at the server they’re sitting on and use the accounts on the server themselves.\nPluggable Authentication Modules (PAM) is the system that allows Linux-based authentication. As of this writing, PAM is the default authentication method for both RStudio Server and JupyterHub.\n\nConceptually PAM is pretty straightforward. You install a service on a Linux machine and configure it to use PAM authentication from the underlying host.\nBy default, PAM just authenticates against the users configured on the Linux server, but it can also be configured to use other modules to authenticate against other systems – most commonly LDAP/AD or Kerberos. PAM can also be used to do things when users login – the most common being initializing tokens or tickets to other systems, like a database.\nPAM is often paired with System Security Services Daemon (SSSD) to automatically create Linux users on a server based on the identities stored in an LDAP/AD instance.\nThough conceptually simple, reading, writing, and managing PAM modules is quite painful. Additionally, as more services move to the cloud, there isn’t necessarily an underlying Linux host where identities live and PAM is generally considered a legacy technology."
  },
  {
    "objectID": "chapters/append/auth.html#ldapad",
    "href": "chapters/append/auth.html#ldapad",
    "title": "Appendix A — Auth Technologies",
    "section": "A.3 LDAP/AD",
    "text": "A.3 LDAP/AD\nFor many years, Microsoft’s Lightweight Directory Access Protocol (LDAP) implementation called Active Directory (AD) was basically the standard in enterprise authentication. As a credential-based system, it is increasingly being retired in favor of token-based systems like SAML and OAuth2.0.\nWhen you login to a service with LDAP, it uses the ldapsearch command to to look you up. LDAP is a protocol, like HTTP. In addition to the query, LDAP sends a set of bind credentials to validate that it’s allowed to be looking things up in the server.\nLDAP can be configured in single-bind mode, where it uses the user’s credentials as the bind credentials. More often, LDAP is configured in double-bind mode, where there is a dedicated service account for binding.\n\nTo give a little more color, let’s look at the results from an ldapsearch against an LDAP server.\nHere’s what my entry in a corporate LDAP directory might look like this:\ncn: Alex Gold\nmail: alex.gold@example.com\nmail: alex.gold@example.org\ndepartment: solutions\nmobile: 555-555-5555\nobjectClass: Person\nYou’ll note that LDAP sends back a lot of information! This is how authorization is done with LDAP – the service is configured to inspect the items that come back from the ldapsearch and do authorization accordingly. This makes it possible to do authorization, but it’s not nearly as tidy as with a token-based system.\nOne of the things LDAP excels at relative to token-based systems is the ability to do lookups ahead of time. Because LDAP is a database, you can do a search for users – for example to make accounts for them – before they show up. In contrast, token-based systems don’t know anything about you until you show up for the first time with a valid token.\nAn example where this matters is if you have a service that itself controls authorization. For example, in Posit Connect, you can assign authorization to see certain apps and reports inside Connect. If you’re using LDAP, you can run a search against the server and add them before they ever log in. If you’re using a token-based system, you’ve gotta wait for them to show up. There are ways around this, but they generally don’t come out-of-the-box."
  },
  {
    "objectID": "chapters/append/auth.html#sec-kerberos",
    "href": "chapters/append/auth.html#sec-kerberos",
    "title": "Appendix A — Auth Technologies",
    "section": "A.4 Kerberos Tickets",
    "text": "A.4 Kerberos Tickets\nKerberos is a relatively old token-based auth technology for use among different enterprise servers. In Kerberos, encrypted tokens called Kerberos Tickets are passed between the servers in the system.\nThough Kerberos is freely available, it was widely adopted along with Active Directory, and it’s used almost exclusively in places that are running a lot of Microsoft products. The most frequent use of Kerberos tickets is to establish connections to Microsoft databases.\nKerberos works by sending information to and from the central Kerberos Domain Controller (KDC).\nWhen a Kerberos session is initialized, the service sends the users credentials off to the KDC and requests something called the Ticket Granting Ticket (TGT) from the KDC. TGTs have a set expiration period. When they expire, the client has to request an updated TGT.\nThis is one reason why Kerberos is considered quite secure - even if someone managed to steal a TGT, they’d only be able to use it for a little while before it went stale and could be revoked.\nWhen the user want to access a service, they send the TGT back to the KDC again and get a session key (sometimes referred to as a service ticket) that allows access, usually with a specified expiration period.\n\nIt’s worth reiterating that Kerberos is used exclusively inside a corporate network. This is one reason it’s still considered secure, even though it’s old. On the other hand, because everything has to live inside the network, it doesn’t work well for providing access to services outside the network, like SaaS software. For that reason, Kerberos is considered a legacy tool."
  },
  {
    "objectID": "chapters/append/auth.html#oauth-saml",
    "href": "chapters/append/auth.html#oauth-saml",
    "title": "Appendix A — Auth Technologies",
    "section": "A.5 Modern systems: OAuth + SAML",
    "text": "A.5 Modern systems: OAuth + SAML\nThese days, most organizations are quickly moving towards implementing a modern token-based authentication system through SAML and/or OAuth.1\nThe way both systems work is that you go to login to a service and are redirected to the SAML/OAuth system to seek a token that will let you in. Assuming all goes well, you’re granted a token and go back to the service to go do your work.\nThere are a few meaningful differences between SAML and OAuth that may help in conversations about these systems with Enterprise IT folks.\nMost of these differences stem from the origin of the technology. The current SAML 2.0 standard was finalized in 2005 – roughly coinciding with the beginning of the modern era of the web. Facebook was started just the prior year.\nOAuth is the next generation. It was started in 2006 and the current 2.0 standard was finalized in 2013.\nWhere SAML was designed to replace older enterprise auth technologies like LDAP/AD and Kerberos, OAuth was designed with the web in mind. People were trying to solve the problem of how to avoid having usernames and passwords for each individual site you might access without giving those sites the usernames and passwords of more sensitive sites.\nYou might’ve used SAML today if you logged into a SSO service for work. In contrast, you’ve almost certainly used OAuth today. Any time you’ve used a Login with Google/Facebook/Twitter/GitHub flow – that’s OAuth.\nWhile OAuth is the default on the web, it’s still somewhat newer in an enterprise context. Increasingly, cloud services are adopting OAuth based auth patterns and many popular database providers are adding the ability to use OAuth.\nSAML is XML-based while OAuth uses JSON Web Tokens (JWT). In both cases, the tokens are usually cryptographically signed. SAML tokens may be encrypted, while JWTs are usually unencrypted and signed. This makes OAuth somewhat easier to debug as JSON is easier to read than XML.\nSAML is a user-centric authentication tool, while OAuth is an application-centric authorization tool. The difference is subtle, but meaningful. For example, let’s say you’re trying to access your organization’s data science workbench. If I was using SAML, I would acquire a token that says “the bearer of this token is Alex Gold”, and it would be up to the workbench to know what to do with that. In contrast, an OAuth token would say, “the bearer of this token should get these specific permissions to Alex Gold’s stuff in this application”. That means that OAuth can be used for authorization at a much more granular level.\nImplementing SSO to the “front door” of a data science service, like your data science workbench or hosting platform is pretty standard these days. Right now, the industry is strongly heading in the direction of “second hop” auth. The idea is that when this works right, you log into the front door of your data science server with SAML or OAuth and then access to data sources like databases, APIs, and cloud storage is handled automatically on the back end – no need for users to (insecurely) input credentials inside the data science environment.2\nTODO: image of passthrough auth\nVery often, IT/Admins describe this experience as “passthrough” auth. The issue is that SAML and OAuth tokens can’t be passed through to another service. SAML tokens are specific to the service provider who asked for them and OAuth tokens describe particular authorizations for the bearer to a particular service.\nIn contrast, a JWT can be issued to any entity and every single request made is accompanied by the OAuth JWT. For example, if you’ve accessed Google Sheets or another Google service from R or Python, you’ve gone through the JWT acquisition dance and then attached that JWT to every request to prove it’s authorized. Similarly, cloud IAM experiences are based on passing JWTs around.\nThis means that there’s always a need to go back to trade in an existing token for a different one for the service you actually want to access. This is almost always an OAuth JWT. Increasingly, data science services are implementing these flows themselves so it does happen automatically, but in some cases you’ll have to figure out how to do the token exchange manually.\nThese days, many organizations are caught in the middle of this work - they are probably using SAML to access many systems and may have mandates to move towards credential-free communication to data sources, but very few have fully implemented such a system.\nHere are quick overviews on how both SAML and OAuth work.\nIn SAML, the service you’re accessing (token consumer) is called the service provider (SP) and the entity issuing the token is called the SAML identity provider (IdP). Most SAML tooling allows you start at either the IdP or the SP.\nIf you start at the SP, you’ll get redirected to the IdP. The IdP will verify your credentials. If all is good, it will put a SAML token in your browser. The SAML token contains claims – information provided by the SAML IdP.\n\nThe OAuth flow is a little more parsimonious. In OAuth, the service you’re trying to visit is called the resource server and the token issuer is the authorization server. When you try to access a service, the service knows to look for a JWT that includes specific claims against a set of scopes defined ahead of time.\nFor example, you may claim read access against the scope of events on Alex’s calendar.\nIf you don’t have that token, you’ll need to go to the authorization server to get it. Unlike in SAML where action is all occurring via HTTP redirects and the SAML token must live in the person’s browser, OAuth makes no assumptions about how this flow happens.\nThe authorization server knows how to accept requests for a token and the resource server knows how to accept them. The process of requesting and getting a token can happen in a number of different ways that might include browser redirects and caches, but also could be done entirely in R or Python.\nThis is why OAuth is used for service-to-service communication."
  },
  {
    "objectID": "chapters/append/auth.html#footnotes",
    "href": "chapters/append/auth.html#footnotes",
    "title": "Appendix A — Auth Technologies",
    "section": "",
    "text": "Modern OAuth is actually OAuth2.0. Whenver I refer to OAuth, I mean OAuth 2.0.↩︎\nWhile OAuth is an authorization technology, there is an authentication technology, Open ID Connect (OIDC) built on top of OAuth. In practice, people use the term OAuth to refer to both OAuth proper and OIDC.↩︎"
  },
  {
    "objectID": "chapters/append/lab-map.html",
    "href": "chapters/append/lab-map.html",
    "title": "Appendix B — Lab Map",
    "section": "",
    "text": "This section aims to clarify the relationship between the assets you’ll make in each portfolio exercise and labs in this book.\n\n\n\n\n\n\n\nChapter\nLab Activity\n\n\n\n\nChapter 1: Environments as Code\nCreate a Quarto side that uses {renv} and {venv} to create standalone R and Python virtual environments, create a page on the website for each.\n\n\nChapter 3: Data Architecture\nMove data into a DuckDB database.\n\n\nChapter 2: Project Architecture\nCreate an API that serves a Python machine-learning model using {vetiver} and {fastAPI}. Call that API from a Shiny App in both R and Python.\n\n\nChapter 4: Logging and Monitoring\nAdd logging to the app from Chapter 2.\n\n\nChapter 5: Code Promotion\nPut a static Quarto site up on GitHub Pages using GitHub Actions that renders the project.\n\n\nChapter 9: Docker\nPut API from Chapter 2 into Docker container.\n\n\nChapter 10: Cloud\nStand up an EC2 instance. Put model into S3.\n\n\nChapter 11: Linux Admin\nAdd R, Python, RStudio Server, JupyterHub, palmer penguin fastAPI + App.\n\n\nChapter 12: Networking\nAdd proxy (nginx) to reach all services from the web.\n\n\nChapter 13: DNS\nAdd a real URL to the EC2 instance. Put the Shiny app into an iFrame on the site.\n\n\nChapter 14: SSL\nAdd SSL/HTTPS to the EC2 instance.\n\n\nChapter 15: Servers\nResize servers."
  }
]