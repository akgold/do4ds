[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DevOps for Data Science",
    "section": "",
    "text": "Welcome!\nThis is the website for the book DevOps for Data Science, currently in draft form.\nIn this book, you’ll learn about DevOps conventions, tools, and practices that can be useful to you as a data scientist. You’ll also learn how to work better with the IT/Admin team at your organization, and even how to do a little server administration of your own if you’re pressed into service.\nThis website is (and always will be) free to use, and is licensed under the Creative Commons Attribution-NonCommercial-NoDerivs 3.0 license.\nIf you’d like a physical copy of the book, they will be available once it’s finished!"
  },
  {
    "objectID": "index.html#software-information",
    "href": "index.html#software-information",
    "title": "DevOps for Data Science",
    "section": "Software information",
    "text": "Software information\nI used the knitr package [@xie2015] and the quarto package [@quarto] to compile my book.\nThis book is published to the web using GitHub Actions from rOpenSci."
  },
  {
    "objectID": "index.html#about-the-author",
    "href": "index.html#about-the-author",
    "title": "DevOps for Data Science",
    "section": "About the Author",
    "text": "About the Author\nAlex Gold is the Director of Solutions Engineering at Posit, formerly RStudio.\nThe Solutions Engineering team works with Posit’s customers to help them deploy, configure, and use Posit’s professional software and open source tooling in R and Python.\nIn his free time, he enjoys landscaping, handstands, and Tai Chi.\nHe occasionally blogs about data, management, and leadership at alexkgold.space."
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "DevOps for Data Science",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nI have so many people to thank for their help in getting this book out the door.\nThe biggest thanks to current and former members of the Solutions Engineering Team at Posit, who taught me so much about DevOps, Data Science, and how to be a great team.\nThanks to my family, especially my brother, who is a great brother and cared enough about this project to insist he appear in the acknowledgments.\nThanks to Randi Cohen at Taylor and Francis, who has been great to work with, and to my editor, Linda Kahn, who’s always been more than an editor to me.\nMost of all, thanks to Shoshana for helping me live my best life.\nHuge thanks to the R4DS bookclub, especially Jon Harmon, Gus Lipkin, and Tinashe Michael Tapera, who read an early (and rough!) copy of this book and gave me amazing feedback.\nThanks to all others who provided improvements that ended up in this book (in alphabetical order): Carl Boettinger, Jon Harmon, Gus Lipkin, and Leungi."
  },
  {
    "objectID": "index.html#color-palette",
    "href": "index.html#color-palette",
    "title": "DevOps for Data Science",
    "section": "Color palette",
    "text": "Color palette\nTea Green: #CAFFDO\nSteel Blue: #3E7CB1\nKombu Green: #273c2c\nBright Maroon: #B33951\nSandy Brown: #FCAA67"
  },
  {
    "objectID": "chapters/intro.html#devops-for-agile-software",
    "href": "chapters/intro.html#devops-for-agile-software",
    "title": "Introduction",
    "section": "DevOps for Agile Software",
    "text": "DevOps for Agile Software\nDevOps is a set of cultural norms, practices, and supporting tooling to help make the process of developing and deploying software smoother and lower risk.\nIf that definition strikes you as unhelpfully vague, you’re right.\nLike Agile software development, to which it is closely related, DevOps is a squishy concept. That’s partially because DevOps isn’t a fixed thing. It’s the application of some principles and process ideas to whatever context you’re working in. That malleability is why DevOps works, but also makes it difficult to pin down.\nThis imprecision furthered by the ecosystem of companies enabling DevOps. There are dozens and dozens of companies proselytizing their own particular flavor of DevOps – one that (shocker) reflects the capabilities of whatever product they’re selling.\nBut underneath the industry hype and the marketing jargon, there are some extremely valuable lessons to take from the field.\nTo understand better, let’s go back to the birth of DevOps.\nAs the story goes, the history of software development before the 1990s involved a waterfall development processes. In these processes, software developers worked with clients and customers to fully define requirements for software, plan the whole thing out, and deliver final software months or years later.\nWhen the application was complete, it was hurled over the metaphorical wall from Development to Operations. IT Administrators in the Ops department would figure out the hardware and networking requirements, get it running, and keep it up.\nThroughout the 1990s, software developers observed that delivering software in small units, quickly collecting feedback, and iterating was a more effective model.\nIn 2001, the Manifesto for Agile Software Development was published, giving a name to this philosophy of software development. Agile development ate the world and, basically, all software is now developed using some form of Agile. Agile work styles have extended far beyond software into other domains as well.\nThere are dozens of Agile software development frameworks you might have heard of including Scrum, Kanban, Extreme Programming (XP), and many, many more. One commonality of these frameworks were really focused on software development. What happened once the software was written?\nThe old pattern clearly wouldn’t work. If you were doing new deployments multiple times a week – or even a day – you needed a complementary process to get that software deployed and into production.\nDevOps arose as this discipline, i.e., a way for Dev and Ops to better collaborate on the process that would take software from development into production. It took a little while for the field to be formalized, with the term DevOps coming into common usage around 2010."
  },
  {
    "objectID": "chapters/intro.html#processes-and-people",
    "href": "chapters/intro.html#processes-and-people",
    "title": "Introduction",
    "section": "Processes and People",
    "text": "Processes and People\nThroughout this book, DevOps refers to the knowledge, practices, and tools that make it easier, safer, and faster to put work into production. So, if you’re a software developer (and as a data scientist, you are) you need to be thinking about DevOps.\nMost organizations also have a set of people and roles who have the permission and responsibility for managing their organization’s servers and software. Their titles vary. They might be named Information Technology (IT), SysAdmin, Site Reliability Engineering (SRE), or DevOps.1\nFor simplicity, I’m going to use the term IT/Admin to refer to these people and teams throughout this book.\nFundamentally, DevOps is about creating good patterns for people to use when collaborating on developing and deploying software. Because these patterns vary by organization, DevOps can and should look different at different organizations.\nAs a data scientist, you are the Dev, so a huge part of making DevOps work for you is finding IT/Admin counterparts with whom you can collaborate. In some cases that will be easier than others. Here are three patterns that are almost always red flags – mostly because they make it hard to develop relationships that can sustain the kind of collaboration DevOps requires.\n\nAt some large organizations, IT/Admin functions are split into small atomic units like security, databases, networking, storage, procurement, cloud, and more. This is useful for keeping the scope-of-work manageable for the people in that group, and often results in deep technical expertise. But it also can be slow to get anything done because you’ll need to bring people together from disparate teams.\nSome organizations have chosen to outsource their IT/Admin functions. While the individuals in those outsourced teams are often quite competent, building relationships can be difficult. Outsourced IT/Admin teams are often in India, so it can be hard to find meeting times with American and European teams. Additionally, turnover on projects and systems tends to be high at outsourced IT/Admin organizations. That means that institutional knowledge is thin and relationships can’t be relied on long term.\nSome organizations, especially small or new ones, don’t have an IT/Admin function. At others, the IT/Admins are preoccupied with other tasks and don’t have the capacity to help the data science team. This isn’t a tragedy, but it probably means you’re going to have to become the IT/Admin if you want to get anything done.\n\nWhether your organization has an IT/Admin setup that facilitates DevOps best practices or not, hopefully this book can help you take the first steps towards making your path to production smoother and simpler."
  },
  {
    "objectID": "chapters/intro.html#about-this-book",
    "href": "chapters/intro.html#about-this-book",
    "title": "Introduction",
    "section": "About this book",
    "text": "About this book\nOver the course of engaging with many organizations, I’ve seen which patterns work to grease the path to production for data scientists and which ones tend to impede it.\nMy goal is that this book helps you create data science projects that are easier and simpler to deploy, and that you have the knowledge and skills to get them into production when it’s time.\nTo that end this book is divided into three sections.\nSection 1 is about applying DevOps best practices to a data science context. There’s a lot data scientists can learn from DevOps, but there are important differences between data science and general purpose software engineering that you’ll learn about.\nSection 2 is a walk through of basic concepts in IT Administration that will get you to the point of being able to host and manage your own small server. If you are a hobbyist or have only a small data science team, this might make you able to operate without any IT/Admin support. Even if you do work at an organization with significant IT/Admin support, it will equip you with the vocabulary to talk to the IT/Admins at your organization and some basic skills of how to do IT/Admin tasks yourself.\nSection 3 is about how all of what you learned in Section 2 changes when you go to enterprise scale. If section 2 explains how to do IT/Admin tasks yourself, section 3 is my attempt to explain why you shouldn’t.\n\nComprehension Questions\nEach chapter in this book includes comprehension questions. As you get to the end of the chapter, take a moment to consider these questions. If you feel comfortable answering them, you’ve probably understood the content of the chapter pretty well.\nAlternatively, feel free to jump ahead to them as you’re reading the chapter. If you can already answer them all, you can probably skip that chapter.\n\n\n\n\n\n\nMental Models + Mental Maps\n\n\n\nThroughout the book, I’ll talk a lot about building a mental model of different concepts.\nA mental map is a way to represent mental models.\nIn a mental map, you draw each of the nouns as nodes and connect them with arrows that are labelled to explain the relationship.\nMental maps are a great way to test your mental models, so I’ll suggest them as comprehension questions in many chapters.\nHere’s an example about this book:\n\nNote how every node is a noun and the edges (labels on the arrows) are verbs. It’s pretty simple! But writing down the relationships between entities like this is a great check on understanding.\n\n\n\n\nLabs\nMany chapters also contain labs. The idea of these labs is to give you hands-on experience with the concepts at hand.\nThese labs all tie together. If you follow the labs in this book, you’ll build up a reasonably complete data science platform that includes a place for you to work, a way to store data, and a deployment environment.\nPalmer Penguins is a public dataset meant for demonstrating data exploration and visualization. We’re going to pretend we care deeply about the relationship between penguin bill length and mass and we’re going to build up an entire data science environment dedicated to exploring that relationship.\nThe front end of this environment is going to be a website that contains an app that allows you to get predictions from a machine learning model of a penguin’s mass based on bill length and other features. We’re also going to include pages dedicated to exploratory data analysis and model building on the website.\nOn the backend, we’re going to build a data science workbench on an AWS EC2 instance where we can do this work. It will include RStudio Server and JupyterHub for working. It will additionally host the machine learning model as an API and the Shiny app that appears on the website.\nThe whole thing will get auto-deployed from a git repo using GitHub Actions.\nFrom an architectural perspective, it’ll look something like this:\n\nIf you’re interested in exactly which pieces get completed in each chapter, check out Appendix C.\n\n\nConventions\nThroughout the book, I will italicize terms of art the first time I introduce them as well as the names of other publications. Because so many of the technical terms in this book are usually referred to by abbreviations or acronyms, I’ll use the abbreviation or acronym in the text and include the full term in parentheses the first time it’s mentioned.\nBolding will be reserved for emphasis.\nR, Python, and system package names will be in code font and will have braces around them like {package}. Networking concepts and terms, including URLs, will appear in \\(\\text{equation font}\\).\nVariables that you would replace with your own values will appear in code font inside angled brackets like &lt;your-variable&gt;."
  },
  {
    "objectID": "chapters/intro.html#footnotes",
    "href": "chapters/intro.html#footnotes",
    "title": "Introduction",
    "section": "",
    "text": "I think a lot of DevOps experts would argue that you’re doing DevOps wrong if you have a standalone DevOps team.↩︎"
  },
  {
    "objectID": "chapters/sec1/1-0-sec-intro.html#labs-in-this-section",
    "href": "chapters/sec1/1-0-sec-intro.html#labs-in-this-section",
    "title": "DevOps Lessons for Data Science",
    "section": "Labs in this section",
    "text": "Labs in this section\nEach chapter in this section has a lab so you can get hands-on experience implementing DevOps best practices in your data science projects.\nIn the labs, you’ll stand up a website to explore the Palmer Penguins dataset, especially the relationship between penguin bill length and mass. Your website will include pages on exploratory data analysis and model building. This website will automatically build and deploy based on changes in a git repo.\nYou’ll also create a Shiny app that visualizes model predictions and an API that hosts the model and provides real-time predictions to the app. Additionally, you’ll get to practice putting that API inside a Docker Container to see how using Docker can make your life easier when moving code around.\nFor more details on exactly what you’ll do in each chapter, see Appendix C."
  },
  {
    "objectID": "chapters/sec1/1-0-sec-intro.html#footnotes",
    "href": "chapters/sec1/1-0-sec-intro.html#footnotes",
    "title": "DevOps Lessons for Data Science",
    "section": "",
    "text": "If you enjoy this introduction, I strongly recommend The Phoenix Project by Gene Kim, Kevin Behr, and George Spafford. It’s a novel about implementing DevOps principles. A good friend described it as, “a trashy romance novel about DevOps”. It’s a very fun read.↩︎"
  },
  {
    "objectID": "chapters/sec1/1-1-env-as-code.html#environments-have-layers",
    "href": "chapters/sec1/1-1-env-as-code.html#environments-have-layers",
    "title": "1  Environments as Code",
    "section": "1.1 Environments have layers",
    "text": "1.1 Environments have layers\nData science environments have three distinct layers. Reasoning clearly about these layers can reveal your actual reproducibility needs, and which environmental layers you need to target putting into code.\nAt the bottom of the environment is the hardware layer. This is the physical and virtual hardware where the code runs. For example, this might be your laptop or a virtual server from a cloud provider. Above that is the system layer, which includes the operating system, important system libraries, and Python and/or R. And above that is the package layer, where your Python and R packages live.\nLayers of data science environments\n\n\n\n\n\n\n\nLayer\nContents\n\n\n\n\nPackages\nPython + R Packages\n\n\nSystem\nPython + R Language Versions\nOther System Libraries\nOperating System\n\n\nHardware\nVirtual Hardware\nPhysical Hardware\n\n\n\nIn an ideal world, the hardware and system layers should be the responsibility of an IT/Admin. It may be the case that you’re responsible for them as well, but then you’re just fulfilling that role.\nAs a data scientist, you can and should be responsible for the package layer, and getting this layer right is where the biggest reproducibility bang for your buck lies. If you do find yourself managing the system or hardware layer, chapters Chapter 7 through Chapter 14 will teach you all about how to manage those layers."
  },
  {
    "objectID": "chapters/sec1/1-1-env-as-code.html#the-package-layer",
    "href": "chapters/sec1/1-1-env-as-code.html#the-package-layer",
    "title": "1  Environments as Code",
    "section": "1.2 The package layer",
    "text": "1.2 The package layer\nThere are three different places packages can live.\n\nIn a repository. You’re used to installing packages from a repositories like PyPI, Conda, CRAN, or BioConductor. These repositories are like the grocery store. The food is packaged up and ready to go, but inert. There are also many varieties there. Repositories hold both current and archival versions of each package.1\nIn a library. Once you install the packages you need with install.packages() or pip install or conda install, they’re in your library, which is the data science equivalent of a pantry. Libraries can hold – at most – one version of any given package. Libraries can be specific to the project, user, or shared across the system.\nLoaded. Loading a package with a library or import command is like taking the food out of the pantry and putting it on the counter so you can actually cook with it.\n\nAs a data scientist, the atomic unit of package reproducibility is in the middle – the library.\nLet’s say you work on one project for a while, installing packages from the repository into your library. You go away for a year of working on other projects or try to share your project with someone else. When you come back, it’s likely that future you or your colleague you won’t have the right versions and your code will break.\nWhat would’ve been better is if you’d had an environment as code strategy that created a portable environment for each project on your system.\nA successful package environment as code setup has two key attributes:\n\nYour package environment is isolated and cannot be disrupted by other activities on the same machine.\nYour package environment can easily be captured and transported elsewhere.\n\nIn Python, there are many different options for virtual environment tooling. In the context of production data science, I recommend {virtualenv}/{venv} and related tools.\nIn R, there’s really only one game in town; the {renv} package.\n\n\n\n\n\n\nA note on Conda\n\n\n\nConda allows you to create a virtual environment in user space on your laptop without having admin access. It’s especially useful when your machine is locked down by IT.\nThat’s not a great fit for a production environment. Conda smashes together the language version, the package management, and, sometimes, the system library management. This is conceptually simple and easy-to-use, but it often goes awry in production environments. In a production environment (or a shared workbench server) I recommend people manage Python packages with a virtual environment tool like {venv} and manage system libraries and versions of Python with tools built for those purposes."
  },
  {
    "objectID": "chapters/sec1/1-1-env-as-code.html#using-a-virtual-environment",
    "href": "chapters/sec1/1-1-env-as-code.html#using-a-virtual-environment",
    "title": "1  Environments as Code",
    "section": "1.3 Using a virtual environment",
    "text": "1.3 Using a virtual environment\nUsing a virtual environment tool is a three-step process.\nAt a high level, you’ll create and use standalone package libraries, use tooling to capture the state of that package environment, and restore that state wherever else you might need the environment.\n\n\n\n\n\n\nNote\n\n\n\nSee the cheatsheet in Appendix D for the exact commands for both R and Python.\n\n\nStep 1: Create standalone package libraries\nEach project should have it’s own {renv}/{venv} library. When you start your project, it should be in a standalone directory that includes everything the project needs – including a virtual environment.\nThis is called a project-oriented workflow. You can do it in either R or Python. The What They Forgot to Teach You About R course (materials available online at rstats.wtf) is a great intro to a project-oriented workflow whether you work in R or Python. The tooling will be somewhat different in Python, but the idea is the same.\n\n\n\n\n\n\nTip\n\n\n\nIf your project includes multiple content items (say an app, API, and ETL script), I recommend using one git repo for the whole project with each content item in its own directory with its own virtual environment.\n\n\nWhen you work on the project, you activate the virtual environment and install and use packages in there.\nStep 2: Document environment state\nThe way to make the environment portable is to document what’s in the package library. Both {renv} and {venv} have standard file formats for documenting the packages, as well as the versions, that are installed in the environment.\nIn {renv}, the file is called a lockfile and it’s a requirements.txt in {venv}.\nSince all this work is occurring in a standalone package environment, you don’t have to worry about what will happen if you come back after a break. You’ll still have those same packages to use.\nStep 3: Collaborate or deploy\nWhen you go to share your project, you don’t want to share your actual package libraries. Package installs are specific to the operating system and the language version you’re using, so you want your target system to install the package specifically for that system.\nFor example, if you’re working on a Mac and you collaborate or deploy to a Windows or Linux machine, you can’t share the actual package files. Those machines will need to install the required set of packages for themselves.\nAdditionally, package files can be large. Just sharing a file of requirements makes downloads and uploads much more manageable, especially if you’re using git. So the process is to check your lockfile or requirements.txt into git with your project.\n\n1.3.1 Step 4: Use a virtual environment\nThen, when your deployment target, collaborator, or future you downloads your project, it will restore the documented environment, again using tools from {renv}/{venv}."
  },
  {
    "objectID": "chapters/sec1/1-1-env-as-code.html#whats-happening-under-the-hood",
    "href": "chapters/sec1/1-1-env-as-code.html#whats-happening-under-the-hood",
    "title": "1  Environments as Code",
    "section": "1.4 What’s happening under the hood",
    "text": "1.4 What’s happening under the hood\nInstalled packages are stored in libraries, which are just directories on your system. Your Python or R session keeps track of the set of libraries it should use with sys.path in Python and .libPaths() in R.\nSo when you install a package, it installs into the first library allowed. And when you load a package with import or library, it searches the directories from sys.path or .libPaths() and returns the package when it finds it.\n\nEach library can contain, at most, one version of any package. So order matters for the directories in sys.path or .libPaths(). Whatever version is found first during the search will be returned.\n\n\n\n\n\n\nNote\n\n\n\nThis works the same for Python modules as for packages. I’m just using the term packages since most modules that aren’t purpose built for a project are in packages.\n\n\nIf you’re not in a virtual environment, the top libraries are user-level libraries by default. Activating a virtual environment puts project-level libraries at the top of the lists in sys.path or .libPaths() so package installs and loads happen from there.\nIn order to economize on space and install time, both {renv} and {venv} do something a little clever. The packages in your project-level library aren’t actually there. Instead, {renv} and {venv} keep user-level package caches of the actual packages and use symlinks so there’s actually only ever one copy of the package installed.\nSometimes, IT/Admins want to further save space by sharing package caches across users. This is usually a mistake. Sharing package caches leads to headaches over user file permissions to write to the package cache versus read. Storage space is cheap, way cheaper than your time. If you have to do it, both {renv} and venv include settings to allow you to relocate the package cache to a shared location on the server."
  },
  {
    "objectID": "chapters/sec1/1-1-env-as-code.html#a-note-on-conda-1",
    "href": "chapters/sec1/1-1-env-as-code.html#a-note-on-conda-1",
    "title": "1  Environments as Code",
    "section": "1.5 A note on Conda",
    "text": "1.5 A note on Conda\n\n\n\n\n\n\nWhy I’m not talking about Conda\n\n\n\nMany data scientists love Conda for managing their Python environments.\nConda allows you to create a virtual environment in user space on your laptop without having admin access. It’s especially useful when your machine is locked down by IT.\nThat’s super useful for working on your laptop, but it’s not a great fit for a production environment. Conda smashes together the language version, the package management, and, sometimes, the system library management. This has the benefit of being conceptually simple and easy-to-use. But I’ve often seen it go awry in production environments. In a production environment (or a shared workbench server) I recommend people use a tool that’s just for package management, like {venv}, as opposed to an all-in-one tool like Conda."
  },
  {
    "objectID": "chapters/sec1/1-1-env-as-code.html#comprehension-questions",
    "href": "chapters/sec1/1-1-env-as-code.html#comprehension-questions",
    "title": "1  Environments as Code",
    "section": "1.6 Comprehension Questions",
    "text": "1.6 Comprehension Questions\n\nWhy does difficulty increase as the level of required reproducibility increase for a data science project. In your day-to-day work, what’s the hardest reproducibility challenge?\nDraw a mental map of the relationships between the seven levels of the reproducibility stack. Pay particular attention to why the higher layers depend on the lower ones.\nWhat are the two key attributes of environments as code? Why do you need both of them? Are there cases where you might only care about one?\nDraw a mental map of the relationships between the following: package repository, package library, package, project-level-library, .libPaths() (R) or sys.path(python), lockfile\nWhy is it a bad idea to share package libraries? What’s the best way to collaborate with a colleague using an environment as code? What are the commands you’ll run in R or Python to save a package environment and restore it later?"
  },
  {
    "objectID": "chapters/sec1/1-1-env-as-code.html#lab1",
    "href": "chapters/sec1/1-1-env-as-code.html#lab1",
    "title": "1  Environments as Code",
    "section": "1.7 Lab 1: Create and use a virtual environment",
    "text": "1.7 Lab 1: Create and use a virtual environment\nIn this lab, we’re going to start working on our penguin explorer website. We’re going to create a simple website using Quarto, which is an open source scientific and technical publishing system that makes it easy to render R and Python code into beautiful documents, websites, reports, and presentations.\nWe’re going to create pages for a simple exploratory data analysis and model building from the Palmer Penguins dataset. In order to get to practice with both R and Python, I’m going to do the EDA page in R and the modeling in Python. By the end of this lab, we’ll have both pages created using standalone Python and R virtual environments.\nIf you’re just getting started, check out the Quarto website to start using Quarto in the editor of your choice.\n\n\n\n\n\n\nTip\n\n\n\nMake sure you add each page below to your _quarto.yml so Quarto knows to render them.\n\n\n\n1.7.1 EDA in R\nLet’s add a simple R-language EDA of the Palmer Penguins data set to our website by adding a file called eda.qmd in the root directory of the project.\nBefore you start adding code, create and activate an {renv} environment with renv::init().\nNow, go ahead and do your analysis. Here’s the contents of my eda.qmd.\n\n\neda.qmd\n\n---\ntitle: \"Penguins EDA\"\nformat:\n  html:\n    code-fold: true\n---\n\n## Penguin Size and Mass by Sex and Species\n\n```{r}\nlibrary(palmerpenguins)\nlibrary(dplyr)\nlibrary(ggplot2)\n\ndf &lt;- palmerpenguins::penguins\n```\n\n```{r}\ndf %&gt;%\n  group_by(species, sex) %&gt;%\n  summarise(\n    across(\n      where(is.numeric), \n      \\(x) mean(x, na.rm = TRUE)\n      )\n    ) %&gt;%\n  knitr::kable()\n```\n\n## Penguin Size vs Mass by Species\n\n```{r}\ndf %&gt;%\n  ggplot(aes(x = bill_length_mm, y = body_mass_g, color = species)) +\n  geom_point() + \n  geom_smooth(method = \"lm\")\n```\n\nFeel free to copy this Quarto doc right into your website or to write your own.\nOnce you’ve finished writing your EDA script and checked that it previews nicely into the website, save the doc, and create your lockfile with renv::snapshot().\n\n\n1.7.2 Modeling in Python\nNow let’s build a {scikit-learn} model for predicting penguin weight based on bill length in a Python notebook by adding a model.qmd to the root of our project.\nAgain, you’ll want to create your virtual environment and activate it before you start pip install-ing packages into the environment.\nHere’s what’s in my model.qmd, but you should feel free to include whatever you want.\n\n\nmodel.qmd\n\n---\ntitle: \"Model\"\nformat:\n  html:\n    code-fold: true\n---\n\n```{python}\nfrom palmerpenguins import penguins\nfrom pandas import get_dummies\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import preprocessing\n```\n\n## Get Data\n\n```{python}\ndf = penguins.load_penguins().dropna()\n\ndf.head(3)\n```\n\n## Define Model and Fit\n\n```{python}\nX = get_dummies(df[['bill_length_mm', 'species', 'sex']], drop_first = True)\ny = df['body_mass_g']\n\nmodel = LinearRegression().fit(X, y)\n```\n\n## Get some information\n\n```{python}\nprint(f\"R^2 {model.score(X,y)}\")\nprint(f\"Intercept {model.intercept_}\")\nprint(f\"Columns {X.columns}\")\nprint(f\"Coefficients {model.coef_}\")\n```\n\nOnce you’re happy with how the page is working, capture your dependencies in a requirements.txt using pip freeze &gt; requirements.txt on the command line."
  },
  {
    "objectID": "chapters/sec1/1-1-env-as-code.html#footnotes",
    "href": "chapters/sec1/1-1-env-as-code.html#footnotes",
    "title": "1  Environments as Code",
    "section": "",
    "text": "They don’t always include archival versions, but they usually do.↩︎"
  },
  {
    "objectID": "chapters/sec1/1-2-proj-arch.html#choose-the-right-type-of-presentation-layer",
    "href": "chapters/sec1/1-2-proj-arch.html#choose-the-right-type-of-presentation-layer",
    "title": "2  Data Project Architecture Guidelines",
    "section": "2.1 Choose the right type of presentation layer",
    "text": "2.1 Choose the right type of presentation layer\nThe presentation layer is the actual thing that will be consumed by your users. A lot of the data flows for your project will be dictated by your presentation layer, so you need to start by figuring out the details of your presentation layer.\nBasically all data science projects fall into one of four categories.\nThe first category is a job. A job matters because it changes something in another system. It might move data around, build a model, or produce plots, graphs, or numbers to be used in a Microsoft Office report.\nFrequently, jobs are written in a SQL-based pipelining tool (dbt has been quickly rising in popularity) or in a .R or .py script.1\nThe second type of data science software is an interactive app. These apps are created in frameworks like Shiny (R or Python), Dash (Python), or Streamlit (Python). In contrast to general purpose web apps, which are for all sorts of purposes, data science web apps are usually used to give non-coders a way to explore data sets and see data insights.\nThe third type is a report. Reports are code you’re turning into an output you care about – like a paper, book, presentation, or website. Reports are the result of rendering an R Markdown doc, Quarto doc, or Jupyter Notebook for people to consume on their computer, in print, or in a presentation. These docs may be completely static (this book is a Quarto doc) or they may have some interactive elements.\n\n\n\n\n\n\nNote\n\n\n\nExactly how much interactivity turns a report into an app is completely subjective. I generally think the distinction is whether there’s a running R or Python process in the background, but it’s not a particularly sharp line.\n\n\nThe fourth type is an API (application programming interface) for machine-to-machine communication. In the general purpose software world, APIs are the backbone of how two distinct pieces of software communicate. In the data science world, APIs are most often used to provide data feeds and on-demand predictions from machine learning models.\nChoosing the right type of presentation layer will make it much easier to design the rest of your project. Here are some guidelines on how to choose a presentation layer.\nIf the results of your software are for machine-to-machine use, you’re thinking about a job or API. It’s a job if it should run in a batched way (i.e. you write a data file or results into a database) and it’s an an API if you want results as queried in real time.\nIf your project is for humans to consume, you’re thinking about creating an app or report, depending on whether you need a live Python or R process on the back-end.\nHere’s a little flow chart for how I think about which of the four things you should build.\n\n\n\n\nflowchart TD\n    A{For human\\n consumption?}\n    B{Static?} \n    C{Lots of data\\n on backend?}\n    D{Should run\\batched?}\n\n    E[\"Report\\n(Static)\"]\n    F[App] \n    G[\"Report\\n(Interactive)\"]\n    H[API]\n    J[Job]\n\n\n    A --&gt;|Yes| B\n    A --&gt;|No| D\n    B --&gt;|Yes| E\n    B --&gt;|No| C\n    C --&gt;|Yes| F\n    C --&gt;|No| G\n    D --&gt;|Yes| H\n    D --&gt;|No| J"
  },
  {
    "objectID": "chapters/sec1/1-2-proj-arch.html#do-less-in-the-presentation-layer",
    "href": "chapters/sec1/1-2-proj-arch.html#do-less-in-the-presentation-layer",
    "title": "2  Data Project Architecture Guidelines",
    "section": "2.2 Do less in the presentation layer",
    "text": "2.2 Do less in the presentation layer\nAs a general rule, data scientists don’t do a great job separating out their presentation layers. It’s not uncommon for me to see apps or reports that are thousands of lines of code, with button definitions, UI bits, and user interaction definitions mixed in among the actual work of the app.\nWith presentation and processing layers that are smushed together, it’s really hard to read your code later or to test or log what’s happening inside your app.\nThe best way to separate the presentation layer is to check if you’ve got anything in your presentation layer that does anything beyond\n\nshowing things to the user\ncollecting interactions from the user\n\nCreating the things that are shown to the user or doing anything with the interactions shouldn’t be in the presentation layer. These should be deferred to the processing layer.\nOnce you’ve identified those things, they should be extracted into functions that are documented and tested – preferably in a package – and use those functions put into standalone scripts.\n\n\n\n\n\n\nTip\n\n\n\nMoving things out of the presentation layer is especially important if you’re writing a Shiny app. You really want to use the presentation layer to do reactive things and move all non-reactive interactions into the processing layer."
  },
  {
    "objectID": "chapters/sec1/1-2-proj-arch.html#aim-for-small-data-in-the-presentation-layer",
    "href": "chapters/sec1/1-2-proj-arch.html#aim-for-small-data-in-the-presentation-layer",
    "title": "2  Data Project Architecture Guidelines",
    "section": "2.3 Aim for small data in the presentation layer",
    "text": "2.3 Aim for small data in the presentation layer\nEverything is easy when your data is small because you can simply load it into your Python or R session as your code starts and never think about it again.\n“Real engineers” may scoff at this pattern, but don’t let their criticism dissuade you. If your data size is small and your project performance is good enough, just read in all of your data and operate on it live. Don’t over-complicate things. These days, this pattern often works well into the range of millions of rows.\nIt may be the case that your data isn’t small – but not all large data is created equal.\nTruly big data can’t fit into the memory on your computer all at once. Data that is actually big is pretty rare for most data science purposes.\nIt’s much more common to encounter medium data. You can load it into memory so it’s not actually big, but it’s big enough that loading it all makes your project’s performance too slow.\nDealing with medium or big data requires being somewhat clever and adopting a design pattern for big data. But being clever is hard.\nSo before you go being clever, it’s worth slowing down and asking yourself a few questions that might let you treat your data as small.\n\n2.3.1 Can I add pre-calculation or use a different project type?\nIf your data is truly big, it’s big. You could always get beefier hardware, but there are limits. But if your data is medium-sized, the thing keeping it from being small isn’t some esoteric hardware issue, its performance.\nAn app requires high performance. Someone is staring at their screen through a 90 second wait is going to think your project stinks.\nBut if you can pre-calculate a lookup table of values – or turn your app into a report that gets re-rendered on a schedule you can turn turn medium or even truly big data into a small data set in the presentation layer.\nThe degree to which you can do this depends a lot on the requirements of your presentation layer.\nTalking to your users and figuring out what cuts of the data they really care about can help you determine whether pre-calculation is feasible or whether you really need to load all the data into the presentation layer.\n\n\n2.3.2 Can I reduce data granularity?\nIf you can pre-calculate results and you’re still hitting performance issues, it’s always worth asking if your data can get smaller.\nLet’s think about a specific project to make this a little clearer.\nSuppose you work for a large retailer and are responsible for creating a dashboard of weekly sales. Your input data is a dataset of every item sold at every store going back for years. Clearly this isn’t naturally small data.\nAs you’re thinking about how to make the presentation layer data smaller, it’s worth keeping in mind that each additional dimension you allow users to cut the data multiplies the amount of data you need in the presentation layer.\nFor example, weekly sales at the department level, requires a lookup table as big as \\(\\text{number of weeks} * \\text{number of stores} * \\text{number of departments}\\). Even with a lot of stores and a lot of departments, you’re probably still squarely in the small data category.\nBut if you have to switch to a daily view, you multiply the amount of data you need by 7. If you break it out across 12 products, your data has to get 12 times bigger. And if you do both, it gets 84 times bigger. It’s not long before you’re back to a big data problem.\nTalking with your users about the tradeoffs between app performance and the number of data dimensions they need can identify opportunities to exclude dimensions and reduce your data size."
  },
  {
    "objectID": "chapters/sec1/1-2-proj-arch.html#big-data-patterns",
    "href": "chapters/sec1/1-2-proj-arch.html#big-data-patterns",
    "title": "2  Data Project Architecture Guidelines",
    "section": "2.4 Adopt a pattern to make big data small",
    "text": "2.4 Adopt a pattern to make big data small\nLet’s say you’ve made your presentation layer as small as possible or you’re trying to do your pre-calculation step to go from big data to small data. You need to figure out how to make your large data smaller.\nThe key insight is that you don’t want to pull all of the data into your Python or R session. Instead, you want to pull in only some of the data.\nHere are a few patterns to consider to make your data smaller. This isn’t an exhaustive list and each of these patterns will only work for some projects, but many can adopt one or more of these patterns.\n\n2.4.1 Be lazy with data pulls\nUp until now, we’ve been assuming that your project pulls in all of the data up front in an eager data pattern. This is often a good first cut at writing an app, as it’s much simpler than doing anything else.\nIf that won’t work for your project, you can try being lazy with your data pulls. In a lazy data pattern, you pull in only the data that’s needed when it’s needed.\nIf your project doesn’t always need all the data – especially if the data it needs depends on what the user does inside a session, it might be worthwhile to pull only exactly the data you need once the user interactions clarify what you need.\n\n\n2.4.2 Sample the data\nFor many tasks, especially machine learning ones, it may be adequate to work on only a sample of the data. In some cases like classification of highly imbalanced classes, it may actually work better to work on a sample of the data rather than the whole data set.\nSampling tends to work well when you’re trying to compute statistical attributes of your datasets. Computing averages or rates and creating machine learning models works just fine on samples of your data. Just be careful to be unbiased with your sampling and consider sampling stratification to make sure one weird sample doesn’t mess with your results.\nBut sampling doesn’t work well on counting tasks – it’s hard to count when you don’t have all the data!\n\n\n2.4.3 Chunk and pull\nIn some cases, there may be natural groups in your data. For example, in our retail dashboard example, it may be the case that we want to compute something by time frame or store or product. In this case, you could pull just that chunk of the data, compute what you need and move on to the next one.\nChunking works well for all kinds of tasks including building machine learning models and creating plots as long as the groups are cleanly separable. When they are, this is an example of an embarrassingly parallel task, which you can easily parallelize in Python or R.\nIf you don’t have distinct chunks in your data, it’s pretty hard to chunk the data.\n\n\n2.4.4 Push work to the data source\nIn most cases, actually transmitting the data from the data source to your project is the most costly step in terms of time. So basically anything you can do before you pull the data out should be done before you pull the data out.\nLet’s say you really have to provide a very high degree of granularity in your weekly sales dashboard. You can at least do any computations them in the data source and just pull the results back, as opposed to loading all the data and doing the computations in Python or R. More on how to do this in Chapter 3.\nThis tends to work quite well when you’re creating simple summary statistics and when your database is reasonably fast. If your data source is slow, or if you’re doing complicated machine learning tasks, you may not be able to push that work off to the data source."
  },
  {
    "objectID": "chapters/sec1/1-2-proj-arch.html#store-intermediate-artifacts-in-the-right-format",
    "href": "chapters/sec1/1-2-proj-arch.html#store-intermediate-artifacts-in-the-right-format",
    "title": "2  Data Project Architecture Guidelines",
    "section": "2.5 Store intermediate artifacts in the right format",
    "text": "2.5 Store intermediate artifacts in the right format\nAs you start breaking your processing layer into the different pieces, you’ll find that you have intermediate artifacts to pass the data from one stage to the next.\nIf all you’re producing is rectangular data frames (or vectors) and you have write access to a database, that’s what you should use.\nBut very often you don’t have write access to a database or you’ve got other sorts of artifacts that you need to save between steps and can’t go into a database, like machine learning models or rendered plots. In that case, you’ll need to choose how to store your data.\nFlat files are data files that can be moved around just like any other file on your computer. You can put them on your computer, and share them through tools like dropbox, google drive, scp, or more.\nThe most common is a comma separated value (csv) file, which is just a literal text file of the values in your data with commas as separators.2 You could open it in a text editor and read it if you wanted to.\nThe advantage of csvs is that they’re completely ubiquitous. Basically every programming language has some way to read in a csv file and work with it.\nOn the downside, csvs are completely uncompressed. That makes them quite large relative to other sorts of files and slow to read and write. Additionally, because csvs aren’t language-specific, complicated data types may not be preserved when saving to csv. For example, dates are often mangled going into a csv file and back.\nThey also can only hold rectangular data, so if you’re trying to save a machine learning model, a csv doesn’t make any sense.\nBoth R and Python have language-specific file types – pickle in Python and rds in R. These are nice because they include some amount of compression and preserve data types when you save a data frame. They also can hold non-rectangular data, which can be great if you want to save a machine learning model.\nIf you don’t have a database but are storing rectangular data, you should strongly consider using DuckDB. Its an in-memory database that’s great for analytics use cases. In contrast to a standard database that runs its own live process, there’s no overhead for setting up DuckDB. You just run it against flat files on disk (usually Parquet files), which you can move around like any other. And unlike a csv, pickle, or rds file, a DuckDB is query-able, so you only load the data you need into memory.\nIt’s hard to stress how cool DuckDB is. Data sets that were big just a few years ago are now medium or even small."
  },
  {
    "objectID": "chapters/sec1/1-2-proj-arch.html#choose-where-based-on-update-frequency",
    "href": "chapters/sec1/1-2-proj-arch.html#choose-where-based-on-update-frequency",
    "title": "2  Data Project Architecture Guidelines",
    "section": "2.6 Choose where based on update frequency",
    "text": "2.6 Choose where based on update frequency\nLet’s say you’ve done your data pre-calculation and have a data set you’re using for the presentation layer. You have to figure out where to keep it.\nWhere you store your data should be dictated by how often the data is updated.\nThe simplest answer is to put it in the presentation bundle, which is the code and assets that make up your presentation layer. For example, let’s say you’re building a simple Dash app, app.py.\nYou could create a project structure like this:\nmy-project/\n├─ app.py\n├─ data/\n│  ├─ my_data.csv\n│  ├─ my_model.pkl\nThis works well only if your data will be updated at the same cadence as the app or report itself. If your project is an annual report that will be rewritten when you update the data, this can work just great.\nBut if your data updates more frequently than your project code, you really want to put the data outside the project bundle.\nThere are a few ways you can do this. The most basic way is just to put the data on a location in your file system that isn’t inside the app bundle.\nBut when it comes to deployment, data on the file system can be complicated. If you’re writing your app and deploying it on the same server, then you can access the same directory. If not, you’ll need to worry about how to make sure that directory is also accessible on the server where you’re deploying your project.\nIf you’re not going to store the flat file on the filesystem and you’re in the cloud, the most common option for where it can go is in blob storage. Blob storage allows you to store and recall things by name.3 Each of the major cloud providers has blob storage – AWS’s has s3 (short for simple storage service), Azure has Azure Blob Store, and Google has Google Storage.\nThe nice thing about blob storage is that it can be accessed from anywhere that has access to the cloud. You can also control access using standard cloud identity management tooling, so you could control who has access using individual credentials or could just say that any request for a blob coming from a particular server would be valid.\nThere are packages in both R and Python for interacting with AWS that are very commonly used for getting access to s3 – {boto3} in Python, and {paws} in R.\nThere’s also the popular {pins} package in both R and Python that basically wraps using blob storage into neater code. It can use a variety of storage backends, including cloud blob storage, networked or cloud drives like Dropbox, Microsoft365 sites, and Posit Connect.\nIf you’re still early in your project lifecycle, a google sheet can be a great way to save and recall a flat file. I wouldn’t recommend a google sheet as a permanent home for data, but it can be a good intermediate step while you’re still figuring out what the right answer is for your pipeline.\nThe primary weakness of a google sheet – that it’s editable by someone who logs in – can also be an asset if that’s something you need."
  },
  {
    "objectID": "chapters/sec1/1-2-proj-arch.html#consider-auth-to-data-up-front",
    "href": "chapters/sec1/1-2-proj-arch.html#consider-auth-to-data-up-front",
    "title": "2  Data Project Architecture Guidelines",
    "section": "2.7 Consider auth to data up front",
    "text": "2.7 Consider auth to data up front\nIf everyone who views your project has the same permissions to see the data, life is easy. You can just allow the project access to the data and check for authorization to view the project.\nBut if you need to provide different data access to different users, you’re much more constrained. First off, you probably need to use an app rather than a report so that you can respond to which user is accessing the app.\nThen you have to figure out how you’re actually going to change data access based on who’s viewing the app.\nSometimes this can be accomplished in the app itself. Many app frameworks pass the username or user groups into the session, and you can write code that changes app behavior based on the user. For example, you can gate access to certain tabs or features of your app based on the user.\nSometimes you’ll actually have to pass database credentials along to the database. If this is the case for you, you’ll need to figure out how to establish the user’s database credentials, how to make sure those credentials stay only in the user’s session, and how those credentials get passed along to the database. This is most commonly done with technologies like Kerberos or OAuth and require coordination with an IT/Admin. More on this topic in Chapter 16."
  },
  {
    "objectID": "chapters/sec1/1-2-proj-arch.html#create-an-api-if-you-need-it",
    "href": "chapters/sec1/1-2-proj-arch.html#create-an-api-if-you-need-it",
    "title": "2  Data Project Architecture Guidelines",
    "section": "2.8 Create an API if you need it",
    "text": "2.8 Create an API if you need it\nIn the case of a general purpose three-layer app, it is almost always the case that the middle tier will be an application programming interface (API). In a data science app, separating processing logic into functions is often sufficient. But if you’ve got a long-running bit of business logic, like training an ML model, it’s often helpful to separate it into an API.\n\n\n\n\n\n\nNote\n\n\n\nYou may have heard the term REST API or REST-ful.\nREST is a set of architectural standards for how to build an API. An API that conforms to those standards is called REST-ful or a REST API.\nIf you’re using standard methods for constructing an API like R’s {plumber} package or {FastAPI} in Python, they’re going to be REST-ful – or at least close enough for standard usage.\n\n\nYou can basically think of an API as a “function as a service”. That is, an API is just one or more functions, but instead of being called within the same process that your app is running or your report is processing, it will run in a completely separate process.\nFor example, let’s say you’ve got an app that allows users to feed in input data and then generate a model based on that data. If you generate the model inside the app, the user will have the experience of pressing the button to generate the model and having the app seize up on them while they’re waiting. Moreover, other users of the app will find themselves affected by this behavior.\nIf, instead, the button in the app ships the long-running process to a separate API, it gives you the ability to think about scaling out the presentation layer separate from the business layer.\nLuckily, if you’ve written functions for your app, turning them into an API is trivial as packages like {fastAPI} and {plumber} let you turn a function into an API with just the addition of some specially-formatted comments."
  },
  {
    "objectID": "chapters/sec1/1-2-proj-arch.html#write-a-data-flow-chart",
    "href": "chapters/sec1/1-2-proj-arch.html#write-a-data-flow-chart",
    "title": "2  Data Project Architecture Guidelines",
    "section": "2.9 Write a data flow chart",
    "text": "2.9 Write a data flow chart\nOnce you’ve figured out the project architecture you need, it can be helpful to write a data flow chart.\nA data flow chart maps the different project components you’ve got into the three parts of the project and documents all the intermediate artifacts you’re creating along the way.\nOnce you’ve mapped your project, figuring out where the data should live and in what format will be much simpler.\nFor example, here’s a very simple data flow chart for the labs in this book. You may want to annotate your data flow charts with other attributes like data types, update frequencies, and where data objects live.\n\n\n\n\nflowchart LR\n    A[Palmer Penguins \\nData Package]\n    B[Model Creation Job] \n    C[Model Serving API]\n    D[Model Explorerer App]\n    E[EDA Report]\n    F[Model Creation Report]\n\n    subgraph Data\n        A\n    end\n\n    subgraph Processing\n        B --&gt;|Model| C\n    end\n\n    subgraph Presentation\n        D\n        E\n        F\n    end\n\n    A --&gt; B\n    B --&gt; F\n    A --&gt; E\n    C --&gt; D"
  },
  {
    "objectID": "chapters/sec1/1-2-proj-arch.html#comprehension-questions",
    "href": "chapters/sec1/1-2-proj-arch.html#comprehension-questions",
    "title": "2  Data Project Architecture Guidelines",
    "section": "2.10 Comprehension Questions",
    "text": "2.10 Comprehension Questions\n\nWhat are the layers of a three-layer application architecture? What libraries could you use to implement a three-layer architecture in R or Python?\nWhat are some questions you should explore to reduce the data requirements for your project?\nWhat are some patterns you can use to make big data smaller?\nWhere can you put intermediate artifacts in a data science project?\nWhat does it mean to “take data out of the bundle”?"
  },
  {
    "objectID": "chapters/sec1/1-2-proj-arch.html#lab2",
    "href": "chapters/sec1/1-2-proj-arch.html#lab2",
    "title": "2  Data Project Architecture Guidelines",
    "section": "2.11 Lab 2: Build the processing layer",
    "text": "2.11 Lab 2: Build the processing layer\nIn the last chapter, we did some EDA of the Palmer Penguins data set and also built an ML model. In this lab, we’re going to take that work we did and turn it into the actual presentation layer for our project.\n\n2.11.1 Step 1: Write the model outside the bundle using {vetiver}\nWhen we originally wrote our model.qmd script in Chapter 1, we didn’t save the model at all.\nIt seems likely that our model will get updated more frequently than our app, so we don’t want to store it in the app bundle. Later on, I’ll show you how to store it in the cloud. For now, I’m just going to store it in a directory on my computer.\nSince\nTo do it, I’m going to use the {vetiver} package, which is an R and Python package to version, deploy, and monitor a machine learning model.\nWe can take our existing model, turn it into a {vetiver} model, and save it to the /data/model folder with\n\n\nmodel.qmd\n\n\n```{python}\nfrom vetiver import VetiverModel\nv = VetiverModel(model, model_name='penguin_model', prototype_data=X)\n```\n\n## Save to Board\n\nIf /data/model doesn’t exist on your machine, you can create it, or use a directory that does exist.\nWhatever path you use, I’d recommend using an absolute file path, rather than a relative one.\n\n\n2.11.2 Step 2: Create an API to serve model predictions\nI’m going to say that I need real-time predictions from my model in this case, so I’ll serve the model from an API.\nAs the point of this lab is to focus on the architecture, I’m just going to use the auto-generation capabilities of {vetiver}. If you’re interested in getting better at writing APIs in general, I encourage you to consult the documentation for {plumber} or {fastAPI}.\nIf you’ve closed your modeling code, you can get your model back from your pin with:\n\nb = pins.board_folder('data/model', allow_pickle_read=True)\nv = VetiverModel.from_pin(b, 'penguin_model')\n\nThen you can auto-generate a {fastAPI} from this model with\n\napp = VetiverAPI(v, check_prototype=True)\n\nYou can run this in your Python session with app.run(port = 8080). You can then access run your model API by navigating to http://localhost:8080 in your browser.\nYou can play around with the front end there, including trying the provided examples."
  },
  {
    "objectID": "chapters/sec1/1-2-proj-arch.html#footnotes",
    "href": "chapters/sec1/1-2-proj-arch.html#footnotes",
    "title": "2  Data Project Architecture Guidelines",
    "section": "",
    "text": "Though I’ll argue in Chapter 4 that you should always use a literate programming tool like Quarto, R Markdown, or Jupyter Notebook.↩︎\nThere are other delimitors you can use. Tab separated value files (tsv) are something you’ll see occasionally.↩︎\nThe term blob is great to describe the thing you’re saving in blob storage, but it’s actually an abbreviation for binary large object. I think that’s very clever.↩︎"
  },
  {
    "objectID": "chapters/sec1/1-3-data-access.html#accessing-and-using-databases",
    "href": "chapters/sec1/1-3-data-access.html#accessing-and-using-databases",
    "title": "3  Using databases and data APIs",
    "section": "3.1 Accessing and using databases",
    "text": "3.1 Accessing and using databases\nDatabases are defined by their query-able interface, usually through structured query language (SQL).\n\n\n\n\n\n\nNote\n\n\n\nThere are many, many different kinds of databases, and choosing the right one for your project is beyond the scope of this book. One recommendation: open source PostgreSQL (Postgres) is a great place to start for most general-purpose data science tasks.\n\n\nRegardless of which database you’re using, you’ll open a connection by creating a connection object at the outset of your code. You’ll then use this object to send SQL queries – either literal ones you’ve written, or the output of a package that generates SQL code, like {sqlalchemy} in Python and {dplyr} in R.\nFor example, in Python you might write the following to connect to a Postgres database:\n\nimport psychopg2\n\ncon = psycopg2.connect()\n\nIn R, it might look like this:\n\ncon &lt;- DBI::dbConnect(RPostgres::postgres())\n\nIn order to develop a mental model for working with databases, let’s reverse engineer this example code.\nPython or R both have standard connection APIs that define operations like connecting and disconnecting, sending queries, and retrieving results.\nIn Python, packages for individual databases like {psychopg2} directly implement the API, which is why the example above calls the connect() method of the {psychopg2} package.\nIn R, the API is split into two parts. The {DBI} package (short for database interface) implements the actual connections. It works with a database driver package, whtich is the first argument to DBI::dbConnect(). Packages that implement the {DBI} interface are called DBI-compliant.\n\n\n\n\n\n\nNote\n\n\n\nThere are Python packages that don’t implement the connections API, and there are non DBI-compliant database connector packages in R. These packages may work for you, but I’d recommend sticking with the standard route.\n\n\nIn a lot of cases, there will be a Python or R package that directly implements your database driver. For example, when you’re connecting to a Postgres database, there are Postgres-specific connectors – {psychopg2} in Python and {RPostgres} in R. For Spark, you’ve got {pyspark} and {sparklyr}.\nIf there’s a package specific to your database, it’s probably faster and may provide additional database-specific functionality than other options.\nIf there isn’t a database-specific package that directly implements the driver, you’ll need to use a generic system driver in concert with a Python or R package that can interface with system drivers.\nIn that case, the example code above might look like in Python\n\nimport pyodbc\n\ncon = pyodbc.connect(\"DSN=MY_DSN\")\n\nIn R, it might look like this:\n\ncon &lt;- DBI::dbConnect(odbc::odbc(), dsn = \"MY_DSN\")\n\nWhile performance sometimes isn’t as good for system drivers, the tradeoff is that IT/Admins can pre-configure details of the connection in a data source name (DSN). If one is pre configured for you, you don’t have to remember – or even learn – the database name, host, and port, even username and password if they’re shared. All you need in your code is the DSN name, which is MY_DSN in the example above.\nSystem drivers come in two main varieties Java Database Connectivity (JDBC) and Open Database Connectivity (ODBC).\nIn Python, {pyodbc} is the main package for using ODBC connections and {JayDeBeApi} for connecting using JDBC. In R, {odbc} is the best package for using system ODBC connections and {RJDBC} is the standard way to use JDBC.\n\n\n\n\n\n\nTip\n\n\n\nIf you’re using R and have the choice between JDBC and ODBC, I strongly recommend ODBC. JDBC requires an extra hop through Java and the {rJava} package, which is painful to configure.1"
  },
  {
    "objectID": "chapters/sec1/1-3-data-access.html#auth-on-behalf-of-data-projects",
    "href": "chapters/sec1/1-3-data-access.html#auth-on-behalf-of-data-projects",
    "title": "3  Using databases and data APIs",
    "section": "3.2 Auth on behalf of data projects",
    "text": "3.2 Auth on behalf of data projects\nLet’s imagine you’ve created a data science project that pulls data from a database. When you’re actively working on the project, it’s easy for you to provide credentials as needed to the database. But what happens when you deploy that project to production and you’re not sitting there to provide credentials as that access happens?\nIn many organizations, you’ll be allowed to use your own data access permissions for the project and then to share the project with others in the company at your discretion. This situation is sometimes called discretionary access control (DAC).\nIn some more restrictive environments, you won’t have this luxury. The IT/Admin team may maintain control of permissions themselves or require that data access be more tightly governed.\nIn some cases, it will be acceptable to create or use a service account, which is a non-human account that exists to hold permissions for a project. You might want this to limit the project’s permissions to exactly what it needs and no more or to be able to manage the app permissions independently of the humans involved.\nIn the most restrictive case, you’ll actually have to use the credentials of the person who’s viewing the content and pass those along. This last option is hard.\n\nIf you have to use the viewer’s credentials for data access, you can write code to collect them from the viewer and pass them along. I don’t recommend this configuration as you have to take on a lot of responsibility for collecting those credentials and storing and using them responsibly.\nIn other cases, the project may be able to run as the viewer when it is accessing the database. The patterns for doing this are complicated and you should talk to your IT/Admin about how this might be able to get done.2"
  },
  {
    "objectID": "chapters/sec1/1-3-data-access.html#connecting-to-apis",
    "href": "chapters/sec1/1-3-data-access.html#connecting-to-apis",
    "title": "3  Using databases and data APIs",
    "section": "3.3 Connecting to APIs",
    "text": "3.3 Connecting to APIs\nSome data sources come in the form of an API (application programming interface).\nIn the data science world, APIs are most often used to provide data feeds and on-demand predictions from machine learning models.\nIt’s common to have Python or R packages that wrap APIs, so you just write Python or R code without needing to think about the API underneath. The usage of these patterns often looks similar to databases – you create and use a connection object that stores the connection details. If your API has a package like this, you should just use it.\nIf you’re consuming a private API at your organization, a helper package probably doesn’t exist or you may have to write it yourself.\n\n\n\n\n\n\nNote\n\n\n\nThere’s increasingly good tooling to auto-generate packages based on API documentation, so you may never have to write an API wrapper package by hand. It’s still helpful to understand how it works.\n\n\nIf find yourself having to call an API directly, you can use the {requests} package in Python or {httr2} in R.\nThese packages idiomatic R and Python ways to call APIs. It’s worth understanding that they’re purely syntactic sugar. There’s nothing special about calling an API from inside Python or R versus using the curl command on the command line and you can go back and forth as you please.\n\n3.3.1 What’s in an API?\nAPIs are the standard way for two computer systems to communicate with each other. It’s an extremely general term that describes the definition of machine-to-machine communication.\nThe APIs used by data scientists are usually http-based REST-ful APIs. What exactly that means isn’t really important, but the rest of this section just addresses the subset of the wild world of APIs you’re likely to encounter as a data scientist.\nhttp operates on a request-response model. So when you use an API, you send a request to the API and it sends a response back.\n\nThe best way to learn about a new API is to read the documentation, which will include a lot of details about how to use it. Let’s go through some of the most salient ones.\n\n\n3.3.2 API Endpoints and Paths\nEach request to an API is directed to a specific endpoint. An API can have many endpoints, each of which you can think of like a function in a package. Each endpoint lives on a path, which is where you find that particular endpoint.\nFor example, if you did the lab in Chapter 2 and used {vetiver} to create an API for serving the penguin mass model, you found your API at http://localhost:8080. By default, you went to the root path at / and found the API documentation there.\nAs you scrolled the documentation, there were two endpoints – /ping and /predict. Those paths are relative to the root, so you could access /ping at http://localhost:8080/ping.\n\n\n3.3.3 HTTP verbs\nWhen you make a request over HTTP, you are asking a server to do something. The http verb, also known as the request method, describes the type of operation you’re asking for. Each endpoint has one or more verbs that it knows how to use.\nIf you look at the penguin mass API, you’ll see that /ping is a GET endpoint and /predict is a POST. This isn’t coincidence. I’d approximate that 95% of the API endpoints you’ll use as a data scientist are GET and POST, which respectively fetch information from the server and provide information to the server.\nTo round out the basic http verbs you might see, PUT and PATCH change or update something and DELETE (you guessed it) deletes something. There are more esoteric ones you’ll probably never see.\n\n\n3.3.4 Request parameters and bodies\nLike a function in a package, each endpoint accepts specific arguments in a required format. Again, like a function, some arguments may be optional and some may be required.\nFor GET requests, the arguments are specified via query parameters that end up embedded in the URL after a ?. So if you ever see a URL in your browser that looks like ?first_name=alex&last_name=gold, those are query parameters.\nFor POST, PUT, and PATCH requests, arguments are provided in a body, which is usually formatted as JSON.3 Both {httr2} and {requests} have built-in functionality for converting standard Python and R data types to their JSON equivalents, but it can sometimes take some experimentation to figure out exactly how to match the argument format. Experimenting with conversions using {json} in Python and {jsonlite} in R can be very useful.\n\n\n3.3.5 (Auth) Headers\nYou will need to figure out how to authenticate to the API. The most common forms of authentication are a username and password combination, an API key, or an OAuth token.\nAPI keys and OAuth tokens are often associated with particular scopes. Scopes are permissions to do particular things. For example, an API key might be scoped to have GET access to a given endpoint, but not POST access.\nRegardless of your authentication type, it will be provided in a header to your API call. Your API documentation will tell you how to provide your username and password, API key, or token to the API in a header. Both {requests} and {httr2} provide easy helpers for adding authentication headers and also general ways to set headers if you need to.\nAside from authentication, headers are also used for a variety of different metadata like the type of machine that is sending the request and cookies that are set. You’ll rarely interact directly with these.\n\n\n3.3.6 Request Status Codes\nThe first thing you’ll consult when you get a result back is the status code. Status codes indicate what happened with your request to the server. You always hope to see 200 codes, which indicate a successful response.\nThere are also a two common types of error codes. 4xx codes indicate that there’s a problem with your request and the API couldn’t understand what you were asking. 5xx codes indicate that your request was fine, but some sort of error happened in processing your request.\nSee Appendix D for a table of common HTTP codes.\n\n\n3.3.7 Response Bodies\nThen there’s the actual contents of the response in the body. You’ll need to turn the body into a Python or R object you can work with.\nMost often, bodies are in JSON and you’ll decode them with {json} or {jsonlite}. Usually JSON is the default and you may be given the option to specify something else if you’ve got a preference.\n\n\n3.3.8 Common API patterns\nHere are a couple of common API patterns that are good to be familiar with:\n\nPagination – many data-feed APIs implement pagination. A paginated API returns only a certain number of results at a time to keep data sizes modest. You’ll need to figure out how to get all the pages back when you make a request.\nJob APIs – HTTP is designed for relatively quick request-response cycles. So if your API kicks off a long-running job, it’s rare to wait until the job is done to get a response. Instead, the API immediately returns an acknowledgement and a job-id which you can use to poll a job-status endpoint to check how things are going and eventually find your result.\nMultiple Verbs – a single endpoint often accepts multiple verbs – for example a GET and a POST at the same endpoint for getting and setting the data that endpoint stores."
  },
  {
    "objectID": "chapters/sec1/1-3-data-access.html#env-vars",
    "href": "chapters/sec1/1-3-data-access.html#env-vars",
    "title": "3  Using databases and data APIs",
    "section": "3.4 Environment variables to secure data connections",
    "text": "3.4 Environment variables to secure data connections\nWhen you take an app to production, authenticating to your data source while keeping your secrets secure is crucial.\nThe single most important thing you can do to secure your credentials is to avoid ever putting credentials in your code. Your username and password or API key should never appear in your code.\nThe simplest way to provide credentials without the values appearing in your code is with an environment variable. Environment variables are set before your code starts – sometimes from completely outside Python or R.\n\n\n\n\n\n\nNote\n\n\n\nThis section assumes you can use a username and password to connect to your database. That may not be true, depending on your organization. See Chapter 16 for how to handle data connections if you can’t directly connect with a username and password.\n\n\n\n3.4.1 Getting environment variables\nThe power of using an environment variable is that you reference them by name. Just sharing that there’s an environment variable called API_KEY doesn’t reveal anything secret, so if your code just includes environment variables, it’s completely safe to share with others.\n\n\n\n\n\n\nNote\n\n\n\nIt is convention to make environment variable names in all caps with words separated by underscores. The values are always simple character values, though these can be cast to some other type inside R or Python.\n\n\nIn Python, you can read environment variables from the os.environ dictionary or by using os.getenv(\"&lt;VAR_NAME&gt;\"). In R, you can get environment variables with Sys.getenv(\"&lt;VAR_NAME&gt;\").\nIt’s common to provide environment variables directly to functions as arguments, though you can also put the values in normal Python or R variables and use them from there.\n\n\n3.4.2 Setting environment variables\nThe most common way to set environment variables in a development environment is to load secrets from a plain text file. In Python, environment variables are usually set by reading a .env file into your Python session. The {python-dotenv} package is a good choice for doing this.\nR automatically reads the .Renviron file as environment variables and also sources the .Rprofile file, where you can set environment variables with Sys.setenv(). I personally prefer putting everything in .Rprofile for simplicity – but that’s not a universal opinion.\nSome organizations don’t ever want credentials files in plain text. After all, if someone stole a plain text secrets file, there’s nothing to stop them from using them.\nThere are packages in both R and Python called {keyring} that allow you to use the system keyring to securely store environment variables and recall them at runtime.\nSetting environment variables in production is a little harder.\nJust moving your secrets from your code into a different file you push to prod is still bad. And using {keyring} in a production environment is quite cumbersome.\nYour production environment may provide environment management tools. For example, GitHub Actions and Posit Connect both provide you the ability to set secrets that aren’t visible to the users, but are accessible to the code at runtime in an environment variable.\nIncreasingly, organizations are using token-based authorization schemes that just exchange one cryptographically secure token for another, never relying on credentials at all. The tradeoff for the enhanced security is that they can be difficult to implement, likely requiring coordination with an IT/Admin to use technologies like Kerberos or OAuth. There’s more on how to do that in Chapter 16."
  },
  {
    "objectID": "chapters/sec1/1-3-data-access.html#data-connection-packages",
    "href": "chapters/sec1/1-3-data-access.html#data-connection-packages",
    "title": "3  Using databases and data APIs",
    "section": "3.5 Data Connection Packages",
    "text": "3.5 Data Connection Packages\nIt’s very common for organizations to write their own data connector packages in Python or R that include all of the shared connection details so users don’t have to remember them. If everyone has their own credentials, it’s also nice if those packages set standard names for the environment variables so they can be more easily set in production.\nWhether you’re using R or Python, the function in your package should return the database connection object for people to use.\nHere’s an example of what that might look like if you were using a Postgres database from R:\n\n#' Return a database connection\n#'\n#' @param user username, character, defaults to value of DB_USER\n#' @param pw password, character, defaults to value of DB_PW\n#' @param ... other arguments passed to \n#' @param driver driver, defaults to RPostgres::Postgres\n#'\n#' @return DBI connection\n#' @export\n#'\n#' @examples\n#' my_db_con()\nmy_db_con &lt;- function(\n    user = Sys.getenv(\"DB_USER\"), \n    pw = Sys.getenv(\"DB_PW\"), \n    ..., \n    driver = RPostgres::Postgres()\n) {\n  DBI::dbConnect(\n    driver = driver,\n    dbname = 'my-db-name', \n    host = 'my-db.example.com', \n    port = 5432, \n    user = user,\n    password = pw, \n    ...\n  )\n}\n\nNote that the function signature defines default environment variables that will be consulted. If those environment variables are set ahead of time by the user, this code will just work."
  },
  {
    "objectID": "chapters/sec1/1-3-data-access.html#comprehension-questions",
    "href": "chapters/sec1/1-3-data-access.html#comprehension-questions",
    "title": "3  Using databases and data APIs",
    "section": "3.6 Comprehension Questions",
    "text": "3.6 Comprehension Questions\n\nDraw two mental map for connecting to a database. One usinga database driver in a Python or R package vs an ODBC or JDBC driver. You should (at a minimum) include the nodes database package, DBI (R only), driver, system driver, ODBC, JDBC, and database.\nDraw a mental map for using an API from R or Python. You should (at a minimum) include nodes for {requests}/{httr2}, request, http verb/request method, headers, query parameters, body, json, response, and response code.\nHow can environment variables be used to keep secrets secure in your code?"
  },
  {
    "objectID": "chapters/sec1/1-3-data-access.html#lab-3-use-a-database-and-an-api",
    "href": "chapters/sec1/1-3-data-access.html#lab-3-use-a-database-and-an-api",
    "title": "3  Using databases and data APIs",
    "section": "3.7 Lab 3: Use a database and an API",
    "text": "3.7 Lab 3: Use a database and an API\nIn this lab, we’re going to build out both the data layer and the presentation layer for our penguin mass model exploration. We’re going to create an app to explore the model, which will look like this: \nLet’s start by moving the data into a real data layer.\n\n3.7.1 Step 1: Put the data in DuckDB\nLet’s start by moving the data into a DuckDB database and use it from there for the modeling and EDA scripts.\nTo start, let’s load the data.\nHere’s what that looks like in R:\ncon &lt;- DBI::dbConnect(duckdb::duckdb(), dbdir = \"my-db.duckdb\")\nDBI::dbWriteTable(con, \"penguins\", palmerpenguins::penguins)\nDBI::dbDisconnect(con)\nOr equivalently, in Python:\nimport duckdb\nfrom palmerpenguins import penguins\n\ncon = duckdb.connect('my-db.duckdb')\ndf = penguins.load_penguins()\ncon.execute('CREATE TABLE penguins AS SELECT * FROM df')\ncon.close()\nNow that the data is loaded, let’s adjust our scripts to use the database.\nIn R, we are just going to replace our data loading with connecting to the database. Leaving out all the parts that don’t change, it looks like\n\n\neda.qmd\n\n\ncon &lt;- DBI::dbConnect(\n  duckdb::duckdb(), \n  dbdir = \"my-db.duckdb\"\n  )\ndf &lt;- dplyr::tbl(con, \"penguins\")\n\nWe also need to call to DBI::dbDisconnect(con) at the end of the script.\nBecause we wrote our data processing code in {dplyr}, we actually don’t have to change anything. Under the hood, {dplyr} can switch seamlessly to a database backend, which is really cool.\n\n\neda.qmd\n\ndf %&gt;%\n  group_by(species, sex) %&gt;%\n  summarise(\n    across(\n        ends_with(\"mm\") | ends_with(\"g\"),\n      \\(x) mean(x, na.rm = TRUE)\n      )\n    ) %&gt;%\n  dplyr::collect() %&gt;%\n  knitr::kable()\n\nIt’s not necessary, but I’ve added a call to dplyr::collect in line 31. It will be implied if I don’t put it there manually, but it helps make obvious that all the work before there has been pushed off to the database. Only the result of this code is coming back to the R process. Obviously it doesn’t matter for this small dataset, but this would be a huge benefit if the dataset were much larger.\nIn Python, we’re just going to load the entire dataset into memory for modeling, so the line loading the dataset changes to\n\n\nmodel.qmd\n\ncon = duckdb.connect('my-db.duckdb')\ndf = con.execute(\"SELECT * FROM penguins\").fetchdf().dropna()\ncon.close()\n\n\nNow let’s switch to figuring out the connection we’ll need to our processing layer in the presentation layer.\n\n\n3.7.2 Step 2: Call the model API from code\nBefore you start, make sure the API is running on your machine from the last lab.\n\n\n\n\n\n\nNote\n\n\n\nI’m assuming it’s running on port 8080 in this lab. If you’ve put it somewhere else, change the 8080 in the code below to match the port on your machine.\n\n\nIf you want to call the model in code, you can use any http request library. In R you should use httr2 and in Python you should use requests.\nHere’s what it looks like to call the API in Python\n\nimport requests\n\nreq_data = {\n  \"bill_length_mm\": 0,\n  \"species_Chinstrap\": False,\n  \"species_Gentoo\": False,\n  \"sex_male\": False\n}\nreq = requests.post('http://127.0.0.1:8080/predict', json = req_data)\nres = req.json().get('predict')[0]\n\nor equivalently in R\n\nreq &lt;- httr2::request(\"http://127.0.0.1:8080/predict\") |&gt;\n  httr2::req_body_json(\n    list(\n      \"bill_length_mm\" = 0,\n      \"species_Chinstrap\" = FALSE,\n      \"species_Gentoo\" = FALSE,\n      \"sex_male\" = FALSE\n    )\n  ) |&gt;\n  httr2::req_perform()\nres &lt;- httr2::resp_body_json(r)$predict[[1]]\n\nNote that there’s no translation necessary to send the request. The {requests} and{httr2} packages automatically know what to do with the Python dictionary and the R list.\nGetting the result back takes a little more work to find the right spot in the JSON returned. This is quite common.\n\n\n\n\n\n\nNote\n\n\n\nThe {vetiver} package also includes the ability to auto-query a {vetiver} API. I’m not using it here to expose the details of calling an API.\n\n\nNow, let’s take this API-calling code and build the presentation layer around it.\n\n\n3.7.3 Step 3: Build a shiny app\nWe’re going to use the {shiny} package, which is an R and Python package for creating interactive web apps using just Python code. If you don’t know much about {shiny}, you can choose to just blindly follow the examples here, or you could spend some time with the Mastering Shiny book to learn to use it yourself.\nEither way, an app that looks like the picture above would look like this in Python\n\n\napp.py\n\nfrom shiny import App, render, ui, reactive\nimport requests\n\napi_url = 'http://127.0.0.1:8080/predict'\n\napp_ui = ui.page_fluid(\n    ui.panel_title(\"Penguin Mass Predictor\"), \n    ui.layout_sidebar(\n        ui.panel_sidebar(\n            [ui.input_slider(\"bill_length\", \"Bill Length (mm)\", 30, 60, 45, step = 0.1),\n            ui.input_select(\"sex\", \"Sex\", [\"Male\", \"Female\"]),\n            ui.input_select(\"species\", \"Species\", [\"Adelie\", \"Chinstrap\", \"Gentoo\"]),\n            ui.input_action_button(\"predict\", \"Predict\")]\n        ),\n        ui.panel_main(\n            ui.h2(\"Penguin Parameters\"),\n            ui.output_text_verbatim(\"vals_out\"),\n            ui.h2(\"Predicted Penguin Mass (g)\"), \n            ui.output_text(\"pred_out\")\n        )\n    )   \n)\n\ndef server(input, output, session):\n    @reactive.Calc\n    def vals():\n        d = {\n            \"bill_length_mm\" : input.bill_length(),\n            \"sex_Male\" : input.sex() == \"Male\",\n            \"species_Gentoo\" : input.species() == \"Gentoo\", \n            \"species_Chinstrap\" : input.species() == \"Chinstrap\"\n\n        }\n        return d\n    \n    @reactive.Calc\n    @reactive.event(input.predict)\n    def pred():\n        r = requests.post(api_url, json = vals())\n        return r.json().get('predict')[0]\n\n    @output\n    @render.text\n    def vals_out():\n        return f\"{vals()}\"\n\n    @output\n    @render.text\n    def pred_out():\n        return f\"{round(pred())}\"\n\napp = App(app_ui, server)\n\nAnd like this in R\n\n\napp.R\n\nlibrary(shiny)\n\napi_url &lt;- \"http://127.0.0.1:8080/predict\"\n\nui &lt;- fluidPage(\n  titlePanel(\"Penguin Mass Predictor\"),\n\n  # Model input values\n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\n        \"bill_length\",\n        \"Bill Length (mm)\",\n        min = 30,\n        max = 60,\n        value = 45,\n        step = 0.1\n      ),\n      selectInput(\n        \"sex\",\n        \"Sex\",\n        c(\"Male\", \"Female\")\n      ),\n      selectInput(\n        \"species\",\n        \"Species\",\n        c(\"Adelie\", \"Chinstrap\", \"Gentoo\")\n      ),\n      # Get model predictions\n      actionButton(\n        \"predict\",\n        \"Predict\"\n      )\n    ),\n\n    mainPanel(\n      h2(\"Penguin Parameters\"),\n      verbatimTextOutput(\"vals\"),\n      h2(\"Predicted Penguin Mass (g)\"),\n      textOutput(\"pred\")\n    )\n  )\n)\n\nserver &lt;- function(input, output) {\n  # Input params\n  vals &lt;- reactive(\n    list(\n      bill_length_mm = input$bill_length,\n      species_Chinstrap = input$species == \"Chinstrap\",\n      species_Gentoo = input$species == \"Gentoo\",\n      sex_male = input$sex == \"Male\"\n    )\n  )\n\n  # Fetch prediction from API\n  pred &lt;- eventReactive(\n    input$predict,\n    httr2::request(api_url) |&gt;\n      httr2::req_body_json(vals()) |&gt;\n      httr2::req_perform() |&gt;\n      httr2::resp_body_json(),\n    ignoreInit = TRUE\n  )\n\n  # Render to UI\n  output$pred &lt;- renderText(pred()$predict[[1]])\n  output$vals &lt;- renderPrint(vals())\n}\n\n# Run the application\nshinyApp(ui = ui, server = server)\n\nOver the next few chapters, we’re going to implement more architectural best practices for the app, and in [Chapter @env-as-code] we’ll actually go to deployment."
  },
  {
    "objectID": "chapters/sec1/1-3-data-access.html#footnotes",
    "href": "chapters/sec1/1-3-data-access.html#footnotes",
    "title": "3  Using databases and data APIs",
    "section": "",
    "text": "I have heard that some write operations may be faster with a JDBC driver than an ODBC one. I would argue that if you’re doing enough writing to a database that speed matters, you probably should be using database-specific data loading tools, not just writing from R or Python.↩︎\nThe most common option is for the app to run as the viewer on the underlying server and use a Kerberos ticket or OAuth token waiting there. For a variety of reasons, this is difficult and you should avoid it if possible.↩︎\nIn a lot of cases, people use POST for things that look like GETs to my eyes. The reason is request bodies. GET endpoints only recently started allowing bodies – and it’s still discouraged. In the {vetiver} API example, I think of fetching results from an ML model as a GET-type operation, but it uses a POST because it also uses a body in the query.↩︎"
  },
  {
    "objectID": "chapters/sec1/1-4-monitor-log.html#observing-correctness",
    "href": "chapters/sec1/1-4-monitor-log.html#observing-correctness",
    "title": "4  Logging and Monitoring",
    "section": "4.1 Observing Correctness",
    "text": "4.1 Observing Correctness\nObservability of general purpose software is primarily concerned with the operational qualities of the software. A software engineer wants to know whether their software is using too much RAM or CPU, whether its fast enough, and whether its crashed.\nFor a general purpose software engineer, an uncaught exception that makes the software crash is about as bad as it gets.\nBut for a data scientist there’s something even scarier – an issue that doesn’t result in code failure but yields incorrect answers. Data joins usually complete even if the merge quality is really bad. Model APIs will return a prediction even if the prediction is very, very bad.\nIt’s hard to check the actual correctness of the numbers and figures work work returns because you’re (basically by definition) doing something novel. So you’re basically left putting process metrics in place that can help reveal a problem before it surfaces.\nOne important tool you have in your toolbox is correctly architecting your project. Jobs are generally much easier to check for correctness than presentation layers. By moving as much processing as possible out of the presentation layer and into the data and processing layers, you can make it easier to observe.\nMoreover, you’re already very familiar with tools for literate programming like Jupyter Notebooks, R Markdown Documents, and Quarto Documents.\nOne of my spicier opinions is that all jobs should be in a literate programming format. These tools, when used well, intersperse code, commentary, and output, which is one of the best ways of observing the correctness of a job.\nOn a job, there are three particular things I always monitor.\nThe first is the quality of data joins. Based on the number of rows (or unique ids), you know how many rows should be in the data set after a join. Checking that the joined data matches expectations can reveal many data quality issues just waiting to ruin your day.\nThe second checking is cross-tabulations before and after recoding a categorical variable. Making sure the recode logic does what you think and that the values coming in aren’t changing over time is always worth the effort.\nThe last is goodness-of-fit metrics of an ML model in production. There are many, many frameworks and products for monitoring model quality and model drift once your model is in production. I don’t have strong opinions on these other than that you need to use one if you’ve got a model that’s producing results you hope to rely on."
  },
  {
    "objectID": "chapters/sec1/1-4-monitor-log.html#observing-operations",
    "href": "chapters/sec1/1-4-monitor-log.html#observing-operations",
    "title": "4  Logging and Monitoring",
    "section": "4.2 Observing Operations",
    "text": "4.2 Observing Operations\nNow let’s turn to the issue of observing the operational qualities of your code. The operational qualities of your project are things like the system resources its consuming, the number of users, and user interactions just before an error occurred.\nThe first step to making your app or API observable is to add logging. You may be used to just adding print statements throughout your code. And, honestly, this is far better than nothing. But purpose-built tooling for logging includes ways to apply consistent formats, emit logs in useful ways, and provide visibility into severity of issues.\nThere are great logging packages in both Python and R. Python’s logging package is standard. There is not a standard logging package in R, but I recommend {log4r}.\nThese packages – and basically every other logging package – work very similarly. At the outset of your code, you’ll create and parameterize a log session that persists as long as the Python or R session. When your code does something you want to love, you’ll you’ll use the log session to write log statements. When the log statement runs, it creates a log entry.\nFor example, here’s what logging for an app starting up might look like in Python\n\n\n\napp.py\n\nimport logging\n\n# Configure the log object\nlogging.basicConfig(\n    format='%(asctime)s - %(message)s',\n    level=logging.INFO\n)\n\n# Log app start\nlogging.info(\"App Started\")\n\n\nAnd here’s what that looks like using {log4r}\n\n\n\napp.R\n\n# Configure the log object\nlog &lt;- log4r::logger()\n\n# Log app start\nlog4r::info(log, \"App Started\")\n\n\nWhen the R or Python interpreter hits either of these lines, it will create a log entry that looks something like this:\n2022-11-18 21:57:50 INFO App Started\nLike all log entries, this entry has three components:\n\nThe log metadata is data what the logging library automatically includes on every entry. It is configured when you initialize logging. In the example above, the only metadata is the timestamp. Log metadata can include additional information, like which server you’re running on.\nThe second component is the log level. The log level indicates the severity of the event you’re logging. In the example above the log level was INFO.\nThe last component is the log data, which provides details on the event you want to log – App Started in this case.\n\n\n4.2.1 Understanding log levels\nThe log level indicates how serious the logged event is. Most logging libraries have 5-7 log levels. As you’re writing statements into your code, you’ll have to think carefully about the appropriate logging level for a given event.\nBoth the Python {logging} library and {log4r} have five levels from least to most scary:\n\nDebug: what the code was doing in detail that will only make sense to someone who really knows the code. For example, you might include which function ran and with what arguments in a debug log.\nInfo: something normal happened in the app. Info statements record things like starting and stopping, successfully making database and other connections, and runtime configuration options.\nWarn/Warning: an unexpected application issue that isn’t fatal. For example, you might include having to retry doing something or noticing that resource usage is high. If something were to go wrong later, these might be helpful breadcrumbs to look at.\nError: an issue that will make an operation not work, but that won’t bring down your app. An example might be a user submitting invalid input and the app recovering.\nCritical: an error so big that the app itself shuts down. This is the SOS your app sends as it shuts down. For example, if your app cannot run without a connection to an outside service, you might log an inability to connect as a Critical error.\n\nWhen you initialize your logging session, you’ll set your log level, which is the least critical level you want to see in the session. In development, you probably want to log everything down to the debug level, while that probably isn’t ideal in prod.\n\n\n4.2.2 Configuring log formats and log handling\nWhen you initialize your logging session, you’ll choose where logs will be written and in what format. You’ll configure the format with a formatter or layout and where it goes with a handler or an appender.\nFor most logging frameworks, the default is to emit logs to the console in plain text.\nFor example, a plain text log of an app starting might put this on your console\n2022-11-18 21:57:50 INFO App Started\nYou’ll decide the format of your log based on how you’re planning to consume them.\nPlain text logs is a great choice if humans are going to be directly reading them. If you’re shipping your logs off to an aggregation service, you might prefer to have structured logs.\nThe most common structured logging format is JSON, though YAML and XML are often options. If you used JSON logging, the same record might be emitted as\n{\n  \"time\": \"2022-11-18 21:57:50\",\n  \"level\": \"INFO\", \n  \"data\": \"App Started\"\n}\nWhere your logs go should be determined by where your code is running.\nIn development, printing logs for the console makes it easy to iterate quickly.\nIn production, the most common way to handle logs is to append them to a file. It makes them easy for humans to access and many tools for aggregating and consuming logs are comfortable watching a file and aggregating lines as they are written.\nIf you are emitting logs to file, you may also want to consider how long those logs stay around.\nLog rotation is the process of periodically creating new log files, storing old logs for a set retention period, and deleting files outside that period. A common log rotation pattern is to have a log file that lasts for 24 hours, is retained for 30 days, and is then deleted.\nThe Python {logging} library does log rotation itself. {log4r} does not, but there is a Linux library called logrotate that you can use in concert with {log4r}.1\nIf you’re running in a Docker container you don’t want to write to a file on disk. As you’ll learn more about in Chapter 6, anything that lives inside a Docker container is ephemeral. This is obviously bad if you’re writing a log that might contain clues for why a Docker container was unexpectedly killed.\nIn that case, it’s common practice for a service running in a container to emit logs inside the container and then have some sort of more permanent service collecting the logs outside. This is usually accomplished by sending normal operating logs to go to stdout (usually pronounced standard out) and failures to go to stderr (standard error).\nIt’s also possible you want to do something else completely custom with your logs. This is most common for critical or error logs. For example, you may want to send an email, slack, or text message immediately if your system emits a high-level log message.\nIt’s also very common to have different format and location settings in development vs in production. The most common way to enable different logging configurations in different environments is with config files and environment variables. More on how to use these tools in Chapter 1.\n\n\n4.2.3 Working with Metrics\nThe most common place to see metrics in a data science context is when deploying and monitoring ML models in production. Additionally, monitoring ETL data quality is ripe for more monitoring.\nIf you are going to configure metrics emission or consumption, most modern metrics stacks are built around the open source tools Prometheus and Grafana.\nPrometheus is an open source monitoring tool that makes it easy to store metrics data, query that data, and alert based on it. Grafana is an open source dashboarding tool that sits on top of Prometheus to do visualization of the metrics. They are usually used together to do monitoring and visualization of metrics.\nYou can run Prometheus and Grafana yourself, but Grafana Labs provides a generous free tier for their SaaS service. This is great because you can just set up their service and point your app to it.\nBecause the Prometheus/Grafana stack started out in the DevOps world, they are most optimized to do monitoring of a whole server or fleet of servers – but it’s not hard to use them to monitor things you might care about like data quality, API response times, or other things.\nIf you want to register metrics from your API or app with Prometheus, there is an official Prometheus client in Python and the {openmetrics} package in R makes it easy to register metrics from a Plumber API or Shiny app.\nThere’s a great Get Started with Grafana and Prometheus doc on the Grafana Labs website if you want to actually try it out."
  },
  {
    "objectID": "chapters/sec1/1-4-monitor-log.html#comprehension-questions",
    "href": "chapters/sec1/1-4-monitor-log.html#comprehension-questions",
    "title": "4  Logging and Monitoring",
    "section": "4.3 Comprehension Questions",
    "text": "4.3 Comprehension Questions\n\nWhat is the difference between monitoring and logging? What are the two halves of the monitoring and logging process?\nIn general, logging is good, but what are some things you should be careful not to log?\nAt what level would you log each of the following events:\n\nSomeone clicks on a particular tab in your Shiny app.\nSomeone puts an invalid entry into a text entry box.\nAn http call your app makes to an external API fails.\nThe numeric values that are going into your computational function."
  },
  {
    "objectID": "chapters/sec1/1-4-monitor-log.html#lab-4-an-app-with-logging",
    "href": "chapters/sec1/1-4-monitor-log.html#lab-4-an-app-with-logging",
    "title": "4  Logging and Monitoring",
    "section": "4.4 Lab 4: An App with Logging",
    "text": "4.4 Lab 4: An App with Logging\nLet’s go back to the prediction generator app from the last lab and add a little logging. This is quite easy in both R and Python. In both, we just declare that we’re using the logger and then we put logging statements into our code.\nI decided to log when the app starts, just before and after each request, and an error logger if an HTTP error code comes back from the API.\nWith the logging now added, here’s what the app looks like in R:\n\n\napp.R\n\nlibrary(shiny)\n\napi_url &lt;- \"http://127.0.0.1:8080/predict\"\nlog &lt;- log4r::logger()\n\nui &lt;- fluidPage(\n  titlePanel(\"Penguin Mass Predictor\"),\n\n  # Model input values\n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\n        \"bill_length\",\n        \"Bill Length (mm)\",\n        min = 30,\n        max = 60,\n        value = 45,\n        step = 0.1\n      ),\n      selectInput(\n        \"sex\",\n        \"Sex\",\n        c(\"Male\", \"Female\")\n      ),\n      selectInput(\n        \"species\",\n        \"Species\",\n        c(\"Adelie\", \"Chinstrap\", \"Gentoo\")\n      ),\n      # Get model predictions\n      actionButton(\n        \"predict\",\n        \"Predict\"\n      )\n    ),\n\n    mainPanel(\n      h2(\"Penguin Parameters\"),\n      verbatimTextOutput(\"vals\"),\n      h2(\"Predicted Penguin Mass (g)\"),\n      textOutput(\"pred\")\n    )\n  )\n)\n\nserver &lt;- function(input, output) {\n  log4r::info(log, \"App Started\")\n  # Input params\n  vals &lt;- reactive(\n    list(\n      bill_length_mm = input$bill_length,\n      species_Chinstrap = input$species == \"Chinstrap\",\n      species_Gentoo = input$species == \"Gentoo\",\n      sex_male = input$sex == \"Male\"\n    )\n  )\n\n  # Fetch prediction from API\n  pred &lt;- eventReactive(\n    input$predict,\n    {\n      log4r::info(log, \"Prediction Requested\")\n      r &lt;- httr2::request(api_url) |&gt;\n        httr2::req_body_json(vals()) |&gt;\n        httr2::req_perform()\n      log4r::info(log, \"Prediction Returned\")\n\n      if (httr2::resp_is_error(r)) {\n        log4r::error(log, paste(\"HTTP Error\"))\n      }\n\n      httr2::resp_body_json(r)\n    },\n    ignoreInit = TRUE\n  )\n\n  # Render to UI\n  output$pred &lt;- renderText(pred()$predict[[1]])\n  output$vals &lt;- renderPrint(vals())\n}\n\n# Run the application\nshinyApp(ui = ui, server = server)\n\nAnd in Python:\n\n\napp.py\n\nfrom shiny import App, render, ui, reactive\nimport requests\nimport logging\n\napi_url = 'http://127.0.0.1:8080/predict'\nlogging.basicConfig(\n    format='%(asctime)s - %(message)s',\n    level=logging.INFO\n)\n\napp_ui = ui.page_fluid(\n    ui.panel_title(\"Penguin Mass Predictor\"), \n    ui.layout_sidebar(\n        ui.panel_sidebar(\n            [ui.input_slider(\"bill_length\", \"Bill Length (mm)\", 30, 60, 45, step = 0.1),\n            ui.input_select(\"sex\", \"Sex\", [\"Male\", \"Female\"]),\n            ui.input_select(\"species\", \"Species\", [\"Adelie\", \"Chinstrap\", \"Gentoo\"]),\n            ui.input_action_button(\"predict\", \"Predict\")]\n        ),\n        ui.panel_main(\n            ui.h2(\"Penguin Parameters\"),\n            ui.output_text_verbatim(\"vals_out\"),\n            ui.h2(\"Predicted Penguin Mass (g)\"), \n            ui.output_text(\"pred_out\")\n        )\n    )   \n)\n\ndef server(input, output, session):\n    logging.info(\"App start\")\n\n    @reactive.Calc\n    def vals():\n        d = {\n            \"bill_length_mm\" : input.bill_length(),\n            \"sex_Male\" : input.sex() == \"Male\",\n            \"species_Gentoo\" : input.species() == \"Gentoo\", \n            \"species_Chinstrap\" : input.species() == \"Chinstrap\"\n\n        }\n        return d\n    \n    @reactive.Calc\n    @reactive.event(input.predict)\n    def pred():\n        logging.info(\"Request Made\")\n        r = requests.post(api_url, json = vals())\n        logging.info(\"Request Returned\")\n\n        if r.status_code != 200:\n            logging.error(\"HTTP error returned\")\n\n        return r.json().get('predict')[0]\n\n    @output\n    @render.text\n    def vals_out():\n        return f\"{vals()}\"\n\n    @output\n    @render.text\n    def pred_out():\n        return f\"{round(pred())}\"\n\napp = App(app_ui, server)\n\nNow, if you load up this app locally, you can see the logs of what’s happening stream in as you’re pressing buttons in the app.\nYou can feel free to log whatever you think is helpful – for example, it’d probably be more useful to get the actual error contents if an HTTP error comes back."
  },
  {
    "objectID": "chapters/sec1/1-4-monitor-log.html#footnotes",
    "href": "chapters/sec1/1-4-monitor-log.html#footnotes",
    "title": "4  Logging and Monitoring",
    "section": "",
    "text": "There are two common naming patterns with rotating log files.\nThe first is to have dated log filenames that look like my-log-20221118.log.\nThe other pattern is to keep one file that’s current and have the older ones numbered. So today’s log would be my-log.log, yesterday’s would be my-log.log.1 , the day before my-log.log.2, etc. This second pattern works particularly well if you’re using logrotate with log4r, because then log4r doesn’t need to know anything about the log rotation. It’s just always writing to my-log.log.↩︎"
  },
  {
    "objectID": "chapters/sec1/1-5-deployments.html#separate-the-prod-environment",
    "href": "chapters/sec1/1-5-deployments.html#separate-the-prod-environment",
    "title": "5  Deployments and code promotion",
    "section": "5.1 Separate the prod environment",
    "text": "5.1 Separate the prod environment\nThe bedrock of a good CI/CD process is a production environment that’s actually separate from non-production environments.\nCI/CD is all about easily promoting code into production, but if the boundaries of production are a mushy mess, it’s all too easily to accidentally mess up code that’s in production.\nIn order to structure a smooth pathway to production, software environments are separated into three – dev, test, and prod. Dev is the development environment where new work is produced, test is where the code is tested for performance, usability, and feature completeness, and prod is the production environment. Depending on your organization you might have just a dev and prod or you might have more environments between dev and prod.\n\nThe number and configuration of lower environments will vary according to your organization and its needs. But like Tolstoy said about happy families, all prod environments are alike.\nSome criteria that all good prod environments meet:\n\nThe environment is created using code. For data science, that managing R and Python packages using environments as code tooling, as discussed in Chapter 1.\nChanges happen via a promotion process. The process combines human approvals that the code is ready for production and automations to run tests and do the actual deployment.\nChanges only happen via the promotion process. This means no manual changes to the environment or the code.\n\nRules 1 and 2 are pretty straightforward to follow. But the first time something breaks in your prod environment, you will be sorely tempted to violate rule 3. Don’t do it.\nIf you want to run a data science project that becomes critical to your organization, keeping a pristine prod environment that you can rely on is critical. Re-create the issue in a lower environment to figure out what’s wrong and push changes through your promotion process.\n\n5.1.1 Dev and test environments\nThese guidelines for a prod environment look almost identical to guidelines for general purpose software engineering. It’s in the composition of lower environments that the needs of data scientists diverge from general purpose software engineers.\nAs a data scientist, dev means working in a lab environment like RStudio, Spyder, VSCode, or PyCharm and experimenting with the data. You’re slicing the data this way or that to see if anything meaningful emerges, creating plots to see if they are the right way to show off a finding, and checking whether certain features improve model performance. All this means that it’s basically impossible to do work without real data.\n“Duh”, you say, “Of course it’s silly to do data science on fake data.”\nThis may be obvious to you, but doing dev data science on real data is a very common source of friction with IT/Admins.\nThat’s because this need is unique to data scientists. For general purpose software engineering, a lower environment needs data that is formatted like the real data, but the actual content doesn’t matter.\nFor example, if you’re building an online store, you need dev and test environments where the API calls from the sales system are in the same format as the real data – but you don’t actually care if it’s real data. In fact, you probably want to create some odd-looking cases for testing purposes.\nOne way to help allay these concerns is to create a data science sandbox. A great data science sandbox provides:\n\nRead-only access to real data for experimentation.\nPlaces to write mock data to test out things you’ll write for real in prod.\nBroad access to R and Python packages to experiment with before things go to prod.\n\nWorking with your IT/Admin team to get these things isn’t always easy. One thing to point out is that creating this environment actually makes things more secure. It gives you a place to do development without any fear that you might actually damage production data or services."
  },
  {
    "objectID": "chapters/sec1/1-5-deployments.html#version-control-implements-code-promotion",
    "href": "chapters/sec1/1-5-deployments.html#version-control-implements-code-promotion",
    "title": "5  Deployments and code promotion",
    "section": "5.2 Version control implements code promotion",
    "text": "5.2 Version control implements code promotion\nYou need a way to actually operationalize your code promotion process. If your process says that your code needs testing and review before it’s pushed to prod, you need a place to actually do that. Version control is the tool to make your code promotion process real.\nVersion control is software that allows you to keep the prod version of your code safe, gives contributors their own copy to work on, and hosts tools to manage merging changes back together. These days, git is the industry standard for version control.\nGit is a system for tracking changes in computer files in a project called a repository. Git is open source and freely available. There are a number of different companies that host git repositories. Many of them allow you to host public – and some private - repositories for free and have enterprise products that your organization may pay for. GitHub is by far the most popular git host, but GitLab, Bitbucket, and Azure DevOps are also common.\nThis is not a book on git. If you’re not already comfortable with using local and remote repositories, branching, and merging, the rest of this chapter is going to be completely useless. I recommend you take a break from this book and spend some time learning git.\n\n\n\n\n\n\nHints on Learning Git\n\n\n\nPeople who say git is easy to learn are either lying or have forgotten. I am sorry our industry has standardized on a tool with such terrible ergonomics, but it’s really worth it to learn.\nWhether you’re an R or Python user, I’d recommend starting with a resource designed to teach git to a data science user. My recommendation is to check out HappyGitWithR by Jenny Bryan.\nIf you’re a Python user, some of the specific tooling suggestions won’t apply, but the general principles will be exactly the same.\n\n\nIf you understand git and just need a reminder of some common commands, see Appendix D for a cheatsheet of common ones.\nThe precise contours of your code promotion process – and therefore your git policies – are up to you and your organization’s needs. Do you need multiple rounds of review? Can anyone promote something to prod, or just certain people? Is automated testing required?\nYou should make these decisions as part of designing your code promotion process, which you can then enshrine in the configuration of your project’s git repository.\nOne important decision you’ll make is on how to configure the branches of your git repository. Here’s how I’d suggest you do it for production data science projects:\n\nMaintain two long running branches – main is the prod version of your project, and test is a long-running pre-prod version.\nCode can only be promoted to main via a merge from test. Direct pushes to main are not allowed.\nNew functionality is developed in short-lived feature branches that are merged into test when you think they’re ready to go. Once sufficient approvals are granted, the feature branch changes in test are merged into main.\n\nThis framework helps maintain a reliable prod version on the main branch, while also leaving sufficient flexibility to accomplish basically any set of approvals and testing you might want.\nHere’s an example of how this might work. Let’s say you were working on a dashboard and were trying to add a new plot.\nYou would create a new feature branch, perhaps called new_plot to work on the plot. When you were happy with how it looked you would merge the feature branch to test. Depending on your organization’s process, you might be able to merge to test yourself or you might require approval.\nIf your testing turned up a bug, you’d fix the bug in the feature branch, merge the bug fix into test, re-test, and merge to main once you were satisfied.\nHere’s what the git graph for that sequence of events might look like:\n\nOne of the tenets of a good CI/CD practice is that changes are merged frequently and incrementally into production.\nA good rule of thumb is that you want your merges to be the smallest meaningful change that can be incorporated into main in a standalone way.\nCreating feature branches for every word of text you might change is clearly too small. Completely rewriting the dashboard in one merge request is also probably too big.\nThere’s no hard and fast rules here. Knowing the appropriate scope for a single merge is an art – one that can take years to develop. Your best resource here is more senior team members who’ve already figured it out."
  },
  {
    "objectID": "chapters/sec1/1-5-deployments.html#cicd-automates-git-operations",
    "href": "chapters/sec1/1-5-deployments.html#cicd-automates-git-operations",
    "title": "5  Deployments and code promotion",
    "section": "5.3 CI/CD automates git operations",
    "text": "5.3 CI/CD automates git operations\nThe role of git is to make your code promotion process happen. Git allows you to configure requirements for whatever approvals and testing you might need. Your CI/CD tool sits on top of that so that all this merging and branching actually does something.1\nTo be more precise a CI/CD pipeline for a project watches the git repository and does something when certain triggers are met, like a merge to a particular branch or a pull request opening.\nThe most common CI/CD operations are pre-merge checks like spell checking, code linting, and automated testing and post-merge deployments.\nThere are a variety of different CI/CD tools available. Because of the tight linkage between GitHub repos and CI/CD, CI/CD pipelines built right into git providers are very popular.\nGitHub Actions (GHA) was released a few years ago and is eating the world of CI/CD. Depending on your organization and the age of your CI/CD pipeline, you might also see Jenkins, Travis, Azure DevOps, or GitLab.\nIf you’re curious how exactly this works, you’ll get your hands dirty in the lab at the end of the chapter.\n\n5.3.1 Configuring per-environment behavior\nAs you promote an app from dev to test and prod, you probably want behavior to look different across the environments. For example, you might want to switch data sources from a dev database to a prod one, or switch a read-only app into write mode, or use a different level of logging.\nThe easiest way to create per-environment behavior is to write code that behaves differently based on on the value of an environment variable and to set that environment variable in each environment.\nMy recommendation is to use a config file to store the values you want for your environment variables for each environment. My preference is to use YAML to store configuration, but there are different ways it can be done.\n\n\n\n\n\n\nNote\n\n\n\nOnly non-secret configuration settings should go in a config file. Secrets should always be configured using secrets management settings in the tooling you’re using so they don’t appear in plain text.\n\n\nFor example, you could write a project that knows whether to write or not based on the value of the config’s write and which database using the config’s db-path. Then you could use the YAML below to specify which environments write and which ones use which database:\n{yaml filename=\"config.yml\"} dev:   write: false   db-path: dev-db test   write: true prod:   write: true   db-path: prod-db\nYou would set a relevant environment variable so your code pulls the dev configuration in dev, test in test, and prod in prod.\nIn Python there are many different ways to set and read in your a per-environment configuration. If you want to use YAML like in the example above, you could save it as config.yml and use the {yaml} package to read it in as a dictionary, and choose which part of the dictionary to at the start of your script.\nIn R, the {config} package is the standard way to load an environmental configuration from a YAML file. The config::get() function uses the value of the R_CONFIG_ACTIVE environment variable to choose which configuration to use. That means that switching from the dev to the prod version of the app is as easy as making sure you’ve got the correct environment variable set on your system."
  },
  {
    "objectID": "chapters/sec1/1-5-deployments.html#comprehension-questions",
    "href": "chapters/sec1/1-5-deployments.html#comprehension-questions",
    "title": "5  Deployments and code promotion",
    "section": "5.4 Comprehension Questions",
    "text": "5.4 Comprehension Questions\n\nWrite down a mental map of the relationship between the three environments for data science?\nWhy is git so important to a good code promotion strategy? Can you have a code promotion strategy without git?\nWhat is the relationship between git and CI/CD? What’s the benefit of using git and CI/CD together?\nWrite out a mental map of the relationship of the following terms: git, GitHub, CI/CD, GitHub Actions, Version Control"
  },
  {
    "objectID": "chapters/sec1/1-5-deployments.html#lab5",
    "href": "chapters/sec1/1-5-deployments.html#lab5",
    "title": "5  Deployments and code promotion",
    "section": "5.5 Lab 5: Host a website with automatic updates",
    "text": "5.5 Lab 5: Host a website with automatic updates\nIn labs 1 through 4, you’ve created a Quarto website for the penguin model. You’ve got sections on EDA and model building. But it’s still just on your computer.\nIn this lab, we’re going to actually deploy that website to a public site on GitHub and and set up GitHub Actions as CI/CD so the EDA and modeling steps re-render every time we make changes.\nBefore we get into the meat of the lab, there are a few things you have to do on your own. If you don’t know how to do these things, there are plenty of great tutorials online.\n\nCreate an empty public git repo on GitHub.\nConfigure the repo as the remote for your Quarto project directory.\n\nOnce you’ve got the GitHub repo connected to your project, you need to set up the Quarto project to publish via GitHub Actions. There are great directions on how to get that configured on the Quarto website.\nFollowing those instructions will accomplish three things for you:\n\nGenerate a _publish.yml, which is a Quarto-specific file for configuring publishing locations.\nConfigure GitHub Pages to serve your website off a long-running standalone branch called gh-pages.\nGenerate a GitHub Actions workflow file, which will live at .github/workflows/publish.yml.\n\nHere’s the basic GitHub Actions file (or close to it) that the process will auto-generate for you.\n\n\n.github/workflows/publish.yml\n\non:\n  workflow_dispatch:\n  push:\n    branches: main\n\nname: Quarto Publish\n\njobs:\n  build-deploy:\n    runs-on: ubuntu-latest\n    permissions:\n      contents: write\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v2\n\n      - name: Set up Quarto\n        uses: quarto-dev/quarto-actions/setup@v2\n\n      - name: Render and Publish\n        uses: quarto-dev/quarto-actions/publish@v2\n        with:\n          target: gh-pages\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n\nOne of the reasons GitHub Actions has gotten so popular is that the actions defined in a very human-readable YAML file and it’s very likely you can read and understand this without much editorializing. But let’s still go through it in some detail.\nThis particular syntax is unique to GitHub Actions, but the idea is universal to all CI/CD systems – you define a trigger and a job to do when it’s triggered.\nIn GitHub Actions, the on section defines when the workflow occurs. In this case, we’ve configured the workflow only to trigger on a push to the main branch.2 Another common case would be to trigger on a pull request to main or another branch.\nThe jobs section defines what happens.\nWhen your action starts up, it’s in a completely standalone environment. This is actually a great thing – if you can easily specify how to start from zero and get your code running in GitHub actions, you can bet it’ll do the same in prod.\nThe runs-on field specifies exactly where we start, which in this case is the latest version of the Ubuntu and not much else.\nOnce that environment is up, each step in jobs runs sequentially.\nThe most common way to define a step is with uses, which calls a preexisting GitHub Actions step that someone else has written. In some cases, you’ll want to specify variable values using with or environment variables with env.\nTake a close look at how this action uses the GITHUB_TOKEN. That’s an environment secret that’s auto-provisioned for an action. By using it as a variable here, it’s easy to see what happens, but the value is still totally secret.\nNow, if you try to run this, it probably won’t work.\nThat’s because the CI/CD process occurs in a completely isolated environment. This auto-generated action doesn’t including setting up versions of R and Python or the packages to run our EDA and modeling scripts. We have to get that configured before this action will work.\n\n\n\n\n\n\nNote\n\n\n\nIf you read the Quarto documentation, they recommend freezing your computations. Freezing is very useful if you want to render your R or Python code only once and just update the text of your document. You wouldn’t need to set up R or Python in CI/CD and the document would render faster.\nThat said, freezing isn’t an option if you intend the R or Python code to re-run because it’s a job you care about.\nBecause the main point here is to learn about getting environments as code working in CI/CD you should not freeze your environment.\n\n\nFirst, add the commands to install R, {renv}, and the packages for your content to the GitHub Actions workflow.\n\n\n.github/workflows/publish.yml\n\n      - name: Install R\n        uses: r-lib/actions/setup-r@v2\n        with:\n          r-version: '4.2.0'\n          use-public-rspm: true\n\n      - name: Setup renv and install packages\n        uses: r-lib/actions/setup-renv@v2\n        with:\n          cache-version: 1\n        env:\n          RENV_CONFIG_REPOS_OVERRIDE: https://packagemanager.rstudio.com/all/latest\n\n\n\n\n\n\n\nNote\n\n\n\nIf you’re having slow package installs in CI/CD for R, I’d strongly recommend using a repos override like in the example above.\nThe issue is that CRAN doesn’t serve binary packages for Linux, which means really slow installs. You’ve got to direct {renv} to install from Public Posit Package Manager, which does have Linux binaries.\n\n\nYou’ll also need to add a workflow to GitHub Actions to install Python and the necessary Python packages from the requirements.txt.\n\n\n.github/workflows/publish.yml\n\n      - name: Install Python and Dependencies\n        uses: actions/setup-python@v4\n        with:\n          python-version: '3.10'\n          cache: 'pip'\n      - run: pip install jupyter\n      - run: pip install -r requirements.txt\n\nNote that in this case, we run the Python environment restore commands with run rather than uses. Where uses takes an existing GitHub Action and runs it, run just runs the shell command natively.\nOnce you’ve made those changes, try pushing or merging your project to main. If you click on the Actions tab on GitHub you’ll be able to see the Action running.\nIn all honesty, it will probably fail the first time or five. You will almost never get your Actions correct on the first try. Just breathe deeply and know we’ve all been there. You’ll figure it out.\nOnce it finishes, you should be able to see your change reflected on your website.\nOnce it’s up, your website will be available at https://&lt;username&gt;.github.io/&lt;repo-name&gt;."
  },
  {
    "objectID": "chapters/sec1/1-5-deployments.html#footnotes",
    "href": "chapters/sec1/1-5-deployments.html#footnotes",
    "title": "5  Deployments and code promotion",
    "section": "",
    "text": "Strictly speaking, this is not true. There are a lot of different ways to kick off CI/CD jobs. But the right way to do it is to base it on git operations.↩︎\nA completed merge counts as a push.↩︎"
  },
  {
    "objectID": "chapters/sec1/1-6-docker.html#container-lifecycle",
    "href": "chapters/sec1/1-6-docker.html#container-lifecycle",
    "title": "6  Demystifying Docker",
    "section": "6.1 Container lifecycle",
    "text": "6.1 Container lifecycle\nDocker is primarily concerned with the creation, movement, and running of containers. A container is a software entity that packages code and its dependencies down to the operating system. Containers make it possible to have completely different environments coexisting side by side on one physical machine.\n\n\n\n\n\n\nNote\n\n\n\nContainers aren’t the only way to run multiple virtual environments side-by-side – they’re just the most popular.\nDocker Containers also aren’t the only type of container, though they’re often treated that way. You may run across other kinds, like Apptainer (formerly Singularity), which is often used in high performance computing contexts.\n\n\nA Docker Image is an immutable snapshot of a container. When you want to run a container, you pull the image and run it as an instance, which is what you’ll actually interact with.\nImages are most often stored in registries, which are similar to git repositories. The most common registry is Docker Hub, which allows public hosting and private hosting of images in free and paid tiers. Docker Hub includes official images for operating systems and programming languages, as well as many, many community-contributed containers. Some organizations run their own private registries, usually using registry as a service offerings from cloud providers.2\nImages are built from Dockerfiles – the code that defines the image. Dockerfiles are usually stored in a git repository. It’s common to build images in a CI/CD pipeline so changes to the Dockerfile are immediately built and pushed to the registry.\nAll instances run on an underlying machine, called a host. A primary feature – also a liability – of using containers is that they are completely ephemeral. Unless configured otherwise, anything inside an instance when it shuts down vanishes without a trace.\nYou can control Docker Containers from the Docker Desktop app. If you’re going to be using Docker on a server, you’ll mostly be interacting via the command line interface (CLI). All Docker CLI commands are formatted as docker &lt;command&gt;.\nThe graphic below shows the different states for a container and the CLI commands to move from one to another.\n\n\n\n\n\n\n\nNote\n\n\n\nI’ve included docker pull on the graphic for completeness, but you’ll almost never run it. docker run auto-pulls the container(s) it needs.\n\n\nSee Appendix D for a cheatsheet with a list of common Docker commands.\n\n6.1.1 Image Names\nIn order to build, push, pull, or run an image, you’ll need to know which image you’re talking about. Every image has a name that consists of an id and a tag.\n\n\n\n\n\n\nNote\n\n\n\nIf you’re using Docker Hub, container ids take the form &lt;user&gt;/&lt;container name&gt;, so I might have the container alexkgold/my-container. This should look familiar to GitHub users.\nOther registries may enforce similar conventions for ids, or they may allow ids in any format they want.\n\n\nTags are used to specify versions of variants of containers and come after the id and :. For example, the official Python docker image has versions for each version of Python like python:3 as well as variants for different operating systems and a slim version that saves space by excluding recommended packages.\nSome tags, usually used for versions, are immutable. For example, the rocker/r-ver container is a container that is built on Ubuntu and has a version of R built in. There’s a rocker/r-ver:4.3.1, which is a container with R 4.3.1.\nOther tags are relative to the point in time. If you don’t see a tag on a container name, it’s using the default latest. Other common relative tags refer to the current development state of the software inside like devel, release, or stable."
  },
  {
    "objectID": "chapters/sec1/1-6-docker.html#running-containers",
    "href": "chapters/sec1/1-6-docker.html#running-containers",
    "title": "6  Demystifying Docker",
    "section": "6.2 Running Containers",
    "text": "6.2 Running Containers\nThe docker run command runs container images as an instance. You can run docker run &lt;image name&gt; to get a running container. However, most things you want to do with your instance require several command line flags.\nThere are a few flags that are useful to manage how your containers run and get cleaned up.\nThe -name &lt;name&gt; flag names an instance. It’s not required, but can be useful to give the instance a name you can remember or use in code. If you don’t provide a name, each instance gets a random alphanumeric id on start.\nThe -rm flag automatically removes the container after its done. If you don’t use the -rm flag, the container will stick around until you clean it up manually with docker rm. The -rm flag can be useful when you’re iterating quickly – especially because you can’t re-use names until you remove the container.\nThe -d flag will run your container in detached mode. This is useful when you want your container to run in the background and not block your terminal session. It’s useful when running containers in production, but you probably don’t want to use it when you’re trying things out and want to see logs streaming out as the container runs.\n\n6.2.1 Getting information in and out\nWhen a container runs, it is isolated from the host. This is a great feature – it means programs running inside the container can address the container’s filesystem and networking without worrying about the host outside. But it also means that using resources on the host requires explicit declarations as part of the docker run command.\nIn order to get data in or out of a container, you need to mount a shared volume (directory) between the container and host with the -v flag. You specify a host directory and a container directory separated by :. Anything in the volume will be available to both the host and the container at the file paths specified.\nFor example, maybe you’ve got a container that runs a job against data it expects in the /data directory. On your host machine, this data lives at /home/alex/data. You could make this happen with\n{bash filename=Terminal} docker run -v /home/alex/data:/data\nHere’s a diagram of how this works.\n\nSimilarly, if you have a service running in a container on a particular port, you’ll need to map the container port to a host port with the -p flag.\n\n\n6.2.2 Other runtime commands\nIf you want to see what containers you’ve got, docker ps lists them. This is especially useful to get instance ids if you didn’t bother with names.\nTo stop a running container docker stop does so nicely and docker kill terminates a container immediately.\nYou can view the logs from a container with docker logs.\nLastly, if you need to run a command inside a running instance, you can use docker exec. This is most commonly used to access the command line inside the container as if SSH-ing to a server with docker exec -it &lt;container&gt; /bin/bash.\nWhile it’s normal to SSH into a server to poke around, it’s somewhat of an anti-pattern to do the same in a container. In general you should prefer to review logs and adjust Dockerfiles and run commands rather than exec in."
  },
  {
    "objectID": "chapters/sec1/1-6-docker.html#building-images-from-dockerfiles",
    "href": "chapters/sec1/1-6-docker.html#building-images-from-dockerfiles",
    "title": "6  Demystifying Docker",
    "section": "6.3 Building Images from Dockerfiles",
    "text": "6.3 Building Images from Dockerfiles\nA Dockerfile is a set of instructions that you use to build a Docker Image. If you know how to accomplish something from the command line, you shouldn’t have too much trouble building a Dockerfile to do the same.\nOne thing to consider when creating Dockerfiles is that the resulting image is immutable. That means anything you build into the image is forever frozen in time. So you’ll definitely want to set up the versions of R and Python and install system requirements. Depending on the purpose of your container, you may want to copy in code, data, and/or R and Python packages, or you may want to mount those in a volume at runtime.\nThere are many Dockerfile commands. You can review them all in the Dockerfile documentation, but here are the handful that are enough to build most images.\n\nFROM – specify the base image. Usually the first line of the Dockerfile.\nRUN – run any command as if you were sitting at the command line inside the container.\nCOPY – copy a file from the host filesystem into the container.\nCMD - Specify what command to run on the container’s shell when it runs, usually the last line of the Dockerfile.3\n\nEvery Dockerfile command defines a new layer.\nA great feature of Docker is that it only rebuilds the layers it needs to when you make changes. For example, take the following Dockerfile:\nFROM ubuntu:latest\n\nCOPY my-data.csv /data/data.csv\n\nRUN [\"head\", \"/data/data.csv\"]\nLet’s say I wanted to change the head command to tail. Rebuilding this container would be nearly instantaneous because the container would only start rebuilding after the COPY command.\nOnce you’ve created your Dockerfile, you build it into an image using docker build -t &lt;image name&gt; &lt;build directory&gt;. If you don’t provide a tag, the default tag is latest.\nYou can then push the image to DockerHub or another registry using docker push &lt;image name&gt;."
  },
  {
    "objectID": "chapters/sec1/1-6-docker.html#comprehension-questions",
    "href": "chapters/sec1/1-6-docker.html#comprehension-questions",
    "title": "6  Demystifying Docker",
    "section": "6.4 Comprehension Questions",
    "text": "6.4 Comprehension Questions\n\nDraw a mental map of the relationship between the following: Dockerfile, Docker Image, Docker Registry, Docker Container\nWhen would you want to use each of the following flags for docker run? When wouldn’t you?\n\n-p, --name, -d, --rm, -v\n\nWhat are the most important Dockerfile commands?"
  },
  {
    "objectID": "chapters/sec1/1-6-docker.html#lab-putting-an-api-in-a-container",
    "href": "chapters/sec1/1-6-docker.html#lab-putting-an-api-in-a-container",
    "title": "6  Demystifying Docker",
    "section": "6.5 Lab: Putting an API in a Container",
    "text": "6.5 Lab: Putting an API in a Container\nPutting an API into a container is popular way to host them. In this lab, we’re going to put the Penguin Model Prediction API from Chapter 2 into a container.\nIf you’ve never used Docker before, start by installing Docker Desktop on your computer.\nYou should feel free to write your own Dockerfile to put the API in a container. If you want to make it easy, the {vetiver} package, which you’ll remember auto-generated the API for us, can also auto-generate a Dockerfile. Look at the package documentation for details.\nI’m using this code to generate my Dockerfile\nfrom pins import board_folder\nfrom vetiver import prepare_docker\n\nboard = board_folder(\"/data/model\", allow_pickle_read=True)\nprepare_docker(board, \"penguin_model\", \"docker\")\nOnce you’ve generated your Dockerfile, take a look at it. Here’s the one for my model:\n\n\nDockerfile\n\n# # Generated by the vetiver package; edit with care\n# start with python base image\nFROM python:3.9\n\n# create directory in container for vetiver files\nWORKDIR /vetiver\n\n# copy  and install requirements\nCOPY vetiver_requirements.txt /vetiver/requirements.txt\n\n#\nRUN pip install --no-cache-dir --upgrade -r /vetiver/requirements.txt\n\n# copy app file\nCOPY app.py /vetiver/app/app.py\n\n# expose port\nEXPOSE 8080\n\n# run vetiver API\nCMD [\"uvicorn\", \"app.app:api\", \"--host\", \"0.0.0.0\", \"--port\", \"8080\"]\n\nThis auto-generated Dockerfile is very nicely commented, so its easy to follow.\n\n\n\n\n\n\nNote\n\n\n\nThis container follows the best practices from Chapter 2. We’d expect the model to be updated much more frequently than the container itself, so the model isn’t built into the container. Instead, the container knows how to fetch the model using the {pins} package.\n\n\nNow build the container using docker build -t penguin-model ..\nYou can run the container using\ndocker run --rm -d \\\n  -p 8080:8080 \\\n  --name penguin-model \\\n  penguin-model\nIf you go to http://localhost:8080 you’ll find that…it doesn’t work? Why? If you run the container attached (remove the -d from the run command) you’ll get some feedback that might be helpful.\nIn line 15 of the Dockerfile, we copied the app.py in to the container. Let’s take a look at that file to see if we can find any hints.\n\n\napp.py\n\nfrom vetiver import VetiverModel\nimport vetiver\nimport pins\n\n\nb = pins.board_folder('./model', allow_pickle_read=True)\nv = VetiverModel.from_pin(b, 'penguin_model', version = '20230422T102952Z-cb1f9')\n\nvetiver_api = vetiver.VetiverAPI(v)\napi = vetiver_api.app\n\nLook at that (very long) line 6. The API is connecting to a local directory to pull the model. Is your spidey sense tingling? Something about container filesystem vs host filesystem?\nThat’s right – we put our model at /data/model on our host machine. But the API inside the container is looking for /data/model inside the container, which doesn’t exist!\nThis is a case where we need to mount a volume into the container like so\ndocker run --rm -d \\\n  -p 8080:8080 \\\n  --name penguin-model \\\n  -v /data/model:/data/model \\\n  penguin-model-local\nAnd NOW you should be able to get your model up in no time.\n\n6.5.1 Lab Extensions\nRight now, logs from the API just stay inside the container instance. But that means that the logs go away when the container does. That’s obviously bad if the container dies because something goes wrong.\nHow might you make sure that the container’s logs get written somewhere more permanent?"
  },
  {
    "objectID": "chapters/sec1/1-6-docker.html#footnotes",
    "href": "chapters/sec1/1-6-docker.html#footnotes",
    "title": "6  Demystifying Docker",
    "section": "",
    "text": "This was truer before the introduction of M-series chips for Macs. Chip architecture differences fall below the level that a container captures, and many popular containers wouldn’t run on new Macs. These issues are getting better over time and will probably fully disappear relatively soon.↩︎\nThe big three container registries are AWS Elastic Container Registry (ECR), Azure Container Registry, and Google Container Registry.↩︎\nYou may also see ENTRYPOINT, which sets the command CMD runs against. Usually the default /bin/sh -c to run CMD in the shell will be the right choice.↩︎"
  },
  {
    "objectID": "chapters/sec2/2-0-sec-intro.html#getting-and-running-a-server",
    "href": "chapters/sec2/2-0-sec-intro.html#getting-and-running-a-server",
    "title": "IT/Admin for Data Science",
    "section": "Getting and running a server",
    "text": "Getting and running a server\nThe most common way to get a server for data science is to rent one from a cloud provider. In order to do data science tasks, many people combine their server with other services from the cloud provider. That’s why Chapter 7 is an introduction to what the cloud is and how you might want to use it for data science purposes.\nUnlike your phone or personal computer, you’ll never touch this cloud server you’ve rented. Instead, you’ll administer the server via a virtual interface from your computer. Moreover, servers generally don’t even have the kind of point-and-click interface you’re used to on your personal devices.\nInstead, you’ll access and manage your server from the text-only command line.That’s why Chapter 8 is all about how to set up the command line on your local machine to make it convenient and ergonomic and how to connect to your server for administration purposes using a technology called SSH.\nUnlike your Apple, Windows, or Android operating systems you’re used to on your personal devices, most servers run the Linux operating system. Chapter 9 will teach you a little about what Linux is and will introduce you to the basics of Linux administration including how to think about files and users on a multi-tenant server.\nBut you’re not interested in just running a Linux server. You want to use it to accomplish data science tasks. In particular, you’ll want to install data science tools like R, Python, RStudio, JupyterHub, and more. So you’ll need to learn how to install, run, and configure applications on your server. That’s why Chapter 10 is about application administration.\nWhen your phone or computer gets slow or you run out of storage, it’s probably time for a new one. But a server is a working machine that can be scaled up or down to accommodate more people or heavier workloads over time. That means that you may have to manage the server’s resources much more actively than your personal devices. That’s why Chapter 11 is all about managing and scaling server resources."
  },
  {
    "objectID": "chapters/sec2/2-0-sec-intro.html#making-it-safely-accessible",
    "href": "chapters/sec2/2-0-sec-intro.html#making-it-safely-accessible",
    "title": "IT/Admin for Data Science",
    "section": "Making it (safely) accessible",
    "text": "Making it (safely) accessible\nUnless you’re doing something really silly, your personal devices aren’t accessible to anyone who isn’t physically touching the device. In contrast, most servers are only useful because they’re addressable on a computer network, perhaps even the open internet.\nMaking a server accessible to people over the internet makes it useful, but it also introduces risk. Many dastardly plans for your personal devices are thwarted because a villain would have to physically steal it to get access. For a server, allowing digital access means there are many more potential threats looking to steal data or hijack your computational resources for nefarious ends. You’ve got to be careful about how you’re providing access to the machine.\nMoreover, risk aside, computer networking is a complicated topic, and making it work right can be somewhat difficult. Following random tutorials on the internet is a great way to eventually get your server working, but have no idea what happened or why it suddenly works.\nThe good news is that it’s not magic. Chapter 12 is all about how computers find each other across a network. Once you understand the basic structure and operations of a computer network, making only the things you intend to be public on your server will be much easier.\nAside from a basic introduction to computer networking, there are two other things you’ll want to configure to make your server safe and accessible. The first is to host your server at a human-friendly URL, which you’ll learn how to configure in Chapter 13. The second is to add SSL/TLS to your server to secure the traffic going to and from your server. You’ll learn how to do that in Chapter 14.\nOnce you’ve finished these chapters, you’ll have a basic understanding of all the main topics in IT/Admin that are likely to come up as you try to administer a simple data science workbench or project hosting platform."
  },
  {
    "objectID": "chapters/sec2/2-0-sec-intro.html#labs-in-this-section",
    "href": "chapters/sec2/2-0-sec-intro.html#labs-in-this-section",
    "title": "IT/Admin for Data Science",
    "section": "Labs in this Section",
    "text": "Labs in this Section\nIn the first section of the book, the labs involved creating a DevOps-friendly data science project. In this section, the labs will revolve around actually putting that project into production.\nYou’ll start by standing up an AWS EC2 instance, configuring your local command line, and connecting to the server via SSH. Once you’ve done that, you’ll learn how to create users on the server and access the server as a particular user.\nAt that point, you’ll be ready to transition into data science work. You’ll add R, Python, RStudio Server, and JupyterHub to your server and get them configured for work. Additionally, you’ll deploy the Shiny App and API you created in the book’s first section onto the server.\nOnce the server itself is configured, you’ll need to configure the server’s networking to make it accessible and secure. You’ll learn how to open the proper ports and configure a proxy to access multiple services on the same server, and you’ll learn to configure DNS records so your server is available at a real URL and SSL so it can all be done securely.\nBy the time you’ve finished the labs in this section, you’ll be able to use your EC2 instance as a data science workbench and add your penguin mass prediction Shiny App to the Quarto website you created in the book’s first section.\nFor more details on what you’ll do in each chapter, see Appendix C."
  },
  {
    "objectID": "chapters/sec2/2-1-cloud.html#the-cloud-is-rental-servers",
    "href": "chapters/sec2/2-1-cloud.html#the-cloud-is-rental-servers",
    "title": "7  The Cloud",
    "section": "7.1 The cloud is rental servers",
    "text": "7.1 The cloud is rental servers\nAt one time, the only way to get servers was to buy physical machines and hire someone to install and maintain them. This is called running the servers on-prem (short for premises). There are some organizations, especially those with highly sensitive data, that still run on-prem servers.\nThe problem is that on-prem servers require a large up-front investment in server hardware and professional capacity. If your company has a use case that only requires a single server or one with uncertain payoff, it probably isn’t worth it to hire someone and buy a bunch of hardware.\nAround the year 2000, Amazon took all the server farms across the company and centralized them so teams would use this central server capacity instead of running their own. Over the next few years, Amazon execs (correctly) realized that other companies and organizations would value this ability to rent server capacity. They launched this “rent a server” business as AWS in 2006.\nThese days, the cloud platform business is enormous – collectively nearly a quarter of a trillion dollars. It’s also highly profitable. AWS was only 13% of Amazon’s revenue in 2021, but a whopping 74% of the company’s profits for that year.2\nAWS is still the biggest cloud platform by a considerable margin, but it’s far from alone. Approximately 2/3 of the market consists of the big three – AWS, Microsoft Azure, and Google Cloud Platform (GCP) with the final third made up of numerous smaller companies.3"
  },
  {
    "objectID": "chapters/sec2/2-1-cloud.html#real-and-fake-cloud-benefits",
    "href": "chapters/sec2/2-1-cloud.html#real-and-fake-cloud-benefits",
    "title": "7  The Cloud",
    "section": "7.2 Real (and fake) cloud benefits",
    "text": "7.2 Real (and fake) cloud benefits\nThe cloud arrived with an avalanche of marketing fluff. Over a decade after the cloud went mainstream, it’s clear that some of the purported benefits are real and some are less so. You can be a much more intelligent cloud consumer if you understand which is which.\nThe most important cloud benefit is flexibility. Moving to the cloud allows you to get a new server or re-scale an existing one in minutes, and you only pay for what you use, often on an hourly basis.4 The risk of incorrectly guessing how much capacity you’ll need is drastically reduced, making it way less risky to get started on a server.\nThe other big benefit of the cloud is that it allows IT/Admin teams to narrow their scope and focus. For most organizations, managing physical servers isn’t part of their core competency and outsourcing that work to a cloud provider is a great choice.\n\n\n\n\n\n\nNote\n\n\n\nOne other dynamic is the incentives of individual IT/Admins. As technical professionals, IT/Admins want evidence on their resumes that they have experience with the latest and greatest technologies, which are generally cloud services these days rather than managing physical hardware.\n\n\nAlong with these very real benefits, the cloud was supposed to enable big time savings relative to on-prem operations. For the most part, that hasn’t materialized.\nThe theory was that the cloud would enable organiations to scale their capacity to match need at any given moment. So even if the hourly price was higher, the organization would turn servers off at night or during slow periods and save money.\nIt turns out that dynamic server scaling loads takes a fair amount of engineering effort and only the most sophisticated IT/Admin organizations have implemented effective autoscaling. And even for the organizations that do autoscale, cloud providers are very good at pricing their products to capture a lot of those savings.\nSome large organizations with stable workloads have actually started doing cloud repatriations – bringing workloads back on-prem for significant cost savings. An a16z study found that for certain organizations, the total cost of repatriated workloads, including staffing, could be 1/3 to 1/2 the cost of using a cloud provider.5\nThat said, even if the cash savings aren’t meaningful, the cloud is a key enabler for many businesses. The ability to start small, focus on what matters, and scale up quickly is more than worth it.\nYou may be interested in buying a physical server or re-purposing an old computer just for fun. You’re in good company; I’ve run Ubuntu Server on more than one old laptop. But if you’re trying to spend more time getting important things done and less time playing, getting a server from the cloud is the way to go."
  },
  {
    "objectID": "chapters/sec2/2-1-cloud.html#understanding-cloud-services",
    "href": "chapters/sec2/2-1-cloud.html#understanding-cloud-services",
    "title": "7  The Cloud",
    "section": "7.3 Understanding cloud services",
    "text": "7.3 Understanding cloud services\nIn the beginning, cloud providers did just one thing – rent you a server. But they didn’t stop there. Instead, they started building layers and layers of services that abstract away more IT/Admin tasks.\nAt the end of the day, all cloud services boil down to “rent me an \\(\\text{X}\\)”. As a data scientist, you should start by decoding “What is the \\(\\text{X}\\)?”\nUnfortunately, cloud marketing materials aren’t usually oriented to the data scientist trying to decide whether to use the services, instead they’re oriented at your boss and your boss’s boss, who wants to hear about benefits of using the service. That can make it difficult to decode what \\(\\text{X}\\) is.\nIt’s helpful to keep in mind that, at the end of the day, every service that isn’t directly renting a server is just renting server that already has certain software pre-installed and configured.6\n\n\n\n\n\n\nLess of serverless computing\n\n\n\nYou might hear people talking about going serverless. There is no such thing as serverless computing. Serverless is a marketing term meant to convey that you don’t have to manage the servers. The cloud provider manages them for you, but they’re still there.\n\n\nCloud services are sometimes grouped into three layers to indicate whether you’re renting a basic computing service or something more complete.\nAn analogy to a more familiar layered object may serve to make things clear. Let’s say you’re throwing a birthday party for a friend. You’re planning to bring a chocolate layer cake with vanilla frosting topped with lavender rosettes and “Happy Birthday!” in teal.7\n\n\n\n\n\n\nBig Three Service Naming\n\n\n\nIn this next section, I’ll mention services for common tasks from the big three. AWS tends use cutesy names that have a tangential relationship to the task at hand. Azure and GCP name their offerings more literally.\nThis makes AWS names a little harder to learn, but much easier to recall once you’ve learned them. A table of all the services mentioned in this chapter is in Appendix D.\n\n\n\n7.3.1 IaaS Offerings\nInfrastructure as a service (IaaS, pronounced eye-ahzz) is the basic rent a server premise from the earliest days of the cloud.\n\n\n\n\n\n\nNote\n\n\n\nWhen you rent a server from a cloud provider, you usually not renting a whole server. Instead, you’re renting a virtualized server or a virtual machine (vm). What you see as your server is probably just a part of a much larger physical server that you’re sharing with other customers of the cloud provider.\n\n\nFrom a data science perspective, a IaaS offering might look like what we’re doing in the lab in this book – acquiring a server, networking, and storage from the cloud provider and assembling it into a data science workbench. This is definitely the best way to learn how to administer a data science environment and it’s the cheapest option, but it’s also the most time-consuming.\nThis would be like choosing to go to the grocery store, buy all the ingredients, and bake and decorate your friend’s cake from scratch.\nAlong with a server, you’ll rent storage and networking to make everything work properly.\nSome common IaaS services you’re likely to use include:\n\nRenting a server from AWS with EC2 (Elastic Cloud Compute), from Azure with Azure VMs, and from GCP with Google Compute Engine Instances.\nUnlike your laptop, rented servers don’t include a hard drive, so you’ll have to attach storage with AWS’s EBS (Elastic Block Store), Azure Managed Disk, or Google Persistent Disk.\nCreating and managing the networking where your servers sit with AWS’s VPC (Virtual Private Cloud), Azure’sVirtual Network, and GCP’s Virtual Private Cloud.\nManaging DNS records via AWS’s Route 53, Azure DNS, and Google Cloud DNS. (More on what this means in Chapter 13.\n\nWhile IaaS means the IT/Admins don’t have to be responsible for physical management of servers, they’re responsible for everything else, including keeping the servers updated and secured. For that reason, many organizations are moving away from IaaS towards something more managed these days.\n\n\n7.3.2 PaaS Offerings\nIn a PaaS (Platform as a Service) solution, you hand off management of the servers, but manage the applications you need via an API specific to that service. From a data science perspective, a PaaS setup might look like hosting a JupyterHub, RStudio, or Posit implementation in EKS, or running an ML API in Lambda.\nIn the cake baking world, PaaS would be like buying a pre-made cake and some tins of frosting and doing only the writing and rosettes yourself.\nOne PaaS service that already came up in the book is blob (Binary Large Object) storage. Blob storage allows you to store individual objects somewhere and recall them to any other machine that has access to the blob store. The major blob stores are AWS’s S3 (Simple Storage Service), Azure Blob Storage, and Google Cloud Storage.\nYou’re also likely to make use of cloud-based database, data lake, and data warehouse offerings. There are numerous different offerings, and the ones that you use will depend a lot on your use case and your organization. The ones I’ve seen used most frequently are RDS or Redshift from AWS, Azure Database or Azure Datalake, and Google Cloud Database and Google BigQuery. This category also includes a number of offerings from outside the big three, most notably Snowflake and Databricks.\nDepending on your organization, you may also use services that run APIs or applications from containers or machine images like AWS’s ECS (Elastic Container Service), Elastic Beanstalk, or Lambda, Azure’s Container Apps or Functions, or GCP’s App Engine or Cloud Functions.\nIncreasingly, organizations are turning to Kubernetes as a way to host services. (More on that in Chapter 17.) Most organizations who do so use a cloud provider’s Kubernetes cluster as a service: AWS’s EKS (Elastic Kubernetes Service) or Fargate, Azure’s AKS (Azure Kubernetes Service), or GCP’s GKE (Google Kubernetes Engine).\nMany organizations are moving to PaaS solutions for hosting applications for internal use. It takes away the hassle of managing and updating actual servers. On the flipside, these offerings are somewhat less flexible than just renting a server, and some applications don’t run well in these environments.\n\n\n7.3.3 SaaS Offerings\nSaaS (Software as a Service) is where you just rent the end-user software for usage, often on the basis of seats or usage. You’re already used to consumer SaaS software like Gmail, Slack, and Office365.\nThe cake equivalent of SaaS would be just going to a bakery and buying a cake for your friend.\nDepending on your organization, you might use a SaaS data science offering like AWS’s SageMaker, Azure’s Azure ML, or GCP’s Vertex AI or Cloud Workstations.\nThe great thing about SaaS offerings is that you get immediate access to the end-user application and it’s usually trivial (aside from cost) to add more users. IT/Admin configuration is generally limited to hooking up integrations, most often authentication and/or data sources.\nThe tradeoff for this ease is that they’re generally more expensive and you’re at the mercy of the provider for configuration and upgrades.\n\n\n7.3.4 Notes on Data Storage\nIn most cases, you’ll have data storage handed to you by an IT/Admin. Here are my thoughts on different kinds of data stores.\nRedshift, Azure Datalake, BigQuery, Databricks, and Snowflake are full-fledged data warehouses that catalog and store data from all across your organization. As a data scientist, you might use one of these, but you probably won’t (and shouldn’t) set one up.\nHowever, it’s likely that you’ll own one or more databases within the data warehouse, and you may have to choose what kind of database to use.\nIn my experience, Postgres is good enough for most things involving rectangular data of moderate size. And if you’re storing non-rectangular data, you can’t go wrong with bucket storage. There are more advanced options, but unless you’ve tried the combo of a Postgres database and bucket storage and found it lacking, I probably wouldn’t do anything more complicated.\n\n\n7.3.5 Common Services\nIrrespective of the particular services you want to use, there are a few basic services you’ll almost certainly have to interact with.\nRegardless of what you’re trying to do, if you’re working in the cloud, you have to make sure that the right people have the right permissions. In order to manage these permissions, AWS has IAM (Identity and Access Management), GCP has Identity Access Management, and Azure has Microsoft Entra ID, which was called Azure Active Directory until the summer of 2023. Your organization might integrate these services with a SaaS identity management solution like Okta or OneLogin.\nAdditionally, some cloud services are geographically specific. Each of the cloud providers has split the world into a number of geographic areas, which they all call regions.\nSome services are region-specific and can only interact with other services in that region by default. If you’re doing things yourself, I recommend just choosing the region where you live and putting everything there. Costs and service availability does vary somewhat across region, but it shouldn’t be materially different for what you’re trying to do.\nRegions are subdivided into availability zones (AZs). AZs are subdivisions of regions that are designed to be independent. Some organizations want to run services that span multiple availability zones to provide protection against outages in any particular geography. If you’re running something sophisticated enough to need multi-AZ configuration, you should really be working with a professional IT/Admin."
  },
  {
    "objectID": "chapters/sec2/2-1-cloud.html#comprehension-questions",
    "href": "chapters/sec2/2-1-cloud.html#comprehension-questions",
    "title": "7  The Cloud",
    "section": "7.4 Comprehension Questions",
    "text": "7.4 Comprehension Questions\n\nWhat are two reasons you should consider going to the cloud? What’s one reason you shouldn’t?\nWhat is the difference between PaaS, IaaS, and SaaS? What’s an example of each that you’re familiar with?\nWhat are the names for AWS’s services for: renting a server, file system storage, blob storage"
  },
  {
    "objectID": "chapters/sec2/2-1-cloud.html#introduction-to-labs",
    "href": "chapters/sec2/2-1-cloud.html#introduction-to-labs",
    "title": "7  The Cloud",
    "section": "7.5 Introduction to Labs",
    "text": "7.5 Introduction to Labs\nWelcome to the lab!\nThe point of these exercises is to get you hands on with running servers and get you practicing the things you’re learning in the rest of the book.\nIf you walk through the labs sequentially, you’ll end up with a working data science workbench. It won’t suffice for any enterprise-level requirements, but it’ll be secure enough for a hobby project or even a small team.\nFor this lab, we’re going to use services from AWS, as they’re the biggest cloud provider and the one you’re most likely to run into in the real world. Because we’ll be mostly using IaaS services, there are very close analogs from Azure and GCP should you want to use one of them instead."
  },
  {
    "objectID": "chapters/sec2/2-1-cloud.html#lab-getting-started-with-aws",
    "href": "chapters/sec2/2-1-cloud.html#lab-getting-started-with-aws",
    "title": "7  The Cloud",
    "section": "7.6 Lab: Getting started with AWS",
    "text": "7.6 Lab: Getting started with AWS\nIn this first lab, we’re going to get you up and running with an AWS account and show you how to manage, start, and stop EC2 instances in AWS.\nThe server we’ll stand up will be from AWS’s free tier – so there will be no cost involved as long as you haven’t used up all your AWS free tier credits before now.\n\n\n\n\n\n\nTip\n\n\n\nThroughout the labs, I’ll suggest you name things in certain ways. You can do what you want, but I’ll be consistent with those names, so you can copy commands straight from the book if you use the same name.\nIf you want to follow along in that way, start by creating a standalone directory for this lab, named do4ds-lab.\n\n\n\n7.6.1 Step 1: Login to the AWS Console\nWe’re going to start by logging into AWS at https://aws.amazon.com.\n\n\n\n\n\n\nNote\n\n\n\nAn AWS account is separate from an Amazon account for ordering stuff online and watching movies. You’ll have to create one if you’ve never used AWS before.\n\n\nOnce you’ve logged in, you’ll be confronted by the AWS console. There are a ton of things here. Poke around if you want and then continue when you’re ready.\n\n\n7.6.2 Step 2: Stand up an EC2 instance\nThere are five attributes about your EC2 instance you’ll want to configure. If it’s not mentioned here, just stick with the defaults for now.\nIn particular, just stick with the default Security Group. We’ll get into what they are and how to configure them later.\n\n7.6.2.1 Name + Tags\nInstance name and tags are human-readable labels so you can remember what this instance is. Neither name nor tag are required, but I’d recommend you name the server something like do4ds-lab in case you stand up others later.\nIf you’re doing this at work, there may be tagging policies so that the IT/Admin team can figure out who servers belong to later.\n\n\n7.6.2.2 Image\nAn image is a snapshot of a system and serves as the starting point for your server. AWS’s are called AMIs (Amazon Machine Images). They range from free images of bare operating system to paid images that come bundled with software you might want.\nChoose an AMI that’s just the newest LTS Ubuntu operating system. As of this writing, that’s 22.04. It should say free tier eligible.\n\n\n7.6.2.3 Instance Type\nThe instance type identifies the capability of the machine you’re renting. An instance type is made up of a family and a size. The family is the category of server and is denoted by letters and numbers, so there are T2s and T3s, C4s, R5s, and many more.\nWithin each family, there are different sizes. Possible sizes vary by the family, but generally range from nano to multiples of xlarge like 24.xlarge.\nFor now, I’d recommend you get the largest server that is free tier eligible. As of this writing, that’s a t2.micro with 1 CPU and 1 Gb of memory.\n\n\n\n\n\n\nServer sizing for the lab\n\n\n\nA t2.micro with 1 CPU and 1 Gb of memory is a very small server. For example, your laptop probably has at least 8 CPUs and 16 Gb of memory.\nA t2.micro should be sufficient for finishing the lab, but you’ll need a substantially larger server to do any real data science work.\nLuckily, it’s easy to upgrade cloud server sizes later. More on how, as well as advice on sizing servers for real data science work in Chapter 11.\n\n\n\n\n7.6.2.4 Keypair\nThe keypair is the skeleton key to your server. We’ll get more into how to use and configure it in Chapter 8. For now, create a new keypair. I’d recommend naming it do4ds-lab-key. Download the .pem version and put it in your do4ds-lab directory.\n\n\n7.6.2.5 Storage\nBump up the storage to as much as you can get under the free tier, because why not? As of this writing, that’s 30 Gb.\n\n\n\n7.6.3 Step 3: Start the Server\nIf you followed these instructions, you should now be looking at a summary that lists the operating system, server type, firewall, and storage. Go ahead an launch your instance.\nIf you go back to the EC2 page and click on Instances you can see your instance as it comes up. When it’s up, it will transition to State: Running.\n\n\n7.6.4 Optional: Stop the Server\nWhenever you’re stopping for the day, you may want to suspend your server so you’re not using up your free tier hours or paying for it. You can suspend an instance in the state it’s in so it can be restarted later. Suspended instances aren’t always free, but they’re generally very cheap.\nWhenever you want to suspend your instance, go to the EC2 page for your server. Under the Instance State drop down in the upper right, choose Stop Instance.\nAfter a couple minutes the instance will stop. Before you come back to the next lab, you’ll need to start the instance back up so it’s ready to go.\nIf you want to completely delete the instance at any point, you can choose to Terminate Instance from that same Instance State dropdown."
  },
  {
    "objectID": "chapters/sec2/2-1-cloud.html#lab-put-the-penguins-data-and-model-in-s3",
    "href": "chapters/sec2/2-1-cloud.html#lab-put-the-penguins-data-and-model-in-s3",
    "title": "7  The Cloud",
    "section": "7.7 Lab: Put the penguins data and model in S3",
    "text": "7.7 Lab: Put the penguins data and model in S3\nWhether or not you’re hosting your own server, most data scientists working at an organization that uses AWS will run into S3, AWS’s blob store.\nOne really common thing to store in S3 is an ML model. So we’re going to store the mass prediction model we created in Chapters Chapter 2 and Chapter 3 in an S3 bucket.\n\n7.7.1 Step 1: Create an S3 bucket\nTo start off with, you’ll have to create a bucket, most commonly from the AWS console. I’m naming mine do4ds-lab.\n\n\n7.7.2 Step 2: Push new models to S3\nLet’s change the code in our Quarto doc to push the model into S3 when the model rebuilds, instead of just saving it locally.\nThere are a variety of different ways to access an S3 bucket. The simplest way is by using the AWS CLI on the command line. There are also R and Python packages for interacting with S3 (and other AWS services). The most common are Python’s {boto3} package or R’s {paws} and {aws.s3}.\nRegardless of what tooling you’re using, you’ll generally configure your credentials in three environment variables – AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, and AWS_REGION. You can get the access key and secret access key from the AWS console and you should know the region.\nAs always, when you’re developing in Python or R, I’d recommend putting these into a .env or a config.yml file and loading them from there.\nSince we built the model using {vetiver}, it’s really easy to push the model to S3 just by changing the board type to board_s3 and making sure our credentials are defined in an environment variable.\nIt’ll look something like this.\n\n\nmodel.qmd\n\nfrom pins import board_s3\nfrom vetiver import vetiver_pin_write\n\nboard = board_s3(\"do4ds-lab\", allow_pickle_read=True)\nvetiver_pin_write(board, v)\n\nUnder the hood, {vetiver} is making use of standard R and Python tooling to access an S3 bucket.\nInstead of using credentials, you could configure an instance profile using IAM, so the entire EC2 instance has access to the S3 bucket without needing credentials. Configuring instance profiles is the kind of thing you should work with a real IT/Admin to do.\n\n\n7.7.3 Step 3: Pull the API model from S3\nYou’ll also have to configure the API to load the model from the S3 bucket. Luckily, this is very easy. Just update the script you used to build your Dockerfile so it pulls from the pin in the S3 bucket rather than the local folder.\nNow, the script to build the Dockerfile looks like this:\n---\ntitle: \"Prepare Dockerfile\"\nformat:\n  html:\n    code-fold: true\n---\n\n## Load Environment\n\n```{python}\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nfrom pins import board_s3\nfrom vetiver import vetiver_prepare_docker\n\nboard = board_s3(\"do4ds-lab\", allow_pickle_read=True)\nvetiver_prepare_docker(board, \"penguin_model\")\n```\n\n\n7.7.4 Step 4: Give GitHub Actions S3 credentials\nWe want our model building to correctly push to S3 even when it’s running in GitHub Actions, but GitHub doesn’t have our S3 credentials by default, so we’ll need to provide them.\nWe’re going to declare the variables we need in the Render and Publish step of the Action.\nOnce you’re done, that section of the publish.yml should look something like this.\n\n\n.github/workflows/publish.yml\n\non:\n  workflow_dispatch:\n  push:\n    branches: main\n\nname: Quarto Publish\n\njobs:\n  build-deploy:\n    runs-on: ubuntu-latest\n    permissions:\n      contents: write\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v2\n\n      - name: Set up Quarto\n        uses: quarto-dev/quarto-actions/setup@v2\n\n      - name: Install R\n        uses: r-lib/actions/setup-r@v2\n        with:\n          r-version: '4.2.0'\n          use-public-rspm: true\n\n      - name: Setup renv and install packages\n        uses: r-lib/actions/setup-renv@v2\n        with:\n          cache-version: 1\n        env:\n          RENV_CONFIG_REPOS_OVERRIDE: https://packagemanager.rstudio.com/all/latest\n\n      - name: Install Python and Dependencies\n        uses: actions/setup-python@v4\n        with:\n          python-version: '3.10'\n          cache: 'pip'\n      - run: pip install jupyter\n      - run: pip install -r requirements.txt\n\n      - name: Render and Publish\n        uses: quarto-dev/quarto-actions/publish@v2\n        with:\n          target: gh-pages\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}\n          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n          AWS_REGION: us-east-1\n\nNow, unlike the GITHUB_TOKEN secret, which GitHub Actions automatically provides to itself, we’ll have to provide these secrets to the GitHub interface.\n\n\n7.7.5 Lab Extensions\nYou might also want to put the actual data you’re using into S3. This can be a great way to separate the data from the project, as recommended in Chapter 2.\nPutting the data in S3 is such a common pattern that DuckDB allows you to directly interface with parquet files stored in S3."
  },
  {
    "objectID": "chapters/sec2/2-1-cloud.html#footnotes",
    "href": "chapters/sec2/2-1-cloud.html#footnotes",
    "title": "7  The Cloud",
    "section": "",
    "text": "Yes, that is a Sound of Music reference.↩︎\nhttps://www.visualcapitalist.com/aws-powering-the-internet-and-amazons-profits/↩︎\nhttps://www.statista.com/chart/18819/worldwide-market-share-of-leading-cloud-infrastructure-service-providers/↩︎\nAlthough these days a huge amount of cloud spending is done via annual pre-commitments. The cloud providers offer big discounts for making an up-front commitment, which the organization then spends down over the course of the year.↩︎\nhttps://a16z.com/2021/05/27/cost-of-cloud-paradox-market-cap-cloud-lifecycle-scale-growth-repatriation-optimization/↩︎\nThere are also some wild services that do specific things, like let you rent you satellite ground station infrastructure or do Internet of Things (IoT) workloads. Those services are really cool, but so far outside the scope of this book that I’m fine with talking like they don’t exist.↩︎\nIf you’re planning my birthday party, this is the correct cake configuration.↩︎"
  },
  {
    "objectID": "chapters/sec2/2-2-cmd-line.html#getting-the-command-line-you-want",
    "href": "chapters/sec2/2-2-cmd-line.html#getting-the-command-line-you-want",
    "title": "8  Customizing the command line",
    "section": "8.1 Getting the command line you want",
    "text": "8.1 Getting the command line you want\nAs you get started on the command line, you’ll soon realize that some customization is in order. Maybe the colors aren’t quite right, or you want shortcuts for commands you type a lot, or you want more information in the default display.\nSome people might argue that customizing your command line isn’t the best use of your time and energy. Those people are no fun. Having a command line that behaves exactly as you like will speed up your work and make you feel like a hacker.\nBut as you get started, you’ll soon find yourself neck deep in Stack Overflow posts on how to customize your .bashrc. Or wait, is it the .zshrc? Or…\nThe reason customizing your command line is somewhat confusing is that the command line you interact with is actually two or three programs that sit on top of each other. You can mix-and-match options for each of them and configure them in a variety of ways.\n\n\n\n\n\n\nNotes on operating systems\n\n\n\nI’ve been using the command line in MacOS for many years, so I have strong opinions to share in this chapter.\nI haven’t used a Windows machine in many years. I’ve collected some recommendations, but I can’t personally vouch for them the same way.\nI don’t include Linux recommendations because people who use Linux on their desktop have already gone deep down the customization rabbit hole and don’t need my help wasting their time."
  },
  {
    "objectID": "chapters/sec2/2-2-cmd-line.html#the-terminal",
    "href": "chapters/sec2/2-2-cmd-line.html#the-terminal",
    "title": "8  Customizing the command line",
    "section": "8.2 The terminal",
    "text": "8.2 The terminal\nThe terminal is the GUI where you’ll type in commands. The terminal program you use will dictate the colors and themes available for the window, how tabs and panes work, and the keyboard shortcuts you’ll use to manage them.\nSome programs you might use, like RStudio or VSCode have terminals built into them. If you do basically all your terminal work from one of these environments, you may not need another. But it can be nice to have a standalone terminal program you like.\n\nMacOSWindows\n\n\nI’d recommend against using the built-in terminal app (called Terminal). It’s fine, but there are better options.\nMy personal favorite is the free iTerm2, which adds a bunch of niceties like better theming and multiple tabs.\n\n\nThe built-in terminal is the favorite of many users. There are a variety of alternatives you can try, but feel free to stick with the default."
  },
  {
    "objectID": "chapters/sec2/2-2-cmd-line.html#the-shell",
    "href": "chapters/sec2/2-2-cmd-line.html#the-shell",
    "title": "8  Customizing the command line",
    "section": "8.3 The shell",
    "text": "8.3 The shell\nThe shell is the program that takes the commands you’re typing and runs them. It’s what matches the words you type to actual commands or programs on your system. Depending on which shell you choose, you’ll get different options for plugins and themes.\nThe shell runs anywhere you’ve got a running operating system. So your computer has one shell and your server would have a different one. Even a Docker Container has a shell available. That means that if you do a lot of work on a server, you may need to configure your shell twice – once locally and once on the server.\n\nMacOSWindows\n\n\nThe default shell for MacOS (and Linux) is called bash. I’d advise you to switch it out for zsh, which is the most popular bash alternative.1 Bash alternatives are programs that extend bash with various bells and whistles.\nRelative to bash, zsh has a few advantages out of the box, like better auto-completion. It also has a huge ecosystem of themes to enhance visual appeal and functionality, and plugins that let your command line do everything from displaying your git status to controlling your Spotify playlist.\nI’d recommend looking up instructions for how to install zsh using Homebrew.\n\n\nWindows comes with two shells built in, the Command Shell (cmd) and the PowerShell.\nThe Command Shell is older and has been superseded by PowerShell. If you’re just getting started, you should just work with PowerShell. If you’ve been using Command Shell on a Windows machine for a long time, most Command Shell commands work in PowerShell, so it may be worth switching over."
  },
  {
    "objectID": "chapters/sec2/2-2-cmd-line.html#configuration-management",
    "href": "chapters/sec2/2-2-cmd-line.html#configuration-management",
    "title": "8  Customizing the command line",
    "section": "8.4 Configuration management",
    "text": "8.4 Configuration management\nNow that you’ve got your shell and terminal installed, you’ll want to customize. It is possible to directly customize both zsh and PowerShell. But the best way to configure them is to use a configuration manager for your themes and plugins.\n\nMacOSWindows\n\n\nPrezto is my favorite configuration and plugin manager for zsh. OhMyZsh is also popular and very good. Feel free to choose either, but you can only use one.\nOnce you’ve installed Prezto, you’ve got (at least) three different places you could configure your command line – the iTerm2 preferences, .zshrc, and .zpreztorc. I’d recommend leaving .zshrc alone, customizing the look of the window and the tab behavior in the iTerm2 preferences, and customizing the text theme and plugins via Prezto in the .zpreztorc file.\nI tend to be a pretty light on customization, but I’d recommend looking into git plugins and some of the advanced auto-completion and command history search functionality.\n\n\nMany people like customizing PowerShell with Oh My Posh."
  },
  {
    "objectID": "chapters/sec2/2-2-cmd-line.html#text-editors",
    "href": "chapters/sec2/2-2-cmd-line.html#text-editors",
    "title": "8  Customizing the command line",
    "section": "8.5 Text Editors",
    "text": "8.5 Text Editors\nIf you’re working on your command line a lot, you’ll probably be working inside text editors a fair bit. There are many, many options for text editors and people have strong preferences.\nMac OS’s default text editor is called TextEdit and it’s bad. Don’t use it. Windows users get Notepad, which is somewhat better than TextEdit, but still not the best option out there.\nIf you like, you can just edit text files inside your IDE of choice like VS Code or RStudio. Others may prefer a standalone text editor. The most popular these days are probably Sublime or Notepad++ (Windows Only).\nUnlike with the terminal, there’s no deep configuration here. Install one from the web, configure it as you like, and make sure it’s the default for opening .txt and other files you might want to edit in your system preferences."
  },
  {
    "objectID": "chapters/sec2/2-2-cmd-line.html#secure-server-connections-with-ssh",
    "href": "chapters/sec2/2-2-cmd-line.html#secure-server-connections-with-ssh",
    "title": "8  Customizing the command line",
    "section": "8.6 Secure server connections with SSH",
    "text": "8.6 Secure server connections with SSH\nOne common IT/Admin task is remotely accessing a server from the command line on your machine. SSH – short for Secure (Socket) Shell – is a tool for making a secure connection to another computer over an unsecured network. It’s most often used to interact with a server’s command line from the command line of your computer.\nUsing SSH requires invoking the ssh command line interface from your computer (local host) with a username and the address of the remote host (server). For example, connecting to the server at server.example.com as the user alex\n\n\nTerminal\n\n&gt; ssh alex@server.example.com\n\nOnce you run this command, your terminal will open a session to the terminal of the server.\n\n8.6.1 Understanding SSH keys\nBefore any of this can work, you’ll have to configure your SSH keys, which come in a set called keypair. Each keypair consists of a public key and a private key. You’ll register your public key anywhere you’re trying to SSH to, like a server or git host, but your private key must be treated as a precious secret.\nWhen you use the ssh command, your local machine sends a request to open an SSH session to the remote and includes the private key with the request. The remote host verifies the private key with the public key and opens an encrypted connection.\n\nThis seems weird – how can you verify your secret identity with something that you can just spread around publicly? The answer is public key cryptography, which makes it easy to check whether a proffered private key is valid, but nearly impossible to fabricate a private key from a public key.\n\n\n\n\n\n\nNote\n\n\n\nI wish public and private keys were named differently. Calling the private key the key and the public key the lock makes the intent much clearer. But no one asked me.\n\n\nThe key to public key cryptography is mathematical operations that are easy in one direction but really hard to reverse. As a simple example, think of the number \\(91\\) and its prime factors. Do you know what the prime factors of \\(91\\) are offhand? I do not.\nIt’ll probably take you a few minutes to try a bunch, even if you use a calculator. But if I give you the numbers \\(7\\) and \\(13\\), it’s easy to verify that \\(7 * 13 = 91\\).\nIn this example, the number \\(91\\) would be the public key and the prime numbers \\(7\\) and \\(13\\) together would be the private key. This wouldn’t actually make for very good public key cryptography because it doesn’t take more than a few moments to figure out that \\(7\\) and \\(13\\) are prime factors of \\(91\\).\nIn real public key cryptography, the idea is similar, but the mathematical operations are more complex and the numbers much, much bigger. So much so that it’s basically impossible to break public SSH keys through guessing.\nBut that doesn’t make SSH foolproof. While it’s basically impossible to fabricate a private key, it’s totally possible to steal one. Your private key must be kept secret. The best practice is to never move it from the computer where it was created and to never share them.\nIn summary, do what you want with your public keys, but don’t share your private keys. Don’t share your private keys. Seriously, do not share your private keys."
  },
  {
    "objectID": "chapters/sec2/2-2-cmd-line.html#practical-ssh-usage",
    "href": "chapters/sec2/2-2-cmd-line.html#practical-ssh-usage",
    "title": "8  Customizing the command line",
    "section": "8.7 Practical SSH usage",
    "text": "8.7 Practical SSH usage\nNow that you’ve got an understanding of how SSH works, the steps should be easy to remember.\n\nCreate an SSH keypair on any machine you’ll be SSH-ing from (local host).\nPut the public key anywhere you’ll be SSH-ing to (remote host).\nUse the ssh command to connect.\n\nIf you’re working on a server, you’ll probably create at least two keypairs. One on your personal computer to SSH to the server, and one on the server to access outside services that use SSH, like GitHub.\n\n8.7.1 Step 1: Create Keypair\nYou’ll create a keypair on any server you’re SSH-ing from.\nTo create an SSH keypair, you should just follow a tutorial online. The keypair will have two parts. The one that ends in .pub is – you guessed it – the public key.\nIn most cases, you’ll only create one private key on each machine. If you follow standard instructions for creating a key, it will use the default name, probably id_ed25519.2 Sticking with the default name is great because the ssh command will automatically use them. If you don’t use the default name, you’ll have to specify.\n\n\n\n\n\n\nNote\n\n\n\nRemember, you should never move your private key. If you think the answer to a problem you’re having is to move your private key, think again.\nInstead of moving your private key, create a new private key on the machine where you need to use SSH and register a second public key on the remote.\n\n\nSome organizations require that you have a unique key for each service you’re using to make it easier to swap keys in the event of a breach. If so, you won’t be able to use the default key names.\n\n\n8.7.2 Step 2: Register the public keys\nTo register a public key to SSH into a server, you’ll add the public key to the end of the user’s .ssh/authorized_keys file in their home directory. You’ll have to make sure the permissions on the authorized_keys file are correct. More on that in Chapter 9.\nIf you’re registering with a service, like GitHub.com, there’s probably a text box in the GUI to add an SSH key. Google for instructions on how to do it.\n\n\n8.7.3 Step 3: Use SSH\nTo use SSH, you type ssh &lt;user&gt;@&lt;host&gt;. There are also other commands that can use SSH under the hood, like git or scp.\n\n\n\n\n\n\nFor Windows users\n\n\n\nFor a long time, Windows didn’t support SSH out of the box, so SSH-ing from Windows required a separate utility called PuTTY. More recent versions of Windows support using SSH directly in PowerShell or in Windows Subsystem for Linux (WSL). If SSH isn’t enabled on your machine, Google for instructions.\n\n\nIf you have multiple SSH keys or didn’t use the default flag, you can specify a particular key with the -i flag.\nIf you’re using SSH a lot, I’d recommend setting up an SSH config file. An SSH config file allows you to create aliases that are shortcuts to SSH commands including users, hosts, and other details. So if you had a long SSH command like ssh -i my-ssh-key alex@server.example.com, you could shorten it to ssh alex-server or whatever you want.\nOne annoyance about SSH is that they block the terminal they’re using and will break when your computer goes to sleep. Many people also like using the tmux command line utility with SSH to help solve these issues.\ntmux is a terminal multiplexer, which allows you to manipulate terminal sessions from the command line, including putting sessions into the background and making sessions durable through sleeps and other operations. To be honest, I’m mentioning tmux because lots of people love it, but I’ve found the learning curve too steep for it to come into regular usage for me. Your mileage may vary.\nIf you ever run into trouble using SSH, it has one of my favorite debugging modes. Just add a -v to your command for verbose mode. If that’s not enough information, add another v for more verbosity, and if that’s not enough, just add another v for super verbose mode."
  },
  {
    "objectID": "chapters/sec2/2-2-cmd-line.html#comprehension-questions",
    "href": "chapters/sec2/2-2-cmd-line.html#comprehension-questions",
    "title": "8  Customizing the command line",
    "section": "8.8 Comprehension Questions",
    "text": "8.8 Comprehension Questions\n\nDraw a mental map that includes the following: terminal, shell, theme manager, operating system, my laptop\nUnder what circumstances should you move or share your SSH private key?\nWhat is it about SSH public keys that makes them safe to share?"
  },
  {
    "objectID": "chapters/sec2/2-2-cmd-line.html#lab-login-to-the-server",
    "href": "chapters/sec2/2-2-cmd-line.html#lab-login-to-the-server",
    "title": "8  Customizing the command line",
    "section": "8.9 Lab: Login to the server",
    "text": "8.9 Lab: Login to the server\nIn the last chapter we got your server up and running. In this lab, we’ll use the provided .pem key to log in for the first time.\n\n8.9.1 Step 1: Grab the server address\nFrom the EC2 page, you can click on the instance ID in blue to see all the details of about your server.\nCopy the Public IPv4 DNS address, which starts with \\(\\text{ec2-}\\) and ends with \\(\\text{amazonaws.com}\\). That little icon on the left of the address copies it. You’ll need it throughout the labs. If you lose it, come back here to get it.\n\n\n\n\n\n\nSet a Server Address Variable\n\n\n\nIn the rest of the labs in this book, I’m going to write the commands using the bash variable SERVER_ADDRESS. That means that if you create that variable, you’ll be able to just copy the commands out of the book.\nFor example, as I write this, my server has the address \\(\\text{ec2-54-159-134-39.compute-1.amazonaws.com}\\). So would set my server address variable on my command line with SERVER_ADDRESS=ec2-54-159-134-39.compute-1.amazonaws.com.\nIf you’re used to R or Python, where it’s best practice to put spaces around =, notice that assigning variables in bash requires no spaces around =.\n\n\n\n\n8.9.2 Step 2: Log on with the .pem key\nThe .pem key you downloaded when you set up the server is the private key for a pre-registered keypair that will let you SSH into your server as the admin user (named ubuntu on a Ubuntu system).\nThe .pem key is just an SSH key, so you can SSH to your server with\n\n\nTerminal\n\nssh -i do4ds-lab-key.pem \\\n  ubuntu@SERVER_ADDRESS\n\nWhen you first try this, you’re probably going to get an alert that looks something like this:\n\n\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\n\n\\@ WARNING: UNPROTECTED PRIVATE KEY FILE! \\@\n\n\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\n\nPermissions 0644 for 'do4ds-lab-key.pem' are too open.\n\nIt is required that your private key files are NOT accessible by others.\n\nThis private key will be ignored.\n\nLoad key \"do4ds-lab-key.pem\": bad permissions\n\nubuntu@ec2-54-159-134-39.compute-1.amazonaws.com: Permission denied (publickey).\nBecause the keypair is so powerful, AWS requires that you restrict the access pretty severely. Before you can use it to open the server, you’ll need to change the permissions. We’ll get into permissions in Chapter 9. Until then, you can just change the permissions by navigating to the right directory with the cd command and running chmod 600 do4ds-lab-key.pem.\nOnce you’ve done that, you should be able to login to your machine as the root user. When you want to exit an SSH session and get back to your machine, you can just type exit.\n\n\n8.9.3 Step 3: Create your own SSH key\nYou really shouldn’t use the AWS-provided .pem key to login to your server after the first time. It’s too powerful. Create a normal SSH key using the instructions earlier in this chapter. In the next lab, we’ll get that SSH key configured for your user on the server."
  },
  {
    "objectID": "chapters/sec2/2-2-cmd-line.html#footnotes",
    "href": "chapters/sec2/2-2-cmd-line.html#footnotes",
    "title": "8  Customizing the command line",
    "section": "",
    "text": "zsh is pronounced by just speaking the letters aloud, zee-ess-aitch. Some people might disagree and say it’s zeesh, but they’re not writing this book, are they?↩︎\nThe pattern is id_&lt;encryption type&gt;. ed25519 is the standard SSH key encryption type as of this writing.↩︎"
  },
  {
    "objectID": "chapters/sec2/2-3-linux.html#a-brief-history-of-linux",
    "href": "chapters/sec2/2-3-linux.html#a-brief-history-of-linux",
    "title": "9  Intro to Linux Administration",
    "section": "9.1 A brief history of Linux",
    "text": "9.1 A brief history of Linux\nA computer’s OS defines how applications can interact with the underlying hardware. OSes dictate how files are stored and accessed, how applications are installed, how network connections work, and more.\nYour laptop probably runs MacOS or Windows. Neither of them is Linux. But Linux is dominant basically everywhere else. Because it’s an open source OS and in order to accommodate that huge variety of use cases and contexts, Linux comes in a wide variety of “flavors” that differ by technical attributes and licensing model. These flavors are called distros, short for distributions.\nBefore the early 1970s, the market for computer hardware and software looked nothing like it does now. Computers of that era had extremely tight linkages between hardware and software. For example, there was no Microsoft Word you could use on a Dell machine or an HP machine or an Apple machine.\nInstead, every hardware company was also a software company. If Example Corp’s computer did text editing, it was because Example Corp had written (or commissioned) text editing software specifically for their machine. If Example Corp’s machine could run a game, Example Corp had written that game just for their computer.\nThen, in the early 1970s, researchers funded by AT&T’s Bell Labs released Unix – the first operating system. Now, there was a piece of middleware that sat between the computer hardware and the end-user software.\nAfter the advent of the OS, the computer market started looking a lot more familiar to 2020s eyes. Hardware manufacturers could build machines that ran Unix and software companies could write applications that ran on Unix.\nThe one issue (for everyone but Bell Labs) was that they were paying Bell Labs a lot of money for licenses to Unix. So in the 1980s, programmers started writing Unix-like OSes. These so-called Unix clones behaved just like Unix, but didn’t include any actual Unix code.2\nIn 1991, Linus Torvalds – then a 21 year-old Finnish grad student – released an open source Unix clone called Linux via a amusingly nonchalant newsgroup posting, saying, “I’m doing a (free) operating system (just a hobby, won’t be big and professional like gnu)…Any suggestions are welcome, but I won’t promise I’ll implement them :-).”3\nObviously, the Linux project outgrew that modest newgroup post. At this point there are over 600 Linux distros, reflecting both the natural fragmentation of popular open source projects, as well as disparate requirements for an OS for your car’s infotainment system versus a smartphone versus the controller for your smart thermostat.\nLuckily, you don’t have to know hundreds of distros. For server use, most organizations standardize on one of a handful. The most common open source distros are Ubuntu or CentOS. Red Hat Enterprise Linux (RHEL) is the most common paid distro.4 Many organizations on AWS are using Amazon Linux, which is independently maintained by Amazon but was originally a RHEL derivative.5\nMost individuals who have a choice in the matter prefer Ubuntu. It’s definitely my personal preference. It’s a little simpler and easier to configure than the others.\n\n\n\n\n\n\nA note on Ubuntu Versioning\n\n\n\nUbuntu versions are numbered by the year and month they were released. Most people use the Long Term Support (LTS) releases, which are released in April of even years.\nUbuntu versions have fun alliterative names, so you’ll hear people refer to releases by name or version. As of this writing, most Ubuntu machines are running Focal (20.04, Focal Fossa) or Jammy (22.04, Jammy Jellyfish)."
  },
  {
    "objectID": "chapters/sec2/2-3-linux.html#bash-basics",
    "href": "chapters/sec2/2-3-linux.html#bash-basics",
    "title": "9  Intro to Linux Administration",
    "section": "9.2 Bash basics",
    "text": "9.2 Bash basics\nLinux is administered from the command line using bash or a bash alternative like zsh. The philosophy behind bash and its derivatives says that you should be able to accomplish anything you want with small programs invoked via a command. Each command should do just one thing, and complicated things should be accomplished by composing commands – taking the output from one as the input to the next.\nInvoking a command is done by typing the command on the command line and hitting enter. If you ever find yourself stuck in a situation you can’t seem to exit, ctrl + c will quit in most cases.\nHelpfully, most bash commands are an abbreviation of the word for what the command does. Unhelpfully, the letters often seem somewhat random.\nFor example, the command to list the contents of a directory is ls, which sorta makes sense. Over time, you’ll get very comfortable with the commands you use frequently.\nBash commands can be modified to behave the way you need them to.\nCommand arguments provide details to the command. They come after the command with a space in between. For example, if I want to run ls on the directory /home/alex, I can run ls /home/alex on the command line.\nSome commands have default arguments. For example the default argument for the ls command is the current directory. So if I’m in /home/alex, I’d get the same thing from either ls or ls /home/alex.\nOptions or flags modify how the command operates and come between the command and arguments. Flags are denoted by having one or more dashes before them. For example, ls allows the -l flag, which displays the output as a list. So, ls -l /home/alex would get the files in /home/alex as a list.\nSome flags themselves have flag arguments. For example, the -D flag allows specifying a format for how the datetime is displayed from ls -l. So running ls -l -D %Y-%m-%dT%H:%M:%S /home/alex lists all the files in /home/alex with the date-time of the last update formatted in ISO-8601 format, which is always the correct format for dates.\nBash commands are always formatted as &lt;command&gt; &lt;flags + flag args&gt; &lt;command args&gt;.\nIt’s nice that this structure is standard. It’s not nice that the main argument is all the way at the end, because it makes long bash commands hard to read. To make commands more readable, you can break the command over multiple lines and include one flag or argument per line. You can tell bash you want it to continue a command after a line break by ending the line with a space and a \\.\nFor example, here’s that long ls command more nicely formatted:\n\n\nTerminal\n\n&gt; ls -l \\\n  -D %Y-%m-%dT%H:%M:%S \\\n  /home/alex\n\nAll of the flags and arguments for commands can be found in the program’s man page (short for manual). You can access the man page for any command with man &lt;command&gt;. You can scroll the man page with arrow keys and exit with q.\nBash is a real – if ugly – programming language, so you can assign variables with &lt;var-name&gt;=&lt;value&gt; (no spaces allowed) and access them $&lt;var-name&gt;. The bash version of print is echo.\nFor example,\n\n\nTerminal\n\n&gt; MSG=\"Hello World!\"\n&gt; echo $MSG\nHello World!\n\nFor the most part, you’ll write commands directly on the command line. It’s also possible to write and run bash scripts that include conditionals, loops, and functions. Bash scripts usually end in .sh and are most often run with the sh command like sh my-script.sh.\nThe advantage of writing bash scripts is that they can run basically anywhere. The disadvantage of writing bash scripts is that bash is a truly ugly programming language that is hard to debug."
  },
  {
    "objectID": "chapters/sec2/2-3-linux.html#commands-run-on-behalf-of-users",
    "href": "chapters/sec2/2-3-linux.html#commands-run-on-behalf-of-users",
    "title": "9  Intro to Linux Administration",
    "section": "9.3 Commands run on behalf of users",
    "text": "9.3 Commands run on behalf of users\nWhenever a program is running in Linux, it is running as a particular user who can be identified by their username.\nOn any Unix-like system, the whoami command returns the username of the active user. So when I run whoami on my MacBook, I get:\n\n\nTerminal\n\n&gt; whoami                                                       \nalexkgold\n\nUsernames have to be unique on the system – but they’re not the true identifier for a user. A user is uniquely identified by their user id (uid), which maps to all the other user attributes like username, password, home directory, groups, and more. The uid for a user is assigned at the time the user is created and usually don’t need to be changed or specified manually.6\nEach human who accesses a Linux server should have their own account. In addition, many applications create service account users for themselves and run as those users. For example, installing RStudio Server will create a user with username rstudio-server. Then, when RStudio Server goes to do something – start an R session for example – it will do so as rstudio-server.\nUser uids start at 10,000 with those below 10,000 reserved for system processes. There’s also one special user – called the admin, root, sudo, or super user who gets the special uid 0.\nUsers belong to groups, which are collections of one or more users. Each user has exactly one primary group and can be a member of secondary groups.7 By default, each user’s primary group is the same as their username.\nLike a user has a uid, a group has a gid. User gids start at 100.\nYou can see a user’s username, uid, groups, and gid with the id command. On my MacBook, I’m a member of a number of different groups, with the primary group staff.\n\n\nTerminal\n\n&gt; id                                                                \nuid=501(alexkgold) gid=20(staff) groups=20(staff),12(everyone),61(localaccounts),79(_appserverusr),80(admin),81(_appserveradm),98(_lpadmin),701(com.apple.sharepoint.group.1),33(_appstore),100(_lpoperator),204(_developer),250(_analyticsusers),395(com.apple.access_ftp),398(com.apple.access_screensharing),400(com.apple.access_remote_ae)\n\nIf you ever need to add users to a server, the easiest way is with the useradd command. Once you have a user, you may need to change the password, which you can do at any time with the passwd command. Both useradd and passwd start interactive prompts, so you don’t need to do much more than run those commands.\nIf you ever need to alter a user – the most common task being to add a user to a group, you would use the usermod command with the -aG flag."
  },
  {
    "objectID": "chapters/sec2/2-3-linux.html#permissions-dictate-what-users-can-do",
    "href": "chapters/sec2/2-3-linux.html#permissions-dictate-what-users-can-do",
    "title": "9  Intro to Linux Administration",
    "section": "9.4 Permissions dictate what users can do",
    "text": "9.4 Permissions dictate what users can do\nIn Linux, everything you can interact with is just a file. Every log – file. Every picture – file. Every application – file. Every configuration – file.\nSo determining whether a user can take an action is really a question of whether they have the right permissions on a particular file.\nBasic Linux permissions (called POSIX permissions) consist of a 3x3 matrix of read, write, and execute for the owner, the owning group, and everyone else. Read means the user can see the contents of a file, write means the user can save a changed version of a file, and execute means they can run the file as a program.\n\n\n\n\n\n\nNote\n\n\n\nThere are more complex ways to manage Linux permissions. For example, you might hear about Access Control Lists (ACLs). They’re beyond the scope of this book.\nThere is more information on different ways organizations manage users and what they’re allowed to do in Chapter 16, which is all about auth.\n\n\nFor example, here’s a set of permissions that you might have for a program if you wanted anyone to be able to run it, group members to inspect it, and only the owner to change it.\n\nDirectories also have permissions – read allows the user see what’s in the directory, write allows the user to alter what’s in the directory, and execute allows the user to enter the directory.\nFile permissions and directory permissions don’t have to match. For example, a user could have read permissions on a directory, so they could see the names of the files, but not actually have read permissions on any of the files, so they can’t look at the contents.\nWhen you’re working on the command line, you don’t get a little grid of permissions. Instead they’re expressed in one of two ways. The first is the string representation, which is a 10-character string that looks something like -rw-r–r--.\nThe first character indicates the type of file: most often - for normal (file) or d for a directory.\nThe next nine characters are indicators for the three permissions for the user, the group, and everyone else. There will be an r for read, a w for write, and an x for execute or - to indicate that they don’t have the permission.\nSo the permissions in the graphic would be -rwxr-x--x for a file and drwxr-x--x for a directory.\nThe best way to get these permissions is to run the ls -l command.\n\n\nTerminal\n\n&gt; ls -l                                                           \n-rw-r--r--  1 alexkgold  staff     28 Oct 30 11:05 config.py\n-rw-r--r--  1 alexkgold  staff   2330 May  8  2017 credentials.json\n-rw-r--r--  1 alexkgold  staff   1083 May  8  2017 main.py\ndrwxr-xr-x 33 alexkgold  staff   1056 May 24 13:08 tests\n\nEach line starts with the string representation of the permissions followed by the owner and group so you can easily understand who should be able to access that file or directory.\nAll of the files in this directory are owned by alexkgold. Only the owner (alexkgold) has write permission, but everyone has read permission. In addition, there’s a tests directory, with read and execute for everyone and write only for alexkgold.\nIn the course of administering a server, you will probably need to change a file’s permissions. You can do so using the chmod command.\nFor chmod, permissions are indicated as a three digit number, like 600, where the first digit is the permission for the user, the second for the group, and the third for everyone else. To get the right number, you sum the permissions as follows: 4 for read, 2 for write, and 1 for execute. You can check for yourself that any set of permissions is uniquely identified by a number between 1 and 7.8\nSo to implement the permissions from the graphic, you’d want the permission set 751 to give the user full permissions (4 + 2 + 1), read and execute (4 + 1) to the group, and execute only (1) to everyone else.\n\n\n\n\n\n\nNote\n\n\n\nIf you spend any time administering a Linux server, you almost certainly will at some point find yourself running into a problem and applying chmod 777 out of frustration to rule out a permissions issue.\nI can’t tell you not to do this – we’ve all been there. But if it’s something important, be sure you change it back once you’re finished figuring out what’s going on.\n\n\nIn some cases you might actually want to change the owner or group of a file. You can change users and groups with either names or ids. You can do so using the chown command. Changing users just uses the username and changing groups get prefixed with a colon.\nIn some cases, you might not be the correct user to take a particular action. If you want to change the user you are, you can use the su (switch user) command. You’ll be prompted for a password to make sure you’re allowed.\nThe admin or root user has full permissions on every file and there are some actions that only the root user can do. When you need to do root-only things, you usually don’t want to su to be the root user. It’s too powerful. Plus if you have user-level configuration, it all gets left behind.\nInstead, individual users can be granted the power to temporarily assume root privileges without changing to be the root user. This is accomplished by making them members of the admin group. If a user is a member of the admin group, they can prefix commands with sudo to run those commands with root privileges.\nThe name of the admin group varies by distro. In Ubuntu, the group is called sudo."
  },
  {
    "objectID": "chapters/sec2/2-3-linux.html#the-linux-filesystem-is-a-tree",
    "href": "chapters/sec2/2-3-linux.html#the-linux-filesystem-is-a-tree",
    "title": "9  Intro to Linux Administration",
    "section": "9.5 The Linux Filesystem is a tree",
    "text": "9.5 The Linux Filesystem is a tree\nRegardless of which Linux distro you’re running, understanding where to find things on your system is crucial.\nAll of the information available to a computer is indexed by its filesystem, which is made up of directories or folders, which are containers for other directories and for files.\nOn your laptop, you’re probably used to browsing the filesystem with your mouse. On your phone, the filesystem is completely obscured by apps, but it’s there.\nOn a Linux server, the only way to traverse the filesystem is with written commands. Having a good mental model for what the filesystem looks like is, therefore, really important.\nOn Linux, the entire filesystem is a tree (or perhaps an upside-down tree). Every directory is contained in by a parent directory, and may contain one or more children or sub-directories. The root directory, / is the base of the tree and is its own parent. A / in between two directories means that it’s a sub-directory.\n\nEvery directory is a sub-directory of / or a sub-directory of a sub-directory of / or…you get the picture. So the /home/alex file path defines a particular location, which is the alex sub-directory of /home, which is a sub-directory of the root directory, /.\n\n\n\n\n\n\nTip\n\n\n\nIt’s never necessary, but sometimes the tree-like layout for a directory is helpful. The tree utility can show you one. It doesn’t always come pre-installed, so you might have to install it.\n\n\nBecause the entire Linux filesystem is based at /, it doesn’t matter what physical or virtual disks you have attached to your system. They will fall somewhere under the main filesystem (often inside /mnt).\nThis will be familiar to MacOS users, because MacOS is based on an operating system called BSD that, like Linux, is a Unix clone.\nIf you’re familiar with Windows, the Linux filesystem may seem a little strange.\nIn Linux, each computer has exactly one filesystem, which is based at the root, /. Network shares or other types of volumes can be mounted somewhere on the filesystem, often below /mnt, but the fact that they’re on separate drives is obscured from the user.\nIn Windows, each physical or logical disk has its own filesystem with its own root. You’re probably familiar with C: as your main filesystem. Your machine may also have a D: drive. If you’ve got network share drives, they’re likely at M: or N: or P:.\nOne other difference is that Windows uses \\ to separate file path elements rather than /. This used to be a big deal, but newer versions of Windows accept file paths using /.\n\n9.5.1 Working with file paths\nWhenever a command runs, it runs at a particular path in the filesystem, called the working directory. You can get the absolute path to your working directory with the pwd command, which is an abbreviation for print working directory.\nWhen you want a command to run the same irrespective of where it’s run from, it’s best to use an absolute path, which is a path specified relative to the root. They operate the same irrespective of the current working directory. Absolute file paths are easy to recognize because they always start with /.\nSometimes it’s convenient to use a relative file path, which starts at the working directory, denoted by .. So, for example, if I want to access the data subdirectory of the working directory, that would be available at ./data.\nThe working directory’s parent is at ... So, you could see everything in the parent directory of your current working directory with ls .. or its parent with ls ../...\nAll accounts that represent actual humans should have a home directory, which usually livelive inside /home.\nThe home directory and all its contents are owned by the user to whom it belongs. The home directory is the user’s space to store the things they need, including user-specific configuration. When navigating the file system, the user can find their home directory is at ~.\nAlong with being able to inspect directories with ls, it’s useful to be able to change your working directory with the cd command, short for change directory. You can use either absolute or relative file paths with cd. So if you were in /home/alex and wanted to navigate to /home, either cd .. or cd /home would work.\nSome files or directories are hidden so they don’t appear in a normal ls. You know a file or directory is hidden because its name starts with .. Hidden files are usually configuration files that you aren’t manipulating in normal usage. These aren’t secret or protected in any way, they’re just skipped by ls for convenience. If you want to display all files in a directory, including hidden ones, you can use the -a flag (for all) with ls.\nYou’ve already seen a couple of hidden files in this book – your GitHub Action was configured in the .github directory and your Prezto configuration was done in the .zpreztorc file. You might also be familiar with .gitignore, .Rprofile, and .Renviron."
  },
  {
    "objectID": "chapters/sec2/2-3-linux.html#moving-files-and-directories",
    "href": "chapters/sec2/2-3-linux.html#moving-files-and-directories",
    "title": "9  Intro to Linux Administration",
    "section": "9.6 Moving files and directories",
    "text": "9.6 Moving files and directories\nYou will frequently need to change where files are on your system, including copying, deleting, moving, and more.\nYou can copy a file or directory from one place to another using the cp command. cp leaves behind the old file or directory and adds the new one at the specified location. You can use the -r flag to recursively copy everything in a directory.\nYou can move a file with the mv command, which does not leave the old file behind. If you want to remove a file entirely, you can use the rm command. The -r (recursive) flag can be used with rm to remove everything within a directory and the -f (force) flag can skip rm double checking you really want to do this.\n\n\n\n\n\n\nWarning\n\n\n\nBe very careful with the rm command, especially with -rf.\nThere’s no recycle bin. Things that are deleted are instantly deleted forever.\n\n\nIf you want to make a directory, mkdir makes a file path. It can be used with relative or absolute file paths, and can include multiple layers of paths to create. For example, if you’re in /home/alex, you could mkdir project1/data to make a project1 directory and data sub-directory.\nThe mkdir command throws an error if you try to create a path that includes some directories that already exist – for example if project1 already existed in the example above. The -p flag can be handy to create only the parts of the path that don’t exist.\nSometimes it’s useful to operate on every file inside a directory. You can get every file that matches a pattern with the wildcard, *. You can also do partial matching with the wildcard to get all the files that match part of a pattern.\nFor example, let’s say I have a /data directory and I want to put a copy of only the .csv files inside into a new data-copy sub-directory. I could do the following:\n\n\nTerminal\n\n&gt; mkdir -p /data/data-copy\n&gt; cp /data/*.csv /data/data-copy\n\n\n9.6.1 Moving things to and from the server\nIt’s very common to have a file on your server you want to move to your desktop or vice versa. There are a few different ways to move files and directories.\nIf you’re moving multiple files, it’s easier to combine them into a single object and move that. The tar command turns a set of files or whole directory into a single archive file, usually with the file suffix .tar.gz. Creating an archive also does some compression. The amount depends on the content.\nIn my opinion, tar is a rare failure of bash to provide standalone commands for anything you need to do. tar is used to both create and unpack (extract) archive files. Telling it which one requires the use of several flags. You’ll basically never use tar without a bunch of flags and the incantation is hard to remember. I google it every time I use it. The flags you’ll use most often are in the cheatsheet in the appendix.\nYou can move files to or from a server with the scp command. scp – short for secure copy – is basically cp, but with an SSH connection in the middle.9\nSince scp establishes an SSH connection, you need to make the request to somewhere that is accepting SSH connections. That means that whether you’re copying something to or from a server, you’ll run scp from a regular terminal on your laptop, not one that’s already SSH-ed into your server.\nRegular ssh options work with scp, like -i and -v."
  },
  {
    "objectID": "chapters/sec2/2-3-linux.html#pipes-and-redirection",
    "href": "chapters/sec2/2-3-linux.html#pipes-and-redirection",
    "title": "9  Intro to Linux Administration",
    "section": "9.7 Pipes and redirection",
    "text": "9.7 Pipes and redirection\nYou can always copy and paste command outputs or write them to a file, but it can also be helpful to just chain a few commands together. Linux provides a few handy operators you can use to make this easy.\nThe simplest operator is the pipe |, which just takes the output of one command and makes it the input for the next command.\nFor example, you might want to see how many files are in a directory. The wc -l (word count, lines) command counts lines, so you could do ls | wc -l since each file returned by ls is counted as a line.\n\n\n\n\n\n\nCeci n’est pas une pipe?\n\n\n\nThe pipe should feel extremely familiar to R users.\nThe pipe from the {magrittr} package, %&gt;%, was introduced in 2013, and is a popular part of the {tidyverse}.10 The {magrittr} pipe was inspired by both the Unix (Linux) pipe and the pipe operator in the F# programming language.\nDue to its popularity, the pipe |&gt; was formally added to the base R language in R 4.1 in 2021.\n\n\nThere are a few operators that write the output of the left hand side into a file.\nThe &gt; command takes the output of a command on the left and writes it as a new file. If the file you specify already exists, it will be overwritten.\nIf you want to append the new text, rather than overwrite, &gt;&gt; appends to the end of the file. I generally default to &gt;&gt;, because it’ll create a new file if one doesn’t exist, and I usually don’t mean to overwrite what’s there.\nA common reason you might want to do this is to add something to the end of your .gitignore. For example, if you want to add your .env file to your .gitignore, you could do that with echo .env &gt;&gt; .gitignore.11 Another great usage is to add a new public key to your .ssh/authorized_keys file.\nThere are times when you want to make files or directories with nothing in them. The touch command makes a blank file at the specified file path. If you touch a preexisting file, it updates the time the file was last updated without actually making any changes. This can be useful because some applications look at the timestamp on files to see if action is needed."
  },
  {
    "objectID": "chapters/sec2/2-3-linux.html#comprehension-questions",
    "href": "chapters/sec2/2-3-linux.html#comprehension-questions",
    "title": "9  Intro to Linux Administration",
    "section": "9.8 Comprehension Questions",
    "text": "9.8 Comprehension Questions\n\nWhat are the parts of a bash command?\nWhere do commands run? How do you know where they’re going to run or specify a relative path?\nHow can you copy, move, or delete a file? What about to or from a server?\nCreate a mind map of the following terms: Operating System, Windows, MacOS, Unix, Linux, Distro, Ubuntu\nWhat are the 3x3 options for Linux file permissions? How are they indicated in an ls -l command?"
  },
  {
    "objectID": "chapters/sec2/2-3-linux.html#lab-set-up-a-user",
    "href": "chapters/sec2/2-3-linux.html#lab-set-up-a-user",
    "title": "9  Intro to Linux Administration",
    "section": "9.9 Lab: Set up a user",
    "text": "9.9 Lab: Set up a user\nWhen you use your server’s .pem key, you login as the root user, but that’s too much power to acquire on a regular basis. Additionally, since your server is probably for multiple people, you’re going to want to create users for them.\nIn this lab, you’ll create a regular user for yourself and add an SSH key for them so you can directly log in from your personal computer.\n\n9.9.1 Step 1: Create a non-root user\nLet’s create a user using the adduser command. This will walk us through a set of prompts to create a new user with a home directory and a password. Feel free to add any information you want – or to leave it blank – when prompted.\nI’m going to use the username test-user. If you want to be able to copy/paste commands, I’d advise doing the same. If you were creating users based on real humans, I’d advise using their names.\n\n\nTerminal\n\n&gt; adduser test-user\n\nWe want this new user to be able to adopt root privileges so let’s add them to the sudo group with\n\n\nTerminal\n\n&gt; usermod -aG sudo test-user\n\n\n\n9.9.2 Step 2: Add an SSH key for your new user\nLet’s register an SSH key for the new user by adding the key from the last lab to the server user’s authorized_users file.\nFirst, you need to get your public key to the server using scp.\nFor me, the command looks like this\n\n\nTerminal\n\n&gt; scp -i ~/Documents/do4ds-lab/do4ds-lab-key.pem \\ \n  ~/.ssh/id_ed25519.pub \\\n  ubuntu@$SERVER_ADDRESS:/home/ubuntu\n\nNote that I’m copying the public key, but SSH access is still using the server’s .pem key because I don’t have another key registered yet.\nNow the public key is on the server, but it’s in the ubuntu user’s home directory. You’ll need to do the following:\n\nCreate .ssh/authorized_keys in test-user’s home directory.\nCopy the contents of the public key you uploaded into the authorized_keys file (recall &gt;&gt;).\nMake sure the .ssh directory and authorized_keys files are owned by test-user with 700 permissions on .ssh and 600 on authorized_keys.\n\nYou could do this all as the admin user, but I’d recommend switching to being test-user at some point with the su command.\n\n\n\n\n\n\nTip\n\n\n\nIf you run into trouble assuming sudo with your new user, try exiting SSH and coming back. Sometimes these changes aren’t picked up until you restart the shell.\n\n\nOnce you’ve done all this, you should be able to log in from your personal computer with ssh test-user@$SERVER_ADDRESS.\nNow that we’re all set up, you should store the .pem key somewhere safe and never use it to log in again."
  },
  {
    "objectID": "chapters/sec2/2-3-linux.html#footnotes",
    "href": "chapters/sec2/2-3-linux.html#footnotes",
    "title": "9  Intro to Linux Administration",
    "section": "",
    "text": "The remainder are mostly Windows servers. There are a few other OSes you might encounter, like Oracle Solaris. There is a product called Mac Server, but it’s just a program for managing Mac desktops and iOS devices, not a server OS.\nThere are also versions on Linux that run on desktop computers. Despite the best efforts of many hopeful nerds, desktop Linux is pretty much only used by professional computer people.↩︎\nOr at least they weren’t supposed to. There’s an interesting history of lawsuits, especially around whether the BSD OS illegally included Unix code.↩︎\nMore in the History of Linux Wikipedia article.\nPedants will scream that the original release of Linux was just the operating system kernel, not a full operating system like Unix. Duly noted, now go away.↩︎\nRHEL and CentOS are related operating systems, but that relationship has changed a lot in the last few years. The details are somewhat complicated, but most people expect less adoption of CentOS in enterprise settings going forward.↩︎\nAs I’m writing this, Amazon Linux 2 is popular, but Amazon Linux 2023 (AL2023) was recently released. I’d expect AL2023 or it’s successor to be dominant by the time you read this.↩︎\nThe one exception to this is when you’ve got the same user accessing resources across multiple machines. Then the uids have to match. If you’re worrying about this kind of thing, it’s probably time to bring in a professional IT/Admin.↩︎\nDepending on your version of Linux, there may be a limit of 16 groups per user.↩︎\nClever eyes may realize that this is just the base-10 representation of a three-digit binary number.↩︎\nIt’s worth noting that scp is now considered “insecure and outdated”. The ways it is insecure are rather obscure and not terribly relevant for most people. But if you’re moving a lot of data, you may want something faster. If so, I’d recommend more modern options like sftp and rsync. I probably wouldn’t bother if you’re only occasionally scp-ing small files to or from your server.↩︎\nThe title of this callout box is also the tagline for the {magrittr} package.↩︎\nNote that echo is needed so that the .env gets repeated as a character string. Otherwise .env would be treated as a command.↩︎"
  },
  {
    "objectID": "chapters/sec2/2-4-app-admin.html#linux-app-install-config",
    "href": "chapters/sec2/2-4-app-admin.html#linux-app-install-config",
    "title": "10  Application administration",
    "section": "10.1 Linux app install + config",
    "text": "10.1 Linux app install + config\nYou can install applications to Linux from a distro-specific repository, much like the app store for your phone or you can just download the application from the internet and install it locally.\nFor Ubuntu, the apt command is used for interacting with repositories of .deb files. For CentOS and RedHat, the yum command is used for installing .rpm files.\n\n\n\n\n\n\nNote\n\n\n\nThe examples below are all for Ubuntu, since that’s what we use in the lab for this book. Conceptually, using yum is very similar, though the exact commands differ somewhat.\n\n\nIn addition to actually installing packages, apt is also the utility for ensuring the lists of available packages you have are up to date with update and that all packages on your system are at their latest version with upgrade. When you find Ubuntu commands online, it’s common to see them prefixed with apt-get update && apt-get upgrade -y. The -y flag bypasses a manual confirmation step.\nPackages are installed with apt-get install &lt;package&gt;. Depending on which user you are, you may need to prefix the command with sudo.\nSometimes, you may want to install packages that aren’t in the repository for your distro. Doing that will generally involve downloading a file directly from a URL – usually with wget and then installing it from the file you’ve downloaded.\n\n10.1.1 Application configuration\nOnce you’ve installed an application, it will generally require some configuration like the default background color, the set of users allowed in, or the frequency something updates.\nOn your personal computer, you’d probably find the setting in a series of dropdown menus at the top of the screen. On a server, no such menu exists.\nFor applications running on your server, application behavior is usually configured through one or more config files. For applications hosted inside a Docker container, behavior is often configured with environment variables, sometimes in addition to config files.\n\n\n10.1.2 Where to find application files\nLinux applications often use several files located in different locations on the filesystem. Here are some of the ones you’ll use most frequently:\n\n/bin, /opt, /usr/local, /usr/bin - installation locations for software.\n/etc - configuration files for applications.\n/var - variable data, most commonly log files in /var/log or /var/lib.\n\nThis means that on a Linux server, the files for a particular application probably don’t all live in the same directory. Instead, you might run the application from the executable in /opt, configure it with files in /etc, and troubleshoot from logs in /var.\n\n\n10.1.3 Command line configuration with vim and nano\nObviously, if you’re administering applications on a server, you’ll need to spend a fair bit of time editing text files. But how? Unlike on your personal computer, where you click a text file to open and edit it, you’ll need to work with a command line text editor when you’re working on a server.\nThere are two command line text editors you’ll probably encounter: nano and vi/vim.1 While they’re both powerful text editing tools, they can also be intimidating if you’ve never used them before.\nYou can open a file in either by typing nano &lt;filename&gt; or vi &lt;filename&gt;.\nAt this point many newbie command line users find themselves completely stuck, unable to do anything – even just exit and try again. But don’t worry, there’s a way out of this labyrinth.\nIf you opened nano, there will be some helpful-looking prompts at the bottom. You’ll see that once you’re ready to go, you can exit with ^x. But the ^ isn’t actually the caret character. On Windows, ^ is short for Ctrl and on Mac it’s for Command (⌘), so Ctrl + x or ⌘ + x will exit.\nWhere nano gives you helpful – if obscure – hints, a first experience in vim is the stuff of command line nightmares. You’ll type words and they won’t appear onscreen. Instead, you’ll experience dizzying jumps around the page and words and lines of text will disappear without a trace.\nThis is because vim uses the letter keys not just to type, but also to navigate the page and interact with vim itself. You see, vim was created before keyboards uniformly had arrow keys.\nWhile they can be intimidating at first, vim keybindings are worth spending some time learning. Vim includes some powerful shortcuts for moving within and between lines and selecting and editing text. Most IDEs you might use, including RStudio, JupyterLab, and VSCode have vim modes.\nWhen you enter vim, you’re in the (poorly named) normal mode, which is for navigation only. Pressing the i key activates insert mode, which will feel normal for those of us used to arrow keys. Once in insert mode, you can type and words will appear and you can navigate with the arrow keys.\nYou can return to normal mode with the escape key.\nOnce you’ve escaped, you may wish never to return to normal mode, but it’s the only way to save files and exit vim. In order to do file operations, you type a colon, :, followed by the shortcut for what you want to do, and enter. The two most common commands you’ll use are save (write) with w and quit with q. You can combine these together, so you can save and quit in one command using :wq.\nSometimes you may want to exit without saving or you might’ve opened and changed a file you don’t actually have permission to edit. If you’ve made changes and try to exit with :q, you’ll find yourself in an endless loop of warnings that your changes won’t be saved. You can tell vim you mean it with the exclamation mark, !, and exit using :q!."
  },
  {
    "objectID": "chapters/sec2/2-4-app-admin.html#reading-logs",
    "href": "chapters/sec2/2-4-app-admin.html#reading-logs",
    "title": "10  Application administration",
    "section": "10.2 Reading logs",
    "text": "10.2 Reading logs\nOnce your applications are up and running, you may run into issues. Or even if you don’t, you may want to take a look at how things are running.\nMost applications write their logs into somewhere inside the /var directory. Some things will get logged to the main log at /var/log/syslog. Other things may get logged to /var/log/&lt;application name&gt; or /var/lib/&lt;application name&gt;.\nIt’s important to get comfortable with the commands to read text files in order to be able to examine logs (and other files). The commands I use most commonly are:\n\ncat is the basic command to print a file, starting at the beginning.\nless prints a file, starting at the beginning, but only a few lines at a time.\nhead prints only the first few lines and exits. It is especially useful to peer at the beginning of a large file, like a csv file – so you can quickly preview the column heads and the first few values.\ntail prints a file going up from the end. This is especially useful for log files, as the newest logs are appended to the end of a file. This is such a common practice that “tailing a log file” is a common phrase.\n\nSometimes, you’ll want to use the -f flag (for follow) to tail a file with a live view as it updates.\n\n\nSometimes you want to search around inside a text file. You’re probably familiar with the power and hassle of regular expressions (regex) to search for specific character sequences in text strings. The Linux command grep is the main regex command.\nIn addition to searching in text files, grep is often useful in combination with other commands. For example, it’s often useful to put the output of ls into grep to search for a particular file in a big directory using the pipe."
  },
  {
    "objectID": "chapters/sec2/2-4-app-admin.html#running-the-right-commands",
    "href": "chapters/sec2/2-4-app-admin.html#running-the-right-commands",
    "title": "10  Application administration",
    "section": "10.3 Running the right commands",
    "text": "10.3 Running the right commands\nLet’s say you want to open Python on your command line. One option would be to type the complete path to a Python install every time. For example, I’ve got a version of Python in /usr/bin, so /usr/bin/python3 works.\nBut in most cases, it’s nice to just type python3 and have the right version open up.\n\n\nTerminal\n\n&gt; python3\nPython 3.9.6 (default, May  7 2023, 23:32:45) \n[Clang 14.0.3 (clang-1403.0.22.14.1)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; \n\nIn some cases, this isn’t optional. Certain applications will rely on others being available, like RStudio Server needing to find R. Or Jupyter Notebook finding your Python kernels.\nSo how does Linux know where to find those applications and files?\nIf you ever want to check which actual executable is being used by a command, you can use the which command. For example, on my system this is the result of which python3.\n\n\nTerminal\n\n&gt; which python3                                                    \n/usr/bin/python3\n\nThe operating system knows how to find the actual runnable programs on your system via the path. The path is a set of directories that the system knows to search when it tries to run a program. The path is stored in an environment variable, conveniently named PATH.\nYou can check your path at any time with echo $PATH. On my MacBook, this is what the path looks like.\n\n\nTerminal\n\n&gt; echo $PATH                                                      \n/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/System/Cryptexes/App/usr/bin:/usr/bin:/bin:/usr/sbin:/sbin\n\nWhen you install a new piece of software you’ll need to add it two the path. Say I was to install a new version of Python in /opt/python. That’s not on my PATH, so my system wouldn’t be able to find it.\nI can get it on the path in one of two ways – the first option would be to add /opt/python to my PATH every time a terminal session starts usually via a file in /etc or the .zshrc.\nThe other option is to create a symlink to the new application in a directory already on the PATH. A symlink does what it sounds like – creates a way to link to a file from a different directory without moving it. Symlinks are created with the ln command."
  },
  {
    "objectID": "chapters/sec2/2-4-app-admin.html#running-applications-as-services",
    "href": "chapters/sec2/2-4-app-admin.html#running-applications-as-services",
    "title": "10  Application administration",
    "section": "10.4 Running applications as services",
    "text": "10.4 Running applications as services\nOn your personal computer, you probably have programs that start every time your computer does. Maybe this happens for Slack, Microsoft Teams, or Spotify. Such applications that execute on startup and run in the background waiting for some sort of input are called a daemon or a service.\nOn a server, most of the applications are configured to run as a service so they’re ready for users who may not have the permissions to start them. For example, on a data science workbench, you’d want JupyterHub and/or RStudio Server to run as a service.\nIn Linux, the tool to turn a regular application into a daemon is called systemd. Some applications automatically configure themselves with systemd when they’re installed. If your application doesn’t, or you want to alter the startup behavior, most applications have their systemd configuration in /etc/systemd/system/&lt;service name&gt;.service.\nDaemonized services are controlled using the systemctl command line tool.\n\n\n\n\n\n\nNote\n\n\n\nBasically all modern Linux distros have coalesced around using systemd and systemctl. Older systems may not have it installed by default and you may have to install it or use a different tool.\n\n\nThe systemctl command has a set of sub-commands that are useful for working with applications. Providing those commands looks like systemctl &lt;subcommand&gt; &lt;application&gt;. Often systemctl has to be run as sudo, since you’re working with an application for all users of the system.\nThe most useful systemctl commands include status for checking whether a program is running or not, start, stop, and restart for a stop followed by a start. Many applications also support a reload command, which reloads configuration settings without having to actually restart the process. Exactly which settings require a restart vs a reload varies from one application to another.\nIf you’ve changed a service’s systemd configuration, you can load changes with daemon-reload. You also can turn a service on or off for the next time the server starts with enable and disable.\n\n10.4.1 Running Docker containers as a service\nOne of the benefits of running Docker Containers is that they run basically anywhere, so it’s a popular way to quickly run an application on a server. To run them as a service, you’ll need to make sure Docker itself is daemonized and then ensure the container you care about comes up whenever Docker does by setting a restart policy for the container.\nHowever, many Docker services involve coordinating more than one container. If that’s the case, you’ll want to use a system that’s custom-built for managing multiple containers. The most popular are Docker Compose or Kubernetes.\nDocker Compose is a relatively lightweight system where you write a YAML file to describe the containers you need and the relationship between them. You can then use a single command to launch the entire set of Docker containers.\nDocker Compose is fantastic for prototyping systems of Docker containers and for running small-scale Docker-ized deployments on a single server. There are many great resources on Docker Compose online to learn more.\nKubernetes is designed for a similar purpose, but instead of running a handful of containers on one server, Kubernetes is a heavy-duty production system designed to schedule hundreds or thousands of Docker-based workloads across a cluster of many individual servers.\nIn general, I’d recommend sticking with Docker Compose for the work you’re doing. If you’re finding yourself needing the full might of Kubernetes to do what you want, you probably should be working closely with a professional IT/Admin"
  },
  {
    "objectID": "chapters/sec2/2-4-app-admin.html#managing-r-and-python",
    "href": "chapters/sec2/2-4-app-admin.html#managing-r-and-python",
    "title": "10  Application administration",
    "section": "10.5 Managing R and Python",
    "text": "10.5 Managing R and Python\nAs the admin of a data science server, Python and R are some of the most critical software you’ll manage.\nIn general, the easiest path to making many users happy is having a bunch of versions of R and Python installed side-by-side. That way you can allow users to upgrade their version of R or Python as it works for their project, not according to your upgrade schedule.\nI’ve seen this work best by installing R and Python into the /opt/python and /opt/R directory. Users can grab them from there to create project-specific virtual environments. You’ll need to make sure permissions are correct on /opt/python for this to work.\nNow, this isn’t what will happen if you just sudo apt-get install python or sudo apt-get install R, so you’ll have to be a little more clever. My favorite route (though I’m obviously biased) is to install Python and R from the pre-built binaries provided by Posit.\n\n10.5.1 Python-specific considerations\nPython is one of the world’s most popular programming languages for general purpose computing. This actually makes configuring Python harder.\nAlmost every system comes with a system version of Python. This is the version of Python that’s used by the operating system for various tasks. It’s almost always very old and you really don’t want to mess with it.\nSo in order to configure Python on your system, you have to install data science specific versions of Python, get them on the path, and get the system version of Python off the path. This is the source of much frustration trying to get Python up and running, both on servers and your personal computer.\nOn your personal computer, Conda is a great solution for this problem. Conda allows you to install a standalone version of Python in user-space. That means that even if your organization doesn’t let you have admin rights on your computer, you can install and manage versions of Python for development.\nFor the same reason, Conda isn’t a great choice for administering a data science server. You’re not a user – you’re an admin. And giving everyone on the server access to Python versions is generally easier without Conda.\nThis is why it’s a good idea to install data science Python versions into /opt/python. It’s easy to add that one directory to the path, and it’s completely distinct from where the system Python lives.\n\n\n10.5.2 R-specific considerations\nPeople pretty much only ever install R to do data science so it’s generally not a huge deal where you install R and get it on the path. If you want just one version of R, using apt-get install is fine.\nIf you want multiple versions, you’ll need to manually install to /opt/R or use rig, which is an R-specific installation manager. Currently, rig only supports Windows, Mac, and Ubuntu."
  },
  {
    "objectID": "chapters/sec2/2-4-app-admin.html#managing-system-libraries",
    "href": "chapters/sec2/2-4-app-admin.html#managing-system-libraries",
    "title": "10  Application administration",
    "section": "10.6 Managing System Libraries",
    "text": "10.6 Managing System Libraries\nAs an admin, you’ll also have to decide what to do about system packages, which are Linux libraries you install from a Linux repository or the internet.\nMany packages in Python and R don’t do any work themselves. Instead, they’re just language-specific interfaces to system packages. For example, any R or Python library that uses a JDBC database connector will need to make use of Java on your system. And many geospatial libraries make use of system packages like GDAL.\nAs the administrator, you’ll need to make sure you understand what system libraries are needed for the Python and R packages you’re using, and you’ll need to make sure they’re available and on the path.\nFor many of these libraries, it’s not a huge pain. You’ll just install the required library using apt or the system package manager for your distro. In some cases (especially Java), more configuration may be required to make sure that the package you need shows up on the path."
  },
  {
    "objectID": "chapters/sec2/2-4-app-admin.html#comprehension-questions",
    "href": "chapters/sec2/2-4-app-admin.html#comprehension-questions",
    "title": "10  Application administration",
    "section": "10.7 Comprehension Questions",
    "text": "10.7 Comprehension Questions\n\nWhat are two different ways to install Linux applications and what are commands to do so?\nWhat does it mean to daemonize a Linux application? What programs and commands are used to do so?\nHow do you know if you’ve opened nano or vim? How would you exit them if you didn’t mean to?\nWhat are 4 commands to read text files?\nHow would you create a file called secrets.txt, open it with vim, write something in, close and save it, and make it so that only you can read it?"
  },
  {
    "objectID": "chapters/sec2/2-4-app-admin.html#lab-installing-applications",
    "href": "chapters/sec2/2-4-app-admin.html#lab-installing-applications",
    "title": "10  Application administration",
    "section": "10.8 Lab: Installing Applications",
    "text": "10.8 Lab: Installing Applications\nAs we’ve started to administer our server, we’ve mostly been doing very generic server administration tasks. Now let’s set up the applications we need to run a data science workbench and get our API and Shiny app set up for using in our portfolio.\n\n10.8.1 Step 1: Install Python\nLet’s start by installing a data science version of Python so we’re not using the system Python for data science purposes.\nIf you want just one version of Python, you can apt-get install a specific version. As of this writing, Python 3.10 is a relatively new version of Python, so we’ll install that one with\n\n\nTerminal\n\nsudo apt-get install python3.10-venv\n\nOnce you’ve installed Python, you can check that you’ve got the right version by running\n\n\nTerminal\n\npython3 --version\n\nThis method, using apt-get, is great when you just want one version of Python. But if you want multiple versions sitting side by side, you’ll need to do something else. I recommend installing using the instructions on the Posit website, which will involve downloading and installing from .deb files.\n\n\n10.8.2 Step 2: Install R\nSince we’re using Ubuntu, we can use rig. There are good instructions on downloading rig and using it to install R on the rlib/rig GitHub repo. Use those instructions to install the current R release on your server.\nOnce you’ve installed R on your server, you can check that it’s running by just typing R into the command line. If that works, you’re good to move on to the next step. If not, you’ll need to make sure R got onto the path.\n\n\n10.8.3 Step 3: Install JupyerHub + JupyterLab\nJupyterHub and JupyterLab are Python programs, so we’re going to run them from within a Python virtual environment. I’d recommend putting that virtual environment inside /opt/jupyterhub.\nHere are the commands to create and activate a jupyterhub virtual environment in /opt/jupyterhub:\n\n\nTerminal\n\nsudo python3 -m venv /opt/jupyterhub\nsource /opt/jupyterhub/bin/activate\n\nNow we’re going to actually get JupyterHub up and running inside the virtual environment we just created. JupyterHub produces docs that you can use to get up and running very quickly. If you have to stop for any reason, make sure to come back, assume sudo, and start the JupyterHub virtual environment we created.\nNote that because we’re working inside a virtual environment, you may have to use the jupyterhub-singleuser version of the binary.\n\n\n10.8.4 Step 4: Daemonize JupyterHub\nBecause JupyterHub is a Python process, not a system process, it won’t automatically get daemonized, so we’ll have to do it manually.\nWe don’t need it right now, but it’ll be easier to manage JupyterHub later on from a config file that’s in /etc/jupyterhub. In order to do so, activate the jupyterhub virtual environment, create a default JupyterHub config (Google for the command), and move it into /etc/jupyterhub/jupyterhub_config.py.\nNow let’s move on to daemonizing JupyterHub. To start, kill the existing JupyterHub process (consult the cheatsheet in Appendix D if you need help). Since JupyterHub wasn’t automatically daemonized, you’ll have to manually create the systemd file.\nHere’s the file I created in /etc/systemd/system/jupyterhub.service.\n\n\n/etc/systemd/system/jupyterhub.service\n\n[Unit]\nDescription=Jupyterhub\nAfter=syslog.target network.target\n\n[Service]\nUser=root\nEnvironment=\"PATH=/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/opt/jupyterhub/bin\"\nExecStart=/opt/jupyterhub/bin/jupyterhub -f /etc/jupyterhub/jupyterhub_config.py\n\n[Install]\nWantedBy=multi-user.target\n\nThere are two things here worth noticing. The first is the Environment line that adds /opt/jupyterhub/bin to the path – that’s where our virtual environment is.\nThe second is ExecStart line, which provides is the startup command and specifies that JupyterHub should use the config we just created, specified by -f /etc/jupyterhub/jupyterhub_config.py.\nNow, you’ll need to use systemctl to reload the daemon, start JupyterHub, and enable it.\n\n\n10.8.5 Step 5: Install RStudio Server\nYou can find the commands to install RStudio server on the Posit website. Make sure to pick the version that matches your operating system. Since you’ve already installed R, you can skip down to the “Install RStudio Server” step.\nUnlike JupyterHub, RStudio Server does daemonize itself right out of the box, so you can check and control the status with systemctl without any further work.\n\n\n10.8.6 Step 6: Run the penguin API from Docker\nFirst, you’ll have to make sure that Docker itself is available on the system. It can be installed from apt using apt-get install docker.io. You may need to adopt sudo privileges to do so.\nOnce Docker is installed, getting the API running is almost trivially easy using the command we used back in Chapter 6 to run our container.\n\n\nTerminal\n\nsudo docker run --rm -d \\\n  -p 8080:8080 \\\n  --name penguin-model \\\n  alexkgold/penguin-model\n\nThe one change you might note is that I’ve changed the port on the server to be 8080, since JupyterHub runs on 8000 by default.\nOnce it’s up, you can check that it’s running with docker ps.\n\n\n10.8.7 Step 7: Put up the Shiny app\nWe’re going to use Shiny Server to host our Shiny app on the server. Start by moving the app code to the server. I put mine in /home/test-user/do4ds-lab/app by cloning the Git repo.\nAfter that, you’ll need to:\n\nOpen R or Python and rebuild the package library with {renv} or {venv}.\nInstall Shiny Server using the instructions from the Admin Guide.\n\nNote that you can skip steps to install R and/or Python, as well as the {shiny} package as we’ve already done that.\n\nEdit Shiny Server’s configuration file to run the right app.\nStart and enable Shiny Server with systemctl.\n\n\n\n10.8.8 Lab Extensions\nThere are a few things you might want to consider before moving into the next chapter, where we’ll start working on giving this server a stable public URL.\nFirst, we haven’t daemonized the API. Feel free to try Docker Compose or setting a restart policy for the container.\nSecond, neither the API nor the Shiny app will automatically update when we make changes to them. You might want to set up a GitHub Action to do so. For Shiny Server, you’ll need to push the updates to the server and then restart Shiny Server. For the API, you’d need to configure a GitHub action to rebuild the container and push it to a registry. You’d then need to tell Docker on the server to re-pull and restart the container.\nFinally, there’s no authentication in front of our API. Now, the API has pretty limited functionality, so that’s not a huge worry. But if you had an API with more functionality that might be a problem. Additionally, someone could try to flood your API with requests to make it unusable. The most common way to solve this is to buy a product that hosts the API for you or to put an authenticating proxy in front of the API (Google for instructions for NGINX if you want to try it)."
  },
  {
    "objectID": "chapters/sec2/2-4-app-admin.html#footnotes",
    "href": "chapters/sec2/2-4-app-admin.html#footnotes",
    "title": "10  Application administration",
    "section": "",
    "text": "vi is the original fullscreen text editor for Linux. vim is its successor (vim stands for vi improved). I’m not going to worry about the distinction.↩︎"
  },
  {
    "objectID": "chapters/sec2/2-5-scale.html#the-briefest-intro-to-computational-theory",
    "href": "chapters/sec2/2-5-scale.html#the-briefest-intro-to-computational-theory",
    "title": "11  Server Resources and Scaling",
    "section": "11.1 The briefest intro to computational theory",
    "text": "11.1 The briefest intro to computational theory\nYou’re probably aware that everything you’ve ever seen on a computer – from this book to your work in R or Python, your favorite internet cat videos, and Minecraft – it’s just 1s and 0s.\nBut the 1s and 0s aren’t actually the interesting part. The interesting part is that these 1s and 0s don’t directly represent a cat or a book. Instead, the 1s and 0s are binary representations of integers (whole numbers) and the only thing the computer does is add these integers.1\nOnce they’ve been added, they’re reinterpreted back into something meaningful. But the bottom line is that every bit of input your computer gets is turned into an addition problem, processed, and the results are reverted back into something we interpret as meaningful.\nSo, a computer is really just a factory that does addition problems. So as you’re thinking about how to size and scale a server, you’re really thinking about how to optimally design your factory.\nThere are three essential resources a server has – compute, memory, and storage. In this chapter, I’m going to share some mental models I find helpful about how to think of each and some recommendations for getting a data science server."
  },
  {
    "objectID": "chapters/sec2/2-5-scale.html#how-computers-compute",
    "href": "chapters/sec2/2-5-scale.html#how-computers-compute",
    "title": "11  Server Resources and Scaling",
    "section": "11.2 How computers compute",
    "text": "11.2 How computers compute\nThe addition assembly line itself – where the work actually gets done – is referred to as compute. It’s where 2+2 gets turned into 4, and where 345619912 + 182347910 gets turned into 527967822.\nThe main compute that all computers have is the central processing unit (CPU), which completes addition problems in a core.\nThe first attribute that determines the speed of compute is the number of cores, which is like the number of conveyor belts in the factory.\nThese days, most consumer-grade laptops have between 4 and 16 physical cores. Many have software capabilities that effectively double that number, so they can do between 4 and 32 simultaneous addition problems.\nThe second speed-determining attribute is how quickly a single addition problem gets completed by a single core. This is called the single core clock speed. You can think of this as how fast the conveyor belt moves.\nClock speeds are measured in operations per second or hertz (hz). The cores in your laptop probably max out between two and five gigahertz (GHz), which means between 2 billion and 5 billion operations per second.\nFor decades, many of the innovations in computing were coming from increases in single core clock speed, but those have fallen off a lot in the last few decades. The clock speeds of consumer-grade chips increased by approximately 10x during the 90s, by 2-3x in the 2000s, and somewhere between not at all and 1.5x in the 2010s.\nBut computers have continued getting a lot faster even as the increase in clock speeds has slowed. The increase has mostly come from increases in the number of cores, better software usage of parallelization, and faster loading and unloading of the CPU (called the bus)."
  },
  {
    "objectID": "chapters/sec2/2-5-scale.html#recommendation-1-fewer-faster-cpu-cores",
    "href": "chapters/sec2/2-5-scale.html#recommendation-1-fewer-faster-cpu-cores",
    "title": "11  Server Resources and Scaling",
    "section": "11.3 Recommendation 1: Fewer, faster CPU cores",
    "text": "11.3 Recommendation 1: Fewer, faster CPU cores\nR and Python are single-threaded. This means that unless you’re using special libraries for parallel processing, you’ll end up red-lining a single CPU core while the others just look on in silence.\nTherefore for most R and Python work, single core clock speed matters more than the number of cores, and fewer, faster cores are usually preferable to many slower\nYou’re probably not used to thinking about this tradeoff from buying a laptop or phone. The reality is that modern CPUs are pretty darn good and you should just buy the one that fits your budget.\nIf you’re standing up a server, you often do have an explicit choice between more slower cores and fewer faster ones, determined by the instance family.\nIf you’re running a multi-user server, the number of cores you need can be hard to estimate. If you’re doing non-ML tasks like counts and dashboarding or relatively light-duty machine learning I might advise the following:\n\\[\n\\text{n cores} = \\text{1 core per user} + 1\n\\]\nThe spare core is for the server to do its own operations apart from the data science usage. On the other hand, if you’re doing heavy-duty machine learning or parallelizing jobs across the CPU, you may need more cores than this rule of thumb."
  },
  {
    "objectID": "chapters/sec2/2-5-scale.html#how-memory-works",
    "href": "chapters/sec2/2-5-scale.html#how-memory-works",
    "title": "11  Server Resources and Scaling",
    "section": "11.4 How memory works",
    "text": "11.4 How memory works\nYour computer’s random access memory (RAM) is its short term storage. You can think of RAM like the stock that’s sitting out on the factory floor ready to go right on an assembly line and the completed work that’s ready to be shipped.\nRAM is very fast to for your computer to access, so you can read and write to it very quickly. The downside is that it’s temporary. When your computer turns off, the RAM gets wiped.2\n\n\n\n\n\n\nNote\n\n\n\nMemory and storage are measured in bytes with metric prefixes.\nCommon sizes for memory these days are in gigabytes (billion bytes) and terrabytes (trillion bytes). Some enterprise data stores run on the scales of thousands of terrabytes (pettabytes) or even thousands of pettabytes (yottabytes).\n\n\nModern consumer-grade laptops come with somewhere between 4 and 16 Gb of memory."
  },
  {
    "objectID": "chapters/sec2/2-5-scale.html#recommendation-2-get-as-much-ram-as-feasible",
    "href": "chapters/sec2/2-5-scale.html#recommendation-2-get-as-much-ram-as-feasible",
    "title": "11  Server Resources and Scaling",
    "section": "11.5 Recommendation 2: Get as much RAM as feasible",
    "text": "11.5 Recommendation 2: Get as much RAM as feasible\nIn most cases, R and Python have to load all of your data into memory. Thus, the size of the data you can use is limited to the size of your machine’s RAM.\nMost other limits of your machine will just result in things being slower than you’d really want, but trying to load too much data into memory will result in a session crash.\n\n\n\n\n\n\nNote\n\n\n\nIf you’re running into this limitation, go back and think about your project architecture as discussed in Chapter 2. Maybe you can load less data into memory.\n\n\nBecause your computer needs memory for things other than R and Python and because you’ll often be doing transformations that temporarily increase the size of your data, you need more memory than your largest data set.\nIn general, you’ll always want more RAM, but a pretty good rule of thumb is that you’ll be happy if:\n\\[\\text{Amount of RAM} \\ge 3 * \\text{max amount of data}\\]\nIf you’re thinking about running a multi-user server, you’ll need to take a step back to think about how many concurrent users you expect and how much data you expect each one to load."
  },
  {
    "objectID": "chapters/sec2/2-5-scale.html#understanding-storage",
    "href": "chapters/sec2/2-5-scale.html#understanding-storage",
    "title": "11  Server Resources and Scaling",
    "section": "11.6 Understanding storage",
    "text": "11.6 Understanding storage\nStorage, or hard disk/drive, is your computer’s place to put things for the long-term. It’s where applications are installed, and where you save things you want to keep.\nRelative to the RAM that’s right next to the factory floor, your computer’s storage is like the warehouse in the next building over. It’s slower to get things from storage than RAM, but it’s also permanent once its stored there.\nUp until a few years ago, storage was much slower than RAM. Those drives, called HDD drives, had a bunch of spinning magnetic disks with magnetized read/write heads that move among the disks to save and read data.\nWhile they spin very fast – 5,400 and 7,200 RPM are common speeds – there were still physical moving parts, and reading and writing data was very slow by computational standards.\nIn the last few years, solid-state drives (SSDs) have become more-or-less standard in laptops. SSDs, which are collections of flash memory chips with no moving parts are up to 15x faster than HDDs."
  },
  {
    "objectID": "chapters/sec2/2-5-scale.html#recommendation-3-get-lots-of-storage-its-cheap",
    "href": "chapters/sec2/2-5-scale.html#recommendation-3-get-lots-of-storage-its-cheap",
    "title": "11  Server Resources and Scaling",
    "section": "11.7 Recommendation 3: Get lots of storage, it’s cheap",
    "text": "11.7 Recommendation 3: Get lots of storage, it’s cheap\nAs for configuring storage on your server – get a lot – but don’t think about it too hard, because it’s cheap and easy to upgrade. Storage is cheap enough these days that it is almost always more cost efficient to buy more storage rather than making a highly-paid professional spend their time trying to figure out how to move things around.\n\n\n\n\n\n\nNote\n\n\n\nIf the IT/Admins at your organization want you to spend a lot of time deleting things from storage that’s usually a red flag that they aren’t thinking much about how to make the overall organization work more smoothly.\n\n\nIf you’re running a multi-user server, the amount of storage you need depends a lot on your data and your workflows.\nIf you’re not running particularly large data, or most of your data won’t be saved into the server’s storage (generally a good thing), a reasonable rule of thumb is to choose\n\\[\n\\text{Amount of Storage} = \\text{1Gb} * \\text{n users}\n\\]\nIf you’re not saving large data files, the amount of space each person needs on the server is small. Code is very small and it’s rare to see R and Python packages take up more than a few dozen Mb per data scientist.\n\n\n\n\n\n\nNote\n\n\n\nIf you’re working with a professional IT admin, they may be concerned about the storage implications of having package copies for each person on their team if you’re following best practices for running environments as code from Chapter 1. I’ve heard this concern a lot from IT/Admins thinking ahead about running their server and almost never of a case where it’s actually been a problem.\n\n\nIf, on the other hand, you will be saving a lot of data, you’ve got to take that into account. In some organizations, each data scientist will save dozens of flat files of a Gb or more for each of their projects.\nIf you’re operating in the cloud, this really isn’t an important choice. As you’ll see in the lab, upgrading the amount of storage you have is a trivial operation, requiring at most a few minutes of downtime. Choose a size you guess will be adequate and add more if you need."
  },
  {
    "objectID": "chapters/sec2/2-5-scale.html#gpus-are-special-purpose-compute",
    "href": "chapters/sec2/2-5-scale.html#gpus-are-special-purpose-compute",
    "title": "11  Server Resources and Scaling",
    "section": "11.8 GPUs are special-purpose compute",
    "text": "11.8 GPUs are special-purpose compute\nAll computers have a CPU. There are also specialized chips where the CPU can offload particular tasks, the most common of which is the graphical processing unit (GPU). GPUs are architected for tasks like editing photo or videos, rendering video game graphics, some kinds of machine learning, and (yes) Bitcoin mining.\nA GPU is an addition factory just like a CPU, but with the opposite architecture. CPUs have only a handful of cores, but those cores are fast. A GPU takes the opposite approach, with many (relatively) slow cores.\nWhere a consumer-grade CPU has 4-16 cores, mid-range GPUs have 700-4,000 cores, with each one running at only about 1% to 10% the single core clock speed speed of a CPU core.\nThe choice of whether you need a GPU to do your work will really depend on what you’re doing and your budget.\nFor the tasks GPUs are good at, the overwhelming parallelism ends up being more important than the speed of any individual core, and GPU computation can be dramatically faster."
  },
  {
    "objectID": "chapters/sec2/2-5-scale.html#recommendation-4-get-a-gpu-maybe",
    "href": "chapters/sec2/2-5-scale.html#recommendation-4-get-a-gpu-maybe",
    "title": "11  Server Resources and Scaling",
    "section": "11.9 Recommendation 4: Get a GPU, maybe",
    "text": "11.9 Recommendation 4: Get a GPU, maybe\nThe tasks that most benefit from GPU computing are training highly parallel machine learning models like deep learning or tree-based models. If you do have one of these use cases, GPU computing can massively speed up your computation – making models trainable in hours instead of days.\nIf you are planning to use cloud resources for your computing, GPU-backed instances are quite pricey, and you’ll want to be careful about only putting those machines up when you’re using them.\nBecause GPUs are expensive, I generally wouldn’t bother with GPU-backed computing unless you’ve already tried without and find that it takes too long to be feasible.\nIt’s also worth noting that using a GPU won’t happen automatically. The tooling has gotten good enough that it’s usually pretty easy to set up, but your computer won’t train your XGBoost models on your GPU unless you tell it to do so."
  },
  {
    "objectID": "chapters/sec2/2-5-scale.html#assessing-ram-cpu-usage",
    "href": "chapters/sec2/2-5-scale.html#assessing-ram-cpu-usage",
    "title": "11  Server Resources and Scaling",
    "section": "11.10 Assessing RAM + CPU usage",
    "text": "11.10 Assessing RAM + CPU usage\nOnce you’ve chosen your server size and gotten up and running, you’ll want to be able to monitor RAM and CPU for problems.\nAny program that is running is called a process. For example, when you type python on the command line to open a REPL, that starts a single Python process. If you were to start a second terminal session and run python again, you’d have a second Python process.\nComplicated programs often involve multiple interlocking processes. For example, running the RStudio IDE involves (at minimum) one process for the IDE itself and one for the R session that it uses in the background. The relationships between these different processes is mostly hidden from you – the end user.\nAs an admin, you may want to inspect the processes running on your system at any given time. The top command is a good first stop. top shows the top CPU-consuming processes in real time along with a number of other facts about them.\nHere’s the top output from my machine as I write this sentence.\n\n\nTerminal\n\nPID    COMMAND      %CPU TIME     #TH    #WQ  #PORT MEM    PURG   CMPRS PGRP\n0      kernel_task  16.1 03:56:53 530/10 0    0     2272K  0B     0B    0\n16329  WindowServer 16.0 01:53:20 23     6    3717  941M-  16M+   124M  16329\n24484  iTerm2       11.3 00:38.20 5      2    266-  71M-   128K   18M-  24484\n29519  top          9.7  00:04.30 1/1    0    36    9729K  0B     0B    29519\n16795  Magnet       3.1  00:39.16 3      1    206   82M    0B     39M   16795\n16934  Arc          1.8  18:18.49 45     6    938   310M   144K   61M   16934\n\nIn most instances, the first three columns are the most useful. The first column is the unique process id (pid) for that process. You’ve got the name of the process (COMMAND) and how much CPU its using. You’ve also got the amount of memory used a few columns over. Right now, nothing is using very much CPU.\nThe top command takes over your whole terminal. You can exit with Ctrl + c.\n\n\n\n\n\n\nSo much CPU?\n\n\n\nFor top (and most other commands), CPU is expressed as a percent of single core availability. So, on a modern machine with multiple cores, it’s very common to see CPU totals well over 100%. Seeing a single process using over 100% of CPU is rarer.\n\n\nAnother useful command for finding runaway processes is ps aux. It lists a snapshot of all processes running on the system, along with how much CPU or RAM they’re using. You can sort the output with the --sort flag and specify sorting by cpu with --sort -%cpu or by memory with --sort -%mem.\nBecause ps aux returns every running process on the system, you’ll probably want to pipe the output into head. In addition to CPU and Memory usage, ps aux gets you who launched the command and the PID.\nFor example, here are the RStudio processes currently running on my system.\n\n\nTerminal\n\nUSER               PID  %CPU %MEM      VSZ    RSS   TT  STAT STARTED      TIME COMMAND\nalexkgold        23583   0.9  1.7 37513368 564880   ??  S    Sat09AM  17:15.27 /Applications/RStudio.app/Contents/MacOS/RStudio\nalexkgold        23605   0.5  0.4 36134976 150828   ??  S    Sat09AM   1:58.16 /Applications/RStudio.app/Contents/MacOS/rsession --config-file none --program-mode desktop \n\nOne of the times you’ll be most interested in the output of top or ps aux is when something is going rogue on your system and using more resources than you intended. If you have some sense of who started the runaway process or what it it, it can be useful to pipe the output of ps aux into grep.\nFor example, the command to get the output above was ps aux | RStudio.\nIf you’ve got a rogue process, the pattern is to try to find the process and make note of its pid. Then you can immediately end the process by pid with the kill command.\nIf I were to find something concerning – perhaps an R process that is using 500% of CPU – I would want to take notice of its pid to kill it with kill."
  },
  {
    "objectID": "chapters/sec2/2-5-scale.html#examining-at-storage-usage",
    "href": "chapters/sec2/2-5-scale.html#examining-at-storage-usage",
    "title": "11  Server Resources and Scaling",
    "section": "11.11 Examining at storage usage",
    "text": "11.11 Examining at storage usage\nA common culprit for weird server behavior is running out of storage space. There are two handy commands for monitoring the amount of storage you’ve got – du and df.\nThese commands are almost always used with the -h flag to put file sizes in human-readable formats.\ndf, for disk free, shows the capacity left on the device where the directory sits.\nFor example, here’s the result of running the df command on the chapters directory on my laptop that includes this chapter.\n\n\nTerminal\n\n&gt; df -h chapters\nFilesystem     Size   Used  Avail Capacity iused      ifree %iused  Mounted on\n/dev/disk3s5  926Gi  227Gi  686Gi    25% 1496100 7188673280    0%   /System/Volumes/Data\n\nSo you can see that the chapters folder lives on a disk called /dev/disk3s5 that’s a little less than 1Tb and is 25% full – no problem. On a server this can be really useful to know, because it’s quite easy to switch a disk out for a bigger one in the same spot.\nIf you’ve figured out that a disk is full, it’s usually most cost effective to just buy a bigger disk. But sometimes something weird happens. Maybe there are a few exceptionally big files, or you think unnecessary copies are being made.\nIf so, the du command, short for disk usage, gives you the size of individual files inside a directory. It’s particularly useful in combination with the sort command.\nFor example, here’s the result of running du on the chapters directory where the text files for this book live.\n\n\nTerminal\n\n&gt; du -h chapters | sort\n12M chapters\n1.7M    chapters/sec1/images\n1.8M    chapters/sec1\n236K    chapters/images\n488K    chapters/sec2/images-traffic\n5.3M    chapters/sec2/images-networking\n552K    chapters/sec2/images\n6.6M    chapters/sec2\n892K    chapters/append/images\n948K    chapters/append\n\nSo if I were thinking about cleaning up this directory, I could see that my sec1/images directory is my biggest single directory. If you find yourself needing to find big files on your Linux server, it’s worth spending some time with the help pages for du. There are lots of really useful options."
  },
  {
    "objectID": "chapters/sec2/2-5-scale.html#running-out-of-resources",
    "href": "chapters/sec2/2-5-scale.html#running-out-of-resources",
    "title": "11  Server Resources and Scaling",
    "section": "11.12 Running out of Resources",
    "text": "11.12 Running out of Resources\nIf you recognize that you’re running out of resources on your current server, you may want to move to something bigger. There are two main reasons servers run out of room.\nThe first reason is because people are running big jobs. This can happen at any scale of organization. There are data science teams of one who have use cases that necessitate terrabytes of data.\nThe second reason is because you have a lot of people using your server. This is generally a feature of big data science teams, irrespective of the size of the workloads.\nEither way, there are two basic options for how to scale your data science workbench. The first is vertical scaling, which is just a fancy way of saying get a bigger server. The second option is horizontal scaling, which means running a whole fleet of servers in parallel and spreading the workload across them.\nAs a data scientist, you shouldn’t be shy about vertically scaling if your budget allows it. The complexity of managing a t3.nano with 2 cores and 0.5 Gb of memory is exactly the same as a C5.24xlarge with 96 cores and 192 Gb of memory. In fact, the bigger one may well be easier to manage, since you won’t have to worry about running low on resources.\nThere are limits to the capacity of vertical scaling. As of this writing, AWS’s general-use instance types max out at 96-128 cores. That can quickly get eaten up by 50 data scientists with reasonably heavy computational demands.\nOnce you’re thinking about horizontal scaling, you’ve got a distributed service problem on your hand, which is inherently difficult. You should almost certainly get an IT/Admin professional involved. See Chapter 17 for more on how to talk to them about it.\n\n11.12.1 AWS Instances for data science\nAWS offers a variety of different EC2 instance types split up by family and size. The family is the category of EC2 instance. Different families of instances are optimized for different kinds of workloads.\nHere’s a table of common instance types for data science purposes:\n\n\n\n\n\n\n\nInstance Type\nWhat it is\n\n\n\n\nt3\nThe “standard” configuration. Relatively cheap. Sizes may be limited.\n\n\nC\nCPU-optimized instances, aka faster CPUs\n\n\nR\nHigher ratio of RAM to CPU\n\n\nP\nGPU instances, very expensive\n\n\n\nWithin each family, there are different sizes available, ranging from nano to multiples of xl. Instances are denoted by &lt;family&gt;.&lt;size&gt;. So, for example, when we put our instance originally on a free tier machine, we put it on a t2.micro.\nIn most cases, going up a size doubles the amount of RAM, the number of cores, and the cost. So you should do some quick math before you stand up a C5.24xlarge or a GPU-based P instance. If your instance won’t be up very long, it may be fine, but make sure you take it down when you’re done lest you rack up a huge bill."
  },
  {
    "objectID": "chapters/sec2/2-5-scale.html#comprehension-questions",
    "href": "chapters/sec2/2-5-scale.html#comprehension-questions",
    "title": "11  Server Resources and Scaling",
    "section": "11.13 Comprehension Questions",
    "text": "11.13 Comprehension Questions\n\nThink about the scenarios below – which part of your computer would you want to upgrade to solve the problem?\n\nYou try to load a big csv file into pandas in Python. It churns for a while and then crashes.\nYou go to build a new ML model on your data. You’d like to re-train the model once a day, but it turns out training this model takes 26 hours on your laptop.\nYou design an visualization Matplotlib , and create a whole bunch in a loop, you want to parallelize the operation. Right now you’re running on a t2.small with 1 CPU.\n\nDraw a mind map of the following: CPU, RAM, Storage, Operations Per Second, Parallel Operations, GPU, Machine Learning\nWhat are the architectural differences between a CPU and a GPU? Why does this make a GPU particularly good for Machine Learning?\nHow would you do the following:\n\nFind all running Jupyter processes that belong to the user alexkgold.\nFind the different disks attached to your server and see how full each one is.\nFind the biggest files in each user’s home directory."
  },
  {
    "objectID": "chapters/sec2/2-5-scale.html#lab-changing-instance-size",
    "href": "chapters/sec2/2-5-scale.html#lab-changing-instance-size",
    "title": "11  Server Resources and Scaling",
    "section": "11.14 Lab: Changing Instance Size",
    "text": "11.14 Lab: Changing Instance Size\nIn this lab, we’re going to upgrade the size of our server. And the best part is that because we’re in the cloud, it’ll take only a few minutes.\n\n11.14.1 Step 1: Confirm current server size\nFirst, let’s confirm what we’ve got available. You can check the number of CPUs you’ve got with lscpu in a terminal. Similarly, you can check the amount of RAM with free -h. This is just so you can prove to yourself later that the instance really changed.\n\n\n11.14.2 Step 2: Change the instance type and bring it back\nNow, you can go to the instance page in the AWS console. The first step is to stop (not terminate!) the instance. This means that changing instance type does require some downtime for the instance, but it’s quite limited.\nOnce the instance has stopped, you can change the instance type under Actions &gt; Instance Settings. Then start the instance. It’ll take a few seconds to start the instance.\n\n\n11.14.3 Step 3: Confirm new server size\nSo, for example, I changed from a t2.micro to a t2.small. Both only have 1 CPU, so I won’t see any difference in lscpu, but running free -h before and after the switch reveals the difference in the total column:\ntest-user@ip-172-31-53-181:~$ free -h\n               total        used        free      shared  buff/cache   available\nMem:           966Mi       412Mi       215Mi       0.0Ki       338Mi       404Mi\nSwap:             0B          0B          0B\ntest-user@ip-172-31-53-181:~$ free -h\n               total        used        free      shared  buff/cache   available\nMem:           1.9Gi       225Mi       1.3Gi       0.0Ki       447Mi       1.6Gi\nSwap:             0B          0B          0B\nI got twice as much RAM!\nThere are some rules around being able to change from one instance type to another, but this is a superpower if you’ve got variable workloads or a team that’s growing. Once you’re done with your larger server, it’s just as easy to scale it back down.\n\n\n11.14.4 Step 4: Upgrade storage (maybe)\nIf you want more storage, it’s similarly easy to resize the EBS volume attached to your server.\nI wouldn’t recommend doing it for this lab, because you can only automatically adjust volume sizes up, so you’d have to manually transfer all of your data if you ever wanted to scale back down.\nIf you do resize the volume, you’ll have to let Linux know so it can resize the filesystem with the new space available. AWS has a great walk through called Extend a Linux filesystem after resizing the volume that I recommend you follow."
  },
  {
    "objectID": "chapters/sec2/2-5-scale.html#footnotes",
    "href": "chapters/sec2/2-5-scale.html#footnotes",
    "title": "11  Server Resources and Scaling",
    "section": "",
    "text": "This was proved in Alan Turing’s 1936 paper on computability. If you’re interested in learning more, I recommend The Annotated Turing: A Guided Tour Through Alan Turing’s Historic Paper on Computability and the Turing Machine by Charles Petzold for a surprisingly readable walkthrough.↩︎\nYou probably don’t experience this personally. Modern computers are pretty smart about dumping RAM onto the hard disk before shutting down, and bringing it back on startup, so you usually won’t notice this happening.↩︎"
  },
  {
    "objectID": "chapters/sec2/2-6-networking.html#understanding-digital-addresses",
    "href": "chapters/sec2/2-6-networking.html#understanding-digital-addresses",
    "title": "12  Intro to Computer Networks",
    "section": "12.1 Understanding digital addresses",
    "text": "12.1 Understanding digital addresses\nYou already use digital addresses all the time in the form of a URL (Uniform Resource Locator).1 A URL fully specifies the network location of a resource and looks like this:\n\\[\\overbrace{\\text{https://}}^\\text{protocol}\\underbrace{\\text{google.com}}_\\text{domain}\\overbrace{\\text{:443}}^\\text{port}\\underbrace{\\text{/}}_\\text{path}\\]\nThis may look a little strange. You’re probably used to using just the domain and maybe a path in your web browser like \\(\\text{google.com}\\) or \\(\\text{google.com/maps}\\). The reason is that you’re usually fine with the default protocol and port, so you may never have realized they’re there.2\nHere’s what each of those four parts are:\n\nThe application layer protocol (often just called the protocol) specifies what type of traffic this is. It’s like agreeing that your letter will be in English or Arabic or Dutch.\nThe domain is a human-readable way of providing the digital street address of the server. We’ll get into the actual address later in the chapter.\nThe port specifies where on the server to direct the traffic. It’s the digital equivalent of the apartment number.\nThe path is a human-friendly way of specifying who you intend the message to go to. It’s like the name of person you’re addressing on your letter.\n\nAs a cloud server admin, most of the networking you’ll do is to ensure someone can just use a domain with a path and access the resources they need from your server."
  },
  {
    "objectID": "chapters/sec2/2-6-networking.html#finding-the-right-digital-address",
    "href": "chapters/sec2/2-6-networking.html#finding-the-right-digital-address",
    "title": "12  Intro to Computer Networks",
    "section": "12.2 Finding the right digital address",
    "text": "12.2 Finding the right digital address\nA domain is the human-readable way of addressing a resource on the internet. But it’s not actually the digital street address of any particular server (host). Instead, hosts are actually identified with an IP Address. When that IP address is valid across the entire internet, it’s a public IP address.\n\n\n\n\n\n\nNote\n\n\n\nIP Addresses are mapped to domains via the Domain Name Service (DNS), which you’ll learn about in Chapter 13\n\n\nNetwork traffic arrives in the form of packets, which are routed to the correct IP address in a process called packet switching. The basic idea is that packets are addressed with a target IP address when they’re created. There is a system of hardware and software tools called routers that are responsible for keeping track of which IP Addresses are assigned to different machines and getting the packets to the right place.3\nOnce the traffic arrives at the server, it has to get to the right service by getting to the right port. Every computer has just over 65,000 ports. Each port is uniquely identified by a number, and there’s a 1-1 mapping between listening services and open ports. Since you’re probably running no more than a handful of services, the overwhelming majority of the ports are closed at any given time.4\nBy default, HTTP traffic goes to port \\(80\\) and HTTPS traffic goes to port \\(443\\). So if there’s just one service, you would configure the application to listen on port \\(80\\) and/or \\(443\\). Then, when people come in, they’d automatically get to the right service.\nBut sometimes you’ve got multiple services on the server. In that case, each one will need to run on a unique port. However, you don’t want to force users to remember, for example, that they access JupyterHub on port \\(8000\\). Instead, you want them just to know that it’s on \\(\\text{/jupyter}\\).\nA proxy is a piece of software that can be used to map paths to another location. There are free and open source proxies, like NGINX and Apache. There are also paid proxies like Citrix, Fortinet, Cloudflare, and F5 (they maintain NGINX). Depending on the configuration, the proxy can be on the same server or a different one.\nIf you’re using a proxy, you’d put each of the server’s services on a different port and use the proxy to redirect incoming traffic to the right place based on the subpath.\nNow, if you were just to put something on port \\(80\\) on your EC2 instance and try to access it from the web, it still wouldn’t work. That’s because there’s a firewall sitting in front of your EC2 instance, which blocks traffic to all but certain ports. In AWS, the default firewall is the security group.\nIn addition to blocking traffic to arriving at certain ports, firewalls can be restricted to allow access only from certain IP Addresses. This can be used, for example, to only allow access from your office to a server. Unless a particular server will only ever be accessed by other servers with known IP addresses, this is a brittle way to configure security and I generally don’t recommend it.\n\n\n\n\n\n\nTip\n\n\n\nIf you think you’ve configured a service correctly and you just can’t seem to access it, one of the first things to check is whether you’ve got the port open in the security group.\nOne symptom that may indicate a security group issue is if you try to go to your service and it just hangs with no response before eventually timing out.\n\n\n\nOnce the traffic makes it to the right server, through the firewall and proxy, and to the right port, it has to communicate using the right application layer protocol. We’ve been talking exclusively about the HTTP and HTTPS application layer protocols, because web traffic arrives as a series of HTTP GET requests, but there are many other application layer protocols, each with its own default port.\nFor example, you’ve already seen a lot about SSH in this book, which is an application layer protocol for allowing secure login and communication over an unsecured network. SSH defaults to port \\(22\\).\nOther important protocols that you might see in this book or elsewhere are SFTP for file transfers, SMTP for emails, LDAP(S) for auth, and websockets, which are used by Shiny and Streamlit."
  },
  {
    "objectID": "chapters/sec2/2-6-networking.html#recognizing-ip-addresses",
    "href": "chapters/sec2/2-6-networking.html#recognizing-ip-addresses",
    "title": "12  Intro to Computer Networks",
    "section": "12.3 Recognizing IP addresses",
    "text": "12.3 Recognizing IP addresses\nIf you’ve seen an IP address before, it probably was an IPv4 address, which are four blocks of 8-bit fields (numbers between \\(0\\) and \\(255\\)) with dots in between, so they look like \\(64.56.223.5\\).\nIf you do the math, you’ll realize there are “only” about 4 billion of these. There are so many things on the public internet that we are running out of IPv4 addresses. The good news is that smart people started planning for this a while ago and the adoption of the new IPv6 standard started a few years ago.\nIPv6 addresses are eight blocks of hexadecimal (\\(\\text{0-9}\\) and \\(\\text{a-f}\\)) digits separated by colons, with certain rules that allow them to be shortened, so \\(\\text{4b01:0db8:85a3:0000:0000:8a2e:0370:7334}\\) or \\(\\text{3da4:66a::1}\\) are both examples of valid IPv6 addresses. There’s no worry about running out of IPv6 addresses any time soon, because the total quantity of IPv6 addresses is a number with 39 zeroes.\nIPv6 will coexist with IPv4 for a few decades and we’ll eventually switch entirely to IPv6.\nThere are a few special IPv4 addresses it’s worth knowing. You’ll probably see \\(127.0.0.1\\) a lot, which is also known as \\(\\text{localhost}\\) or loopback. This is the way a machine refers to itself.\nFor example, if you open a Shiny app in RStudio Desktop, the app will pop up in a little window along with a notice that says\nListening on http://127.0.0.1:6311\nThat means that the Shiny app is running on the same computer and is available on port \\(6311\\). You can open that location in your browser to view the app as it runs.\nThere are also a few blocks of IPv4 addresses – those that start with \\(192.168\\), \\(172.16\\), and \\(10\\) that are reserved for use on private networks, so they’re never assigned in public."
  },
  {
    "objectID": "chapters/sec2/2-6-networking.html#basic-network-administration",
    "href": "chapters/sec2/2-6-networking.html#basic-network-administration",
    "title": "12  Intro to Computer Networks",
    "section": "12.4 Basic network administration",
    "text": "12.4 Basic network administration\nNetworking can be difficult to manage because there are so many layers. It will frequently happen that you think a service is configured, but you just can’t seem to access it. Here are some basic tools for network debugging.\n\n12.4.1 Browser devtools\nOne of the most useful tools for debugging networking issues can be found in the menus of your web browser. It has developer tools that allow you to inspect the network traffic going back and forth between your machine and a remote.\nThis can be really handy if things are going slowly or if you’re not sure why the page isn’t loading. By inspecting the status codes of different HTTP calls and the time they take, you can develop a pretty good idea of where things might be getting stuck.\n\n\n12.4.2 SSH tunneling/port forwarding\nWhen you start a new EC2 instance in AWS, the default security group opens only port \\(22\\), so only SSH traffic is allowed.\nSo far, you’ve seen SSH used to access the command line on that remote server, but SSH can actually be used to access any port in a process called tunneling or port forwarding.\nWhen you tunnel, you make a port on the remote host available at the same port on \\(\\text{localhost}\\) on your machine. The most common usage is to use your browser to look at what’s available at a specific port on a server via HTTP without configuring the host to accept HTTP traffic on that port.\nYou can create an SSH tunnel to a remote host with\n\n\nTerminal\n\nssh -L &lt;remote port&gt;:localhost:&lt;local port&gt; &lt;user&gt;@&lt;server&gt;\n\nI find that the syntax for port forwarding completely defies my memory and I have to google it every time I use it.5\nSo, for example, if your server were running at \\(64.56.223.5\\) and you has the SSH user test-user, you might forward JupyterHub on port \\(8000\\) with ssh -L 8000:localhost:8000 test-user@64.56.223.5. Once the tunnel is established, you could access JupyerHub in your browser on \\(\\text{localhost:8000}\\).\n\n\n12.4.3 Checking what ports are open\nSometimes you can just forget what ports are open on your machine and for what purposes. Or you want to double check that a configuration change took. In that case, you want to use the netstat command to get the services that are running and their associated ports.\nFor this use, netstat is generally most useful with the -tlp flags to show programs that are listening and the programs associated.\n\n\n12.4.4 Checking if a host is accessible\nThe ping command can be useful for checking whether your server is reachable on the network. For example, the server where this book lives is at \\(185.199.110.153\\). So I can ping that domain to check if it’s accessible.\n&gt; ping -o 185.199.110.153\n\nPING 185.199.110.153 (185.199.110.153): 56 data bytes\n64 bytes from 185.199.110.153: icmp_seq=0 ttl=58 time=23.322 ms\n\n--- 185.199.110.153 ping statistics ---\n1 packets transmitted, 1 packets received, 0.0% packet loss\nround-trip min/avg/max/stddev = 23.322/23.322/23.322/nan ms\nThe -o flag tells ping to try just once as opposed to pinging continuously. The fact that I transmitted and received one packet means that everything is working properly.\nSeeing an unreachable host or packet loss would be an indication that my networking probably isn’t configured correctly somewhere between me and the server. That means it’s time to check that the server is actually up, followed by firewalls (security groups) and proxies. You can also use ping with a domain, so it can also be used to see if DNS is working properly.\nIf ping succeeds but a particular resource is inaccessible, curl is can be useful. curl actually attempts to fetch the website at a particular URL. It’s often useful to use curl with the -I option so it just returns a simple status report, not the full contents of what it finds there.\nFor example, here’s what I get when I curl the website for this book.\n&gt; curl -I https://do4ds.com                                         \n\nHTTP/2 200\nserver: GitHub.com\ncontent-type: text/html; charset=utf-8\nlast-modified: Tue, 04 Jul 2023 16:23:38 GMT\naccess-control-allow-origin: *\netag: \"64a4478a-79cb\"\n...\nThe important thing here is that first line. The server is returning a 200 HTTP status code, which means all is well. If you get something else, take a look at the http code cheatsheet in Appendix D.\nIf ping succeeds, but curl does not, it means that the server is up, but the path or port is incorrect. If you’re running inside a container, you should check that you’ve properly configured the port inside container to be forwarded to the outside."
  },
  {
    "objectID": "chapters/sec2/2-6-networking.html#comprehension-questions",
    "href": "chapters/sec2/2-6-networking.html#comprehension-questions",
    "title": "12  Intro to Computer Networks",
    "section": "12.5 Comprehension Questions",
    "text": "12.5 Comprehension Questions\n\nWhat are the 4 components of a URL? What’s the significance of each?\nAre there any inherent differences between public and private IP addresses?\nDraw a mind map of trying to access the following in your browser. Include the following terms: URL, domain, IP Address, port, path, \\(80\\), \\(443\\), 8000, proxy, server, HTTP, HTTPS, status code, protocol\n\nA Shiny app on a server at \\(\\text{http://my-shiny.com}\\) where Shiny Server is sitting on port \\(80\\).\nJupyterHub on a server at \\(\\text{https://example.com/jupyter}\\) where Jupyter is on port \\(8000\\)."
  },
  {
    "objectID": "chapters/sec2/2-6-networking.html#lab-making-it-accessible-in-one-place",
    "href": "chapters/sec2/2-6-networking.html#lab-making-it-accessible-in-one-place",
    "title": "12  Intro to Computer Networks",
    "section": "12.6 Lab: Making it accessible in one place",
    "text": "12.6 Lab: Making it accessible in one place\nIn this lab, we’re going to set up a proxy to be able to access all of our services over HTTP.\nBut first, you might want to try out accessing the various services where they are.\nYou could either try SSH tunneling to them and seeing them on localhost or you could apply custom TCP rules to your security group to temporarily allow access directly to the services. If you want to try, here’s a reminder of where everything is:\n\n\n\nService\nPort\n\n\n\n\nJupyterHub\n\\(8000\\)\n\n\nRStudio\n\\(8787\\)\n\n\nPenguin model API\n\\(8080\\)\n\n\nShiny App\n\\(3838\\)\n\n\n\nIf you’re through playing and ready to get everything configured, let’s go ahead. If you changed your security group rules, change them back.\n\n12.6.1 Step 1: Configure Nginx\nHonestly, configuring proxies is a somewhat advanced networking topic, and in most cases you’d just put one service per server. But if you want to be able to save money and run everything on one server, you’ll want a proxy.\nConfiguring Nginx is pretty straightforward – you install Nginx, put the configuration file into place, and restart the service to pick up the changes. The hard part is figuring out the right configuration. Configuring proxies can be quite painful, as the configuration is very sensitive to seemingly meaningless syntax issues.\nHere are the steps to configure your proxy on your server for JupyterHub and RStudio Server:\n\nSSH into your server.\nInstall Nginx with sudo apt install nginx.\nSave a backup of the default nginx.conf, cp /etc/nginx/nginx.conf /etc/nginx/nginx-backup.conf.6\nEdit the Nginx configuration with sudo vim /etc/nginx/nginx.conf and replace it with:\n\n\n\n/etc/nginx/nginx.conf\n\nuser www-data;\nworker_processes auto;\npid /run/nginx.pid;\ninclude /etc/nginx/modules-enabled/*.conf;\n\nevents {\n\tworker_connections 768;\n\t# multi_accept on;\n}\n\nhttp {\n\n map $http_upgrade $connection_upgrade {\n    \t\tdefault upgrade;\n    \t\t''      close;\n  }\n\n  server {\n    listen 80;\n\n    location /rstudio/ {\n      # Needed only for a custom path prefix of /rstudio\n      rewrite ^/rstudio/(.*)$ /$1 break;\n\n      # Use http here when ssl-enabled=0 is set in rserver.conf\n      proxy_pass http://localhost:8787;\n\n      proxy_http_version 1.1;\n      proxy_set_header Upgrade $http_upgrade;\n      proxy_set_header Connection $connection_upgrade;\n      proxy_read_timeout 20d;\n\n      # Not needed if www-root-path is set in rserver.conf\n      proxy_set_header X-RStudio-Root-Path /rstudio;\n\n      # Optionally, use an explicit hostname and omit the port if using 80/443\n      proxy_set_header Host $host:$server_port;\n    }\n\n    location /jupyter/ {\n      # NOTE important to also set bind url of jupyterhub to /jupyter in its config\n      proxy_pass http://127.0.0.1:8000;\n\n      proxy_redirect   off;\n      proxy_set_header X-Real-IP $remote_addr;\n      proxy_set_header Host $host;\n      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n      proxy_set_header X-Forwarded-Proto $scheme;\n\n      # websocket headers\n      proxy_set_header Upgrade $http_upgrade;\n      proxy_set_header Connection $connection_upgrade;\n    }\n  }\n}\n\n\n\nTest that your configuration is valid sudo nginx -t.\nStart Nginx with sudo systemctl start nginx. If you see nothing all is well.\n\nIf you need to change anything, update the config and then restart with sudo systemctl restart nginx.\n\n\n12.6.2 Step 2: Open port 80\nNow, if you try to go to your your server’s public IP address or DNS, your browser will spin for a while and nothing will happen. That’s because the AWS security group still only allows SSH access on port \\(22\\). We need to add a rule that will allow HTTP access on port \\(80\\).\nOn the AWS console page for your instance, find the Security section and click into the security group for your instance. You want to add a new inbound HTTP rule that allows access on port \\(80\\) from anywhere. Make sure not to get rid of the rule that allows SSH access on \\(22\\). You still need that one too.\nOnce you do this, you should be able to visit your server address and get the default Nginx landing page.\n\n\n12.6.3 Step 3: Configure your subpaths\nComplex web apps like RStudio and JupyterHub frequently proxy traffic back to themselves. For example, when you launch a Shiny app in RStudio, you’re actually just opening a “headless” browser window that gets proxied back into your session.\nThis works by default when those apps are on the root path \\(\\text{/}\\). That’s not true in this case, so we’ve got to let the services know where they’re actually located.\nConfiguring RStudio Server is already done. The X-RStudio-Root-Path line in the Nginx configuration adds a header to each request coming through the proxy that tells RStudio Server that it’s on the \\(\\text{/rstudio}\\) path.\nJupyterHub needs a configuration update to let it know that it’s on a subpath. Luckily it’s a very simple change. You can edit the Jupyter configuration with\n\n\nTerminal\n\nsudo vim /etc/jupyterhub/jupyterhub_config.py\n\nFind the line that reads # c.JupyterHub.bind_url = 'http://:8000'.\n\n\n\n\n\n\nTip\n\n\n\nYou can search in vim from normal mode with / &lt;thing you're searching for&gt;. Go to the next hit with n.\n\n\nDelete the # to uncomment the line and add the subpath on the end. If you’re using the \\(\\text{/jupyter}\\) subpath and the default \\(8000\\) port, that line will read c.JupyterHub.bind_url = 'http://:8000/jupyter'.\nJupyterHub should pick up the new config when it’s restarted with\n\n\nTerminal\n\nsudo systemctl restart jupyterhub\n\n\n\n12.6.4 Step 4: Try it out!\nNow we should have each service configured on a subpath. RStudio Server at \\(\\text{/rstudio}\\), JupyterHub at \\(\\text{/jupyter}\\). For example, with my server at \\(\\text{64.56.223.5}\\), I can get to RStudio Server at \\(\\text{http://64.56.223.5/rstudio}\\).\nNote that right now, this server is on HTTP, which is not a best practice. In fact, it’s such a bad practice that your browser will probably autocorrect the url to https and you’ll have to manually correct it back to http and ignore some scary warnings. Don’t worry, we’ll fix this in Chapter 14.\n\n\n12.6.5 Lab Extensions\nIf you’ve gone to the root URL for your server, you’ve probably noticed that it’s just the default Nginx landing page, which is not very attractive.\nYou might want to create a landing page with links to the subpath by serving a static html page off of \\(\\text{/}\\). Or maybe you want one of the services at \\(\\text{/}\\) and the others at a different subpath.\nRight now, neither the penguins model API or the Shiny app are available from the outside. You might want to add them to the proxy to make them accessible. I’ll leave that as an exercise for you.\n\n\n\n\n\n\nTip\n\n\n\nIt’s very common to put an API and/or a Shiny app behind a proxy. Googling “Shiny app behind nginx” or “FastAPI with nginx” will yield good results.\n\n\nOne thing to consider is whether the model API should be publicly accessible at all. If the only thing calling it is the Shiny app, maybe it shouldn’t be?"
  },
  {
    "objectID": "chapters/sec2/2-6-networking.html#footnotes",
    "href": "chapters/sec2/2-6-networking.html#footnotes",
    "title": "12  Intro to Computer Networks",
    "section": "",
    "text": "URLs are a subset of a broader category called Uniform Resource Identifiers (URIs), which look like a URL and are used to identify a resource by may not be a valid address. I mention them only because you may run across them in certain contexts, like configuring SSO.↩︎\nDifferent resources divide URLs into somewhere between three and seven parts. I think these four are the most useful for this chapter’s purpose.↩︎\nThe idea is actually pretty clever. Routers are arranged in a tree-like structure. Each router only keeps track of any downstream addresses and a single upstream default address. So the packet gets passed upstream until it hits a router that knows about the target IP address and then back downstream to the right place.↩︎\nPorts are also used for outbound communication. Computers know how to automatically open ports for outbound communication and specify that’s where the response should come, so we’re not going to get into them here.↩︎\nAs you might guess from this complicated syntax, you can do a lot more than this with SSH tunneling, but this is most often what I use it for.↩︎\nThis is generally a good practice before you start messing with config files. Bad configuration is usually preferable to a service that can’t start at all because you’ve messed up the config so badly. It happens.↩︎"
  },
  {
    "objectID": "chapters/sec2/2-7-dns.html#basics-of-dns-and-domains",
    "href": "chapters/sec2/2-7-dns.html#basics-of-dns-and-domains",
    "title": "13  DNS allows for human-readable addresses",
    "section": "13.1 Basics of DNS and domains",
    "text": "13.1 Basics of DNS and domains\nWhen you create or launch a website, you’ll purchase (rent, really) a domain like \\(\\text{do4ds.com}\\) from a domain name registrar. Purchasing a domain gives you the right to attach that domain to an IP Address.\nWhen someone comes to visit your website, their computer resolves the IP Address via a DNS lookup against public DNS nameservers. So when you purchase a domain, you register the association between your domain and the IP Address with the DNS nameservers so users can look them up.\n\nA complete domain is called a Fully-Qualified Domain Name (FQDN) and consists of three parts:\n\\[\n\\overbrace{\\text{blog}}^{\\text{Subdomain}}.\\overbrace{\\underbrace{\\text{example}}_{\\text{Domain Name}}.\\underbrace{\\text{com}}_{\\text{Top-Level Domain}}}^{\\text{Root Domain}}\n\\]\nWhen you get a domain from a registrar, the thing you’ll actually rent is the root domain. You can choose any root domain you want, as long as it’s not already taken. Domain names are unique only within top-level domains, so you might be able to get \\(\\text{example.fun}\\) even if someone else owns \\(\\text{example.com}\\).\n\n\n\n\n\n\nTop-level domains\n\n\n\nWhen the web first launched, there were only a limited number of top-level domains like \\(\\text{.com}\\), \\(\\text{.org}\\), and \\(\\text{.net}\\). ICANN, the group that controls how domains are assigned, controlled them all.\nIn 2013, ICANN decided to allow people and organizations to register their own top-level domains. That’s why there’s been an explosion in websites at top level domains like \\(\\text{.io}\\), \\(\\text{.ai}\\), and \\(\\text{.fun}\\) in the last decade or so.\nI think it’d be fun to have my own top-level domain, so it’s unfortunate that owning a top-level domain is not something to do on a whim. In 2012, the initial application fee was $185,000.\n\n\nSubdomains are a way to specify a part of a domain, usually to signify to users that its for a distinct purpose. You can generally register as many subdomains as you want against a root domain."
  },
  {
    "objectID": "chapters/sec2/2-7-dns.html#how-dns-is-configured",
    "href": "chapters/sec2/2-7-dns.html#how-dns-is-configured",
    "title": "13  DNS allows for human-readable addresses",
    "section": "13.2 How DNS is configured",
    "text": "13.2 How DNS is configured\nDNS is configured by giving the proper IP Address information to the domain name registrar where you bought your domain. There’s often some minor configuration on the server side as well to let the server know where it lives.\nMost places you might serve a website from have instructions on how to do configure it you can google. The point of this section is just to make you comfortable with whatever instructions you find in your googling.\nThere are a variety of different domain name registrars. Each of the big 3 has their own and there are a number of others including Namecheap (my personal favorite as it is indeed, cheap), Cloudflare, and GoDaddy.\nCosts for domain names vary widely. Buying a meaningless domain in a less popular top-level domain can cost as little as a few dollars per year. For example, I was able to get the domain \\(\\text{do4ds-lab.shop}\\) for $1.98 for a year on Namecheap.\nOn the other hand, buying a \\(\\text{.com}\\) domain that’s a real word or phrase can be thousands of dollars. There are articles every few years about some major company accidentally allowing their domain name to lapse and ransoming it back for stupid amounts of money.\nOnce you’ve purchased your domain, you need to configure the public DNS records of your domain to point to the IP Address you want. Configuration of DNS is done by way of records. Records map a path or host to a target.\nDepending on the type of mapping, records fall into a number of different categories, but there are only a few you’re likely to see:\n\nA records (or their IPv6 cousin AAAA records) map a domain to an actual IP Address.\nCNAME records alias subdomains to another record.\nNS records tells the DNS server to forward the request on to another namespace server. This is usually only used by big organizations that run their own domain name servers for their subdomains.\n\nWhen you go to configure DNS with your domain name registrar, you’ll configure the records in a record table. Here’s an imaginary DNS record table for the domain \\(\\text{example.com}\\):\n\n\n\nPath/Host\nType\nTarget\n\n\n\n\n\\(\\text{@}\\)\nA\n\\(\\text{64.56.223.5}\\)\n\n\n\\(\\text{www}\\)\nCNAME\n\\(\\text{example.com}\\)\n\n\n\\(\\text{blog}\\)\nA\n\\(\\text{114.13.56.77}\\)\n\n\n\\(\\text{*}\\)\nA\n\\(\\text{64.56.223.5}\\)\n\n\n\nThe first row provides an A record for the special \\(\\text{@}\\) symbol meaning exact match. So by this configuration, any traffic to \\(\\text{example.com}\\) will be passed straight through to the specified IP Address.\nThe second row deals with traffic to the \\(\\text{www}\\) subdomain. Since this is a CNAME record, this record indicates that traffic to \\(\\text{www.example.com}\\) should be treated exactly like traffic to the bare \\(\\text{example.com}\\). Some domain providers do automatic redirection of \\(\\text{www}\\) traffic, and so this row may not be necessary in some configurations.\nThe next record sends the \\(\\text{blog}\\) subdomain to a completely different IP Address. This is a common configuration when the subdomain might be owned by a completely different group than the main website or is served from a different server.\nThe last record uses the wildcard symbol \\(\\text{*}\\) to send all subdomain traffic that’s not already spoken for back to the main IP Address.\nOther than the \\(\\text{www}\\) subdomain, which stands for world wide web, and is generally routed to the same place as the bare root domain, using subdomains and choosing between subdomains and paths is entirely about organizing how users experience your website and what’s easiest for your organization to maintain.\n\n\n\n\n\n\n\\(\\text{www}\\) is just a subdomain\n\n\n\nWhen the internet was first started, it seemed like it might be important to differentiate the subdomain where the website would live from, for example, the email domain people at that organization would use.\nThat turned out not really to be the case and these days \\(\\text{www}\\) and the bare root domain are generally used interchangeably.\n\n\nOnce you’ve configured your DNS records, you will need to wait an annoyingly long time to see if you did it correctly.\nWhen your computer does a DNS lookup, there are often at least three nameservers involved. First, your computer talks to a resolver, which is a server that keeps track of where the nameservers are. Then you’re routed to the correct nameserver for the top-level domain, which routes you to the nameserver for your actual domain.\nAnd then there are various geographic repeats of this whole set of servers to make sure your query doesn’t get slowed down by having to travel too far.\nThat means that when you update a DNS record, it can take some time to propagate to all these servers. The other issue is that your computer, and each of these servers, tries to do as few lookups as possible, because they’re slow.\nWhen you configure a DNS record, you configure something called the TTL (time to live). Each level of DNS lookup, may cache the results of looking up a domain to avoid having to ask the next level. How long that cache lasts is configured by the TTL.\nThis is great if you are using the internet and don’t want to wait for DNS lookups, but when you’re changing the domains on servers you control, there are many DNS servers that a request could get routed to, and many of them may have outdated cache entries. That’s why DNS changes can take up to 24 hours to propagate.\nThat means that if you make a change and it’s not working, you have no idea whether you made a mistake or it just hasn’t propagated yet. It’s very annoying."
  },
  {
    "objectID": "chapters/sec2/2-7-dns.html#comprehension-questions",
    "href": "chapters/sec2/2-7-dns.html#comprehension-questions",
    "title": "13  DNS allows for human-readable addresses",
    "section": "13.3 Comprehension Questions",
    "text": "13.3 Comprehension Questions\n\nWhat are the parts of a fully-qualified domain name? How does each of them get created?\nHow does your computer find the IP Address for a domain? Why could it sometimes be wrong?\nWhat are the different kinds of DNS records you’re likely to use?"
  },
  {
    "objectID": "chapters/sec2/2-7-dns.html#lab-configuring-dns-for-your-server",
    "href": "chapters/sec2/2-7-dns.html#lab-configuring-dns-for-your-server",
    "title": "13  DNS allows for human-readable addresses",
    "section": "13.4 Lab: Configuring DNS for your server",
    "text": "13.4 Lab: Configuring DNS for your server\nIn the last lab, we configured the server so that all of the services were served off of one single port that redirected to various subpaths.\nNow we want to get a real, memorable domain for the server so that you and your users don’t have to remember some random ec2- domain or an IP Address. In this lab, we’ll configure DNS records for our server so it’s available at a real domain.\n\n13.4.1 Step 1: Allocate an Elastic IP\nEC2 instances get assigned a public IP when they are started. The issue is that every time the server stops that IP is released and a new one is assigned when it comes back up. This means you’d have to change your DNS record every time you temporarily take your server offline – no good.\nLuckily, AWS has a great solution here called Elastic IP. which gives you a stable public IP Address you can move from one instance to another as you wish.\n\n\n\n\n\n\nPaying for Elastic IPs\n\n\n\nElastic IPs are free as long as they’re attached to a running EC2 instance. You pay when they’re not in use to discourage hoarding them.\nIf you do take your server down for a short time, it’s no big deal. As of this writing, it’s 12 cents per day for a single IP. But do make sure to release the Elastic IP if/when you take your server down permanently.\n\n\nTo set up your Elastic IP, find it in the AWS console and allocate an address. Then you will associate your Elastic IP as the default public IP Address for your instance.\nNote that once you make this change, your server will no longer be available at its old IP Address, so you’ll have to SSH in at the new one. If you have SSH terminals open when you make the change, they will break.\n\n\n\n\n\n\nNote\n\n\n\nNext time you stand up a server, you should start by giving it an Elastic IP so you are immediately using it’s permanent IP Address. The labs in this book are ordered to promote learning, not the right order to configure things.\n\n\n\n\n13.4.2 Step 2: Buy a domain\nYou can buy a domain from any of the many domain purchasing services on the web. This won’t be free, but many domains are very cheap.\nThe easiest place to buy a domain is via AWS’s Route53 service, but you can feel free to use another provider. I usually use Namecheap just because all of the domains I own are there.\n\n\n13.4.3 Step 3: Configure DNS\nOnce you’ve got your domain, you have to configure your DNS. You’ll have to create 2 A records – one each for the \\(\\text{@}\\) host and the \\(\\text{*}\\) host pointing to your IP and one for the CNAME at the \\(\\text{www}\\) with the value being your bare domain.\nExactly how you configure this will depend on the domain name provider you choose. In NameCheap, you configure this via a table under Advanced DNS, which looks like this.\n\n\n\nType\nHost\nValue\nTTL\n\n\n\n\nA Record\n\\(\\text{*}\\)\n\\(\\text{64.56.223.5}\\)\nAutomatic\n\n\nCNAME Record\n\\(\\text{www}\\)\n\\(\\text{do4ds-lab.shop}\\)\nAutomatic\n\n\nA Record\n\\(\\text{@}\\)\n\\(\\text{64.56.223.5}\\)\nAutomatic\n\n\n\nI would recommend sticking with the default for TTL.\n\n\n13.4.4 Step 4: Wait an annoyingly long time\nNow you just have to be patient. Unfortunately DNS takes time to propagate. After a few minutes (or hours?), your server should be reachable at your domain.\nIf it’s not (yet) reachable, try seeing if an incognito browser works because that sidesteps the browser level of caching. If it doesn’t, wait some more. When you run out of patience, try reconfiguring everything and check if it works now.\n\n\n\n\n\n\nTip\n\n\n\nWe still haven’t configured HTTPS, so you’ll need to manually input the URL as \\(\\text{http://}\\), because your browser will otherwise assume it’s HTTPS.\n\n\n\n\n13.4.5 Step 5: Add the Shiny app to your site\nNow that the Shiny app is at a stable URL, let’s put it on our site so people can look at our penguin size prediction model. I put the app at the subpath \\(\\text{/penguins}\\), so it’s now at \\(\\text{http://do4ds-lab.shop/penguins}\\).\nWe’re going to use something called an iFrame, which lets you embed one website inside another. An iFrame is a basic HTML construct and it’s easy to put one in a Quarto site.\n\n\n\n\n\n\nNote\n\n\n\nOnce you change your website to go over HTTPS in the next section, you’ll have to adjust the iFrame URL as well.\n\n\nIn Quarto, you can just add an html block to a any document and it will get loaded in automatically. I want the app on the landing page of my site, index.qmd. So I’ve added a block that looks like:\n\n\nindex.qmd\n\n&lt;iframe width=\"780\" height=\"500\" src=\"http://do4ds-lab.shop/penguins/\" title=\"Penguin Model Explorer\"&gt;&lt;/iframe&gt;"
  },
  {
    "objectID": "chapters/sec2/2-7-dns.html#footnotes",
    "href": "chapters/sec2/2-7-dns.html#footnotes",
    "title": "13  DNS allows for human-readable addresses",
    "section": "",
    "text": "This is really a very shallow intro to DNS. If you want to go a little deeper, I highly recommend Julia Evans’s zines on a variety of technical topics, including DNS. You can find them at wizardzines.com.↩︎"
  },
  {
    "objectID": "chapters/sec2/2-8-ssl.html#what-https-does",
    "href": "chapters/sec2/2-8-ssl.html#what-https-does",
    "title": "14  You should use SSL/HTTPS",
    "section": "14.1 What HTTPS does",
    "text": "14.1 What HTTPS does\nHTTPS is the same as HTTP, but secured with a technology called SSL/TLS (secure sockets layer/transport layer security). SSL/TLS works by configuring the resource to provide an SSL certificate upon demand, which is used to verify the site’s identity and establish an encrypted session.\n\n\n\n\n\n\nSSL vs TLS\n\n\n\nTLS is actually what’s in use these, but you’ll mostly talk about SSL. TLS is the successor to SSL, but the configuration is identical, so most people didn’t update how they talk about it.\n\n\nYou use HTTPS constantly. Go to a website in your browser and look for a little lock icon near the search bar. That little lock indicates that the domain is secured using HTTPS. If you click on it, you can get more information about the site’s SSL certificate.\nIf you’re of a certain age, you may recall warnings that you shouldn’t use the WiFi at your neighborhood Starbucks. The issue was twofold.\nHTTP has no way to verify that the website you think you’re interacting with is actually that website. So a bad actor could put up a fake WiFi network that makes \\(\\text{bankofamerica.com}\\) resolve to a lookalike website to capture your banking information. That’s called a man-in-the-middle attack.\nAnd even if they didn’t do that, they could inspect the traffic going back and forth and just read whatever you’re sending over the web in what’s called a packet sniffing attack.\nIn 2015, Google Chrome began the process of marking any site using plain HTTP as insecure, which led to nearly complete adoption of HTTPS across the internet. The risk of both of these kinds of attacks has been neutered and it’s actually pretty safe to use any random WiFi network you want these days – because of HTTPS.\nAs a website administrator, securing your website or server with HTTPS is one of the most basic things you can do to make sure your website traffic is safe. You should always configure HTTPS for a public website – full stop.\nIt’s worth noting that this SSL/TLS security can be applied to a number of different application layer protocols, including (S)FTP and LDAP(S). You may run across these depending on your organization. In any case, the SSL/TLS part works the same and all that changes is what’s inside the secure digital envelope."
  },
  {
    "objectID": "chapters/sec2/2-8-ssl.html#how-ssltls-works",
    "href": "chapters/sec2/2-8-ssl.html#how-ssltls-works",
    "title": "14  You should use SSL/HTTPS",
    "section": "14.2 How SSL/TLS works",
    "text": "14.2 How SSL/TLS works\nSSL/TLS uses public key encryption (remember, we learned about that in Chapter 8) to do two things – validate that the site you’re visiting is the site you intend and encrypt the traffic back and forth to the site.\nTo set up SSL for a website, you create or acquire an SSL certificate, which has a public and a private component (sound familiar?).1 Then, verify the public certificate with a trusted Certificate Authority (CA) and put the private certificate in the right place on the website.2\nThen, when you go to access that resource, the first thing your machine asks for is a signature. The site uses its private key to generate the signature and your machine verifies the signature against it’s internal trusted CA store.\nNow your machine knows it’s communicating with the right host on the other side and you’re not falling victim to a man-in-the-middle attack.\nOnce the verification process is done, your machine and the remote on the other side create temporary session keys to establish encryption with the website on the other end.3 Only then does it start sending real data, now encrypted securely inside a digital envelope."
  },
  {
    "objectID": "chapters/sec2/2-8-ssl.html#getting-and-using-ssl-certificates",
    "href": "chapters/sec2/2-8-ssl.html#getting-and-using-ssl-certificates",
    "title": "14  You should use SSL/HTTPS",
    "section": "14.3 Getting and using SSL certificates",
    "text": "14.3 Getting and using SSL certificates\nWhen you buy a computer, it comes configured out of the box to trust a small number of official CAs. So if you’re a website wanting to get a certificate, you generally want to get it from one of those CAs.\nThis used to be kinda a pain. The CAs charged to issue certificates. While it was only $10 per year to for a basic SSL certificate, they typically would only cover a single subdomain. A wildcard certificate to cover all the subdomains of a root domain was expensive enough to discourage hobbyists.\nIf you wanted a free certificate, your only option was to use a self-signed certificate, which you’d create similarly to creating an SSH key. This was a pain because you had to manually add the public certificate to the CA store of every machine that would be accessing the site, and then re-add it when the certificate expired.4\nLuckily there’s now another option. For most small organizations, I recommend getting a free SSL certificate from the nonprofit CA Let’s Encrypt. They even have some nice tooling that makes it super easy to create and configure your certificate right on your server.\nFor most organizations, using a public CA to get SSL on public-facing resources is sufficient and use plain HTTP inside private networks. Some large organizations want to encrypt their private traffic as well and run their own private CA. If this is the case, your organization’s policies will make it clear. This can be a pain, because you’ve got to make sure every host inside the network trusts the private CA.\nOnce you’ve configured SSL/TLS, you generally want to only allow HTTPS traffic to your site. You’ll accomplish this by redirecting all HTTP traffic on port \\(80\\) to come in via HTTPS on port \\(443\\).\nSome web applications support configuring a certificate directly, while others only accept HTTP traffic, so you’d need to do SSL termination with a proxy in front of the application."
  },
  {
    "objectID": "chapters/sec2/2-8-ssl.html#comprehension-questions",
    "href": "chapters/sec2/2-8-ssl.html#comprehension-questions",
    "title": "14  You should use SSL/HTTPS",
    "section": "14.4 Comprehension Questions",
    "text": "14.4 Comprehension Questions\n\nWhat are the two risks of using plain HTTP and how does HTTPS mitigate them?\nWrite down a mental map of how SSL secures your web traffic. Include the following: public certificate, private certificate, certificate authority, encrypted traffic, port 80, port 443"
  },
  {
    "objectID": "chapters/sec2/2-8-ssl.html#lab-configure-ssl",
    "href": "chapters/sec2/2-8-ssl.html#lab-configure-ssl",
    "title": "14  You should use SSL/HTTPS",
    "section": "14.5 Lab: Configure SSL",
    "text": "14.5 Lab: Configure SSL\nWe’re going to use Let’s Encrypt’s certbot utility to automatically generate an SSL certificate, share it with the CA, install it on the server, and even update your NGINX configuration.\nIf you’ve never had to manually configure SSL in the past, let me tell you, this is magical!\n\n14.5.1 Step 1: Follow instructions to add SSL for NGINX\nUsing Let’s Encrypt to add an SSL certificate to NGINX configuration is a super common task. As of this writing, there’s a great blog post entitled Using Free Let’s Encrypt SSL/TLS Certificates with NGINX. I’d encourage you to look for that article (or something similar) and follow the steps there.\nAt a high level, what you’ll do is\n\nConfigure the NGINX configuration to know what domain its on.\nInstall certbot on the system.\nRun certbot to get the certificate, apply it to the server, and update the NGINX configuration.\n\nBefore you move along, I’d recommend you take a moment and inspect the /etc/nginx/nginx.conf file to look at what certbot added.\nRelative to the old version, you’ll notice two things. First, the line that read listen 80 is gone from the server block because we’re no longer listening for HTTP traffic. In it’s place, there’s now a listen 443 along with a bunch of stuff that tells NGINX where to find the certificate on the server.\nScrolling down a little, there’s a new server block that is listening on \\(80\\). This block returns a 301 status code (permanent redirect) and sends traffic to HTTPS on \\(443\\).\n\n\n14.5.2 Step 2: Let RStudio Server know it’s on HTTPS\nBefore we exit and test it out, let’s do one more thing. As mentioned when we configured NGINX the first time, RStudio Server does a bunch of proxying traffic back to itself, so it needs to know that it’s on HTTPS.\nYou can let RStudio Server know that it’s on HTTPS by adding a header to all traffic letting RStudio Server know the protocol is HTTPS. You can add this line to your nginx.conf:\n\n\n/etc/nginx/nginx.conf\n\nproxy_set_header X-Forwarded-Proto https;\n\nOk, now try to visit RStudio Server at your URL, and you’ll find that…it’s broken again.\nBefore you read along, think for just a moment. Why is it broken?\n\n\n14.5.3 Step 3: Configure security groups\nIf your thoughts went to something involving ports and AWS security groups, you’re right!\nBy default, our server was open to SSH traffic on port \\(22\\). Since then, we may have opened or closed port \\(80\\), \\(8000\\), \\(8080\\), \\(8787\\), and/or \\(3838\\).\nBut now that we’re exclusively sending HTTPS traffic into the proxy on \\(443\\) and letting the proxy redirect things elsewhere. So you have to go into the security group settings and change it so there are only 2 rules – one that allows SSH traffic on \\(22\\) and one that allows HTTPS traffic on \\(443\\).\nIt’s up to you whether you want to leave port \\(80\\) open. If you do, it will redirect people to HTTPS on \\(443\\). If you close it entirely, people who come to port \\(80\\) will be blocked and will eventually get a timeout. If people are used to coming to the server via HTTP, it might be nice to leave \\(80\\) open so they get a nice redirect experience instead of getting confusingly blocked.\n\n\n14.5.4 Step 4: We did it!\nThis is the end of the labs in this book.\nAt this point your server is fully configured. You have three real data science services available on a domain of your choosing protected by HTTPS and you can SSH in to do admin work.\nTake a moment to celebrate. It’s very cool to be able to stand up and administer your own data science workbench. If you’re working at a small organization or are a hobbyist, you can really use this server to do real data science work.\nBut this server isn’t enterprise-ready. If you work at a large organization or one with stringent security or privacy rules, your IT/Admin group is going to have concerns. Read on to learn more about what they are and why they’re completely valid."
  },
  {
    "objectID": "chapters/sec2/2-8-ssl.html#footnotes",
    "href": "chapters/sec2/2-8-ssl.html#footnotes",
    "title": "14  You should use SSL/HTTPS",
    "section": "",
    "text": "Like with SSL, this makes more sense if you think key where you see private and lock where you see public.↩︎\nThe CA verifies the certificate by signing it. Your machine just keeps public certificates for the CAs. Then, when it gets an SSL certificate that’s signed by one of those CAs, it can validate that the CA actually stamped this certificate as valid.↩︎\nUnlike the asymmetric encryption used by SSL and SSH for the public key encryption, the session keys are symmetric, so they work the same in both directions.↩︎\nYou also could skip that step, in which case you got the session encryption benefits of SSL/TLS, but not the verification.↩︎"
  },
  {
    "objectID": "chapters/sec3/3-0-sec-intro.html#creating-a-devops-culture",
    "href": "chapters/sec3/3-0-sec-intro.html#creating-a-devops-culture",
    "title": "Enterprise-grade data science",
    "section": "Creating a DevOps culture",
    "text": "Creating a DevOps culture\nAs a data scientist, your primary concern about your data science environment is that it’s useful. You want the latest versions of Python or R, abundant access to packages, and data right at your fingertips.\nGreat IT/Admin teams also care about the usefulness of the system to users (that’s you), but it’s usually a distant third to their primary concerns of security and stability. And there’s a benefit to you from that. You may be primarily focused on getting stuff done minute-to-minute, but an insecure or unstable data science platform is not useful.\nThere’s a reason why these concerns primarily arise in an enterprise context. If you’re a team of three data scientists who sit next to each other, someone crashing your workbench server is probably a fun chance to drink some coffee and learn something new about how servers work.\nBut if you’re working on an enterprise-grade data science workbench, it’s really frustrating if someone three teams over can disturb your work. And you definitely don’t want to work in an environment where you constantly have to be thinking about security risks.\nBalancing security, stability, and usefulness is always about tradeoffs. After all, the most secure and stable computing environment is the one that doesn’t exist at all. Organizations that do DevOps right embrace this tension and are constantly figuring out the right stance for the organization as a whole.\nIt is an unfortunate truth that many IT/Admin organizations don’t act as partners. They act as gatekeepers to the resources you need to do your job, which can be incredibly frustrating. That means you’ll have to figure out how to communicate with those teams, understand what matters to them, help them understand what matters to you, and reach minimally acceptable organizational outcomes."
  },
  {
    "objectID": "chapters/sec3/3-0-sec-intro.html#a-hierarchy-of-itadmin-concerns",
    "href": "chapters/sec3/3-0-sec-intro.html#a-hierarchy-of-itadmin-concerns",
    "title": "Enterprise-grade data science",
    "section": "A hierarchy of IT/Admin concerns",
    "text": "A hierarchy of IT/Admin concerns\nThe primary concerns of IT/Admins are security and stability. A system that is secure and stable gives valid users access to the systems they need to get work done and doesn’t give access to people who shouldn’t have access. If you understand and communicate with IT/Admins about the risks they perceive, it can help generate buy-in from them that you’re taking their concerns seriously.\nThe worst outcome for a supposedly secure data science platform would be for an unauthorized person to gain access to and steal data. In the most extreme form, this is someone completely outside the organization (outsider threat). But it also could be someone inside the organization who is disgruntled, seeking personal gain, or just careless (insider threat). A related risk is someone hijacking systems that don’t contain sensitive data and appropriating your organization’s computational resources for nefarious ends like crypto mining or virtual DDOS attacks on Turkish banks.1\nSomewhat less concerning, but still the stuff of IT/Admin nightmares is platform instability that results in the erasure of important data, called data loss. And even if data isn’t permanently lost, instability that results in lost time for platform users is also bad.\nDepending on the context, IT/Admins may have some stake in ensuring that they don’t allow in software that results in bad or incorrect work. And last, way down the list, is that the users don’t have a bad experience using the environment."
  },
  {
    "objectID": "chapters/sec3/3-0-sec-intro.html#enterprise-tools-and-techniques",
    "href": "chapters/sec3/3-0-sec-intro.html#enterprise-tools-and-techniques",
    "title": "Enterprise-grade data science",
    "section": "Enterprise tools and techniques",
    "text": "Enterprise tools and techniques\nConceptually, enterprise IT/Admins are always trying to implement security in layers. This means that an application or server has multiple different kinds of security in place, making it harder for an accident or breach to occur.\nAt every layer, sophisticated organizations are trying to implement the principle of least privilege. The idea is that they are trying to keep systems safe and stable by giving people the permissions they need to get their work done – and no more. For example, while you might want to have root access on your data science workbench, you are almost certainly not getting it if you work in an enterprise context, because you don’t really need it.\nThere is no one-size-fits-all (or even most) way to implement these things. Depending on the relative value of keeping information tightly controlled or more broadly available at your organization, you may have different solutions in place.\n\n\n\n\n\n\nBuild or Buy?\n\n\n\nOne big question any enterprise IT/Admin faces when creating a data science environment is whether to buy something or build it. Some IT/Admin organizations prefer to build data science platforms straight from open source tools like JupyterHub and RStudio Server on a handful of VMs or in Kubernetes. Others prefer to buy a whole solution from a vendor.\nIn my experience, enterprises with extraordinarily competent IT/Admin teams can be better off building, but they are extremely rare. Most organizations that try to build their own environments from scratch end up with major pain points that would be solved by purchasing a solution.\nI am obviously biased on this front, working for a company that sells this software, but I have seen many organizations decide to build something and decline to buy Posit’s Pro Products, only to come back 6 or 12 months later having discovered that DIY-ing an enterprise-ready data science environment from open source components is hard.\n\n\nThe most blunt tool an IT/Admin team has to keep bad actors out is networking. If the network is secure, it’s hard for bad actors to infiltrate and hard for insiders to accidentally or intentionally exfiltrate valuable data. That’s why Chapter 15 is about how enterprises approach networking to create highly secure environments.\nIf you work in a small organization, it’s likely that everyone in the organization has access to nearly everything. As the organization gets larger or security concerns rise, making sure that people have access to the systems they need – and only the systems they need – becomes a higher priority and sophisticated approaches are needed to keep the solutions manageable. In Chapter 16, you’ll learn the basics of how sophisticated organizations think about providing a way to log in to different systems and how you can make use of those tools.\nIf the environment is secure, concerns turn to how to make sure it has sufficient horsepower to support the users that need it, and how to make sure that upgrades and other changes to the environment can be done smoothly. That’s why Chapter 17 is all about how enterprises manage computational resources to ensure stability, especially when a lot of them are needed.\nLastly, there are your concerns as a data scientist. In particular, using open source R and Python packages can be complicated in an enterprise context. That’s why Chapter 18 is all about the difficulties I’ve observed for organizations using open source packages and the solutions I’ve seen work for those environments.\nMy hope is that by the time you’ve finished these chapters, you’ll be in great shape to articulate exactly the needs for your enterprise data science environment and to be a great partner to IT/Admin when issues, tension, or problems arise."
  },
  {
    "objectID": "chapters/sec3/3-0-sec-intro.html#no-labs-in-this-section",
    "href": "chapters/sec3/3-0-sec-intro.html#no-labs-in-this-section",
    "title": "Enterprise-grade data science",
    "section": "(No) labs in this section",
    "text": "(No) labs in this section\nAs this section is all about developing mental models and language to have great collaboration with IT/Admins, there are no labs in this section of the book. There are a lot of pictures and comprehension questions to make sure you’ve really grasped the material."
  },
  {
    "objectID": "chapters/sec3/3-0-sec-intro.html#footnotes",
    "href": "chapters/sec3/3-0-sec-intro.html#footnotes",
    "title": "Enterprise-grade data science",
    "section": "",
    "text": "Yes, both things I’ve actually seen happen.↩︎"
  },
  {
    "objectID": "chapters/sec3/3-1-ent-networks.html#enterprise-networks-are-private",
    "href": "chapters/sec3/3-1-ent-networks.html#enterprise-networks-are-private",
    "title": "15  Enterprise Networking",
    "section": "15.1 Enterprise networks are private",
    "text": "15.1 Enterprise networks are private\nAn enterprise network houses dozens or hundreds of servers, each with its own connection requirements. Some are accessible to the outside, while others are only accessible to other servers inside the network.\nFor that reason, the servers an enterprise controls all live inside one or more private networks. Every host in a private network has a private IP Address, which is valid only inside the private network. Those IP Addresses are handed out by the private router that governs the network. And like a public IP Address is aliased to a domain for ease of remembering, many private networks make use of hostnames to have human-friendly ways to talk about servers.\n\n\n\n\n\n\nWhere’s my private network?\n\n\n\nIn AWS, every server lives inside a private network called a virtual private cloud (VPC). When we set up our data science workbench throughout Section 2, we basically just ignored the VPC and assigned the instance a public IP address, which is why this is the first time you’re hearing about it.\nIn an enterprise context, this kind of configuration would be a no-go.\n\n\nWhen there are this many services running inside a network, networking requirements can get somewhat byzantine. For example, you probably want to set up a data science workbench and a data science hosting environment that should be reachable from user’s laptops and that needs to reach one or more databases. They also may need to access a public package repository.\nBut the servers that house the databases should be accessible from the workbench and wherever the data is being loaded from, but probably shouldn’t be directly accessible from anyone’s laptop. And in order to comply with the principle of least privilege, you don’t want any of these servers to be more available than needed.\n\nAnd that’s just the servers for actually doing work. Enterprise networks also host a variety of devices that are purely for controlling the flow of traffic through the network. So when you’re working in a data science environment and running into trouble, one of the first questions to ask yourself is whether the issue could be with network traffic struggling to get into, out of, or across the private network."
  },
  {
    "objectID": "chapters/sec3/3-1-ent-networks.html#the-shape-of-an-enterprise-network",
    "href": "chapters/sec3/3-1-ent-networks.html#the-shape-of-an-enterprise-network",
    "title": "15  Enterprise Networking",
    "section": "15.2 The shape of an enterprise network",
    "text": "15.2 The shape of an enterprise network\nWhen you access something important inside a private network, the IP Address is almost never a server that actually does work. Instead, it’s usually the address of a proxy server or proxy, which is an entire server that exists just to run proxy software that routes traffic around the network.\nRouting all traffic through the proxy ensures that the important servers only ever get traffic from other servers the organization controls. This decreases the number of attack vectors to the important servers. Proxy servers may also do other tasks like terminate SSL or authentication.\n\n\n\n\n\n\nVPN vs VPC?\n\n\n\nYou may have to log into a VPN (Virtual Private Network) for work or school from your personal computer. Where a VPC is a private network inside a cloud environment, a VPN is a private network for remote access to a shared network. You generally don’t directly log into an enterprise VPC (or on-prem private network), but you might login to an adjacent VPN ensuring that anyone who accesses the network is coming from an authenticated machine.\n\n\nEnterprise networks are almost always subdivided into subnets, which are logically separate partitions of the private network.1 In most cases, the private network is divided between the public subnet or demilitarized zone (DMZ) where the proxies live and the private subnet where all the important servers live.2\n\nAside from the security benefits, putting the important servers in the private subnet is also more convenient because the IT/Admin use hostnames and IP Addresses without worrying about uniqueness outside of the private subnet. For example, they could use the hostname \\(\\text{google.com}\\) for a server because it only needs to be valid inside the private network. But that’s also bound to be confusing and I wouldn’t recommend it."
  },
  {
    "objectID": "chapters/sec3/3-1-ent-networks.html#networking-pain-follows-proxies",
    "href": "chapters/sec3/3-1-ent-networks.html#networking-pain-follows-proxies",
    "title": "15  Enterprise Networking",
    "section": "15.3 Networking pain follows proxies",
    "text": "15.3 Networking pain follows proxies\nThe simplest networking issue is that a connection simply doesn’t exist where one is needed. This is usually pretty obvious using tools like ping and curl and is straightforward to solve by working with your IT/Admin team.\nDifficulties tend to be more subtle when there are proxies involved, and enterprise networks can feature proxies all over the place. Much like the watertight bulkheads between every room on a naval ship, they show up between any two parts of the network you might want to be able to seal off at some point. And where a proxy exists, it can cause you trouble.\nIn fact, there are actually two proxies that might be messing with traffic you care about. There could be a proxy that intercepts inbound traffic and also a a proxy that intercepts outbound traffic.\n\n\n\n\n\n\nNote\n\n\n\nI’ve made up the terms inbound and outbound.\nTraditionally, proxies are classified as forward or reverse. They’re classified as if you’re a host inside the private network, so inbound proxies are reverse and outbound ones are forward. I find it nearly impossible to remember which is which. I started using inbound and outbound to keep myself clear, and I’ve always been understood on the first try. I recommend you do the same.\n\n\n\nThe first step in debugging networking issues is to ask whether there might be a proxy in the middle. You can jumpstart that discussion by clearly describing the path of the traffic so the IT/Admin can consider whether there’s a proxy in the way.\nPeople often get tripped up in this, especially when using their laptop to access a server. When you’re accessing a data science project running on a server, the only inbound traffic to the private network is the connection from your laptop to the server. Code that runs on the server can only generate outbound traffic. So nearly all the traffic you care about is outbound, including package installation, making API calls in your code with {requests} or {httr}, connecting to a git repo, or connecting to data sources.\n\n15.3.1 Issues with inbound proxies\nAlmost all private networks feature inbound proxies that handle traffic coming in from the internet. This can cause problems in a data science environment if everything isn’t configured correctly.\n\n\n\n\n\n\nWhat ports do I need?\n\n\n\nOne of the first questions IT/Admins ask is what ports need to be open in the proxy.\nDatabase traffic often runs using non-HTTP traffic to special ports. For example, Postgres runs on port \\(5432\\). However, your database traffic should probably all occur inside the private network and this won’t be an issue.\nAlmost other traffic, including package downloads, is standard HTTP(S) traffic, so it can happily run over \\(80\\) or \\(443\\).\n\n\nInbound redirection issues can be quite hairy to debug. Very often, these issues arise because the application you’re using expects to be able to redirect you back to itself. If the proxy isn’t configured quite correctly, everything can break in really funky ways.3 This is particularly likely to show up when starting new sessions or launching something from inside the platform. Usually, your application will have an admin guide that includes directions on how to host it behind a proxy. You should confirm with your admin those steps have been followed.\nProxies also often impose file size limits and/or session duration timeouts. If weird things are happening during file uploads or downloads or sessions ending unexpectedly, checking on inbound proxy settings is a good first step.\nSome data science app frameworks, including Shiny and Streamlit, use a technology called websockets for maintaining the connection between the user and the app session. Most modern proxies support websockets, but some older on-prem proxies don’t, and you may have to figure out a workaround if you can’t get websockets enabled on your proxy."
  },
  {
    "objectID": "chapters/sec3/3-1-ent-networks.html#airgapping-with-outbound-proxies",
    "href": "chapters/sec3/3-1-ent-networks.html#airgapping-with-outbound-proxies",
    "title": "15  Enterprise Networking",
    "section": "15.4 Airgapping with outbound proxies",
    "text": "15.4 Airgapping with outbound proxies\nUnlike inbound proxies, which occur on virtually every enterprise private network, outbound proxies generally only occur when the IT/Admin team needs to restrict the ability of people to go outside. This can be necessary to avoid data exfiltration or to ensure that users only acquire resources that have been explicitly allowed into the environment.\nEnvironments with limited outbound access are called offline or airgapped. The term airgap originates with machines that are physically disconnected from the outside world with a literal air gap, but truly airgapped networks are very rare. In most cases, airgapping is accomplished by putting in an outbound proxy that disallows (nearly) all connections.\nThe biggest issue in an airgapped environment is that you can’t install anything from the outside, including Python and R packages. You will be to make sure your IT/Admin understands that you cannot do your job without a way to work with packages. There’s more on managing packages in an airgapped environment in Chapter 18.\nAdditionally, your IT/Admin will have to figure out how they’re going to manage operating system updates, system library installations, and licensing any paid software that’s inside the environment.4 They likely already have solutions that might include some sort of transfer system, internal repositories, and/or temporarily opening the firewall."
  },
  {
    "objectID": "chapters/sec3/3-1-ent-networks.html#comprehension-questions",
    "href": "chapters/sec3/3-1-ent-networks.html#comprehension-questions",
    "title": "15  Enterprise Networking",
    "section": "15.5 Comprehension Questions",
    "text": "15.5 Comprehension Questions\n\nWhat is the advantage of adopting a more complex networking setup than a server just deployed directly on the internet? Are there advantages other than security?\nDraw a mental map with the following entities: inbound traffic, outbound traffic, proxy, private subnet, public subnet, VPC\nLet’s say you’ve got a private VPC that hosts an instance of RStudio Server, an instance of JupyterHub, and a Shiny Server that has an app deployed. Here are a few examples of traffic – are they outbound, inbound, or within the network?\n\nSomeone connecting to and starting a session on RStudio Server.\nSomeone SFTP-ing an app and packages from RStudio Server to Shiny Server.\nSomeone installing a package to the Shiny Server.\nSomeone uploading a file to JupyterHub.\nA call in a Shiny app using httr2 or requests to a public API that hosts data.\nAccessing a private corporate database from a Shiny for Python app using sqlalchemy.\n\nWhat are the most likely pain points for running a data science workbench that is fully offline/airgapped?"
  },
  {
    "objectID": "chapters/sec3/3-1-ent-networks.html#footnotes",
    "href": "chapters/sec3/3-1-ent-networks.html#footnotes",
    "title": "15  Enterprise Networking",
    "section": "",
    "text": "Subnets are defined as a range of IP addresses by something called a CIDR (Classless Inter-Domain Routing) block.\nEach CIDR block is defined by a starting address and a suffix that indicates the size of the range. For example, the \\(10.33.0.0/26\\) CIDR block is the 64 addresses from \\(10.33.0.0\\) to \\(10.33.0.63\\).\nEach CIDR number is half the size of the prior block, so the \\(10.33.0.0/26\\) CIDR can be split into the \\(10.33.0.0/27\\) block of 32 addresses from \\(10.33.0.0\\) to \\(10.33.0.31\\) and the \\(10.33.0.32/27\\) block for \\(10.33.0.32\\) through \\(10.33.0.63\\).\nDon’t try to remember this. There are online CIDR block calculators if you ever need to create them.↩︎\nThe public subnet usually hosts at least two proxies – one to handle regular HTTP(S) traffic and one just to route SSH traffic to hosts in the private network. The SSH proxy is often called a bastion host or jump box.\nThere are also network infrastructure devices to translate public and private IP addresses back and forth that go alongside the proxies. Private subnets have a device that only allows outbound traffic called a NAT (Network Address Translation) Gateway by AWS. Public subnets have a two-way device called an Internet Gateway by AWS.\nIt’s also very common to actually have 4 subnets and duplicate the public/private subnet configuration across two availability zones to be resilient to failures in one availability zone.↩︎\nFor example, remember those headers we had to add to traffic to RStudio Server in Chapters Chapter 12 and Chapter 14 so it knew it was on a subpath and on HTTPS.\nThis can be particularly gnarly if your proxy also does authentication. If your proxy expects that ever request has credentials attached, but your application doesn’t realize it has to go through the proxy, weird behavior can ensue.↩︎\nLicenses are often applied by reaching out to a license server owned by the software vendor.↩︎"
  },
  {
    "objectID": "chapters/sec3/3-2-auth.html#a-gentle-introduction-to-auth",
    "href": "chapters/sec3/3-2-auth.html#a-gentle-introduction-to-auth",
    "title": "16  Auth in Enterprise",
    "section": "16.1 A gentle introduction to auth",
    "text": "16.1 A gentle introduction to auth\nImagining yourself as an enterprise IT/Admin, you’ve got dozens or hundreds of different services that require auth – email, databases, servers, social media accounts, HR systems, and more. Let’s imagine each of those services as a room in a building.\nYour job is to give everyone access to the rooms they need and only the rooms they need.\nFirst, you need a way to ascertain and validate the identity of anyone who’s trying to enter a room in the building. And for that, you’ll need to issue and verify credentials. The most common computer credential is a username and password, but there are other kinds of credentials including biometrics like fingerprints and facial identification, multi-factor codes, push notifications on your phone, and ID cards. The process of verifying credentials is called authentication (authn).\nBut in an enterprise context, just knowing that someone has valid credentials is insufficient. Remember, not every person in an enterprise gets to access every system or every feature of every system. So you’ll also need a way to check their permissions, which is the binary choice of whether they’re allowed to access the service. The permissions checking process is called authorization (authz). The combination of authn and authz comprise auth.\n\nMany organizations start out simply with auth. They just add services one at a time and allow each service to use it’s own built-in functionality to issue service-specific usernames and passwords to users. This would be similar to posting a guard at the door to each room who works out a unique passphrase with each user.\n\nThis quickly becomes a mess for everyone. It’s bad for users because they need to either keep many credentials straight or reuse them over and over, which is insecure. And as an IT/Admin, adding, removing, or changing permissions is a pain, because the permissions are managed individually with each system."
  },
  {
    "objectID": "chapters/sec3/3-2-auth.html#centralizing-user-management-with-ldapad",
    "href": "chapters/sec3/3-2-auth.html#centralizing-user-management-with-ldapad",
    "title": "16  Auth in Enterprise",
    "section": "16.2 Centralizing user management with LDAP/AD",
    "text": "16.2 Centralizing user management with LDAP/AD\nIn the mid-1990s, an open protocol called LDAP (Lightweight Directory Access Protocol, pronounced ell-dapp) became popular. LDAP allows services to collect usernames and passwords from users and use them to query the central LDAP database. The LDAP server sends back information on their user, including their username and groups, which the service can use to authorize the user. Microsoft implemented LDAP as a piece of software called Active Directory (AD) that became so synonymous with LDAP that the technology is often just called LDAP/AD.\nSwitching to LDAP/AD is like changing the process in our building so the guard will radio in the credentials from the user and you’ll radio back if those credentials are valid. This is a big improvement. Having only one set of credentials makes life easier for users. We know now that all the rooms are using a similar level of security and if credential are compromised, it’s easy to swap them out.\n\nLDAP/AD provides a straightforward way to create Linux users with a home directory, so it’s often used in data science workbench contexts that have that requirement.\nLDAP/AD predates the rise of the cloud and SaaS services, and is considered a legacy technology. In particular, LDAP/AD can’t be used to centrally manage authorization. With LDAP/AD who can do what still has to be managed with each service.\nAdditionally, LDAP/AD requires the service request and pass along the user credentials which is a potential security risk and usually means that it’s not possible to incorporate now-common requirements like multi-factor authentication (MFA). These days, many organizations are getting rid of their LDAP/AD implementations and are adopting a smoother user and admin experience with cloud-friendly technologies.\n\n\n\n\n\n\nNote\n\n\n\nIt’s worth noting that LDAP/AD actually isn’t an auth technology at all. It’s a type of database that happens to be particularly well-suited to managing users in an organization. So even as many organizations are switching to more modern systems, they may just be wrappers around user data stored in an existing LDAP/AD system."
  },
  {
    "objectID": "chapters/sec3/3-2-auth.html#the-rise-of-single-sign-on-sso",
    "href": "chapters/sec3/3-2-auth.html#the-rise-of-single-sign-on-sso",
    "title": "16  Auth in Enterprise",
    "section": "16.3 The rise of Single Sign On (SSO)",
    "text": "16.3 The rise of Single Sign On (SSO)\nSingle Sign On (SSO) is when you login once at the start of your workday to a standalone identity provider and then are granted access to every service you need when you go there. These days, SSO is almost always done through a standalone identity provider like Okta, Onelogin, Ping, or Microsoft Entra ID.1\nAdvantages of SSO includes central management of authorization at the identity provider and making it easier to implement more sophisticated forms of credentialling like MFA, because they only need to be implemented by the identity provider. For many organizations, especially enterprise ones, SSO is a requirement for any new service.\n\n\n\n\n\n\nNote\n\n\n\nThe term SSO is somewhat ill-defined. It usually means the experience described here, but sometimes it just means the centralized user and credential management in an LDAP/AD system. It’s important to follow up when an IT/Admin says SSO is a requirement.\n\n\nSSO is analogous to having users exchange their credentials for a building access pass at the central security office. Each room has a machine to send a request to the central security office, where the room can be remotely unlocked if it’s ok.\n\nSSO is a description of a user and admin experience, usually accomplised by one of two technologies – SAML (Security Assertion Markup Language) or OAuth (Open Identity Connect/OAuth2.0).2 From the user perspective, using SAML and OAuth are very similar. Your organization’s IT/Admins are likely to use OAuth to work with external SaaS services and may have a slight preference for SAML for services inside your organization’s firewall.\nAs I write this in 2023, there’s a major shift underway from legacy systems like LDAP/AD to cloud-friendly SSO systems and enhanced security they enable. In particular, the use of non-password credentials like biometrics and passphrases and the use of OAuth to do sophisticated authorization management inside enterprises are relatively uncommon now, but are likely to be standard practices within a few years."
  },
  {
    "objectID": "chapters/sec3/3-2-auth.html#managing-permissions",
    "href": "chapters/sec3/3-2-auth.html#managing-permissions",
    "title": "16  Auth in Enterprise",
    "section": "16.4 Managing permissions",
    "text": "16.4 Managing permissions\nIrrespective of the technology used, your organization is going to have policies about how they manage permissions that you’ll need to incorporate or adopt.\n\n\n\n\n\n\nNote\n\n\n\nThere are meaningful differences in how LDAP, SAML, and OAuth communicate to services about permissions. That’s a level of detail beyond this chapter. More in Appendix A if you’re interested.\n\n\nIf your organization has a policy you’re going to need to be able to enforce inside the data science environment, it’s most likely a Role Based Access Control (RBAC) policy. In RBAC, permissions are assigned to an abstraction called a role. Users and groups are then given roles depending on their needs.\nFor example, there might be a manager role that should have access to certain permissions in the HR software. This role would be applied to anyone in the data-science-manager group as well as the data-engineering-manager group.\nThere are a few issues with RBAC. Most importantly, if there are lots of idiosyncratic permissions, it’s often easier to create tons and tons of special roles rather than figure out how to harmonize them into a system.\nMany organizations haven’t reached the complexity of adopting RBAC. They often use simple Access Control Lists (ACLs) of who’s allowed to access each service.3 ACLs have the advantage of being conceptually simple, but with a lot of services or a lot of users, it can be a pain to maintain individual lists for each service.\nSome organizations are going more granular techniques than RBAC or ACLs and are adopting Attribute Based Access Control (ABAC). In ABAC, permissions are granted based on an interaction of different attributes and a rules engine.\nFor example, you can imagine three distinct attributes a user could have: data-science, data-engineer, and manager. You could create a rules engine that provides access to different resources based on the combinations of these attributes.\nRelative to RBAC, ABAC is a more powerful system that allows for more granular permissions, but it’s a much bigger lift to initially configure. You’ve already encountered an ABAC system in the AWS IAM system. If you tried to configure anything in IAM, you were probably completely befuddled. You can thank the power and complexity of ABAC."
  },
  {
    "objectID": "chapters/sec3/3-2-auth.html#connecting-to-data-sources",
    "href": "chapters/sec3/3-2-auth.html#connecting-to-data-sources",
    "title": "16  Auth in Enterprise",
    "section": "16.5 Connecting to data sources",
    "text": "16.5 Connecting to data sources\nWhether you’re working directly on a data science workbench or deploying a project to a hosting platform, you’re almost certainly connecting to a database, storage bucket, or data API along the way.\nIt used to be the case that most data sources had simple username and password auth, so you could authenticate by just passing those credentials along to the data source, preferably via environment variables (see Chapter 3). This is still the easiest way to connect to data sources.\nBut that doesn’t work if your data source is expecting a secure SSO token rather than a username and password.\nThe good news is that connecting data sources to SSO makes them more secure than directly passing a username and password to a database. The bad news is that it’s still early in the adoption of these technologies, and your ability to acquire and use an SSO token may be limited.\nIn some cases, you can manually acquire an SSO token. In that case, you can simply do it at the outset of your script and use the token. This is a relatively rare case.\nBut most times, the IT/Admin wants you to have the experience of logging in and automatically having access to your data systems. This situation is sometimes termed passthrough auth. This is a great user experience and is highly secure, because there are never any credentials in the data science environment, just the exchange of one cryptographically secure token for another.\n\nThe downside is that this isn’t easy to accomplish. Data science platforms have to implement this kind of token exchange on a service-by-service basis for every type of data source you might want to access.\n\n\n\n\n\n\nNote\n\n\n\nOne thing you definitely can’t do, though it seems like a great idea at first, is just use the SAML or OAuth token that got you into the data science environment to authenticate to the data source.\nFor most types of SSO, the token that gets you access to an individual service isn’t your overall building access pass and the service doesn’t get access to the building access pass for security reasons. That means “passthrough” is a complete misnomer for how this works and it’s actually a much more complicated token exchange.\n\n\nOne technology people use for this purpose is an old, but very secure, technology called Kerberos. There are a variety of different ways to configure Kerberos, but the upshot is that you get Kerberos Ticket available to you to attach to database calls.\nHowever, there are a few issues with this setup. First, Kerberos exclusively works inside a private network, so you can’t use it to authenticate to a cloud database. Second, it basically is only used to connect to databases and only some types of databases support Kerberos authentication. Third, using Kerberos generally requires you to actually login to the underlying server. This generally isn’t a problem in the Workbench environment, but can be a pain if you’re trying to use Kerberos with deployed content.\nOAuth is quickly becoming an industry standard on this front, but it’s not fully implemented in a number of places. I expect this will be a solved problem within the next few years, but for right now you’ll need to talk to your IT/Admin team about whether you can use a username and password to connect to the data source or whether you’ll have to cross your fingers that there’s an integration that exists so you can seamlessly use your SSO token to access a data source."
  },
  {
    "objectID": "chapters/sec3/3-2-auth.html#comprehension-questions",
    "href": "chapters/sec3/3-2-auth.html#comprehension-questions",
    "title": "16  Auth in Enterprise",
    "section": "16.6 Comprehension Questions",
    "text": "16.6 Comprehension Questions\n\nWhat is the difference between authentication and authorization?\nWhat are some different ways to manage permissions? What are the advantages and drawbacks of each?\nWhat is some advantages of token-based auth? Why are most organizations adopting it? Are there any drawbacks?\nFor each of the following, is it a username + password method or a token method? PAM, LDAP, Kerberos, SAML, ODIC/OAuth"
  },
  {
    "objectID": "chapters/sec3/3-2-auth.html#footnotes",
    "href": "chapters/sec3/3-2-auth.html#footnotes",
    "title": "16  Auth in Enterprise",
    "section": "",
    "text": "Until recently, Microsoft Entra ID was called Azure Active Directory, which confusingly was for SSO, not Active Directory. That’s probably why they changed the name.↩︎\nThere is a technology called Kerberos that some organizations use to accomplish SSO with LDAP/AD. This is rare.↩︎\nStandard Linux permissions (POSIX permissons) that were discussed in Chapter 9 are basically a special case of ACLs. ACLs allow setting individual-level permissions for any number of users and groups, as opposed to the one owner, one group, and everyone else permissions set for POSIX.\nLinux distros now have support for ACLs on top of the standard POSIX permissions.↩︎"
  },
  {
    "objectID": "chapters/sec3/3-3-ent-scale.html#devops-best-practices",
    "href": "chapters/sec3/3-3-ent-scale.html#devops-best-practices",
    "title": "17  Compute at enterprise scale",
    "section": "17.1 DevOps best practices",
    "text": "17.1 DevOps best practices\nIn an enterprise, the IT/Admin group is managing dozens or hundreds or thousands of servers. In order to keep that many servers manageable, the IT/Admin group tries to keep those servers very standardized and interchangeable. This idea is often encompassed in the adage that “servers should be cattle, not pets”. That means that you almost certainty won’t be allowed to SSH in and just make changes that you might want as a data scientist.\nIndeed, in many organizations, no one is allowed to SSH in and make changes. Instead, all changes have to go through a robust change management process and are deployed via Infrastructure as Code (IaC) tools so the environment can always be torn down and replaced easily.\n\n\n\n\n\n\nNote\n\n\n\nAvoiding this complexity is the major reason many organizations are moving away from directly managing servers at all. Instead, they’re outsourcing server patches and management by acquiring PaaS or SaaS software from cloud providers.\n\n\nThere are actually two parts to standing up an environment. First the servers and networking need to be stood up, called provisioning. Once that’s done, they need to be configured, which involves installing and activating applications like Python, R, JupyterHub, RStudio Server. In many enterprises, provisioning and configuration are actually done by separate groups, which server group and the application administration group.\nThere are many different IaC tools you may hear the IT/Admins at your organization talk about. These include Terraform, Ansible, CloudFormation (AWS’s IaC tool), Chef, Puppet, and Pulumi. Most of these tools can do both provisioning and configuration, but most specialize in one or the other, so many organizations use a pair of them together.\n\n\n\n\n\n\nNo Code IaC\n\n\n\nSome enterprises manage servers without IaC. These usually involve writing extensive run books to tell another person how to configure the servers. If your spidey sense is tingling that this probably isn’t nearly as good as code, you’re right. Finding that your enterprise IT/Admin organizations doesn’t use IaC tooling is definitely a red flag.\n\n\nAlong with making deployments via IaC, organizations that follow DevOps best practices use a Dev/Test/Prod setup for making changes to servers and applications. The Dev and Test environments, often called lower environments are solely for testing changes to the environment itself. In order to differentiate this environment from the data scientist’s Dev and Test environments, I often refer to this as staging.1\nGenerally, the you won’t have access to the staging environment at all, except for potentially doing user acceptance testing for changes there.\nIn this kind of setup, environment promotion is a two-dimensional grid, with IT/Admins working on changes to the environment in staging and data scientists working in Dev and Test within the Prod IT/Admin environment. The ultimate goal of all of this is to create an extremely reliable prod-prod environment.\n\nIn enterprises, moves from staging to prod, including upgrades to applications or operating systems or adding system libraries often have rules around them. They may need to be validated or approved by security. In some highly-regulated environments, the IT/Admin group may only be able to make changes during certain windows of time. This can be a source of tension between a data scientist who wants a new library or version now and an IT/Admin who isn’t allowed to move fast.\nIn addition to changes that go from staging to prod, enterprises also sometimes undergo a complete rebuild of their environments. These days, many of those rebuilds are the result of a move to the cloud, which can be a multi-year affair."
  },
  {
    "objectID": "chapters/sec3/3-3-ent-scale.html#compute-for-many-users",
    "href": "chapters/sec3/3-3-ent-scale.html#compute-for-many-users",
    "title": "17  Compute at enterprise scale",
    "section": "17.2 Compute for many users",
    "text": "17.2 Compute for many users\nAt some number of data scientists, you outstrip the ability of any one server – even a big one – to accommodate all of the work that needs to get done. How many data scientists it takes to overtax a single server depends entirely on what data scientists do at your organization. If you’re doing highly intensive simulation work or deep learning, you may hit it with only one person. On the other hand, I’ve seen organizations that mostly work on small data sets that can comfortably fit 50 concurrent users on a single server.\nOnce you need multiple servers to support the data science team(s), you have to horizontally scale. There is a simple way to horizontally scale, which is just to give every user or every group their own disconnected server. In some organizations this can work very well. The downside is that this either leaves a lot of hassle for the IT/Admin to manage or they just delegate server management to the individual teams.\nMany enterprises don’t want to do things this way. Instead, they want to run one centralized service that everyone, or at least a lot of users, in the company tome to. Managing just one environment makes things simpler in some ways, but it drastically increases the cost of downtime. For example, one hour of downtime for a platform that supports 500 data scientists wastes over $25,000.2\n\n\n\n\n\n\nMeasuring Uptime\n\n\n\nOrganizations often introduce Service Level Agreements (SLAs) or Operating Level Agreements (OLAs) about how much downtime is allowed. These limits are often measured in nines of uptime, which refers to the proportion of the time that the service is guaranteed to be online. So a one-nine service is guaranteed to be up 90% of the time, allowing for 36 days of downtime a year. A five-nine service is guaranteed up for 99.999% of the time, allowing for only about 5 1/4 minutes of downtime a year.\n\n\nFor that reason, organizations that support enterprise data science platforms take avoiding downtime very seriously. Most have some sort of disaster recovery policy. Some maintain frequent (often nightly) snapshots of the state of the environment so they can roll back to a known good state in the event of a failure. Sometimes it means that there is actually a copy of the environment waiting on standby in the event of an issue with the main environment.\nOther times, there are stiffer requirements such that nodes in the cluster could fail and the users wouldn’t be meaningfully affected. This requirement for limited cluster downtime is often called high availability. If you’re talking to an IT/Admin about high availability, it’s worth understanding that it’s a description of a desired outcome, not a particular technology or technical approach."
  },
  {
    "objectID": "chapters/sec3/3-3-ent-scale.html#computing-in-clusters",
    "href": "chapters/sec3/3-3-ent-scale.html#computing-in-clusters",
    "title": "17  Compute at enterprise scale",
    "section": "17.3 Computing in clusters",
    "text": "17.3 Computing in clusters\nWhether it’s for horizontal scaling or high availability reasons, most enterprises run their data science environments in a cluster, which is a set of servers (nodes) that operate together as one unit. Ideally, working in a cluster feels the same as working in a single-server environment, but there are multiple servers to add computational power or provide resilience if one server were to fail.\nIn order to have a cluster operate as a single environment, you need to solve two problems. First, you want to provide a single front door that routes users to a node in the cluster, preferably without them being aware of it happening. This is accomplished with a load balancer, which is a kind of proxy server.\nSecond, you need to make sure that the user is able to persist things (state) on the server even if they end up on a different node later. This is accomplished by setting up storage so that persistent data doesn’t actually live on any of the nodes. Instead, it lives in separate storage that is symmetrically accessible to all the nodes in the cluster.\n\nIf you are a solo data scientist reading this, please do not try to run your own load balanced data science cluster. When you undertake load balancing, you’ve taken on a distributed systems problem and those are inherently difficult.\nHigh-availability is often phrased as “no single point of failure”. It’s worth noting that just load balancing doesn’t eliminate single points of failure. In fact, it’s totally possible to make your system less stable by carelessly load balancing a bunch of nodes. For example, what if your load balancer were to fail? Or the place where the state is stored? What if your NAS has low performance relative to a non-networked drive? Sophisticated IT/Admin organizations have answers to these questions and standard ways they implement high availability.\n\n\n\n\n\n\nNote\n\n\n\nFor technical information on how load balancers work and different types of configuration, see Appendix B."
  },
  {
    "objectID": "chapters/sec3/3-3-ent-scale.html#docker-in-enterprise-kubernetes",
    "href": "chapters/sec3/3-3-ent-scale.html#docker-in-enterprise-kubernetes",
    "title": "17  Compute at enterprise scale",
    "section": "17.4 Docker in enterprise = Kubernetes",
    "text": "17.4 Docker in enterprise = Kubernetes\nOriginally created at Google and released in 2014, the open source Kubernetes (K8S, pronounced koo-ber-net-ees or kates for the abbreviation) is the way to run production services out of Docker containers.3 Many organizations are moving towards running much or all of their production work in Kubernetes.\n\n\n\n\n\n\nNote\n\n\n\nApparently Kubernetes is an ancient Greek word for “helmsman”.\n\n\nRelative to managing individual nodes in a load balanced cluster, Kubernetes makes things easy because it completely separates provisioning and configuration. In Kubernetes, you create a cluster of worker nodes. Then, you separately tell the cluster’s control plane that you want it to run a certain set of Docker containers with a certain amount of computational power allocated to each one. These Docker Containers running in Kubernetes are called pods.\nThe elegance of Kubernetes is that you don’t have to think at all about where each pod goes. The control plane schedules the pods on the nodes without you having to think about where the nodes actually are or how networking will work.\n\nFrom the perspective of the IT/Admin, this is wonderful because you just make sure you’ve got enough horsepower in the cluster and then all the app requirements go in the container, which makes node configuration trivial.\nIn fact, almost everyone running Kubernetes can just add nodes with a few button clicks because they’re using a cloud provider’s managed service: AWS’s EKS (Elastic Kubernetes Service, Azure’s AKS (Azure Kubernetes Service), or GCP’s GKE (Google Kubernetes Engine).4\nFor production purposes, pod deployments are usually managed with Helm charts, which are the standard IaC way to declare what pods you want, how many of each you need, and what their relationship is.\nKubernetes is an extremely powerful tool and there are many reasons to like it. But like most powerful tools, it’s also complicated and becoming a proficient Kubernetes admin is a skill unto itself. These days, many IT/Admins are trying to add Kubernetes to their list of skills. If you have a competent Kubernetes admin, you should be in good shape, but you should be careful that setting up your data science environment doesn’t become someone else’s chance to learn Kubernetes."
  },
  {
    "objectID": "chapters/sec3/3-3-ent-scale.html#consider-hpc-over-kubernetes",
    "href": "chapters/sec3/3-3-ent-scale.html#consider-hpc-over-kubernetes",
    "title": "17  Compute at enterprise scale",
    "section": "17.5 Consider HPC over Kubernetes",
    "text": "17.5 Consider HPC over Kubernetes\nAs you add more people, you also are likely to add more variety in requirements. For example, you might want to be able to incorporate jobs that require very large nodes. Or maybe you want to run GPU-backed nodes. Or maybe you want to have “burst-able” capacity for particularly busy days or times.\nIt’s unlikely that your organization wants to be running all of these nodes all the time, especially expensive large or GPU-backed ones. By far the simplest way to manage this complexity is to run a “main” cluster for everyday kind of workloads and stand up additional special environments as needed. If the need is just for burst capacity over a relatively long time scales (e.g. a full day), manually adding or removing nodes from a cluster is easy.\nSome organizations really want to run a single environment that contains different kinds of nodes or that can autoscale to accommodate quick bursts of needed capacity.\nOften, the best option for doing these kinds of activities is a high-performance computing (HPC) framework. HPC is particularly appropriate when you need very large machines. For example, Slurm is an HPC framework that supports multiple queues for different sets of machines and allows you to treat an entire cluster as if it were one machine with many – even thousands – of CPU cores. AWS has a service called ParallelCluster that allows users to easily set up a Slurm cluster – and with no additional cost relative to the cost of the underlying AWS hardware.\nSome organizations want to accomplish this kind of approach in Kubernetes. That is generally more difficult. It is not as easy to manage multiple kinds of nodes in one cluster with Kubernetes. Where HPC frameworks are designed to let you combine an arbitrary number of nodes into what acts like a single machine with thousands or tens of thousands of cores, it’s usually not possible to schedule pods larger than an actual node in your cluster.\nAutoscaling is also easier in many HPC frameworks compared to Kubernetes. Regardless of the framework, scaling up is always easy. You just add more nodes and you can schedule more work. But it’s scaling down that’s hard.\nBecause of its history, Kubernetes assumes that pods can easily be moved from one node to another and it wouldn’t be a problem to shut down a node on someone and just move their pod to another node. That doesn’t work well when what’s in the pod is your Jupyter or RStudio session. Relative to Kubernetes, HPC is much more “session-aware” and often does a better job scaling down the kinds of workloads that happen in a data science environment.\nThe upshot is that most IT/Admins will reach for Kubernetes to solve the problem of multiple use cases in one cluster or to autoscale it, but HPC may actually be a better solution in a data science context."
  },
  {
    "objectID": "chapters/sec3/3-3-ent-scale.html#comprehension-questions",
    "href": "chapters/sec3/3-3-ent-scale.html#comprehension-questions",
    "title": "17  Compute at enterprise scale",
    "section": "17.6 Comprehension Questions",
    "text": "17.6 Comprehension Questions\n\nWhat is the difference between horizontal and vertical scaling? For each of the following examples, which one would be more appropriate?\n\nYou’re the only person using your data science workbench and run out of RAM because you’re working with very large data sets in memory.\nYour company doubles the size of the team that will be working in your data science workbench. Each person will be working with reasonably small data, but there’s going to be a lot more of them.\nYou have a big modeling project that’s too large for your existing machine. The modeling you’re doing is highly parallelizable.\n\nWhat is the role of the load balancer in horizontal scaling? When do you really need a load balancer and when can you go without?\nWhat are the biggest strengths of Kubernetes as a scaling tool? What are some drawbacks? What are some alternatives?"
  },
  {
    "objectID": "chapters/sec3/3-3-ent-scale.html#footnotes",
    "href": "chapters/sec3/3-3-ent-scale.html#footnotes",
    "title": "17  Compute at enterprise scale",
    "section": "",
    "text": "You’ll have to fight out who gets to claim the title Dev/Test/Prod for their environments with the IT/Admins at your organization. Be nice, they probably had the idea long before you did.↩︎\nAssuming a (probably too low), fully-loaded cost of $100,000 and 2,000 working hours per year.↩︎\nIf you are pedantic, there are other tools for deploying Docker containers like Docker Swarm and Kubernetes is not limited to Docker containers. But for all practical purposes, \\(\\text{production Docker = Kubernetes}\\).↩︎\nIt’s rare, but some organizations do run an on-prem Kubernetes cluster with Oracle’s OpenShift.↩︎"
  },
  {
    "objectID": "chapters/sec3/3-4-ent-pm.html#ensuring-packages-are-safe",
    "href": "chapters/sec3/3-4-ent-pm.html#ensuring-packages-are-safe",
    "title": "18  Package Management in the Enterprise",
    "section": "18.1 Ensuring packages are safe",
    "text": "18.1 Ensuring packages are safe\nThe biggest concern most IT/Admins have about packages is that they might be unsafe. Unsafe packages might introduce exploitable bugs in your code to allow outside actors to get in or may themselves be trojan horses that exfiltrate data.\nSome of these security concerns can be ameliorated because most data science projects are run entirely inside a private environment. For example, there are many security concerns with running Javascript on public websites that are sharply reduced when the only people who can access your application are already staff at your organization. Similarly, a package that maliciously grabs data from your system and exports it will be rendered ineffective in an airgapped environment.\nDepending on your industry, IT/Admins may also take some responsibility for creating “validated” environments full of only packages that are known to create good results. This is particularly common in industries that have longstanding statistical practices, like Pharma. In other cases, organizations will only want to use packages that are known to be well-maintained and will be in the future.\nA basic, but effective, form of package security is to limit allowed packages to popular packages, packages from known organizations/authors, or packages that outside groups have validated to be safe.\nIncreasingly, there are industry groups that are validating that certain packages have met quality and security standards and that anyone in the industry should feel comfortable using them. For example, the R Validation Hub is a pharmaceutical industry group that is working to create lists of packages that are broadly agreed to meet certain quality standards. There are also paid products that may serve this validation function.\nOther organizzations may want to check that incoming packages don’t contain known security vulnerabilities.\nEvery day, security vulnerabilities in software are identified and publicized. These vulnerabilities are maintained in the CVE (Common Vulnerabilities and Exposures) system. Each CVE is assigned an identifier and a severity score that ranges from None to Critical.\nVery often, companies have policies that disallow the usage of software with too many vulnerabilities. These policies often completely ban software with Critical CVEs and only temporarily allow software with a few High CVEs.\nSome organizations try to ensure the security of incoming packages via a code scanner. A code scanner is a piece of software that scans all incoming code and detects potential security risks – like usage of insecure encryption libraries, calls to external web services, or places where it accesses a database.\nThese are almost always paid tools. It is my personal opinion that the creators of these tools often overstate the potential benefits and that a lot of code scanning is security theater. Unfortunately, that doesn’t change the reality that getting open source packages into your environment may require them going through a code scanner.\nThe sophistication of these tools is roughly in proportion to how popular the language is. So Javascript, which is both extremely popular and also makes up most public websites, has reasonably well-developed software scanning. Python, which is very popular, but is only rarely on the front end of websites has fewer scanners, and R, which is far less popular and is never in a website front end has none as far as I’m aware."
  },
  {
    "objectID": "chapters/sec3/3-4-ent-pm.html#open-source-licensing-issues",
    "href": "chapters/sec3/3-4-ent-pm.html#open-source-licensing-issues",
    "title": "18  Package Management in the Enterprise",
    "section": "18.2 Open source licensing issues",
    "text": "18.2 Open source licensing issues\nIn addition to security issues, some organizations are concerned about the legal implications of using free and open source software (FOSS) in their environment. These organizations, most often organizations that themselves write software, want to limit the use of certain types of licenses inside their environment.\n\n\n\n\n\n\nNote\n\n\n\nI am not a lawyer and this is not legal advice, but hopefully this is helpful context on the legal issues around FOSS software.\n\n\nWhen someone releases software, they can choose a license, which isa legal document explaining what consumers are allowed to do with that software.\nThe type of license you’re probably most familiar with is a copyright. A copyright gives the owner exclusivity to distribute the software and charge for it. For example, if you buy a copy of Microsoft Word, you have a limited license to use the software, but you’re not allowed to inspect the source code of Microsoft Word and you’re not allowed to share the software.\nIn 1985, the Free Software Foundation (FSF) was created to support the creation of free software. They wanted to facilitate using, reusing, and sharing software. In particular, the FSF supported four freedoms for software:1\n\nRun the program however you wish for any purpose.\nStudy the source code of the program and change it as you wish.\nRedistribute the software as you wish to others.\nDistribute copies of the software once you’ve made changes so everyone can benefit.\n\nNow, you could just do this without applying a license to your software. But from a lawyer’s perspective, that’s dangerous and unsustainable. Creating and applying FOSS licenses to software made it something that was enforceable.\n\n\n\n\n\n\nWhat does “free” mean?\n\n\n\nIt’s expensive to create and maintain FOSS. For that reason, the free in FOSS is about freedom, not about zero cost. As a common saying goes – it means free as in free speech, not free as in free beer.\nThere are many different flavors of open-source licenses. All of them I’m aware of, even the anti-capitalist one, allow you to charge for software.\nOrganizations have attempted to support FOSS with a variety of different business models to varying degrees of success. These models include donations, paid features or products, advertising or integrations, and paid support, services, or hosting.\n\n\nThere isn’t just one FOSS license, instead there are dozens that fall into two categories. Permissive licenses allow you to do essentially whatever you want with the FOSS software. For example, the permissive MIT license allows you to, “use, copy, modify, merge, publish, distribute, sublicense, and/or sell” MIT-licensed software without attribution. Most organizations have basically no concerns about using software with a permissive open source license.\nThe bigger concern is using software that has a copyleft or viral license. Software licensed under a copyleft regime requires that any derivative works are themselves released under a similar license. The idea is that open source software should beget more open source software and not silently be used by big companies to make megabucks.\nThe concern enterprises have with copyleft licenses is that they might propagate into the private work you are doing inside your company. This concern is especially keen at organizations that themselves sell proprietary software. For example, what if a court were to rule that Apple or Google had to suddenly open source all their software because of the use of copyleft licenses by developers there?\nMuch of the concern centers around what it means for a piece of software to be a derivative work of another. Most people agree that artifacts created with copyleft-licensed software, like your plots, reports, and apps, are not themselves derivative works. But the treatment of software that incorporates copyleft-licensed software is murky. The reality is that there have been basically no court cases on this topic and nobody knows how it would shake out if it did get to court, so some organizations err on the side of caution.\nThese concerns are somewhat less for Python than for R. Python is released under a permissive Python Software Foundation (PSF) license and Jupyter Notebook under a permissive modified BSD. R is released under the copyleft GPL license and RStudio under a copyleft AGPL.\nHowever, every single package author can choose a license for themselves. In an enterprise context, these discussions tend to focus on knowing – and potentially blocking – the use of packages under copyleft licenses inside the enterprise. Some package repository software surfaces the license type of individual packages to help organizations make their own decisions."
  },
  {
    "objectID": "chapters/sec3/3-4-ent-pm.html#controlling-package-flows",
    "href": "chapters/sec3/3-4-ent-pm.html#controlling-package-flows",
    "title": "18  Package Management in the Enterprise",
    "section": "18.3 Controlling package flows",
    "text": "18.3 Controlling package flows\nWhether your organization is trying to limit CVE exposure, run a code scanner, limit copyleft exposure, or stick to a known list of good packages, they need a way to actually restrict the packages that are available inside the environment.\nIf you’ve giving someone access to Python or R, it’s not possible to just remove their ability to run pip install or install.packages. That’s one reason why many enterprise environments are airgapped – it’s the only way to ensure data scientists can’t install packages from outside the environment.\nMost IT/Admins understand that airgapping is the best way to stop unauthorized package installs. The next bit – that they need to provide you some way to install packges – is the part that may require some convincing.\nIn order to allow packages to be installed in their environments, many enterprises run package repository software inside their firewall. Common package repositories include Jfrog Artifactory, Sonatype Nexus, Anaconda Business, and Posit Package Manager.\nArtifactory and Nexus are generalized library and package management solutions for all sorts of software, while Anaconda and Posit Package Manager are more narrowly tailored for data science use cases. If possible, I’d try working with your IT/Admins to get a data science focused repository software. Often these repositories can run alongside general-purpose repositories if you already have them.\nDepending on the repository software you use, it may connect to an outside sync service or support manual file transfers for package updates. In many cases, IT/Admins are comfortable having narrow exceptions so the package repository can download packages, but no one in the data science environment can reach the internet.\n\nThis tends to work best when the IT/Admin is the one who controls which packages are allowed into the repository and when. Then you, as the data scientist, have the freedom to install those packages into any individual project and manage them there using environment as code tooling, as discussed in Chapter 1."
  },
  {
    "objectID": "chapters/sec3/3-4-ent-pm.html#comprehension-questions",
    "href": "chapters/sec3/3-4-ent-pm.html#comprehension-questions",
    "title": "18  Package Management in the Enterprise",
    "section": "18.4 Comprehension Questions",
    "text": "18.4 Comprehension Questions\n\nWhat are the concerns IT/Admins have about packages in an enterprise context?\nWhat are three tools IT/Admins might use to ensure packages are safe?\nWhat is the difference between permissive and copyleft open source licenses? Why are some organizations concerned about using code that includes copyleft licenses?"
  },
  {
    "objectID": "chapters/sec3/3-4-ent-pm.html#footnotes",
    "href": "chapters/sec3/3-4-ent-pm.html#footnotes",
    "title": "18  Package Management in the Enterprise",
    "section": "",
    "text": "They’re numbered 1-4 here for clarity in writing, but like many computer science topics, the numbering actually starts at 0.↩︎"
  },
  {
    "objectID": "chapters/append/auth.html#service-based-auth",
    "href": "chapters/append/auth.html#service-based-auth",
    "title": "Appendix A — Auth Technologies",
    "section": "A.1 Service-based auth",
    "text": "A.1 Service-based auth\nMany pieces of software come with integrated authentication. When you use those systems, the service stores encrypted username and password pairs in a database it owns. These setups are often really easy from an admin perspective – you just set up individual users on the service.\nOn the other hand, everything has to be managed service-by-service and you’re only as secure as what the service has implemented. Almost any organization with an IT/Admin group will prefer not to use service-based auth."
  },
  {
    "objectID": "chapters/append/auth.html#system-linux-accounts",
    "href": "chapters/append/auth.html#system-linux-accounts",
    "title": "Appendix A — Auth Technologies",
    "section": "A.2 System (Linux) Accounts",
    "text": "A.2 System (Linux) Accounts\nMany pieces of software – especially data science workbenches – are able to look at the server they’re sitting on and use the user accounts and groups from the server themselves.\nOn a Linux server, Pluggable Authentication Modules (PAM) is the system that allows a service to make use of the Linux host’s users and groups. As of this writing, PAM is the default authentication method for both RStudio Server and JupyterHub.\nAs the name might suggest, PAM includes a number of different modules that allow it to authenticate against different systems. The most common is to authenticate against the underlying Linux server, but it can also use LDAP/AD or Kerberos tickets (uncommon).\n\nPAM can also be used to do things when users login – the most common being initializing Kerberos tickets to connect with databases.\nWhen PAM is used in concert with LDAP/AD, the Linux users are usually created automatically on the system using a system called SSSD (System Security Services Daemon). This process is called joining the domain.\nThough conceptually simple, reading, writing, and managing PAM modules is quite painful. Additionally, as more services move to the cloud, there isn’t necessarily an underlying Linux host where identities live and PAM is generally considered a legacy technology."
  },
  {
    "objectID": "chapters/append/auth.html#ldapad",
    "href": "chapters/append/auth.html#ldapad",
    "title": "Appendix A — Auth Technologies",
    "section": "A.3 LDAP/AD",
    "text": "A.3 LDAP/AD\nFor many years, Microsoft’s Lightweight Directory Access Protocol (LDAP) implementation called Active Directory (AD) was basically the standard in enterprise authentication. It is increasingly being retired in favor of token-based systems like SAML and OAuth2.0.\nDepending on the service, it may use LDAP/AD indirectly via PAM, or it may be directly configured to talk to LDAP/AD.\n\n\n\n\n\n\nNote\n\n\n\nLDAP is an application-layer protocol, like HTTP. And like HTTP, there is an SSL-secured version called LDAPS. Because LDAP is almost always used only inside a private network, adoption of LDAPS is uneven. The default port for LDAP is \\(389\\) and for LDAPS it’s \\(636\\).\n\n\nLDAP/AD actually isn’t a type of authentication. It’s a hierarchical tree database that is good for storing organizational entities. Doing authentication with LDAP/AD consists of sending a search for the provided username/password combination to the LDAP/AD database using the ldapsearch command.\nWhen you configure LDAP/AD in an application, you’ll configure a search base, which is the subtree to look for users inside. Additionally, you may configure LDAP/AD with bind credentials to validate that it’s allowed to be looking things up in the server.\n\n\n\n\n\n\n\nNote\n\n\n\nDepending on your application and LDAP/AD configuration, it may be possible to skip the bind credentials and use the user’s credentials to look them up in single-bind mode (as opposed to double-bind when there are bind credentials). Single-bind is generally inferior and shouldn’t be used unless you can’t get bind credentials.\n\n\nWhen you run an ldapsearch, you get back the distinguished name (DN) of the entity that you are looking for (assuming it’s found).\nHere’s what my entry in a corporate LDAP directory might look like this:\ncn: Alex Gold\nmail: alex.gold@example.com\nmail: alex.gold@example.org\ndepartment: solutions\nmobile: 555-555-5555\nobjectClass: Person\nThis is helpful information, but you’ll note that there’s no direct information about authorization. Instead, you configure the service authorize certain users or groups. This is a pain, as each service needs to be configured separately.\nOne of the big issues with LDAP/AD is that credentials are provided to the service, which passes them along to the LDAP/AD server. This is one reason why LDAP/AD is inferior to token-based technologies, where the credentials are only ever provided to the identity provider."
  },
  {
    "objectID": "chapters/append/auth.html#sec-kerberos",
    "href": "chapters/append/auth.html#sec-kerberos",
    "title": "Appendix A — Auth Technologies",
    "section": "A.4 Kerberos Tickets",
    "text": "A.4 Kerberos Tickets\nKerberos is a relatively old, but very secure, token-based auth technology for use inside a private network. In Kerberos, encrypted tokens called Kerberos tickets are passed between the servers in the system. A system that is designed to authenticate against a Kerberos ticket is called kerberized.2\nThough Kerberos is freely available, it was widely adopted along with Active Directory, and it’s used almost exclusively in places that are running a lot of Microsoft products. The most frequent use of Kerberos tickets these days is to establish connections to Microsoft databases.\nKerberos is tightly linked to the underlying servers, one of the reasons why it is so secure. In a Kerberos-based system, users often store their credentials in a secure file called a keytab. They can manually initialize a ticket using the kinit command, though PAM sessions are often used to automatically fetch a ticket upon user login.\nWhen a Kerberos session is initialized, the service sends the users’ credentials off to the central Kerberos Domain Controller (KDC) and requests something called the Ticket Granting Ticket (TGT) from the KDC. Like most token authentication, TGTs have a set expiration period and must be reacquired when they expire.\nWhen the user wants to access a service, they send the TGT back to the KDC again along with the service they’re trying to access and get a session key (sometimes referred to as a service ticket) that allows access to a particular service.\n\nKerberos is basically only used inside a corporate network. This is one reason it’s still considered secure. On the other hand, because everything has to live inside the network, it doesn’t work well for providing access to services outside the network, like SaaS software. For that reason, Kerberos is considered a legacy tool."
  },
  {
    "objectID": "chapters/append/auth.html#oauth-saml",
    "href": "chapters/append/auth.html#oauth-saml",
    "title": "Appendix A — Auth Technologies",
    "section": "A.5 Modern systems: OAuth + SAML",
    "text": "A.5 Modern systems: OAuth + SAML\nThese days, most organizations are quickly moving towards implementing a modern token-based authentication system through SAML and/or OAuth2.0.\nWhen you go to login to a service that uses SAML or OAuth, you are redirected to the SAML/OAuth identity provider to seek a token that will let you in. Assuming all goes well, you’re granted a token and go back to the service to go do your work.\nBoth OAuth and SAML rely on plain HTTP traffic, making them easier to configure than LDAP/AD or Kerberos from a networking standpoint.\n\nA.5.1 SAML\nThe current SAML 2.0 standard was finalized in 2005 – roughly coinciding with the beginning of the modern era of the web. Facebook was started just the prior year.\nSAML was invented to be a modern successor to enterprise auth methods like LDAP/AD and Kerberos. SAML uses encrypted and cryptographically-signed XML-based tokens that are generated through a browser redirect flow.\nIn SAML, the service you’re accessing is called the service provider (SP) and the entity issuing the token is called the SAML identity provider (IdP). Most SAML tooling allows you start at either the IdP or the SP.\nIf you start at the SP, you’ll get redirected to the IdP. The IdP will verify your credentials. If all is good, it will put a SAML token in your browser, which the SP will use to authenticate you.3\n\nA SAML token contains a number of claims, which usually include a username and may include groups or other attributes. Whoever controls the IdP can configure what claims appear on the token at the IdP. The SAML standard itself doesn’t do authorization, but it’s very common for an application to have required or optional claims that it can interpret to do authorization.\n\n\nA.5.2 OAuth\nOAuth was started in 2006 and the current 2.0 standard was finalized in 2013. OAuth 2.1 is under development as of 2023.\nFrom the very beginning, OAuth was designed to be used with different services across the web. Any time you’ve used a Login with Google/Facebook/Twitter/GitHub flow – that’s OAuth.\nOAuth relies on passing around cryptographically-signed JSON Web Tokens (JWT). This makes OAuth much easier to debug compared to SAML because the JWT is just plain-text JSON with a signature that proves it’s valid.\nUnlike a SAML token that always lives in a browser cache, JWTs can go basically anywhere. They can live in the browser, but they also can pass from one server to another to do authorization or can be saved in a user’s home directory. For example, if you’ve accessed Google Sheets or another Google service from R or Python, you may have manually handled the resulting OAuth token in your home directory.\nOAuth is actually an authorization scheme, so the contents of a JWT explicitly are about the permissions of the bearer of the token. There is a related standard called OpenID Connect (OIDC) that does authentication with OAuth tokens. Over the next few years, I fully expect basically all data access to move towards using OAuth tokens.\nIn OAuth, the service you’re trying to visit is called the resource server and the token issuer is the authorization server. When you try to access a service, the service knows to look for a JWT that includes specific claims against a set of scopes defined ahead of time.\nFor example, if you want to read my Google Calendar, you would need to have a JWT that includes a claim granting read access against the scope of events on Alex’s calendar.\nIf you don’t have that token, you’ll need to go to the authorization server to get it. Unlike in SAML where action is all occurring via HTTP redirects, OAuth makes no assumptions about how this flow happens.\n\nThe authorization server knows how to accept requests for a token and the resource server knows how to accept them. The process of requesting and getting a token can happen in a number of different ways that might include browser redirects and caches, but also could be done entirely in R or Python."
  },
  {
    "objectID": "chapters/append/auth.html#user-provisioning",
    "href": "chapters/append/auth.html#user-provisioning",
    "title": "Appendix A — Auth Technologies",
    "section": "A.6 User Provisioning",
    "text": "A.6 User Provisioning\nWhen you’re using a service, users often need to be created (provisioned) in that system. In some cases, the users will be provisioned the first time they log in. In other cases, you may want the ability to provision them ahead of time.\nLDAP/AD is very good for user provisioning. You can often configure your application to provision everyone who comes back from a particular ldapsearch. In contrast, token-based systems don’t know anything about you until you show up for the first time with a valid token.\nThere is a SAML-based provisioning system called SCIM (System for Cross-Domain Identity Management) that is slowly being adopted by many IdPs and SPs."
  },
  {
    "objectID": "chapters/append/auth.html#footnotes",
    "href": "chapters/append/auth.html#footnotes",
    "title": "Appendix A — Auth Technologies",
    "section": "",
    "text": "To be precise, possible if integrated with Kerberos, but unlikely.↩︎\nYuck, what a gross word.↩︎\nThe diagram below assumes you don’t already have a token in your browser. What’s listed as step 6 below actually occurs as soon as the user goes to the SP. If the user has a token already, they skip the rest of the flow.↩︎"
  },
  {
    "objectID": "chapters/append/lb.html#load-balancer-settings",
    "href": "chapters/append/lb.html#load-balancer-settings",
    "title": "Appendix B — Technical Detail: Load balancers",
    "section": "B.1 Load balancer settings",
    "text": "B.1 Load balancer settings\nRegardless of which load balancer you’re using, a basic requirement is that it knows what nodes are accepting traffic. This is accomplished by configuring a health check/heartbeat for the application on the node. A health check is a feature of the application that responds to periodic pings from the load balancer. If no response comes back, the load balancer treats that node as unhealthy and doesn’t send traffic there.\nOne other feature that may come up is sticky sessions or sticky cookies. For stateful applications, like Shiny apps, you want to get back to the same node in the cluster so you can resume a previous session. In most load balancers, this is just an option you can activate."
  },
  {
    "objectID": "chapters/append/lb.html#ways-to-configure-load-balancing",
    "href": "chapters/append/lb.html#ways-to-configure-load-balancing",
    "title": "Appendix B — Technical Detail: Load balancers",
    "section": "B.2 Ways to configure load balancing",
    "text": "B.2 Ways to configure load balancing\nThe simplest form of load balancing is to just rotate traffic to each node that is healthy in a round-robin configuration. Depending on the capabilities of the load balancer and what metrics are emitted by the application, it may also be possible or desirable to do more complicated load balancing that pays attention to how loaded different nodes are.\nUsually, load balancers are configured to send traffic to all the nodes in the cluster in an active/active configuration. It is also possible to configure the load balancer in an active/passive configuration to send traffic to only some of the nodes, with the rest remaining inert until they are switched on, usually in the event of a failure in the active ones. This is sometimes called a blue/green or red/black configuration when it’s used to diminish downtime in upgrades and migrations."
  },
  {
    "objectID": "chapters/append/lb.html#shared-state",
    "href": "chapters/append/lb.html#shared-state",
    "title": "Appendix B — Technical Detail: Load balancers",
    "section": "B.3 Shared state",
    "text": "B.3 Shared state\nAside from the load balancer, the nodes need to be able to share state with each other so users can have the same experience on each node. The requirements for that shared state depend on the software.\nOften the shared state takes the form of a database (often Postgres) and/or Network Attached Storage (NAS, pronounced naahz) for things that get stored in a filesystem.\nIf your NAS is exclusively for Linux, you it would use NFS (Network File System). If Windows is involved, you’d use SMB (Server Message Block) or Samba to connect SMB to a Linux server. There’s also an outdated Windows NAS called CIFS (Common Internet File System) that you might see in older systems.\nEach of the cloud providers has a NAS offering. AWS has EFS (Elastic File System) and FSx. Azure has Azure File, and GCP has Filestore."
  },
  {
    "objectID": "chapters/append/lb.html#upgrades-in-ha",
    "href": "chapters/append/lb.html#upgrades-in-ha",
    "title": "Appendix B — Technical Detail: Load balancers",
    "section": "B.4 Upgrades in HA",
    "text": "B.4 Upgrades in HA\nSometimes IT/Admins want to run an HA cluster with software that supports zero downtime upgrades. In order to do a zero-downtime upgrade, you need to take some nodes offline, upgrade them, put them back online, and then upgrade the rest of the nodes.\nThere are a two features the application you’re upgrading needs to support to accomplish this feat. If it doesn’t support both, you’ll need to endure some downtime to do an upgrade.\nThe first is node draining. If you just naively removed a node, you might kill someone’s active session. Instead, you’d want to configure the node so that it doesn’t kill any existing sessions but also doesn’t accept any new ones. As the current sessions end, the node empties and you can safely take it offline when all the sessions are gone.\nThe second is rolling upgrade. Some software supports being in a load balanced cluster with different versions of the software and other does not. If your software doesn’t support a cluster with mixed versions, a rolling upgrade won’t be possible. Supporting a rolling upgrade is relatively rare. It requires that the cluster understand how to maintain metadata across different versions simultaneously, which is a tricky bit of application programming.\nIf your application doesn’t support zero downtime upgrades, some organizations like to get close by building a second copy of the environment, getting it almost live, and then taking downtime just to switch the networking over. That’s generally much faster than building the whole thing during downtime."
  },
  {
    "objectID": "chapters/append/lab-map.html",
    "href": "chapters/append/lab-map.html",
    "title": "Appendix C — Lab Map",
    "section": "",
    "text": "This section aims to clarify the relationship between the assets you’ll make in each portfolio exercise and labs in this book.\n\n\n\n\n\n\n\nChapter\nLab Activity\n\n\n\n\nChapter 1: Environments as Code\nCreate a Quarto side that uses {renv} and {venv} to create standalone R and Python virtual environments, create a page on the website for each.\n\n\nChapter 3: Data Architecture\nMove data into a DuckDB database.\n\n\nChapter 2: Project Architecture\nCreate an API that serves a Python machine-learning model using {vetiver} and {fastAPI}. Call that API from a Shiny App in both R and Python.\n\n\nChapter 4: Logging and Monitoring\nAdd logging to the app from Chapter 2.\n\n\nChapter 5: Code Promotion\nPut a static Quarto site up on GitHub Pages using GitHub Actions that renders the project.\n\n\nChapter 6: Docker\nPut API from Chapter 2 into Docker container.\n\n\nChapter 7: Cloud\nStand up an EC2 instance. Put model into S3.\n\n\nChapter 9: Linux Admin\nAdd R, Python, RStudio Server, JupyterHub, palmer penguin fastAPI + App.\n\n\nChapter 12: Networking\nAdd proxy (nginx) to reach all services from the web.\n\n\nChapter 13: DNS\nAdd a real URL to the EC2 instance. Put the Shiny app into an iFrame on the site.\n\n\nChapter 14: SSL\nAdd SSL/HTTPS to the EC2 instance.\n\n\nChapter 11: Servers\nResize servers."
  },
  {
    "objectID": "chapters/append/cheatsheets.html#environments-as-code",
    "href": "chapters/append/cheatsheets.html#environments-as-code",
    "title": "Appendix D — Cheatsheets",
    "section": "D.1 Environments as Code",
    "text": "D.1 Environments as Code\n\nD.1.1 Checking library + repository status\n\n\n\n\n\n\n\n\nStep\nR Command\nPython Command\n\n\n\n\nCheck whether library in sync with lockfile.\nrenv::status()\nNone\n\n\n\n\n\nD.1.2 Creating and Using a Standalone Project Library\nMake sure you’re in a standalone project library.\n\n\n\n\n\n\n\n\nStep\nR Command\nPython Command\n\n\nCreate a standalone library.\nTip: Make sure you’ve got {renv}/{venv}: in stall.packages(\"renv\") {venv} included w/ Python 3.5+\nrenv::init()\npython -m venv &lt;dir&gt;\nConvention: use.venv for &lt;dir&gt;\n\n\nActivate project library.\nrenv::activate()\nHappens automatically if in RStudio project.\nsource  &lt;dir&gt; /bin/activate\n\n\nInstall packages as normal.\ninstall.p ackages(\"&lt;pkg&gt;\")\npython -m pip install &lt;pkg&gt;\n\n\nSnapshot package state.\nrenv::snapshot()\npip freez e &gt; requirements.txt\n\n\nExit project environment.\nLeave R project or re nv::deactivate()\ndeactivate\n\n\n\n\n\nD.1.3 Collaborating on someone else’s project\nStart by downloading the project into a directory on your machine.\n\n\n\n\n\n\n\n\nStep\nR Command\nPython Command\n\n\nMove into project directory.\nset wd (\"&lt; project-dir&gt;\")\nOr open project in RStudio.\ncd &lt;project-dir&gt;\n\n\nCreate project environment.\nrenv::init()\npython -m venv &lt;dir&gt;\nRecommend: use .venv for &lt;dir&gt;\n\n\nEnter project environment.\nHappens automatically or re nv::activate()\nsource &lt;dir&gt; /bin/activate\n\n\nRestore packages.\nHappens automatically or r env::restore()\npi p install -r requirements.txt"
  },
  {
    "objectID": "chapters/append/cheatsheets.html#cheat-http",
    "href": "chapters/append/cheatsheets.html#cheat-http",
    "title": "Appendix D — Cheatsheets",
    "section": "D.2 HTTP Code Cheatsheet",
    "text": "D.2 HTTP Code Cheatsheet\nAs you work more with HTTP traffic, you’ll learn some of the common codes. Here’s a cheatsheet for some of the most frequent you’ll see.\n\n\n\n\n\n\n\nCode\nMeaning\n\n\n\n\n200\nEveryone’s favorite, a successful response.\n\n\n3xx\nYour query was redirected somewhere else, usually ok.\n\n\n4xx\nErrors with the request\n\n\n400\nBad request. This isn’t a request the server can understand.\n\n\n401 and 403\nUnauthorized or forbidden. Required authentication hasn’t been provided.\n\n\n404\nNot found. There isn’t any content to access here.\n\n\n5xx\nErrors with the server once your request got there.\n\n\n500\nGeneric server-side error. Your request was received, but there was an error processing it.\n\n\n504\nGateway timeout. This means that a proxy or gateway between you and the server you’re trying to access timed out before it got a response from the server."
  },
  {
    "objectID": "chapters/append/cheatsheets.html#cheat-git",
    "href": "chapters/append/cheatsheets.html#cheat-git",
    "title": "Appendix D — Cheatsheets",
    "section": "D.3 Git",
    "text": "D.3 Git\n\n\n\n\n\n\n\nCommand\nWhat it does\n\n\ngit clone &lt;remote&gt;\nClone a remote repo – make sure you’re using SSH URL.\n\n\ngit add &lt;files/dir&gt;\nAdd files/dir to staging area.\n\n\ngit commit -m &lt;message&gt;\nCommit staging area.\n\n\ngit p ush origin &lt;branch&gt;\nPush to a remote.\n\n\ngit p ull origin &lt;branch&gt;\nPull from a remote.\n\n\ngit che ckout &lt;branch name&gt;\nCheckout a branch.\n\n\ngit checko ut -b &lt;branch name&gt;\nCreate and checkout a branch.\n\n\ngit bran ch -d &lt;branch name&gt;\nDelete a branch."
  },
  {
    "objectID": "chapters/append/cheatsheets.html#cheat-docker",
    "href": "chapters/append/cheatsheets.html#cheat-docker",
    "title": "Appendix D — Cheatsheets",
    "section": "D.4 Docker",
    "text": "D.4 Docker\n\nD.4.1 Docker CLI Commands\n\n\n\n\n\n\n\n\n\nS ta ge\nCommand\nWhat it does\nNotes and helpful options\n\n\nB ui ld\ndocker b uild &lt;directory&gt;\nBuilds a directory into an image.\n-t &lt;name:tag&gt; provides a name to the container.\ntag is optional, defaults to latest.\n\n\nMo ve\ndoc ker push &lt;image&gt;\nPush a container to a registry.\n\n\n\nMo ve\ndoc ker pull &lt;image&gt;\nPull a container from a registry.\nRarely needed because run pulls the container if needed.\n\n\nR un\ndo cker run &lt;image&gt;\nRun a container.\nSee flags in next table.\n\n\nR un\ndocker stop &lt;container&gt;\nStop a running container.\ndocker kill can be used if stop fails.\n\n\nR un\ndocker ps\nList running containers.\nUseful to get container id to do things to it.\n\n\nR un\ndocker exec &lt;cont ainer&gt; &lt;command&gt;\nRun a command inside a running container.\nBasically always used to open a shell with docker exec -it  &lt;container&gt; /bin/bash\n\n\nR un\ndocker logs &lt;container&gt;\nViews logs for a container.\n\n\n\n\n\n\nD.4.2 Flags for docker run\n\n\n\n\n\n\n\n\nFlag\nEffect\nNotes\n\n\n--n ame &lt;name&gt;\nGive a name to container.\nOptional. Auto-assigned if not provided\n\n\n--rm\nRemove container when its stopped.\nDon’t use in production. You probably want to inspect failed containers.\n\n\n-d\nDetach container (don’t block the terminal).\nAlmost always used in production.\n\n\n-p &lt;po rt&gt;:&lt;port&gt;\nPublish port from inside running inside container to outside.\nNeeded if you want to access an app or API inside the container.\n\n\n-v&lt; dir&gt;:&lt;dir&gt;\nMount volume into the container.\n\n\n\n\nReminder: Order for -p and -v is &lt;host&gt;:&lt;container&gt;\n\n\nD.4.3 Dockerfile Commands\nThese are the commands that go in a Dockerfile when you’re building it.\n\n\n\n\n\n\n\n\nCommand\nPurpose\nExample\n\n\nFROM\nIndicate base container.\nFROM rocker/r-ver:4.1.0\n\n\nRUN\nRun a command when building.\nRUN apt-get update\n\n\nCOPY\nCopy from build directory into the container.\nCOPY . /app/\n\n\nCMD\nSpecify the command to run when the container starts.\nCMD quarto render ."
  },
  {
    "objectID": "chapters/append/cheatsheets.html#cloud-services",
    "href": "chapters/append/cheatsheets.html#cloud-services",
    "title": "Appendix D — Cheatsheets",
    "section": "D.5 Cloud Services",
    "text": "D.5 Cloud Services\n\n\n\n\n\n\n\n\n\nService\nAWS\nAzure\nGCP\n\n\nKubernetes cluster\nEKS or Fargate\nAKS\nGKE\n\n\nRun a container or application\nECS or Elastic Beanstalk\nAzure Container Apps\nGoogle App Engine\n\n\nRun an API\nLambda\nAzure Functions\nGoogle Cloud Functions\n\n\nDatabase\nRDS\nAzure SQL\nGoogle Cloud Database\n\n\nData Warehouse\nRedshift\nDataLake\nBigQuery\n\n\nML Platform\nSageMaker\nAzure ML\nVertex AI\n\n\nNAS\nEFS or FSx\nAzure File\nFilestore"
  },
  {
    "objectID": "chapters/append/cheatsheets.html#cheat-cli",
    "href": "chapters/append/cheatsheets.html#cheat-cli",
    "title": "Appendix D — Cheatsheets",
    "section": "D.6 Command Line",
    "text": "D.6 Command Line\n\nD.6.1 General Command Line\n\n\n\nSymbol\nWhat it is\n\n\n\n\nman &lt;command&gt;\nOpen manual for command\n\n\nq\nQuit the current screen\n\n\n\\\nContinue bash command on new line\n\n\nctrl + c\nQuit current execution\n\n\necho &lt;string&gt;\nPrint string (useful for piping)\n\n\n\n\n\nD.6.2 Linux Navigation\n\n\n\n\n\n\n\n\nCommand\nWhat it does/is\nNotes + Helpful options\n\n\n\n\n/\nSystem root or file path separator\n\n\n\n.\ncurrent working directory\n\n\n\n..\nParent of working directory\n\n\n\n~\nHome directory of the current user\n\n\n\nls &lt;dir&gt;\nList objects in a directory\n-l - format as list\n-a - all (include hidden files that start with .)\n\n\npwd\nPrint working directory\n\n\n\ncd &lt;dir&gt;\nChange directory\nCan use relative or absolute paths\n\n\n\n\n\nD.6.3 Reading Text Files\n\n\n\n\n\n\n\n\nCommand\nWhat it does\nNotes + Helpful options\n\n\ncat &lt;file&gt;\nPrint a file from the top.\n\n\n\nless &lt;file&gt;\nPrint a file, but just a little.\nCan be very helpful to look at a few rows of csv.\nLazily reads lines, so can be much faster than cat for big files.\n\n\nhead &lt;file&gt;\nLook at the beginning of a file.\nDefaults to 10 lines, can specify a different number with -n &lt;n&gt;.\n\n\ntail &lt;file&gt;\nLook at the end of a file.\nUseful for logs where the newest part is last.\nThe -f flag is useful to follow for a live view.\n\n\ngre p &lt;expression&gt;\nSearch a file using regex.\nWriting regex can be a pain. I suggest testing on \\(\\text{regex101.com}\\).\nOften useful in combination with the pipe.\n\n\n|\nThe pipe\n\n\n\nwc &lt;file&gt;\nCount words in a file\nUse -l to count lines, useful for .csv files.\n\n\n\n\n\nD.6.4 Manipulating Files\n\n\n\n\n\n\n\n\nCommand\nWhat it does\nNotes + Helpful Options\n\n\nrm &lt;path&gt;\nRemove\n-r - recursively remove everything below a file path\n-f - force - don’t ask for each file\nBe very careful, it’s permanent\n\n\n\n\n\n\n\nc p &lt;from&gt; &lt;to&gt;\nCopy\n\n\n\nm v &lt;from&gt; &lt;to&gt;\nMove\n\n\n\n*\nWildcard\n\n\n\nmkdir/rmdir\nMake/remove directory\n-p - create any parts of path that don’t exist\n\n\n\n\n\nD.6.5 Move things to/from server\n\n\n\n\n\n\n\n\nCommand\nWhat it does\nNotes + Helpful Options\n\n\ntar\nCreate/extract archive file\nAlmost always used with flags.\nCreate is usually tar -czf &lt;a rchive name&gt; &lt;file(s)&gt;\nExtract is usually t ar -xfv &lt;archive name&gt;\n\n\nscp\nSecure copy via ssh\nRun on laptop to server\nCan use most ssh flags (like -i and -v)\n\n\n\n\n\nD.6.6 Write files from the command line\n\n\n\n\n\n\n\n\nCommand\nWhat it does\nNotes\n\n\n\n\ntouch\nCreates file if doesn’t already exist.\nUpdates last updated to current time if it does exist.\n\n\n&gt;\nOverwrite file contents\nCreates a new file if it doesn’t exist\n\n\n&gt;&gt;\nConcatenate to end of file\nCreates a new file if it doesn’t exist\n\n\n\n\n\nD.6.7 Command Line Text Editors (Vim + Nano)\n\n\n\n\n\n\n\n\nCommand\nWhat it does\nNotes + Helpful options\n\n\n\n\n^\nPrefix for file command in nano editor.\nIts the ⌘ or Ctrl key, not the caret symbol.\n\n\ni\nEnter insert mode (able to type) in vim\n\n\n\nescape\nEnter normal mode (navigation) in vim.\n\n\n\n:w\nWrite the current file in vim (from normal mode)\nCan be combined to save and quit in one, :wq\n\n\n:q\nQuit vim (from normal mode)\n:q! quit without saving"
  },
  {
    "objectID": "chapters/append/cheatsheets.html#cheat-ssh",
    "href": "chapters/append/cheatsheets.html#cheat-ssh",
    "title": "Appendix D — Cheatsheets",
    "section": "D.7 ssh",
    "text": "D.7 ssh\nssh &lt;user&gt;@&lt;host&gt;\n\n\n\n\n\n\n\n\nFlag\nWhat it does\nNotes\n\n\n\n\n-v\nVerbose, good for debugging.\nAdd more vs as you please, -vv or -vvv.\n\n\n-i\nChoose identity file (private key)\nNot necessary with default key names."
  },
  {
    "objectID": "chapters/append/cheatsheets.html#linux-admin",
    "href": "chapters/append/cheatsheets.html#linux-admin",
    "title": "Appendix D — Cheatsheets",
    "section": "D.8 Linux Admin",
    "text": "D.8 Linux Admin\n\nD.8.1 Users\n\n\n\n\n\n\n\n\nCommand\nWhat it does\nHelpful options + notes\n\n\ns u &lt;username&gt;\nChange to be a different user.\n\n\n\nwhoami\nGet username of current user.\n\n\n\nid\nGet full user + group info on current user.\n\n\n\npasswd\nChange password.\n\n\n\nuseradd\nAdd a new user.\n\n\n\nusermo d &lt;username&gt;\nModify user username\n-aG &lt;group&gt; adds to a group (e.g. sudo)\n\n\n\n\n\nD.8.2 Permissions\n\n\n\n\n\n\n\n\nCommand\nWhat it does\nHelpful options + notes\n\n\n\n\nchmod  &lt;permissions&gt; &lt;file&gt;\nModifies permissions on a file or directory.\nNumber indicates permissions for user, group, others: add 4 for read, 2 for write, 1 for execute, 0 for nothing, e.g. 644.\n\n\nchow n &lt;user/group&gt; &lt;file&gt;\nChange the owner of a file or directory.\nCan be used for user or group, e.g. :my-group.\n\n\nsudo &lt;command&gt;\nAdopt root permissions for the following command.\n\n\n\n\n\n\nD.8.3 Install applications (Ubuntu)\n\n\n\n\n\n\n\nCommand\nWhat it does\n\n\napt-get u pdate && apt-get upgrade -y\nFetch and install upgrades to system packages\n\n\napt-get install &lt;package&gt;\nInstall a system package.\n\n\nwget\nDownload a file from a URL.\n\n\ngdebi\nInstall local .deb file.\n\n\n\n\n\nD.8.4 Storage\n\n\n\n\n\n\n\n\nCommand\nWhat it does\nHelpful options\n\n\n\n\ndf\nCheck storage space on device.\n-h for human readable file sizes.\n\n\ndu\nCheck size of files.\nMost likely to be used as d u - h &lt;dir&gt; | sort -h\nAlso useful to combine with head.\n\n\n\n\n\nD.8.5 Processes\n\n\n\n\n\n\n\n\nCommand\nWhat it does\nHelpful options\n\n\n\n\ntop\nSee what’s running on the system.\n\n\n\nps aux\nSee all system processes.\nConsider using --sort and pipe into head or grep\n\n\nkill\nKill a system process.\n-9 to force kill immediately\n\n\n\n\n\nD.8.6 Networking\n\n\n\n\n\n\n\n\nCommand\nWhat it does\nHelpful Options\n\n\nnetstat\nSee ports and services using them.\nUsually used with -tlp, for tcp listening applications, including pid\n\n\nssh -L &lt;port&gt;:&lt;i p&gt;:&lt;port&gt;:&lt;host&gt;\nPort forwards a remote port on remote host to local.\nRemote ip is usually localhost.\nChoose local port to match remote port.\n\n\n\n\n\nD.8.7 The path\n\n\n\n\n\n\n\nCommand\nWhat it does\n\n\nwhich &lt;command&gt;\nFinds the location of the binary that runs when you run command.\n\n\nln -s &lt;location to l ink&gt;:&lt;location of symlink&gt;\nCreates a symlink from file at location to link to location of symlink.\n\n\n\n\n\nD.8.8 systemd\nDaemonizing services is accomplished by configuring them in /etc/systemd/system/&lt;service name&gt;.service.\nThe format of all commands is systemctl &lt;command&gt; &lt;application&gt;.\n\n\n\n\n\n\n\nCommand\nNotes/Tips\n\n\n\n\nstatus\nReport status\n\n\nstart\n\n\n\nstop\n\n\n\nrestart\nstop then start\n\n\nreload\nReload configuration that doesn’t require restart (depends on service)\n\n\nenable\nDaemonize the service\n\n\ndisable\nUn-daemonize the service"
  },
  {
    "objectID": "chapters/append/cheatsheets.html#cheat-ports",
    "href": "chapters/append/cheatsheets.html#cheat-ports",
    "title": "Appendix D — Cheatsheets",
    "section": "D.9 IP Addresses and Ports",
    "text": "D.9 IP Addresses and Ports\n\nD.9.1 Special IP Addresses\n\n\n\n\n\n\n\nAddress\nMeaning\n\n\n\n\n\\(\\text{127.0.0.1}\\)\n\\(\\text{localhost}\\) or loopback – the machine that originated the request.\n\n\n\\(\\text{192.168.x.x}\\)\n\\(\\text{172.16.x.x.x}\\)\n\\(\\text{10.x.x.x}\\)\nProtected address blocks used for private IP addresses.\n\n\n\n\n\nD.9.2 Special Ports\nAll ports below \\(1024\\) are reserved for server tasks and cannot be assigned to admin-controlled services.\n\n\n\nProtocol/Application\nDefault Port\n\n\n\n\nHTTP\n\\(80\\)\n\n\nHTTPS\n\\(443\\)\n\n\nSSH\n\\(22\\)\n\n\nPostgreSQL\n\\(5432\\)\n\n\nRStudio Server\n\\(8787\\)\n\n\nShiny Server\n\\(3939\\)\n\n\nJupyterHub\n\\(8000\\)"
  }
]