[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DevOps for Data Science",
    "section": "",
    "text": "Welcome!\nThis is the website for the book DevOps for Data Science, currently in draft form.\nIn this book, you’ll learn about DevOps conventions, tools, and practices that can be useful to you as a data scientist. You’ll also learn how to work better with the IT/Admin team at your organization, and even how to do a little server administration of your own if you’re pressed into service.\nThis website is (and always will be) free to use, and is licensed under the Creative Commons Attribution-NonCommercial-NoDerivs 3.0 license.\nIf you’d like a physical copy of the book, they will be available once it’s finished!"
  },
  {
    "objectID": "index.html#software-information",
    "href": "index.html#software-information",
    "title": "DevOps for Data Science",
    "section": "Software information",
    "text": "Software information\nI used the knitr package [@xie2015] and the quarto package [@quarto] to compile my book.\nThis book is published to the web using GitHub Actions from rOpenSci."
  },
  {
    "objectID": "index.html#about-the-author",
    "href": "index.html#about-the-author",
    "title": "DevOps for Data Science",
    "section": "About the Author",
    "text": "About the Author\nAlex Gold is the Director of Solutions Engineering at Posit, formerly RStudio.\nThe Solutions Engineering team works with Posit’s customers to help them deploy, configure, and use Posit’s professional software and open-source tooling in R and Python.\nIn his free time, he enjoys landscaping, handstands, and Tai Chi.\nHe occasionally blogs about data, management, and leadership at alexkgold.space."
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "DevOps for Data Science",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nI have so many people to thank for their help in getting this book out the door.\nThe biggest thanks to current and former members of the Solutions Engineering Team at Posit, who taught me so much about DevOps, Data Science, and how to be a great team.\nThanks to my family, especially my brother, who is a great brother and cared enough about this project to insist he appear in the acknowledgments.\nThanks to Randi Cohen at Taylor and Francis, who has been great to work with, and to my editor, Linda Kahn, who’s always been more than an editor to me.\nMost of all, thanks to Shoshana for helping me live my best life.\nHuge thanks to the R4DS book club, especially Jon Harmon, Gus Lipkin, and Tinashe Michael Tapera, who read an early (and rough!) copy of this book and gave me amazing feedback.\nThanks to all others who provided improvements that ended up in this book (in alphabetical order): Carl Boettinger, Jon Harmon, Gus Lipkin, and Leungi."
  },
  {
    "objectID": "index.html#color-palette",
    "href": "index.html#color-palette",
    "title": "DevOps for Data Science",
    "section": "Color palette",
    "text": "Color palette\nTea Green: #CAFFDO\nSteel Blue: #3E7CB1\nKombu Green: #273c2c\nBright Maroon: #B33951\nSandy Brown: #FCAA67"
  },
  {
    "objectID": "chapters/intro.html#devops-for-agile-software",
    "href": "chapters/intro.html#devops-for-agile-software",
    "title": "Introduction",
    "section": "DevOps for Agile Software",
    "text": "DevOps for Agile Software\nDevOps is a set of cultural norms, practices, and tooling to help make developing and deploying software smoother and lower risk.\nIf that definition strikes you as unhelpfully vague, you’re right.\nDevOps is a squishy concept, much like the closely related Agile software development process. That’s partially because DevOps isn’t a fixed thing. It’s the application of some principles and ideas about process to whatever context you’re working in. That malleability is why DevOps works, but it makes it difficult to pin down.\nThe ecosystem of companies selling DevOps tools furthers this imprecision. There are dozens and dozens of companies proselytizing their particular flavor of DevOps – one that (shocker) reflects the capabilities of their product.\nBut there are some precious lessons to learn underneath the industry hype and the marketing jargon.\nTo understand better, let’s go back to the birth of DevOps. As the story goes, the history of software development before the 1990s involved a waterfall development process. Software developers worked with clients and customers to fully define project requirements, plan the entire development process, and deliver completed software months or years later.\nWhen the application was complete, it was hurled over the metaphorical wall from Development to Operations. Professionals in the Ops department would figure out the hardware and networking requirements, get it running, and maintain it.\nThis working method came with a lot of problems. It was hard to estimate how long each bit of work would take and to divine how the finished software should look and work ahead of time. Software developers observed that delivering working software in small units, quickly collecting feedback, and iterating was a more effective model.\nIn 2001, the Manifesto for Agile Software Development was published, giving a name to this new software development philosophy. Agile development ate the world. Essentially all software is now developed using some form of Agile. Agile work patterns have also extended beyond software into more general project management.\nYou may have heard of some of the dozens of Agile software development frameworks, including Scrum, Kanban, Extreme Programming (XP), and many, many more. These frameworks laid out effective software development methods, but a question remained. What should happen once the software is written?\nThe old pattern clearly wouldn’t work. If you were writing code in small chunks that resulted in new deployments multiple times a week – or even a day – you needed a way to get software into production that complemented agile software development.\nDevOps arose as this discipline, i.e., a way for Dev and Ops to better collaborate on the process that would take software from development into production. It took a little while for the field to be formalized, with the term DevOps coming into common usage around 2010."
  },
  {
    "objectID": "chapters/intro.html#processes-and-people",
    "href": "chapters/intro.html#processes-and-people",
    "title": "Introduction",
    "section": "Processes and People",
    "text": "Processes and People\nThroughout this book, DevOps refers to the knowledge, practices, and tools that make it easier, safer, and faster to put work into production. So, if you’re a software developer (and as a data scientist, you are), you need to be thinking about DevOps.\nMost organizations also have a set of people and roles who have the permission and responsibility for managing their organization’s servers and software. Their titles vary. They might be named Information Technology (IT), SysAdmin, Site Reliability Engineering (SRE), or DevOps.1\nFor simplicity, I will use the term IT/Admin to refer to these people and teams throughout this book.\nAs a data scientist, you are the Dev, so a huge part of making DevOps work for you is finding IT/Admin counterparts with whom you can collaborate. In some cases, that will be easier than others. Here are three patterns that are almost always red flags – mainly because they make it hard to develop durable relationships to sustain the kind of collaboration DevOps requires.\n\nAt some large organizations, IT/Admin functions are split into small atomic units like security, databases, networking, storage, procurement, cloud, and more. This is useful for keeping the scope of work manageable for the people in that group and often yields deep technical expertise. But it also can be slow to get anything done because you’ll need to bring people together from disparate teams.\nSome organizations have chosen to outsource their IT/Admin functions. While the individuals in those outsourced teams are often competent, building relationships can be difficult. Outsourced IT/Admin teams are often in India, so it can be hard to find meeting times with American and European teams. Additionally, turnover on projects and systems tends to be high, so institutional knowledge is thin, and relationships can’t be relied on long term.\nSome organizations, especially small or new ones, don’t have an IT/Admin function. At others, the IT/Admins are preoccupied with other tasks and lack the capacity to help the data science team. This isn’t a tragedy, but it probably means you’ll have to become the IT/Admin if you want to get anything done.\n\nWhether your organization has an IT/Admin setup that facilitates DevOps best practices or not, this book can help you take the first steps toward making your path to production smoother and simpler."
  },
  {
    "objectID": "chapters/intro.html#a-data-science-platform",
    "href": "chapters/intro.html#a-data-science-platform",
    "title": "Introduction",
    "section": "A data science platform",
    "text": "A data science platform\nA lot of data science is done on personal computers. Data scientists download Jupyter Notebook or RStudio, install Python and R, and get to work. However, organizations are increasingly centralizing data science operations onto a centralized data science platform or data science environment.\nIt’s easier to secure connections between a centralized platform and data sources compared to providing access to everyone’s laptops. Similarly, providing more computational resources is much easier in a centralized environment compared to distributing new laptops.\nThere are two essential components of an organizational data science platform. The first is the workbench. This is where data scientists go to get work done. It has Python, R, data access, sufficient computational resources, and the open-source Python and R packages you need to do work.\nA good workbench drastically speeds onboarding for the data science team. Compared to the days, weeks, or months to provide each laptop access to each data source, adding a new person to the platform takes minutes, and they arrive with all of their tools pre-provisioned.\nOnce data science projects are complete, they need to go somewhere to be shared. That means the data science environment needs to include a deployment platform where data science projects can be hosted and shared with other people and systems.\nIn most organizations, especially enterprises, everything in the data science environment will also be subject to access control to ensure that only the right people and systems have access.\n\nThis book will help you understand the needs of each component of the data science platform and how to articulate them to the IT/Admins at your organization who will help you get one."
  },
  {
    "objectID": "chapters/intro.html#about-this-book",
    "href": "chapters/intro.html#about-this-book",
    "title": "Introduction",
    "section": "About this book",
    "text": "About this book\nWhile engaging with many organizations, I’ve seen which patterns grease the path to production for data scientists and which tend to impede it.\nMy goal is that this book helps you create data science projects that are easier and simpler to deploy and that you have the knowledge and skills to get them into production when it’s time.\nTo that end, this book is divided into three sections.\nSection 1 is about applying DevOps best practices to a data science context. Adhering to these best practices will make it easier to take projects into production and ensure their security and stability once they’re there. While these best practices are inspired by DevOps, data science and data science projects are different enough from general-purpose sofware engineering that some re-thinking is required.\nSection 2 is a walkthrough of basic concepts in IT Administration that will get you to the point of being able to host and manage a basic data science environment. If you are a hobbyist or have only a small data science team, this might make you able to operate without any IT/Admin support. Even if you work at an organization with significant IT/Admin support, it will equip you with the vocabulary to talk to the IT/Admins at your organization and some basic skills of how to do IT/Admin tasks yourself.\nSection 3 is about how everything you learned in Section 2 is inadequate at organizations that operate at enterprise scale. If section 2 explains how to do IT/Admin tasks yourself, section 3 is my attempt to explain why you shouldn’t.\n\nComprehension Questions\nEach chapter in this book includes comprehension questions. As you get to the end of the chapter, take a moment to consider these questions. If you feel comfortable answering them, you’ve probably understood the chapter’s content.\n\n\n\n\n\n\nMental Models + Mental Maps\n\n\n\nI’ll frequently discuss building mental models throughout the book. A mental model is an understanding of each of the components in a system and how they fit together.\nA mental map is a way to represent mental models. In a mental map, you draw each entity in the system as a node in a graph and connect them with labeled arrows to explain the relationship.\nMental maps are a great way to test your mental models, so I’ll suggest them as comprehension questions in many chapters.\nHere’s an example for this book:\n\nNote how every node is a noun, and the edges (labels on the arrows) are verbs. You’ve probably understood the content if you can write down the relationships in this compact form.\n\n\n\n\nLabs\nMany chapters also contain labs. The idea of these labs is to give you hands-on experience with the concepts at hand.\nThese labs all tie together. If you follow the labs, you’ll build up a reasonably complete data science platform, including a data science workbench and a project hosting platform.\nIn the labs, we’ll use the Palmer Penguins data, a public dataset meant to demonstrate data exploration and visualization. We’re going to pretend we care deeply about the relationship between penguin bill length and mass, and we’re going to build up an entire data science environment dedicated to exploring that relationship.\nThe front end of this environment will be a website built in Quarto. It will include an app for fetching penguin mass predictions from a machine learning model based on bill length and other features. The website will also have pages dedicated to exploratory data analysis and model building.\nOn the backend, we will build a data science workbench on an AWS EC2 instance where we can do this work. It will include RStudio Server and JupyterHub for working. It will also host the machine learning model as an API and the Shiny app for the website.\nThe whole thing will get auto-deployed from a git repo using GitHub Actions.\nFrom an architectural perspective, it’ll look something like this:\n\nIf you’re interested in which pieces are completed in each chapter, check out Appendix C.\n\n\nConventions\nThroughout the book, I will italicize terms of art when introducing them and the names of other publications. Because so many of the technical terms in this book are usually referred to by abbreviations or acronyms, I’ll use the abbreviation or acronym in the text and include the entire name in parentheses the first time it’s mentioned.\nBolding will be reserved for emphasis.\nR and Python packages will appear inside braces in code font like {package}, and system commands will be in code font. Networking concepts and terms, including URLs, will appear in \\(\\text{equation font}\\).\nVariables you would replace with your values will appear in code font inside angled brackets like &lt;your-variable&gt;."
  },
  {
    "objectID": "chapters/intro.html#footnotes",
    "href": "chapters/intro.html#footnotes",
    "title": "Introduction",
    "section": "",
    "text": "I think a lot of DevOps experts would argue that you’re doing DevOps wrong if you have a standalone DevOps team, but such teams exist.↩︎"
  },
  {
    "objectID": "chapters/sec1/1-0-sec-intro.html#labs-in-this-section",
    "href": "chapters/sec1/1-0-sec-intro.html#labs-in-this-section",
    "title": "DevOps Lessons for Data Science",
    "section": "Labs in this section",
    "text": "Labs in this section\nEach chapter in this section has a lab so you can get hands-on experience implementing DevOps best practices in your data science projects.\nYou’ll create a website in the labs to explore the Palmer Penguins dataset, especially the relationship between penguin bill length and mass. Your website will include pages on exploratory data analysis and model building. This website will automatically build and deploy based on changes in a git repo.\nYou’ll also create a Shiny app that visualizes model predictions and an API that hosts the model and provides real-time predictions to the app. Additionally, you’ll get to practice putting that API inside a Docker Container to see how using Docker can make your life easier when moving code around.\nFor more details on precisely what you’ll do in each chapter, see Appendix C."
  },
  {
    "objectID": "chapters/sec1/1-0-sec-intro.html#footnotes",
    "href": "chapters/sec1/1-0-sec-intro.html#footnotes",
    "title": "DevOps Lessons for Data Science",
    "section": "",
    "text": "If you enjoy this introduction, I strongly recommend The Phoenix Project by Gene Kim, Kevin Behr, and George Spafford. It’s a novel about implementing DevOps principles. A good friend described it as, “a trashy romance novel about DevOps”. It’s a very fun read.↩︎"
  },
  {
    "objectID": "chapters/sec1/1-1-env-as-code.html#environments-have-layers",
    "href": "chapters/sec1/1-1-env-as-code.html#environments-have-layers",
    "title": "1  Environments as Code",
    "section": "1.1 Environments have layers",
    "text": "1.1 Environments have layers\nData science environments have three distinct layers. Reasoning clearly about these layers can reveal your actual reproducibility needs and which environmental layers you need to target putting into code.\nAt the bottom of the environment is the hardware layer. This is the physical and virtual hardware where the code runs. For example, this might be your laptop or a virtual server from a cloud provider. Above that is the system layer, which includes the operating system, essential system libraries, and Python and/or R. Above that is the package layer, where your Python and R packages live.\nLayers of data science environments\n\n\n\n\n\n\n\nLayer\nContents\n\n\n\n\nPackages\nPython + R Packages\n\n\nSystem\nPython + R Language Versions\nOther System Libraries\nOperating System\n\n\nHardware\nVirtual Hardware\nPhysical Hardware\n\n\n\nIn an ideal world, the hardware and system layers should be the responsibility of an IT/Admin. You may be responsible for them, but then you’re fulfilling the IT/Admin role.\nAs a data scientist, you can and should be responsible for the package layer, and getting this layer right is where the biggest reproducibility bang for your buck lies. If you find yourself managing the system or hardware layer, chapters Chapter 7 through Chapter 14 will teach you how to manage those layers."
  },
  {
    "objectID": "chapters/sec1/1-1-env-as-code.html#the-package-layer",
    "href": "chapters/sec1/1-1-env-as-code.html#the-package-layer",
    "title": "1  Environments as Code",
    "section": "1.2 The package layer",
    "text": "1.2 The package layer\nThere are three different places packages can live.\n\nIn a repository. You’re used to installing packages from repositories like PyPI, Conda, CRAN, or BioConductor. These repositories are like the grocery store. The food is packaged up and ready to go, but inert. There are also many varieties there. Repositories hold both current and archival versions of each package.1\nIn a library. Once you install the packages you need with install.packages() or pip install or conda install, they’re in your library, the data science equivalent of a pantry. Libraries can hold – at most – one version of any given package. Libraries can be specific to the project, user, or shared across the system.\nLoaded. Loading a package with a library or import command is like taking the food out of the pantry and putting it on the counter so you can cook with it.\n\nAs a data scientist, the atomic unit of package reproducibility is in the middle – the library.\nLet’s say you work on one project for a while, installing packages from the repository into your library. You go away for a year to work on other projects or try to share your project with someone else. When you come back, it’s likely that future you or your colleague won’t have the correct versions and your code will break.\nWhat would’ve been better is if you’d had an environment as code strategy that created a portable environment for each project on your system.\nA successful package environment as code setup has two key attributes:\n\nYour package environment is isolated and cannot be disrupted by other activities on the same machine.\nYour package environment can easily be captured and transported elsewhere.\n\nIn Python, there are many different options for virtual environment tooling. I recommend {virtualenv}/{venv} and related tools for production data science.\nIn R, there’s really only one game in town: the {renv} package.\n\n\n\n\n\n\nA note on Conda\n\n\n\nConda allows you to create a virtual environment in user space on your laptop without having admin access. It’s especially useful when your machine is locked down by IT.\nThat’s not a great fit for a production environment. Conda smashes together the language version, the package management, and, sometimes, the system library management. This is conceptually simple and easy-to-use, but it often goes awry in production environments. In a production environment (or a shared workbench server) I recommend people manage Python packages with a virtual environment tool like {venv} and manage system libraries and versions of Python with tools built for those purposes."
  },
  {
    "objectID": "chapters/sec1/1-1-env-as-code.html#using-a-virtual-environment",
    "href": "chapters/sec1/1-1-env-as-code.html#using-a-virtual-environment",
    "title": "1  Environments as Code",
    "section": "1.3 Using a virtual environment",
    "text": "1.3 Using a virtual environment\nUsing a virtual environment tool is a three-step process.\nAt a high level, you’ll create and use standalone package libraries, use tooling to capture the state of that package environment, and restore that state wherever else you might need the environment.\n\n\n\n\n\n\nNote\n\n\n\nSee the cheatsheet in Appendix D for the exact commands for both R and Python.\n\n\nStep 1: Create standalone package libraries\nEach project should have its own {renv}/{venv} library. When you start your project, it should be in a standalone directory that includes everything the project needs – including a virtual environment.\nThis is called a project-oriented workflow. You can do it in either R or Python. The What They Forgot to Teach You About R course (materials available online at rstats.wtf) is an excellent intro to a project-oriented workflow whether you work in R or Python. The tooling will be somewhat different in Python, but the idea is the same.\n\n\n\n\n\n\nTip\n\n\n\nSuppose your project includes multiple content items (say an app, API, and ETL script). In that case, I recommend using one git repo for the whole project with each content item in its own directory with its own virtual environment.\n\n\nWhen you work on the project, you activate the virtual environment and install and use packages in there.\nStep 2: Document environment state\nThe way to make the environment portable is to document what’s in the package library. Both {renv} and {venv} have standard file formats for documenting the packages, as well as the versions, that are installed in the environment.\nIn {renv}, the file is called a lockfile and it’s a requirements.txt in {venv}.\nSince all this work occurs in a standalone package environment, you don’t have to worry about what will happen if you return after a break. You’ll still have those same packages to use.\nStep 3: Collaborate or deploy\nWhen you go to share your project, you don’t want to share your actual package libraries. Package installs are specific to the operating system and the language version you’re using, so you want your target system to install the package specifically for that system.\nFor example, if you’re working on a Mac and you collaborate or deploy to a Windows or Linux machine, you can’t share the actual package files. Those machines will need to install the required set of packages for themselves.\nAdditionally, package files can be large. Sharing a requirements file makes downloads and uploads much more manageable, especially if you’re using git. So, check your lockfile or requirements.txt into git with your project.\n\n1.3.1 Step 4: Use a virtual environment\nThen, when your deployment target, collaborator, or future you downloads your project, it will restore the documented environment, again using tools from {renv}/{venv}."
  },
  {
    "objectID": "chapters/sec1/1-1-env-as-code.html#whats-happening-under-the-hood",
    "href": "chapters/sec1/1-1-env-as-code.html#whats-happening-under-the-hood",
    "title": "1  Environments as Code",
    "section": "1.4 What’s happening under the hood",
    "text": "1.4 What’s happening under the hood\nInstalled packages are stored in libraries, which are just directories on your system. Your Python or R session keeps track of the set of libraries it should use with sys.path in Python and .libPaths() in R.\nSo when you install a package, it installs into the first library allowed. And when you load a package with import or library, it searches the directories from sys.path or .libPaths() and returns the package when it finds it.\n\nEach library can contain, at most, one version of any package. So order matters for the directories in sys.path or .libPaths(). Whatever version is found first during the search will be returned.\n\n\n\n\n\n\nNote\n\n\n\nThis works the same for Python modules as for packages. I’m just using the term packages since most modules that aren’t purpos-built for a project are in packages.\n\n\nIf you’re not in a virtual environment, the top libraries are user-level libraries by default. Activating a virtual environment puts project-level libraries at the top of the lists in sys.path or .libPaths() so package installs and loads happen from there.\nTo economize on space and install time, both {renv} and {venv} do something clever. The packages in your project-level library aren’t there. Instead, {renv} and {venv} keep user-level package caches of the actual packages and use symlinks, so only one copy of the package is installed.\nSometimes, IT/Admins want to save space further by sharing package caches across users. This is usually a mistake. Sharing package caches leads to headaches over user file permissions to write to the package cache versus read. Storage space is cheap, way cheaper than your time. If you have to do it, both {renv} and venv include settings to allow you to relocate the package cache to a shared location on the server."
  },
  {
    "objectID": "chapters/sec1/1-1-env-as-code.html#comprehension-questions",
    "href": "chapters/sec1/1-1-env-as-code.html#comprehension-questions",
    "title": "1  Environments as Code",
    "section": "1.5 Comprehension Questions",
    "text": "1.5 Comprehension Questions\n\nWhy does difficulty increase as the level of required reproducibility increase for a data science project. In your day-to-day work, what’s the hardest reproducibility challenge?\nDraw a mental map of the relationships between the seven levels of the reproducibility stack. Pay particular attention to why the higher layers depend on the lower ones.\nWhat are the two key attributes of environments as code? Why do you need both of them? Are there cases where you might only care about one?\nDraw a mental map of the relationships between the following: package repository, package library, package, project-level-library, .libPaths() (R) or sys.path(python), lockfile\nWhy is it a bad idea to share package libraries? What’s the best way to collaborate with a colleague using an environment as code? What commands will you run in R or Python to save a package environment and restore it later?"
  },
  {
    "objectID": "chapters/sec1/1-1-env-as-code.html#lab1",
    "href": "chapters/sec1/1-1-env-as-code.html#lab1",
    "title": "1  Environments as Code",
    "section": "1.6 Lab: Create and use a virtual environment",
    "text": "1.6 Lab: Create and use a virtual environment\nIn this lab, we will start working on our penguin explorer website. We will create a simple website using Quarto, an open-source scientific and technical publishing system that makes it easy to render R and Python code into beautiful documents, websites, reports, and presentations.\nWe will create pages for a simple exploratory data analysis and model building from the Palmer Penguins dataset. To get to practice with both R and Python, I’m going to do the EDA page in R and the modeling in Python. By the end of this lab, we’ll have both pages created using standalone Python and R virtual environments.\nIf you’re starting, check out the Quarto website to use Quarto in the editor of your choice.\n\n\n\n\n\n\nTip\n\n\n\nEnsure you add each page below to your _quarto.yml so Quarto knows to render them.\n\n\n\n1.6.1 EDA in R\nLet’s add a simple R-language EDA of the Palmer Penguins data set to our website by adding a file called eda.qmd in the project’s root directory.\nBefore you add code, create and activate an {renv} environment with renv::init().\nNow, go ahead and do your analysis. Here’s the contents of my eda.qmd.\n\n\neda.qmd\n\n---\ntitle: \"Penguins EDA\"\nformat:\n  html:\n    code-fold: true\n---\n\n## Penguin Size and Mass by Sex and Species\n\n```{r}\nlibrary(palmerpenguins)\nlibrary(dplyr)\nlibrary(ggplot2)\n\ndf &lt;- palmerpenguins::penguins\n```\n\n```{r}\ndf %&gt;%\n  group_by(species, sex) %&gt;%\n  summarise(\n    across(\n      where(is.numeric), \n      \\(x) mean(x, na.rm = TRUE)\n      )\n    ) %&gt;%\n  knitr::kable()\n```\n\n## Penguin Size vs Mass by Species\n\n```{r}\ndf %&gt;%\n  ggplot(aes(x = bill_length_mm, y = body_mass_g, color = species)) +\n  geom_point() + \n  geom_smooth(method = \"lm\")\n```\n\nFeel free to copy this Quarto doc into your website or to write your own.\nOnce you’ve finished writing your EDA script and checked that it previews nicely into the website, save the doc, and create your lockfile with renv::snapshot().\n\n\n1.6.2 Modeling in Python\nNow let’s build a {scikit-learn} model for predicting penguin weight based on bill length in a Python notebook by adding a model.qmd to the root of our project.\nAgain, you’ll want to create and activate your virtual environment before you start pip install-ing packages.\nHere’s what’s in my model.qmd, but you should feel free to include whatever you want.\n\n\nmodel.qmd\n\n---\ntitle: \"Model\"\nformat:\n  html:\n    code-fold: true\n---\n\n```{python}\nfrom palmerpenguins import penguins\nfrom pandas import get_dummies\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import preprocessing\n```\n\n## Get Data\n\n```{python}\ndf = penguins.load_penguins().dropna()\n\ndf.head(3)\n```\n\n## Define Model and Fit\n\n```{python}\nX = get_dummies(df[['bill_length_mm', 'species', 'sex']], drop_first = True)\ny = df['body_mass_g']\n\nmodel = LinearRegression().fit(X, y)\n```\n\n## Get some information\n\n```{python}\nprint(f\"R^2 {model.score(X,y)}\")\nprint(f\"Intercept {model.intercept_}\")\nprint(f\"Columns {X.columns}\")\nprint(f\"Coefficients {model.coef_}\")\n```\n\nOnce you’re happy with how the page works, capture your dependencies in a requirements.txt using pip freeze &gt; requirements.txt on the command line."
  },
  {
    "objectID": "chapters/sec1/1-1-env-as-code.html#footnotes",
    "href": "chapters/sec1/1-1-env-as-code.html#footnotes",
    "title": "1  Environments as Code",
    "section": "",
    "text": "They don’t always include archival versions, but they usually do.↩︎"
  },
  {
    "objectID": "chapters/sec1/1-2-proj-arch.html#choose-the-right-presentation-layer",
    "href": "chapters/sec1/1-2-proj-arch.html#choose-the-right-presentation-layer",
    "title": "2  Data Project Architecture",
    "section": "2.1 Choose the right presentation layer",
    "text": "2.1 Choose the right presentation layer\nThe presentation layer is the thing your users will consume. The data flows for your project will be dictated by your presentation layer choices so you should start by figuring out the presentation layer for your project.\nBasically all data science projects fall into the following categories.\n\nA job. A job matters because it changes something in another system. It might move data around, build a model, or produce plots, graphs, or numbers for a Microsoft Office report.\nFrequently, jobs are written in a SQL-based pipelining tool (dbt has been quickly rising in popularity) or in a .R or .py script.1 Depending on your organization, the people who write jobs may be called data engineers.\nAn app. Data science apps are created in frameworks like Shiny (R or Python), Dash (Python), or Streamlit (Python). In contrast to general-purpose web apps, which are for all sorts of purposes, data science web apps are usually used to give non-coders a way to explore data sets and see data insights.\nA report. Reports are code you’re turning into an output you care about – like a paper, book, presentation, or website. Reports result from rendering an R Markdown doc, Quarto doc, or Jupyter Notebook for people to consume on their computer, in print, or in a presentation. These docs may be completely static (this book is a Quarto doc) or have some interactive elements.2\nAn API (application programming interface). An API is for machine-to-machine communication. In general-purpose software, APIs are the backbone of how two distinct pieces of software communicate. In data science, APIs are mostly used to provide data feeds and on-demand predictions from machine learning models.\n\nChoosing the right type of presentation layer will make designing the rest of your project much easier. Here are some guidelines on how to choose which to use.\nIf the results of your software are for machine-to-machine use, you’re thinking about a job or API. You should create a job if it runs in a batched way, i.e. you write a data file or results into a database. If you want results to be queried in real-time, it’s an API.\nIf your project is for humans to consume, you’re thinking about creating an app or report. Reports are great if you don’t need to do data processing that depends on user input and apps are great if you do.\nThis flow chart illustrates how I decide which of the four types to build."
  },
  {
    "objectID": "chapters/sec1/1-2-proj-arch.html#do-less-in-the-presentation-layer",
    "href": "chapters/sec1/1-2-proj-arch.html#do-less-in-the-presentation-layer",
    "title": "2  Data Project Architecture",
    "section": "2.2 Do less in the presentation layer",
    "text": "2.2 Do less in the presentation layer\nData scientists usually don’t do a great job separating out their presentation layers. It’s not uncommon to see apps or reports that are thousands of lines of code, with UI components, code for plots, and data cleaning all mixed up together. These smushed up layers make it hard to reason about the code or to add testing or logging.\nThe only code that belongs in the presentation layer is code that shows something to the user or that collects input from the user. Creating the things shown to the user or doing anything with the interactions shouldn’t be in the presentation layer. These should be deferred to the processing layer.\nOnce you’ve identified what belongs in the processing layer, you should extract the code into functions that can be put in a package for easy documentation and testing and create scripts that do the processing.\n\n\n\n\n\n\nTip\n\n\n\nMoving things out of the presentation layer is especially important if you’re writing a Shiny app. You want to use the presentation layer to do reactive things and move all non-reactive interactions into the processing layer."
  },
  {
    "objectID": "chapters/sec1/1-2-proj-arch.html#small-data-in-the-presentation-layer",
    "href": "chapters/sec1/1-2-proj-arch.html#small-data-in-the-presentation-layer",
    "title": "2  Data Project Architecture",
    "section": "2.3 Small data in the presentation layer",
    "text": "2.3 Small data in the presentation layer\nEverything is easy when your data is small. You can load it into your Python or R session as your code starts and never think about it again.\n“Real engineers” may scoff at this pattern, but don’t let their criticism dissuade you. If your data size is small and your project performance is good enough, just read in all of your data and operate on it. Don’t over-complicate things. This pattern often works well into the range of millions of rows.\nIt may be the case that your data isn’t small – but not all large data is created equal.\nTruly big data can’t fit into the memory on your computer all at once. As computer memory gets more plentiful, truly big data is getting rarer.\nIt’s much more common to encounter medium data. You can technically load it into memory, but it’s substantial enough that loading it all makes your project’s performance too slow.\nDealing with medium or big data requires being somewhat clever and adopting a design pattern appropriate for big data (more on that in a bit). But being clever is hard.\nBefore you go ahead being clever, it’s worth asking a few questions that might let you treat your data as small.\n\n2.3.1 Can you pre-calculate anything?\nIf your data is truly big, it’s big. But if your data is medium-sized, the thing keeping it from being small isn’t some esoteric hardware issue, its performance.\nAn app requires high performance. Someone staring at their screen through a 90-second wait may think your project stinks depending on expectations.\nBut if you can pre-calculate a lookup table of values or turn your app into a report that gets re-rendered on a schedule, you can turn large data into a small data set in the presentation layer.\nTalking to your users and figuring out what cuts of the data they care about can help you determine whether pre-calculation is feasible or whether you need to load all the data into the presentation layer.\n\n\n2.3.2 Can you reduce data granularity?\nIf you can pre-calculate results and you’re still hitting performance issues, it’s always worth asking if your data can get smaller.\nLet’s think about a specific project to make this a little more straightforward.\nSuppose you work for a large retailer and are responsible for creating a dashboard of weekly sales. Your input data is a dataset of every item sold at every store for years. This isn’t naturally small data.\nBut you might be able to make the data small if you don’t need to allow the user to slice the data in too many different dimensions. Each additional dimension you allow multiplies the amount of data you need in the presentation layer.\nFor example, weekly sales at the department level only requires a lookup table as big as \\(\\text{number of weeks} * \\text{number of stores} * \\text{number of departments}\\). Even with a lot of stores and a lot of departments, you’re probably still squarely in the small data category.\nBut if you have to switch to a daily view, you multiply the amount of data you need by 7. If you break it out across 12 products, your data has to get 12 times bigger. And if you do both, it gets 84 times bigger. It’s not long before you’re back to a big data problem.\nTalking with your users about the tradeoffs between app performance and the number of data dimensions they need can identify opportunities to exclude dimensions and reduce your data size."
  },
  {
    "objectID": "chapters/sec1/1-2-proj-arch.html#big-data-patterns",
    "href": "chapters/sec1/1-2-proj-arch.html#big-data-patterns",
    "title": "2  Data Project Architecture",
    "section": "2.4 Make big data small",
    "text": "2.4 Make big data small\nThe way to make big data small is to avoid pulling all the data into your Python or R session. Instead, you want to pull in only some of the data.\nThere are a few different ways you can avoid pulling all the data. This isn’t an exhaustive list; each pattern will only work for some projects, but adopting one or more can be helpful.\n\n2.4.1 Push work to the data source\nIn most cases, the most time-consuming step is transmitting the data from the data source to your project. So, as a general rule, you should do anything you can do before you pull the data out.\nThis works quite well when you’re creating simple summary statistics and when your database is reasonably fast. It may be unwise if your data source is slow or impossible if you’re doing complicated machine learning tasks on a database that only supports SQL.\n\n\n2.4.2 Be lazy with data pulls\nAs you’re pushing more work into the database, it’s also worth considering when the project pulls its data during its runtime. The most basic pattern is to include the data pull in the project setup in an eager pattern. This is often a good first cut at writing an app, as it’s much simpler than doing anything else.\nIf that turns out to be too slow, consider being lazy with your data pulls. In a lazy data pattern, you have a live connection to your data source and pull in only the data that’s needed when it’s needed.\nIf you don’t always need all the data, especially if the required data depends on what the user does inside a session, it might be worthwhile to pull only once the user interactions clarify what you need.\n\n\n2.4.3 Sample the data\nIt may be adequate to work on only a sample of the data for many tasks, especially machine learning ones. In some cases, like classification of highly imbalanced classes, it may work better to work on a sample rather than the whole data set.\nSampling tends to work well when you’re trying to compute statistical attributes of your datasets. Computing averages or rates and creating machine learning models works just fine on samples of your data. Be careful to consider the statistical implications of sampling, especially remaining unbiased. You may also want to consider stratifying your sampling to ensure good representation across important dimensions.\nSampling doesn’t work well on counting tasks. It’s hard to count when you don’t have all the data!\n\n\n2.4.4 Chunk and pull\nIn some cases, there may be natural groups in your data. For example, in our retail dashboard example, it may be the case that we want to compute something by time frame or store or product. In this case, you could pull just that chunk of the data, compute what you need and move on to the next one.\nChunking works well for all kinds of tasks including building machine learning models and creating plots. The big requirement is that the groups are cleanly separable. When they are, this is an example of an embarrassingly parallel task, which you can easily parallelize in Python or R.\nIf you don’t have distinct chunks in your data, it’s pretty hard to chunk the data."
  },
  {
    "objectID": "chapters/sec1/1-2-proj-arch.html#choose-location-by-update-frequency",
    "href": "chapters/sec1/1-2-proj-arch.html#choose-location-by-update-frequency",
    "title": "2  Data Project Architecture",
    "section": "2.5 Choose location by update frequency",
    "text": "2.5 Choose location by update frequency\nWhere you store your data should be dictated by how often the data is updated.\nThe simplest answer is to put it in the presentation bundle, the code and assets that comprise your presentation layer. For example, let’s say you’re building a simple Dash app, app.py.\nYou could create a project structure like this:\nmy-project/\n├─ app.py\n├─ data/\n│  ├─ my_data.csv\n│  ├─ my_model.pkl\nThis works well only if your data will be updated at the same cadence as the app or report itself. This works well if your project is something like an annual report that will be rewritten when you update the data.\nBut if your data updates more frequently than your project code, you want to put the data outside the project bundle.\n\n2.5.1 Filesystem\nThere are a few ways you can do this. The most basic is to put the data on a location in your file system that isn’t inside the app bundle.\nBut when it comes to deployment, data on the file system can be complicated. You can use the same directory if you write and deploy your project on the same server. If not, you’ll need to worry about how to make sure that directory is also accessible on the server where you’re deploying your project.\n\n\n2.5.2 Blob Storage or Pins\nIf you’re not going to store the flat file on the filesystem and you’re in the cloud, it’s most common to use blob storage. Blob storage allows you to store and recall things by name.3 Each of the major cloud providers has blob storage – AWS’s has s3 (short for simple storage service), Azure has Azure Blob Store, and Google has Google Storage.\nThe nice thing about blob storage is that it can be accessed from anywhere with access to the cloud. You can also control access using standard cloud identity management tooling.\nThere are packages in both R and Python for interacting with AWS that are very commonly used to access s3 – {boto3} in Python and {paws} in R.\nThe popular {pins} package in both R and Python wraps using blob storage into neater code. It can use a variety of storage backends, including cloud blob storage, networked or cloud drives like Dropbox, Microsoft365 sites, and Posit Connect.\n\n\n2.5.3 Google Sheets\nIf you’re still early in your project lifecycle, a Google Sheet can be a great way to save and recall a flat file. I wouldn’t recommend a Google Sheet as a permanent home for data, but it can be a good intermediate step while you’re still figuring out the right solution for your pipeline.\nThe primary weakness of a Google Sheet – that it’s editable by someone who logs in – can also be an asset if that’s something you need."
  },
  {
    "objectID": "chapters/sec1/1-2-proj-arch.html#store-intermediate-artifacts-in-the-right-format",
    "href": "chapters/sec1/1-2-proj-arch.html#store-intermediate-artifacts-in-the-right-format",
    "title": "2  Data Project Architecture",
    "section": "2.6 Store intermediate artifacts in the right format",
    "text": "2.6 Store intermediate artifacts in the right format\nAs you break your processing layer into components, you’ll probably have intermediate artifacts like analysis datasets, models, and lookup tables to pass from one stage to the next.\nIf you’re producing rectangular data frames (or vectors) and you have write access to a database, use that.\nBut you often don’t have write access to a database or have other sorts of artifacts that you need to save between steps and can’t go into a database, like machine learning models or rendered plots. In that case, you must choose how to store your data.\n\n2.6.1 Flat files\nFlat files are data files that can be moved around just like any other file on your computer.\n\n\n2.6.2 CSV\nThe most common is a comma separated value (csv) file, which is just a literal text file of the values in your data with commas as separators.4 You could open it in a text editor and read it if you wanted to.\nThe advantage of .csvs is that they’re completely ubiquitous. Every programming language has some way to read in a .csv file and work with it.\nOn the downside, .csvs are completely uncompressed. That makes them quite large relative to other files and slow to read and write. Additionally, because .csvs aren’t language-specific, complicated data types may not be preserved when saving to .csv. For example, dates are often mangled in the roundtrip to a .csv file and back.\nThey also can only hold rectangular data, so if you’re trying to save a machine learning model, a .csv doesn’t make sense.\n\n\n2.6.3 Pickle or RDS\nR and Python have language-specific flat file types – pickle in Python and rds in R. These are nice because they include some compression and preserve data types when you save a data frame. They also can hold non-rectangular data, which can be great if you want to save a machine learning model.\n\n\n2.6.4 DuckDB\nIf you don’t have a database but store rectangular data, you should strongly consider using DuckDB. Its an in-memory database that’s great for analytics use cases. In contrast to a standard database that runs its own live process, there’s no overhead for setting up DuckDB.\nYou just run it against flat files on disk (usually Parquet files), which you can move around like any other. And unlike a .csv, pickle, or rds file, a DuckDB is query-able, so you only load the data you need into memory.\nIt’s hard to stress how cool DuckDB is. Data sets that were big just a few years ago are now medium or even small.5"
  },
  {
    "objectID": "chapters/sec1/1-2-proj-arch.html#consider-data-auth-upfront",
    "href": "chapters/sec1/1-2-proj-arch.html#consider-data-auth-upfront",
    "title": "2  Data Project Architecture",
    "section": "2.7 Consider data auth upfront",
    "text": "2.7 Consider data auth upfront\nLife is easy if everyone who views your project has the same permissions to see the data. You can allow the project access to the data and check for authorization to view the project.\nBut you’re much more constrained if you need to provide different data access to different users. First, you probably need to use an app rather than a report so that you can respond to which user is accessing the app.\nSometimes, you can adjust data access in the app itself. Many app frameworks pass the username or user groups into the session, and you can write code that changes app behavior based on the user. For example, you can gate access to specific tabs or features of your app based on the user.\nSometimes, you’ll need to pass database credentials along to the database. If this is the case for you, you’ll need to figure out how to establish the user’s database credentials, ensure those credentials stay only in the user’s session, and how those credentials get passed along to the database. More on this topic in Chapter 16."
  },
  {
    "objectID": "chapters/sec1/1-2-proj-arch.html#create-an-api-if-you-need-it",
    "href": "chapters/sec1/1-2-proj-arch.html#create-an-api-if-you-need-it",
    "title": "2  Data Project Architecture",
    "section": "2.8 Create an API if you need it",
    "text": "2.8 Create an API if you need it\nIn the case of a general-purpose three-layer app, it is almost always the case that the middle tier will be an API. Separating processing logic into functions is often sufficient in a data science app. But separating it into an API is often helpful if you’ve got a long-running bit of business logic, like training an ML model.\n\n\n\n\n\n\nNote\n\n\n\nYou may have heard the term REST API or REST-ful.\nREST is a set of architectural standards for how to build an API. An API that conforms to those standards is called REST-ful or a REST API.\nIf you’re using standard methods for constructing an API like R’s {plumber} package or {FastAPI} in Python, they will be REST-ful – or at least close enough for standard usage.\n\n\nYou can think of an API as a “function as a service”. That is, an API is just one or more functions, but instead of being called within the same process that your app is running or your report is processing, it will run in a completely separate process.\nFor example, let’s say you’ve got an app that allows users to feed in input data and then generate a model based on that data. If you generate the model inside the app, the user will have the experience of pressing the button to generate the model and having the app seize up on them while they’re waiting. Moreover, other app users will find themselves affected by this behavior.\nIf, instead, the button in the app ships the long-running process to a separate API, it allows you to think about scaling out the presentation layer separate from the business layer.\nLuckily, if you’ve written functions for your app, turning them into an API is trivial since packages like {fastAPI} and {plumber} let you turn a function into an API by adding some specially-formatted comments."
  },
  {
    "objectID": "chapters/sec1/1-2-proj-arch.html#write-a-data-flow-chart",
    "href": "chapters/sec1/1-2-proj-arch.html#write-a-data-flow-chart",
    "title": "2  Data Project Architecture",
    "section": "2.9 Write a data flow chart",
    "text": "2.9 Write a data flow chart\nOnce you’ve figured out the project architecture you need, writing a data flow chart can be helpful.\nA data flow chart maps the different project components into the project’s three parts and documents all the intermediate artifacts you’re creating along the way.\nOnce you’ve mapped your project, figuring out where the data should live and in what format will be much simpler.\nFor example, here’s a simple data flow chart for the labs in this book. You may want to annotate your data flow charts with other attributes like data types, update frequencies, and where data objects live."
  },
  {
    "objectID": "chapters/sec1/1-2-proj-arch.html#comprehension-questions",
    "href": "chapters/sec1/1-2-proj-arch.html#comprehension-questions",
    "title": "2  Data Project Architecture",
    "section": "2.10 Comprehension Questions",
    "text": "2.10 Comprehension Questions\n\nWhat are the layers of a three-layer application architecture? What libraries could you use to implement a three-layer architecture in R or Python?\nWhat are some questions you should explore to reduce the data requirements for your project?\nWhat are some patterns you can use to make big data smaller?\nWhere can you put intermediate artifacts in a data science project?\nWhat does it mean to “take data out of the bundle”?"
  },
  {
    "objectID": "chapters/sec1/1-2-proj-arch.html#lab2",
    "href": "chapters/sec1/1-2-proj-arch.html#lab2",
    "title": "2  Data Project Architecture",
    "section": "2.11 Lab: Build the processing layer",
    "text": "2.11 Lab: Build the processing layer\nIn Chapter 1, we did some EDA of the Palmer Penguins data set and built an ML model. In this lab, we will take that work we did and turn it into the actual presentation layer for our project.\n\n2.11.1 Step 1: Write the model outside the bundle\nWhen we originally wrote our model.qmd script, we didn’t save the model at all.\nOur model will likely be updated more frequently than our app, so we don’t want to store it in the app bundle. Later in the book, I’ll show you how to store it in the cloud. For now, I will store it in a directory on my computer.\nI will use the {vetiver} package, an R and Python package to version, deploy, and monitor a machine learning model.\nWe can take our existing model, turn it into a {vetiver} model, and save it to the /data/model folder with\n\n\nmodel.qmd\n\n\n```{python}\nfrom vetiver import VetiverModel\nv = VetiverModel(model, model_name='penguin_model', prototype_data=X)\n```\n\n## Save to Board\n\nIf /data/model doesn’t exist on your machine, you can create it, or use a directory that does exist.\n\n\n2.11.2 Step 2: Create an API for model predictions\nI’ll serve the model from an API to allow for real-time predictions.\nAs the point of this lab is to focus on the architecture, I’m just going to use the auto-generation capabilities of {vetiver}. If you want to improve at writing APIs, I encourage you to consult the documentation for {plumber} or {fastAPI}.\nIf you’ve closed your modeling code, you can get your model back from your pin with:\n\nb = pins.board_folder('data/model', allow_pickle_read=True)\nv = VetiverModel.from_pin(b, 'penguin_model')\n\nThen you can auto-generate a {fastAPI} from this model with\n\napp = VetiverAPI(v, check_prototype=True)\n\nYou can run this in your Python session with app.run(port = 8080). You can then access run your model API by navigating to http://localhost:8080 in your browser.\nYou can play around with the front end there, including trying the provided examples."
  },
  {
    "objectID": "chapters/sec1/1-2-proj-arch.html#footnotes",
    "href": "chapters/sec1/1-2-proj-arch.html#footnotes",
    "title": "2  Data Project Architecture",
    "section": "",
    "text": "Though I’ll argue in Chapter 4 that you should always use a literate programming tool like Quarto, R Markdown, or Jupyter Notebook.↩︎\nExactly how much interactivity turns a report into an app is completely subjective. I generally think the distinction is whether there’s a running R or Python process in the background, but it’s not a particularly sharp line.↩︎\nThe term blob is great to describe the thing you’re saving in blob storage, but it’s actually an abbreviation for binary large object. Very clever, in my opinion.↩︎\nThere are other delimitors you can use. Tab separated value files (tsv) are something you’ll see occasionally.↩︎\nSome people use SQLite in a similar way, but DuckDB is much more optimized for analytics purposes.↩︎"
  },
  {
    "objectID": "chapters/sec1/1-3-data-access.html#accessing-and-using-databases",
    "href": "chapters/sec1/1-3-data-access.html#accessing-and-using-databases",
    "title": "3  Using databases and data APIs",
    "section": "3.1 Accessing and using databases",
    "text": "3.1 Accessing and using databases\nDatabases are defined by their query-able interface, usually through structured query language (SQL).\n\n\n\n\n\n\nNote\n\n\n\nThere are many kinds of databases, and choosing the right one for your project is beyond the scope of this book. One recommendation: open-source PostgreSQL (Postgres) is a great place to start for most general-purpose data science tasks.\n\n\nAny database connection starts by creating a connection object at the outset of your code. You’ll then use this object to send SQL queries, which you can generate by writing them yourself or using a package that generates SQL like {sqlalchemy} in Python or {dplyr} in R.\nFor example, in Python you might write the following to connect to a Postgres database:\n\nimport psychopg2\ncon = psycopg2.connect()\n\nIn R, it might look like this:\n\ncon &lt;- DBI::dbConnect(RPostgres::postgres())\n\nPython or R have standard connection APIs that define operations like connecting and disconnecting, sending queries, and retrieving results.\nIn Python, packages for individual databases like {psychopg2} directly implement the API, which is why the example above calls the connect() method of the {psychopg2} package.\nIn R, the API is split into two parts. The {DBI} package (short for database interface) implements the actual connections. It works with a database driver package, which is the first argument to DBI::dbConnect(). Packages that implement the {DBI} interface are called DBI-compliant.\n\n\n\n\n\n\nNote\n\n\n\nThere are Python packages that don’t implement the connections API and non DBI-compliant database packages in R. I’d recommend sticking with the standard route if possible.\n\n\nOften, a Python or R package will directly implement your database driver. For example, when you’re connecting to a Postgres database, there are Postgres-specific connectors – {psychopg2} in Python and {RPostgres} in R. For Spark, you’ve got {pyspark} and {sparklyr}.\nIf a package exists for your database, you should probably prefer it. It’s probably faster and may provide additional database-specific functionality compared to other options.\nIf there isn’t a database-specific package, you’ll need to use a generic system driver with a Python or R package to interface with system drivers.\nWhile performance sometimes isn’t as good for system drivers, the tradeoff is that IT/Admins can pre-configure connection details in a data source name (DSN). If one is pre configured for you, you don’t have to remember the database name, host, port, or even username and password if they’re shared.\nFor example, you might connect with something that looks like:\n\nimport pyodbc\ncon = pyodbc.connect(\"DSN=MY_DSN\")\n\nIn R, it might look like this:\n\ncon &lt;- DBI::dbConnect(odbc::odbc(), dsn = \"MY_DSN\")\n\nSystem drivers come in two main varieties Java Database Connectivity (JDBC) and Open Database Connectivity (ODBC).\nIn Python, {pyodbc} is the main package for using ODBC connections and {JayDeBeApi} for connecting using JDBC. In R, {odbc} is the best package for using system ODBC connections and {RJDBC} is the standard way to use JDBC.\n\n\n\n\n\n\nTip\n\n\n\nIf you’re using R and have the choice between JDBC and ODBC, I strongly recommend ODBC. JDBC requires an extra hop through Java and the {rJava} package, which is painful to configure.1"
  },
  {
    "objectID": "chapters/sec1/1-3-data-access.html#providing-credentials-to-data-projects",
    "href": "chapters/sec1/1-3-data-access.html#providing-credentials-to-data-projects",
    "title": "3  Using databases and data APIs",
    "section": "3.2 Providing credentials to data projects",
    "text": "3.2 Providing credentials to data projects\nImagine you’ve created a data science project that pulls data from a database. When you’re actively working on the project, it’s easy for you to provide credentials as needed to the database. But what happens when you deploy that project to production and you’re not sitting there to provide credentials?\nIn many organizations, you’ll be allowed to use your data access permissions for the project and then to share the project with others in the company at your discretion. This situation is sometimes called discretionary access control (DAC).\nIn some more restrictive environments, you won’t have this luxury. The IT/Admin team may maintain control of permissions or require that data access be more tightly governed.\nIn some cases, it will be acceptable to create or use a service account, which is a non-human account that exists to hold permissions for a project. You might want to use a service account to limit the project’s permissions to exactly what it needs or to be able to manage the project’s permissions independently of the humans involved.\nIn the most restrictive case, you’ll have to use the credentials of the person viewing the content and pass those along. This last option is much more complex than the other two.\n\nIf you have to use the viewer’s credentials for data access, you can write code to collect them from the viewer and pass them along. I don’t recommend this as you have to take responsibility for managing those credentials and storing and using them responsibly.\nIn other cases, the project may be able to run as the viewer when it is accessing the database. The patterns for doing this are complicated and require working with an IT/Admin. More on this topic in Chapter 16."
  },
  {
    "objectID": "chapters/sec1/1-3-data-access.html#connecting-to-apis",
    "href": "chapters/sec1/1-3-data-access.html#connecting-to-apis",
    "title": "3  Using databases and data APIs",
    "section": "3.3 Connecting to APIs",
    "text": "3.3 Connecting to APIs\nSome data sources come in the form of an API.\nIt’s common to have Python or R packages that wrap APIs, so you write Python or R code without thinking about the API underneath. Using these patterns often looks similar to databases – you create and use a connection object that stores the connection details. If your API has a package like this, you should use it.\nIf you’re consuming a private API at your organization, a helper package probably doesn’t exist, or you may have to write it yourself.\n\n\n\n\n\n\nNote\n\n\n\nThere’s increasingly good tooling to auto-generate packages based on API documentation, so you may never have to write an API wrapper package by hand. It’s still helpful to understand how it works.\n\n\nIf you have to call an API directly, you can use the {requests} package in Python or {httr2} in R.\nThese packages provide idiomatic R and Python ways to call APIs. It’s worth understanding that they’re purely syntactic sugar. There’s nothing special about calling an API from inside Python or R versus using the command line and you can go back and forth as you please. It is sometimes helpful to try to replicate Python or R API calls without the language wrapper for debugging reasons.\n\n3.3.1 What’s in an API?\nAPIs are the standard way for two computer systems to communicate. API is a general term that describes machine-to-machine communication. For our purposes, we’re talking about http-based REST-ful APIs.\nhttp operates on a request-response model. So when you use an API, you send a request to the API and it sends a response back.\n\nThe best way to learn about a new API is to read the documentation, which will include many details about usage. Let’s go through some of the most salient ones.\n\n\n3.3.2 API Endpoints and Paths\nEach request to an API is directed to a specific endpoint. An API can have many endpoints, each of which you can think of like a function in a package. Each endpoint lives on a path, where you find that particular endpoint.\nFor example, if you did the lab in Chapter 2 and used {vetiver} to create an API for serving the penguin mass model, you found your API at http://localhost:8080. By default, you went to the root path at / and found the API documentation there.\nAs you scrolled the documentation, there were two endpoints – /ping and /predict. You can read the definition to see what parameters you could send them and what you’d get back. Those paths are relative to the root, so you could access /ping at http://localhost:8080/ping.\n\n\n3.3.3 HTTP verbs\nWhen you make an HTTP request, you ask a server to do something. The http verb, also known as the request method, describes the type of operation you’re asking for. Each endpoint has one or more verbs that it knows how to use.\nLooking at the penguin mass API, you’ll see that /ping is a GET endpoint and /predict is a POST. This isn’t a coincidence. I’d approximate that 95% of the API endpoints you’ll use as a data scientist are GET and POST, which respectively fetch information from the server and provide information to the server.\nTo round out the basic http verbs you might see, PUT and PATCH change or update something and DELETE (you guessed it) deletes something. There are also more esoteric ones you’ll probably never see.\n\n\n3.3.4 Request parameters and bodies\nLike a function in a package, each endpoint accepts specific arguments in a required format. Again, like a function, some arguments may be optional while others may be required.\nFor GET requests, the arguments are specified via query parameters embedded in the URL after a ?. When you see a URL in your browser that looks like ?first_name=alex&last_name=gold, those are query parameters.\nFor POST, PUT, and PATCH requests, arguments are provided in a body, which is usually formatted as JSON.2 Both {httr2} and {requests} have built-in functionality for converting standard Python and R data types to their JSON equivalents. APIs often require their arguments to be nested in particular ways. You can experiment with how your objects get converted to JSON with {json} in Python and {jsonlite} in R to figure out how to get it nested correctly.\n\n\n3.3.5 (Auth) Headers\nMost APIs require authentication. The most common forms of authentication are a username and password combination, an API key, or an OAuth token.\nAPI keys and OAuth tokens are often associated with particular scopes. Scopes are permissions to do particular things. For example, an API key might be scoped to have GET access to a given endpoint but not POST access.\nRegardless of your authentication type, it will be provided in a header to your API call. Your API documentation will tell you how to provide your username and password, API key, or token to the API in a header. Both {requests} and {httr2} provide easy helpers for adding authentication headers and more general ways to set headers if needed.\nAside from authentication, headers are also used for metadata like the type of machine sending the request and cookies. You’ll rarely interact directly with these.\n\n\n3.3.6 Request Status Codes\nThe status code is the first thing you’ll consult when you get a result. Status codes indicate what happened with your request to the server. You always hope to see 200 codes, which indicate a successful response.\nThere are also two common error codes. 4xx codes indicate a problem with your request and the API couldn’t understand what you were asking. 5xx codes indicate that your request was fine, but some error happened in processing your request.\nSee Appendix D for a table of common HTTP codes.\n\n\n3.3.7 Response Bodies\nThe contents of the response are in the body. You’ll need to turn the body into a Python or R object you can work with.\nMost often, bodies are in JSON and you’ll decode them with {json} or {jsonlite}. Depending on the API, you may have the option to request something other than JSON as the return. I rarely need anything other than JSON.\n\n\n3.3.8 Common API patterns\nHere are a couple of common API patterns that are good to know about:\n\nPagination – many data-feed APIs implement pagination. A paginated API returns only a certain number of results to keep data sizes modest. Check out the API documentation to learn how to get all your pages.\nJob APIs – HTTP is designed for relatively quick request-response cycles. If your API kicks off a long-running job, it’s rare to wait until the job is done to get a response. Instead, a common pattern is to return a job-id, which you can use to poll a job-status endpoint to check how things are going and eventually claim your result.\nMultiple Verbs – a single endpoint often accepts multiple verbs, such as a GET and a POST at the same endpoint, for getting and setting the data that the endpoint stores."
  },
  {
    "objectID": "chapters/sec1/1-3-data-access.html#env-vars",
    "href": "chapters/sec1/1-3-data-access.html#env-vars",
    "title": "3  Using databases and data APIs",
    "section": "3.4 Environment variables to secure data connections",
    "text": "3.4 Environment variables to secure data connections\nWhen you take an app to production, authenticating to your data source while keeping your secrets secure is crucial.\nThe most important thing you can do to secure your credentials is to avoid ever putting credentials in your code. Your username and password or API key should never appear in your code.\nThe simplest way to provide credentials without the values appearing in your code is with an environment variable. Environment variables are set before your code starts – sometimes from completely outside Python or R.\n\n\n\n\n\n\nNote\n\n\n\nThis section assumes you can use a username and password or an API key to connect to your data source. That may not be true, depending on your organization. See Chapter 16 for handling data connections if you can’t directly connect with a username and password.\n\n\n\n3.4.1 Getting environment variables\nThe power of using an environment variable is that you reference them by name. Using only a name makes it easy to swap out the value in production versus other environments. Also, it means it’s safe to share code since all it does is reveal that an environment variable exists.\n\n\n\n\n\n\nNote\n\n\n\nIt is a convention to make environment variable names in all caps with words separated by underscores. The values are always simple character values, though these can be cast to some other type inside R or Python.\n\n\nIn Python, you can read environment variables from the os.environ dictionary or by using os.getenv(\"&lt;VAR_NAME&gt;\"). In R, you can get environment variables with Sys.getenv(\"&lt;VAR_NAME&gt;\").\nIt’s common to provide environment variables directly to functions as arguments, including as defaults, though you can also put the values in normal Python or R variables and use them from there.\n\n\n3.4.2 Setting environment variables\nThe most common way to set environment variables in a development environment is to load secrets from a text file. Environment variables are usually set in Python by reading a .env file into your session. The {python-dotenv} package is a good choice for doing this.\nR automatically reads the .Renviron file as environment variables and also sources the .Rprofile file, where you can set environment variables with Sys.setenv(). Personally, I prefer putting everything in the .Rprofile so I’m only using one file, but that’s not a universal opinion.\nSome organizations don’t ever want credential files in plain text. After all, if someone steals a plaintext secrets file, nothing can stop the thief from using them.\nThere are packages in both R and Python called {keyring} that allow you to use the system keyring to securely store environment variables and recall them at runtime.\nSetting environment variables in production is a little harder.\nMoving your secrets from your code into a different file you push to prod doesn’t solve the problem of putting secrets in code. And using {keyring} in a production environment is quite cumbersome.\nYour production environment may provide environment management tools. For example, GitHub Actions and Posit Connect both allow you to set secrets that aren’t visible to the users, but are accessible to the code at runtime in an environment variable.\nOrganizations increasingly use token-based authorization schemes that exchange one cryptographically secure token for another, never relying on credentials at all. The tradeoff for the enhanced security is that they can be difficult to implement, likely requiring coordination with an IT/Admin to use technologies like Kerberos or OAuth. There’s more on how to do that in Chapter 16."
  },
  {
    "objectID": "chapters/sec1/1-3-data-access.html#data-connection-packages",
    "href": "chapters/sec1/1-3-data-access.html#data-connection-packages",
    "title": "3  Using databases and data APIs",
    "section": "3.5 Data Connection Packages",
    "text": "3.5 Data Connection Packages\nIt’s widespread for organizations to write data connector packages in Python or R that include all of the shared connection details so users don’t have to remember them. If everyone has their own credentials, it’s also nice if those packages set standard names for the environment variables so they can be more easily set in production.\nWhether you’re using R or Python, the function in your package should return the database connection object for people to use.\nHere’s an example of what that might look like if you were using a Postgres database from R:\n\n#' Return a database connection\n#'\n#' @param user username, character, defaults to value of DB_USER\n#' @param pw password, character, defaults to value of DB_PW\n#' @param ... other arguments passed to \n#' @param driver driver, defaults to RPostgres::Postgres\n#'\n#' @return DBI connection\n#' @export\n#'\n#' @examples\n#' my_db_con()\nmy_db_con &lt;- function(\n    user = Sys.getenv(\"DB_USER\"), \n    pw = Sys.getenv(\"DB_PW\"), \n    ..., \n    driver = RPostgres::Postgres()\n) {\n  DBI::dbConnect(\n    driver = driver,\n    dbname = 'my-db-name', \n    host = 'my-db.example.com', \n    port = 5432, \n    user = user,\n    password = pw, \n    ...\n  )\n}\n\nNote that the function signature defines default environment variables that will be consulted. If those environment variables are set ahead of time by the user, this code will just work."
  },
  {
    "objectID": "chapters/sec1/1-3-data-access.html#comprehension-questions",
    "href": "chapters/sec1/1-3-data-access.html#comprehension-questions",
    "title": "3  Using databases and data APIs",
    "section": "3.6 Comprehension Questions",
    "text": "3.6 Comprehension Questions\n\nDraw two mental maps for connecting to a database, one using a database driver in a Python or R package vs an ODBC or JDBC driver. You should (at a minimum) include the nodes database package, DBI (R only), driver, system driver, ODBC, JDBC, and database.\nDraw a mental map for using an API from R or Python. You should (at a minimum) include nodes for {requests}/{httr2}, request, http verb/request method, headers, query parameters, body, json, response, and response code.\nHow can environment variables be used to keep secrets secure in your code?"
  },
  {
    "objectID": "chapters/sec1/1-3-data-access.html#lab-use-a-database-and-an-api",
    "href": "chapters/sec1/1-3-data-access.html#lab-use-a-database-and-an-api",
    "title": "3  Using databases and data APIs",
    "section": "3.7 Lab: Use a database and an API",
    "text": "3.7 Lab: Use a database and an API\nIn this lab, we will build the data and the presentation layers for our penguin mass model exploration. We’re going to create an app to explore the model, which will look like this: \nLet’s start by moving the data into an actual data layer.\n\n3.7.1 Step 1: Put the data in DuckDB\nLet’s start by moving the data into a DuckDB database and use it from there for the modeling and EDA scripts.\nTo start, let’s load the data.\nHere’s what that looks like in R:\ncon &lt;- DBI::dbConnect(duckdb::duckdb(), dbdir = \"my-db.duckdb\")\nDBI::dbWriteTable(con, \"penguins\", palmerpenguins::penguins)\nDBI::dbDisconnect(con)\nOr equivalently, in Python:\nimport duckdb\nfrom palmerpenguins import penguins\n\ncon = duckdb.connect('my-db.duckdb')\ndf = penguins.load_penguins()\ncon.execute('CREATE TABLE penguins AS SELECT * FROM df')\ncon.close()\nNow that the data is loaded, let’s adjust our scripts to use the database.\nIn R, we will replace our data loading with connecting to the database. Leaving out all the parts that don’t change, it looks like\n\n\neda.qmd\n\n\ncon &lt;- DBI::dbConnect(\n  duckdb::duckdb(), \n  dbdir = \"my-db.duckdb\"\n  )\ndf &lt;- dplyr::tbl(con, \"penguins\")\n\nWe also need to call to DBI::dbDisconnect(con) at the end of the script.\nWe don’t have to change anything because we wrote our data processing code in {dplyr}. Under the hood, {dplyr} can switch seamlessly to a database backend, which is really cool.\n\n\neda.qmd\n\ndf %&gt;%\n  group_by(species, sex) %&gt;%\n  summarise(\n    across(\n        ends_with(\"mm\") | ends_with(\"g\"),\n      \\(x) mean(x, na.rm = TRUE)\n      )\n    ) %&gt;%\n  dplyr::collect() %&gt;%\n  knitr::kable()\n\nIt’s unnecessary, but I’ve added a call to dplyr::collect() in line 31. It will be implied if I don’t put it there manually, but it helps make it obvious that all the work before there has been pushed off to the database. It doesn’t matter for this small dataset, but it could benefit a larger dataset.\nIn Python, we’re just going to load the entire dataset into memory for modeling, so the line loading the dataset changes to\n\n\nmodel.qmd\n\ncon = duckdb.connect('my-db.duckdb')\ndf = con.execute(\"SELECT * FROM penguins\").fetchdf().dropna()\ncon.close()\n\n\nNow let’s switch to figuring out the connection we’ll need to our processing layer in the presentation layer.\n\n\n3.7.2 Step 2: Call the model API from code\nBefore you start, ensure the API is running on your machine from the last lab.\n\n\n\n\n\n\nNote\n\n\n\nI’m assuming it’s running on port 8080 in this lab. If you’ve put it somewhere else, change the 8080 in the code below to match the port on your machine.\n\n\nIf you want to call the model in code, you can use any http request library. In R you should use httr2 and in Python you should use requests.\nHere’s what it looks like to call the API in Python\n\nimport requests\n\nreq_data = {\n  \"bill_length_mm\": 0,\n  \"species_Chinstrap\": False,\n  \"species_Gentoo\": False,\n  \"sex_male\": False\n}\nreq = requests.post('http://127.0.0.1:8080/predict', json = req_data)\nres = req.json().get('predict')[0]\n\nor equivalently in R\n\nreq &lt;- httr2::request(\"http://127.0.0.1:8080/predict\") |&gt;\n  httr2::req_body_json(\n    list(\n      \"bill_length_mm\" = 0,\n      \"species_Chinstrap\" = FALSE,\n      \"species_Gentoo\" = FALSE,\n      \"sex_male\" = FALSE\n    )\n  ) |&gt;\n  httr2::req_perform()\nres &lt;- httr2::resp_body_json(r)$predict[[1]]\n\nNote that there’s no translation necessary to send the request. The {requests} and{httr2} packages automatically know what to do with the Python dictionary and the R list.\nGetting the result back takes more work to find the right spot in the JSON returned. This is quite common.\n\n\n\n\n\n\nNote\n\n\n\nThe {vetiver} package also includes the ability to auto-query a {vetiver} API. I’m not using it here to expose the details of calling an API.\n\n\nLet’s take this API-calling code and build the presentation layer around it.\n\n\n3.7.3 Step 3: Build a shiny app\nWe will use the {shiny} R and Python package for creating interactive web apps using just Python code. If you don’t know much about {shiny}, you can unthinkingly follow the examples here or spend time with the Mastering Shiny book to learn to use it yourself.\nEither way, an app that looks like the picture above would look like this in Python\n\n\napp.py\n\nfrom shiny import App, render, ui, reactive\nimport requests\n\napi_url = 'http://127.0.0.1:8080/predict'\n\napp_ui = ui.page_fluid(\n    ui.panel_title(\"Penguin Mass Predictor\"), \n    ui.layout_sidebar(\n        ui.panel_sidebar(\n            [ui.input_slider(\"bill_length\", \"Bill Length (mm)\", 30, 60, 45, step = 0.1),\n            ui.input_select(\"sex\", \"Sex\", [\"Male\", \"Female\"]),\n            ui.input_select(\"species\", \"Species\", [\"Adelie\", \"Chinstrap\", \"Gentoo\"]),\n            ui.input_action_button(\"predict\", \"Predict\")]\n        ),\n        ui.panel_main(\n            ui.h2(\"Penguin Parameters\"),\n            ui.output_text_verbatim(\"vals_out\"),\n            ui.h2(\"Predicted Penguin Mass (g)\"), \n            ui.output_text(\"pred_out\")\n        )\n    )   \n)\n\ndef server(input, output, session):\n    @reactive.Calc\n    def vals():\n        d = {\n            \"bill_length_mm\" : input.bill_length(),\n            \"sex_Male\" : input.sex() == \"Male\",\n            \"species_Gentoo\" : input.species() == \"Gentoo\", \n            \"species_Chinstrap\" : input.species() == \"Chinstrap\"\n\n        }\n        return d\n    \n    @reactive.Calc\n    @reactive.event(input.predict)\n    def pred():\n        r = requests.post(api_url, json = vals())\n        return r.json().get('predict')[0]\n\n    @output\n    @render.text\n    def vals_out():\n        return f\"{vals()}\"\n\n    @output\n    @render.text\n    def pred_out():\n        return f\"{round(pred())}\"\n\napp = App(app_ui, server)\n\nAnd like this in R\n\n\napp.R\n\nlibrary(shiny)\n\napi_url &lt;- \"http://127.0.0.1:8080/predict\"\n\nui &lt;- fluidPage(\n  titlePanel(\"Penguin Mass Predictor\"),\n\n  # Model input values\n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\n        \"bill_length\",\n        \"Bill Length (mm)\",\n        min = 30,\n        max = 60,\n        value = 45,\n        step = 0.1\n      ),\n      selectInput(\n        \"sex\",\n        \"Sex\",\n        c(\"Male\", \"Female\")\n      ),\n      selectInput(\n        \"species\",\n        \"Species\",\n        c(\"Adelie\", \"Chinstrap\", \"Gentoo\")\n      ),\n      # Get model predictions\n      actionButton(\n        \"predict\",\n        \"Predict\"\n      )\n    ),\n\n    mainPanel(\n      h2(\"Penguin Parameters\"),\n      verbatimTextOutput(\"vals\"),\n      h2(\"Predicted Penguin Mass (g)\"),\n      textOutput(\"pred\")\n    )\n  )\n)\n\nserver &lt;- function(input, output) {\n  # Input params\n  vals &lt;- reactive(\n    list(\n      bill_length_mm = input$bill_length,\n      species_Chinstrap = input$species == \"Chinstrap\",\n      species_Gentoo = input$species == \"Gentoo\",\n      sex_male = input$sex == \"Male\"\n    )\n  )\n\n  # Fetch prediction from API\n  pred &lt;- eventReactive(\n    input$predict,\n    httr2::request(api_url) |&gt;\n      httr2::req_body_json(vals()) |&gt;\n      httr2::req_perform() |&gt;\n      httr2::resp_body_json(),\n    ignoreInit = TRUE\n  )\n\n  # Render to UI\n  output$pred &lt;- renderText(pred()$predict[[1]])\n  output$vals &lt;- renderPrint(vals())\n}\n\n# Run the application\nshinyApp(ui = ui, server = server)\n\nOver the next few chapters, we will implement more architectural best practices for the app and eventually go to deployment."
  },
  {
    "objectID": "chapters/sec1/1-3-data-access.html#footnotes",
    "href": "chapters/sec1/1-3-data-access.html#footnotes",
    "title": "3  Using databases and data APIs",
    "section": "",
    "text": "I have heard that some write operations may be faster with a JDBC driver than an ODBC one. I would argue that if you’re doing enough writing to a database that speed matters, you probably should be using database-specific data loading tools, and not just writing from R or Python.↩︎\nYou may see POST for things that look like GETs. For example, fetching a model prediction from an API feels like a GET to me. The reason is the HTTP spec only recently allowed GET endpoints to use request bodies and it’s still discouraged. So if the API wants to use a body in the request, it’s likely to be a POST even if it’s more of a GET activity.↩︎"
  },
  {
    "objectID": "chapters/sec1/1-4-monitor-log.html#observing-correctness",
    "href": "chapters/sec1/1-4-monitor-log.html#observing-correctness",
    "title": "4  Logging and Monitoring",
    "section": "4.1 Observing Correctness",
    "text": "4.1 Observing Correctness\nObservability of general-purpose software is primarily concerned with the operational qualities of the software. A software engineer wants to know whether their software uses too much RAM or CPU, is fast enough, or has crashed.\nFor a general-purpose software engineer, an uncaught exception that makes the software crash is about as bad as it gets.\nFor a data scientist, something even scarier is an issue that doesn’t result in code failure but yields incorrect answers. Data joins usually complete even if the merge quality is terrible. Model APIs will return a prediction even if the prediction is very, very bad.\nChecking the correctness of the numbers and figures you produce is hard because data scientists are (basically by definition) doing something novel. The solution is to use process metrics to reveal a problem before it surfaces in your results.\nOne crucial tool is correctly architecting your project. Jobs are generally much simpler to check for correctness than presentation layers. By moving as much processing as possible out of the presentation layer and into the data and processing layers, you can make it easier to observe.\nMoreover, you’re already probably familiar with tools for literate programming like Jupyter Notebooks, R Markdown Documents, and Quarto Documents.\nOne of my spicier opinions is that all jobs should be in a literate programming format. When used well, these tools intersperse code, commentary, and output – having the output of a run weaved in with context makes it much easier to spot issues.\nThere are a few particular things I always make sure to include in job output.\nThe first is the quality of data joins. Based on the number of rows (or unique IDs), I know how many rows should be in the data set after a join. Figuring out how many rows to expect can take a minute, but checking that the size of the joined data matches my expectations has avoided many gnarly issues.\nThe second is checking cross-tabulations before and after recoding a categorical variable. I’ve caught many mistakes in my recode logic by checking that the post-recode values match what I think they should. Input values also can change over time. Checking recode values is a good way to spot novel values to ensure they’re recoded correctly.\nThe last is goodness-of-fit metrics of an ML model in production. There are many frameworks and products for monitoring model quality and model drift once your model is in production. I don’t have strong opinions on these other than that you need to use one if you’ve got a model producing results you hope to rely on."
  },
  {
    "objectID": "chapters/sec1/1-4-monitor-log.html#observing-operations",
    "href": "chapters/sec1/1-4-monitor-log.html#observing-operations",
    "title": "4  Logging and Monitoring",
    "section": "4.2 Observing Operations",
    "text": "4.2 Observing Operations\nAs a data scientist, you can’t only pay attention to correctness. You still need to pay attention to the operational qualities of your code like the speed and responsiveness, system resources it’s consuming, the number of users, and user interactions just before an error occurs.\nThe first step to making your app or API observable is to add logging. You may be accustomed to just adding print statements throughout your code. Honestly, this is far better than nothing. But purpose-built tooling for logging allows you to apply consistent formats within logs, emit logs in useful formats, and provide visibility into the severity of issues.\nThere are great logging packages in both Python and R. Python’s logging package is standard and included. There is no standard logging package in R, but I recommend {log4r}.\nThese packages – and basically every other logging package – work very similarly. At the outset of your code, you’ll create and parameterize a log session that persists as long as the Python or R session. You’ll use the log session to write log statements about what your code does. When the log statement runs, it creates a log entry.\nFor example, here’s what logging for an app starting up might look like in Python\n\n\n\napp.py\n\nimport logging\n\n# Configure the log object\nlogging.basicConfig(\n    format='%(asctime)s - %(message)s',\n    level=logging.INFO\n)\n\n# Log app start\nlogging.info(\"App Started\")\n\n\nAnd here’s what that looks like using {log4r}\n\n\n\napp.R\n\n# Configure the log object\nlog &lt;- log4r::logger()\n\n# Log app start\nlog4r::info(log, \"App Started\")\n\n\nWhen the R or Python interpreter hits either of these lines, it will create a log entry that looks something like this:\n2022-11-18 21:57:50 INFO App Started\nLike all log entries, this entry has three components:\n\nThe log metadata is data the logging library automatically includes on every entry. It is configured when you initialize logging. In the example above, the only metadata is the timestamp. Log metadata can include additional information, such as which server you’re running on or the user.\nThe second component is the log level. The log level indicates the severity of the event you’re logging. In the example above the log level was INFO.\nThe last component is the log data, which details the event you want to log – App Started in this case.\n\n\n4.2.1 Understanding log levels\nThe log level indicates how serious the logged event is. Most logging libraries have 5-7 log levels.\nBoth the Python {logging} library and {log4r} have five levels from least to most scary:\n\nDebug: detail on what the code was doing. Debug statements are designed to be useful to make sense to someone who knows the code. For example, you might include which function ran and with what arguments in a debug log.\nInfo: something normal happened in the app. Info statements record actions like starting and stopping, successfully making database and other connections, and runtime configuration options.\nWarn/Warning: an unexpected application issue that isn’t fatal. For example, you might include having to retry doing something or noticing that resource usage is high. If something were to go wrong later, these might be helpful breadcrumbs to look at.\nError: an issue that will make an operation not work, but that won’t crash your app. An example might be a user submitting invalid input and the app recovering.\nCritical: an error so big that the app itself shuts down. This is the SOS your app sends as it shuts down. For example, if your app cannot run without a connection to an outside service, you might log an inability to connect as a Critical error.\n\nWhen you initialize your logging session, you’ll set your session’s log level, which is the least critical level you want to see in the logs for the session. In development, you probably want to log everything down to the debug level, though that probably isn’t ideal in prod.\n\n\n4.2.2 Configuring log formats\nWhen you initialize your logging session, you’ll choose where logs will be written and in what format. You’ll configure the format with a formatter or layout.\nThe default for most logging is to emit logs in plain text. For example, a plain text log of an app starting might put this on your console:\n2022-11-18 21:57:50 INFO App Started\nPlain text logging is an excellent choice if the only use of the logs is for humans to read them. You might prefer structured logs if you’re shipping your logs to an aggregation service.\nThe most common structured logging format is JSON, though YAML and XML are often options. If you used JSON logging, the same record might be emitted as\n{\n  \"time\": \"2022-11-18 21:57:50\",\n  \"level\": \"INFO\", \n  \"data\": \"App Started\"\n}"
  },
  {
    "objectID": "chapters/sec1/1-4-monitor-log.html#where-logs-go",
    "href": "chapters/sec1/1-4-monitor-log.html#where-logs-go",
    "title": "4  Logging and Monitoring",
    "section": "4.3 Where logs go",
    "text": "4.3 Where logs go\nBy default, logs go to the console. This is an excellent development choice because it makes it easy to see the logs as you go. If you want to choose a different place for them, you can configure it with a handler or an appender.\nIn production, the most common place to send logs is to a file on disk. Over time, logs can become quite voluminous, so it’s common to delete logs after a retention period via log rotation.\nA typical log rotation pattern is to have each log file last for 24 hours and then be retained for 30 days before it is deleted. The Python {logging} library does log rotation itself. {log4r} does not, but there is a Linux library called logrotate that you can use in concert with {log4r}.1\nIf you run in a relatively sophisticated organization, they probably want to move logs off the server and into a centralized monitoring location. In many cases, they’ll just run a log collection agent on your server, which is configured to send logs to the central monitoring platform.\nIf you’re running in a Docker Container, you need to direct the logs outside the Container. This is usually accomplished by sending normal operating logs to stdout (usually pronounced standard out) and failures to stderr (standard error).\n\n\n\n\n\n\nNote\n\n\n\nAs you’ll learn more about in Chapter 6, anything that lives inside a Docker Container is ephemeral. This is bad if you’re writing a log that might contain clues for why a Docker Container was unexpectedly killed.\n\n\nYou may also want to do something else completely custom with your logs. This is most common for critical or error logs. For example, you may want to send an email, slack, or text message immediately if your system emits a high-level log message.\n\n4.3.1 Working with Metrics\nThe most common place to see metrics in a data science context is when deploying and monitoring ML models in production. While it’s relatively nascent in 2023, I think it’s likely that more organizations will start monitoring ETL data quality over time.\nIf you will configure metrics emission or consumption, most modern metrics stacks are built around the open-source tools Prometheus and Grafana.\nPrometheus is an open-source monitoring tool that makes it easy to store metrics data, query it, and alert based on it. Grafana is an open source dashboarding tool that sits on top of Prometheus to do visualization of metrics. They are usually used together to do monitoring and visualization of metrics.\nYou can run Prometheus and Grafana, but Grafana Labs provides a generous free tier for their SaaS service. This is great because you can set up their service and point your app to it. Many organizations also use DataDog, a SaaS service for log aggregation and monitoring.\nBecause the Prometheus/Grafana stack started in the DevOps world, they are most optimized to monitor a whole server or fleet of servers; however, it’s not hard to use them to monitor things you might care about, like data quality or API response times. There is an official Prometheus client in Python and the {openmetrics} package in R makes registering metrics from a Plumber API or Shiny app easy.\nThere’s a great Get Started with Grafana and Prometheus doc on the Grafana Labs website if you want to try it out."
  },
  {
    "objectID": "chapters/sec1/1-4-monitor-log.html#comprehension-questions",
    "href": "chapters/sec1/1-4-monitor-log.html#comprehension-questions",
    "title": "4  Logging and Monitoring",
    "section": "4.4 Comprehension Questions",
    "text": "4.4 Comprehension Questions\n\nWhat is the difference between monitoring and logging? What are the two halves of the monitoring and logging process?\nLogging is generally good, but what are some things you should be careful not to log?\nAt what level would you log each of the following events:\n\nSomeone clicks on a particular tab in your Shiny app.\nSomeone puts an invalid entry into a text entry box.\nAn HTTP call your app makes to an external API fails.\nThe numeric values that are going into your computational function."
  },
  {
    "objectID": "chapters/sec1/1-4-monitor-log.html#lab-an-app-with-logging",
    "href": "chapters/sec1/1-4-monitor-log.html#lab-an-app-with-logging",
    "title": "4  Logging and Monitoring",
    "section": "4.5 Lab: An App with Logging",
    "text": "4.5 Lab: An App with Logging\nLet’s return to the last lab’s prediction generator app and add a little logging. This is easy in both R and Python. We declare that we’re using the logger and then put logging statements into our code.\nI decided to log when the app starts, just before and after each request, and an error logger if an HTTP error code comes back from the API.\nWith the logging now added, here’s what the app looks like in R:\n\n\napp.R\n\nlibrary(shiny)\n\napi_url &lt;- \"http://127.0.0.1:8080/predict\"\nlog &lt;- log4r::logger()\n\nui &lt;- fluidPage(\n  titlePanel(\"Penguin Mass Predictor\"),\n\n  # Model input values\n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\n        \"bill_length\",\n        \"Bill Length (mm)\",\n        min = 30,\n        max = 60,\n        value = 45,\n        step = 0.1\n      ),\n      selectInput(\n        \"sex\",\n        \"Sex\",\n        c(\"Male\", \"Female\")\n      ),\n      selectInput(\n        \"species\",\n        \"Species\",\n        c(\"Adelie\", \"Chinstrap\", \"Gentoo\")\n      ),\n      # Get model predictions\n      actionButton(\n        \"predict\",\n        \"Predict\"\n      )\n    ),\n\n    mainPanel(\n      h2(\"Penguin Parameters\"),\n      verbatimTextOutput(\"vals\"),\n      h2(\"Predicted Penguin Mass (g)\"),\n      textOutput(\"pred\")\n    )\n  )\n)\n\nserver &lt;- function(input, output) {\n  log4r::info(log, \"App Started\")\n  # Input params\n  vals &lt;- reactive(\n    list(\n      bill_length_mm = input$bill_length,\n      species_Chinstrap = input$species == \"Chinstrap\",\n      species_Gentoo = input$species == \"Gentoo\",\n      sex_male = input$sex == \"Male\"\n    )\n  )\n\n  # Fetch prediction from API\n  pred &lt;- eventReactive(\n    input$predict,\n    {\n      log4r::info(log, \"Prediction Requested\")\n      r &lt;- httr2::request(api_url) |&gt;\n        httr2::req_body_json(vals()) |&gt;\n        httr2::req_perform()\n      log4r::info(log, \"Prediction Returned\")\n\n      if (httr2::resp_is_error(r)) {\n        log4r::error(log, paste(\"HTTP Error\"))\n      }\n\n      httr2::resp_body_json(r)\n    },\n    ignoreInit = TRUE\n  )\n\n  # Render to UI\n  output$pred &lt;- renderText(pred()$predict[[1]])\n  output$vals &lt;- renderPrint(vals())\n}\n\n# Run the application\nshinyApp(ui = ui, server = server)\n\nAnd in Python:\n\n\napp.py\n\nfrom shiny import App, render, ui, reactive\nimport requests\nimport logging\n\napi_url = 'http://127.0.0.1:8080/predict'\nlogging.basicConfig(\n    format='%(asctime)s - %(message)s',\n    level=logging.INFO\n)\n\napp_ui = ui.page_fluid(\n    ui.panel_title(\"Penguin Mass Predictor\"), \n    ui.layout_sidebar(\n        ui.panel_sidebar(\n            [ui.input_slider(\"bill_length\", \"Bill Length (mm)\", 30, 60, 45, step = 0.1),\n            ui.input_select(\"sex\", \"Sex\", [\"Male\", \"Female\"]),\n            ui.input_select(\"species\", \"Species\", [\"Adelie\", \"Chinstrap\", \"Gentoo\"]),\n            ui.input_action_button(\"predict\", \"Predict\")]\n        ),\n        ui.panel_main(\n            ui.h2(\"Penguin Parameters\"),\n            ui.output_text_verbatim(\"vals_out\"),\n            ui.h2(\"Predicted Penguin Mass (g)\"), \n            ui.output_text(\"pred_out\")\n        )\n    )   \n)\n\ndef server(input, output, session):\n    logging.info(\"App start\")\n\n    @reactive.Calc\n    def vals():\n        d = {\n            \"bill_length_mm\" : input.bill_length(),\n            \"sex_Male\" : input.sex() == \"Male\",\n            \"species_Gentoo\" : input.species() == \"Gentoo\", \n            \"species_Chinstrap\" : input.species() == \"Chinstrap\"\n\n        }\n        return d\n    \n    @reactive.Calc\n    @reactive.event(input.predict)\n    def pred():\n        logging.info(\"Request Made\")\n        r = requests.post(api_url, json = vals())\n        logging.info(\"Request Returned\")\n\n        if r.status_code != 200:\n            logging.error(\"HTTP error returned\")\n\n        return r.json().get('predict')[0]\n\n    @output\n    @render.text\n    def vals_out():\n        return f\"{vals()}\"\n\n    @output\n    @render.text\n    def pred_out():\n        return f\"{round(pred())}\"\n\napp = App(app_ui, server)\n\nNow, if you load up this app locally, you can see the logs of what’s happening stream in as you press buttons.\nYou can feel free to log whatever you think is helpful. For example, getting the actual error contents would probably be helpful if an HTTP error comes back."
  },
  {
    "objectID": "chapters/sec1/1-4-monitor-log.html#footnotes",
    "href": "chapters/sec1/1-4-monitor-log.html#footnotes",
    "title": "4  Logging and Monitoring",
    "section": "",
    "text": "There are two common naming patterns with rotating log files.\nThe first is to have dated log filenames that look like my-log-20221118.log.\nThe other pattern is to keep one file that’s current and have the older ones numbered. So today’s log would be my-log.log, yesterday’s would be my-log.log.1 , the day before my-log.log.2, etc. This second pattern works particularly well if you’re using logrotate with log4r, because then log4r doesn’t need to know anything about the log rotation. It’s just always writing to my-log.log.↩︎"
  },
  {
    "objectID": "chapters/sec1/1-5-deployments.html#separate-the-prod-environment",
    "href": "chapters/sec1/1-5-deployments.html#separate-the-prod-environment",
    "title": "5  Deployments and code promotion",
    "section": "5.1 Separate the prod environment",
    "text": "5.1 Separate the prod environment\nCI/CD is all about quickly promoting code into production. It’s all too easy to mess up production if you don’t have a clear boundary between what is in production and what isn’t. That’s why software environments are often divided into dev, test, and prod.\nDev is the development environment where new work is produced, test is where the code is tested for performance, usability; and feature completeness; and prod is the production environment. Sometimes dev and test are collectively called the lower environments and prod the higher environment.\n\nWhile the dev/test/prod triad is the most traditional, some organizations have more than two lower environments and some have only dev and prod. That’s all fine. The number and configuration of lower environments should vary according to your organization’s needs. But, just as Tolstoy said about happy families, all prod environments are alike.\nSome criteria that all good prod environments meet:\n\nThe environment is created using code. For data science, that means managing R and Python packages using environments as code tooling, as discussed in Chapter 1.\nChanges happen via a promotion process. The process combines human approvals validating code is ready for production with automations to run tests and deploy.\nChanges only happen via the promotion process. This means no manual changes to the environment or the active code in production.\n\nRules 1 and 2 tend to be easy to follow. It might even be fun to figure out how to create the environment with code and design a promotion process. But the first time something breaks in your prod environment, you will be sorely tempted to violate rule 3. Please don’t do it.\nKeeping a pristine prod environment is necessary if you want to run a data science project that becomes critical to your organization. When an issue arises, you must reproduce it in a lower environment before pushing changes through your promotion process. Keeping your environments in sync is crucial to reproduce prod issues in lower environments.\nThese guidelines for a prod environment look almost identical to guidelines for general-purpose software engineering. The divergent needs of data scientists and general-purpose software engineers show up in the composition of lower environments.\n\n5.1.1 Dev and test environments\nAs a data scientist, dev means working in a lab environment like RStudio, Spyder, VSCode, or PyCharm and experimenting with the data. You’re slicing the data this way or that to see if anything meaningful emerges, creating plots to see if they are the right way to show off a finding, and checking whether certain features improve model performance. All this means it is impossible to work without real data.\n“Duh”, you say, “Of course you can’t do data science without real data.”\nThis may be obvious to you, but needing to do data science on real data in dev is a common source of friction with IT/Admins.\nThat’s because this need is unique to data scientists. For general-purpose software engineering, a lower environment needs data formatted like the real data, but the content doesn’t matter.\nFor example, if you’re building an online store, you need dev and test environments where the API calls from the sales system are in the same format as the real data, but you don’t care if it’s real sales data. In fact, you probably want to create some odd-looking cases for testing purposes.\nOne way to help alleviate concerns about using real data is to create a data science sandbox. A great data science sandbox provides:\n\nRead-only access to real data for experimentation.\nPlaces to write mock data to test things you’ll write for real in prod.\nExpanded access to R and Python packages for experiments before promoting to prod.\n\nWorking with your IT/Admin team to get these things isn’t always easy. They might not want to give you real data in dev. One point to emphasize is that creating this environment makes things more secure. It gives you a place to do development without fear that you might damage production data or services."
  },
  {
    "objectID": "chapters/sec1/1-5-deployments.html#version-control-implements-code-promotion",
    "href": "chapters/sec1/1-5-deployments.html#version-control-implements-code-promotion",
    "title": "5  Deployments and code promotion",
    "section": "5.2 Version control implements code promotion",
    "text": "5.2 Version control implements code promotion\nOnce you’ve invented your code promotion process, you need a way to operationalize it. If your process says that your code needs testing and review before it’s pushed to prod, you need a place to do that. Version control is the tool to make your code promotion process real.\nVersion control is software that allows you to keep the prod version of your code safe, gives contributors a copy to work on, and hosts tools to manage merging changes back together. These days, Git is the industry standard for version control.\nGit is an open-source system for tracking changes to computer files in a project-level collection called a repository. You can host repositories on your own Git server, but most organizations host their repositories with free or paid plans from tools like GitHub, GitLab, Bitbucket, or Azure DevOps.\nThis is not a book on git. If you’re not comfortable using local and remote repositories, branching, and merging, then the rest of this chapter will not be useful right now. I recommend you take a break from this book and learn about Git.\n\n\n\n\n\n\nHints on learning Git\n\n\n\nPeople who say learning Git is easy are either lying or have forgotten. I am sorry our industry has standardized on a tool with such terrible ergonomics It’s worth your time to learn.\nWhether you’re an R or Python user, I’d recommend starting with a resource designed to teach Git to a data science user. My recommendation is to check out HappyGitWithR by Jenny Bryan.\nIf you’re a Python user, some specific tooling suggestions won’t apply, but the general principles will be the same.\nIf you know Git and need a reminder of commands, see Appendix D for a cheatsheet of common ones.\n\n\nThe precise contours of your code promotion process and, therefore your Git policies – are up to you and your organization’s needs. Do you need multiple rounds of review? Can anyone promote something to prod or just certain people? Is automated testing required?\nYou should make these decisions as part of your code promotion process, which you can enshrine in the project’s Git repository configuration.\nOne important decision you’ll make is how to configure the branches of your Git repository. Here’s how I’d suggest you do it for production data science projects:\n\nMaintain two long-running branches – main is the prod version of your project and test is a long-running pre-prod version.\nCode can only be promoted to main via a merge from test. Direct pushes to main are not allowed.\nNew functionality is developed in short-lived feature branches that are merged into test when you think they’re ready to go. Once sufficient approvals are granted, the feature branch changes in test are merged into main.\n\nThis framework helps maintain a reliable prod version on the main branch while leaving sufficient flexibility to accomplish any set of approvals and testing you might want.\nHere’s an example of how this might work. Let’s say you were working on a dashboard and were trying to add a new plot.\nYou would create a new feature branch with your work, perhaps called new_plot. When you were happy with it, you would merge the feature branch to test. Depending on your organization’s process, you might be able to merge to test yourself or you might require approval.\nIf your testing turned up a bug, you’d fix the bug in the feature branch, merge the bug fix into test, re-test, and merge to main once you were satisfied.\nHere’s what the Git graph for that sequence of events might look like:\n\nOne of the tenets of a good CI/CD practice is that changes are merged frequently and incrementally into production.\nA good rule of thumb is that you want your merges to be the smallest meaningful change that can be incorporated into main in a standalone way.\nThere are no hard and fast rules here. Knowing the appropriate scope for a single merge is an art – one that can take years to develop. Your best resource here is more senior team members who’ve already figured it out."
  },
  {
    "objectID": "chapters/sec1/1-5-deployments.html#cicd-automates-git-operations",
    "href": "chapters/sec1/1-5-deployments.html#cicd-automates-git-operations",
    "title": "5  Deployments and code promotion",
    "section": "5.3 CI/CD automates Git operations",
    "text": "5.3 CI/CD automates Git operations\nThe role of Git is to make your code promotion process happen. Git allows you to configure requirements for whatever approvals and testing you need. Your CI/CD tool sits on top of that so that all this merging and branching does something.1\nTo be more precise, a CI/CD pipeline for a project watches the Git repository and does something based on a trigger. Common triggers include a push or merge to a particular branch or a pull request opening.\nThe most common CI/CD operations are pre-merge checks like spell checking, code linting, automated testing, and post-merge deployments.\nThere are a variety of different CI/CD tools available. Because of the tight linkage between CI/CD operations and Git repos, CI/CD pipelines built into Git providers are very popular.\nGitHub Actions (GHA) was released a few years ago and is eating the world of CI/CD. Depending on your organization and the age of your CI/CD pipeline, you might also see Jenkins, Travis, Azure DevOps, or GitLab.\n\n5.3.1 Configuring per-environment behavior\nAs you promote an app from dev to test and prod, you probably want behavior to look different across the environments. For example, you might want to switch data sources from a dev database to a prod one, switch a read-only app into write mode, or use a different logging level.\nThe easiest way to create per-environment behavior is to:\n\nWrite code that includes flags that change behavior (e.g. write to the database or no).\nCapture the intended behavior for each environment in a YAML config file.\nRead in the config file as your code starts.\nChoose the values for the current environment based on an environment variable.\n\n\n\n\n\n\n\nNote\n\n\n\nOnly non-secret configuration settings should go in a config file. Secrets should always be configured using secrets management tools so they don’t appear in plain text.\nMost CI/CD tooling supports injecting secrets into the pipeline as environment variables.\n\n\nFor example, let’s say you have a project that should use a special read-only database in dev and switch to writing in the prod database in prod. You might write the config file below to describe this behavior:\n{yaml filename=\"config.yml\"} dev:   write: false   db-path: dev-db test   write: true prod:   write: true   db-path: prod-db\nThen, you’d set that an environment variable to have the value dev in dev, test in test, and prod in prod. Your code would grab the correct set of values based on the environment variable.\nIn Python there are many different ways to set and read a per-environment configuration. The easiest way to use YAML is to read it in with the {yaml} package and treat it as a dictionary.\nIn R, the {config} package is the standard way to load a configuration from a YAML file. The config::get() function uses the value of the R_CONFIG_ACTIVE environment variable to choose which configuration to use."
  },
  {
    "objectID": "chapters/sec1/1-5-deployments.html#comprehension-questions",
    "href": "chapters/sec1/1-5-deployments.html#comprehension-questions",
    "title": "5  Deployments and code promotion",
    "section": "5.4 Comprehension Questions",
    "text": "5.4 Comprehension Questions\n\nWrite down a mental map of the relationship between the three environments for data science. Include the following terms: Git, Promote, CI/CD, automation, deployment, dev, test, prod\nWhy is Git so important to a good code promotion strategy? Can you have a code promotion strategy without Git?\nWhat is the relationship between Git and CI/CD? What’s the benefit of using Git and CI/CD together?\nWrite out a mental map of the relationship of the following terms: Git, GitHub, CI/CD, GitHub Actions, Version Control"
  },
  {
    "objectID": "chapters/sec1/1-5-deployments.html#lab5",
    "href": "chapters/sec1/1-5-deployments.html#lab5",
    "title": "5  Deployments and code promotion",
    "section": "5.5 Lab: Host a website with automatic updates",
    "text": "5.5 Lab: Host a website with automatic updates\nIn labs 1 through 4, you’ve created a Quarto website for the penguin model. You’ve got sections on EDA and model building. But it’s still just on your computer.\nIn this lab, we will deploy that website to a public site on GitHub and set up GitHub Actions as CI/CD so the EDA and modeling steps re-render every time we make changes.\nBefore we get into the meat of the lab, there are a few things you need to do on your own. If you don’t know how, there are plenty of great tutorials online.\n\nCreate an empty public repo on GitHub.\nConfigure the repo as the remote for your Quarto project directory.\n\nOnce you’ve connected the GitHub repo to your project, you will set up the Quarto project to publish via GitHub Actions. There are great directions on configuring that on the Quarto website.\nFollowing those instructions will accomplish three things for you:\n\nGenerate a _publish.yml, which is a Quarto-specific file for configuring publishing locations.\nConfigure GitHub Pages to serve your website off a long-running standalone branch called gh-pages.\nGenerate a GitHub Actions workflow file, which will live at .github/workflows/publish.yml.\n\nHere’s the basic GitHub Actions file (or close to it) that the process will auto-generate for you.\n\n\n.github/workflows/publish.yml\n\non:\n  workflow_dispatch:\n  push:\n    branches: main\n\nname: Quarto Publish\n\njobs:\n  build-deploy:\n    runs-on: ubuntu-latest\n    permissions:\n      contents: write\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v2\n\n      - name: Set up Quarto\n        uses: quarto-dev/quarto-actions/setup@v2\n\n      - name: Render and Publish\n        uses: quarto-dev/quarto-actions/publish@v2\n        with:\n          target: gh-pages\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n\nLike all GitHub Actions, this action is defined in a .yml file in the .github/workflows directory of a project. It contains several sections:\n\nThe on section defines when the workflow occurs. In this case, we’ve configured the workflow only to trigger on a push to the main branch.2 Another common case would be to trigger on a pull request to main or another branch.\nThe jobs section defines what happens in steps, that occur in sequential order.\n\nThe runs-on field specifies which runner to use. A runner is a virtual machine that is started from scratch every time the action runs. GitHub Actions offers runners with Ubuntu, Windows, and MacOS. You can also add custom runners.\nMost steps are defined with uses, which calls a preexisting GitHub Actions step that someone else has written.\nYou can inject variables using with and environment variables using env.\n\n\nIf you try to run this, it probably won’t work.\nThe CI/CD process occurs in a completely isolated environment. This auto-generated action doesn’t include setting up versions of R and Python or the packages to run our EDA and modeling scripts. We need to get that configured before this action will work.\n\n\n\n\n\n\nNote\n\n\n\nIf you read the Quarto documentation, they recommend freezing your computations. Freezing is useful if you want to render your R or Python code only once and update only the text of your document. You wouldn’t need to set up R or Python in CI/CD, and the document would render faster.\nThat said, freezing isn’t an option if you intend the CI/CD environment to re-run the R or Python code.\nBecause the main point here is to learn about getting environments as code working in CI/CD you should not freeze your environment.\n\n\nFirst, add the commands to install R, {renv}, and the packages for your content to the GitHub Actions workflow.\n\n\n.github/workflows/publish.yml\n\n      - name: Install R\n        uses: r-lib/actions/setup-r@v2\n        with:\n          r-version: '4.2.0'\n          use-public-rspm: true\n\n      - name: Setup renv and install packages\n        uses: r-lib/actions/setup-renv@v2\n        with:\n          cache-version: 1\n        env:\n          RENV_CONFIG_REPOS_OVERRIDE: https://packagemanager.rstudio.com/all/latest\n\n\n\n\n\n\n\nNote\n\n\n\nIf you’re having slow package installs in CI/CD for R, I’d strongly recommend using a repos override like in the example above.\nThe issue is that CRAN doesn’t serve binary packages for Linux, which means slow installs. You need to direct {renv} to install from Public Posit Package Manager, which does have Linux binaries.\n\n\nYou’ll also need to add a workflow to GitHub Actions to install Python and the necessary Python packages from the requirements.txt.\n\n\n.github/workflows/publish.yml\n\n      - name: Install Python and Dependencies\n        uses: actions/setup-python@v4\n        with:\n          python-version: '3.10'\n          cache: 'pip'\n      - run: pip install jupyter\n      - run: pip install -r requirements.txt\n\nNote that, we run the Python environment restore commands with run rather than uses. Where uses takes an existing GitHub Action and runs it, run executes the shell command natively.\nOnce you’ve made those changes, try pushing or merging your project to main. If you click on the Actions tab on GitHub you’ll be able to see the Action running.\nIn all honesty, it will probably fail the first time or five. You will rarely get your Actions correct on the first try. Breathe deeply and know we’ve all been there. You’ll figure it out.\nOnce it’s up, your website will be available at https://&lt;username&gt;.github.io/&lt;repo-name&gt;."
  },
  {
    "objectID": "chapters/sec1/1-5-deployments.html#footnotes",
    "href": "chapters/sec1/1-5-deployments.html#footnotes",
    "title": "5  Deployments and code promotion",
    "section": "",
    "text": "Strictly speaking, this is not true. There are a lot of different ways to kick off CI/CD jobs. But the right way to do it is to base it on Git operations.↩︎\nA completed merge counts as a push.↩︎"
  },
  {
    "objectID": "chapters/sec1/1-6-docker.html#container-lifecycle",
    "href": "chapters/sec1/1-6-docker.html#container-lifecycle",
    "title": "6  Demystifying Docker",
    "section": "6.1 Container lifecycle",
    "text": "6.1 Container lifecycle\nDocker is primarily concerned with the creation, movement, and running of containers. A container is a software entity that packages code and its dependencies down to the operating system. Containers are one way to have completely different environments coexisting side by side on one physical machine.\n\n\n\n\n\n\nNote\n\n\n\nContainers aren’t the only way to run multiple virtual environments on one host. They’re just the most talked about right now.\nAnd Docker Containers aren’t the only type of container. You may run across other kinds, like Apptainer (formerly Singularity), often used in high performance computing (HPC) contexts.\n\n\nA Docker Image is an immutable snapshot of a container. When you want to run a container, you pull the image and run it as an instance or container that you’ll interact with.\n\n\n\n\n\n\nNote\n\n\n\nConfusingly, the term container is used both to refer to a running instance (“Here’s my running container”) as well as which image (“I used the newest Ubuntu container”).\nI prefer the term instance for the running container to eliminate this confusion.\n\n\nImages are usually stored in registries, which are similar to Git repositories. The most common registry for public containers is Docker Hub, which allows public and private hosting of images in free and paid tiers. Docker Hub includes official images for operating systems and programming languages, as well as many community-contributed containers. Some organizations run private registries, usually using registry as a service offerings from cloud providers.2\nImages are built from Dockerfiles – the code that defines the image. Dockerfiles are usually stored in a git repository. Building and pushing images in a CI/CD pipeline is common so changes to the Dockerfile are immediately reflected in the registry.\nYou can control Docker Containers from the Docker Desktop app. If you’re using Docker on a server, you’ll mostly interact via the command line interface (CLI). All Docker CLI commands are formatted as docker &lt;command&gt;.\nThe graphic below shows the different states for a container and the CLI commands to move from one to another.\n\n\n\n\n\n\n\nNote\n\n\n\nI’ve included docker pull on the graphic for completeness, but you’ll rarely run it. docker run auto-pulls the container(s) it needs.\n\n\nInstances run on an underlying machine called a host. A primary feature – also a liability – of using containers is that they are ephemeral. Unless configured otherwise, anything inside an instance when it shuts down vanishes without a trace.\nSee Appendix D for a cheatsheet listing common Docker commands.\n\n6.1.1 Image Names\nYou must know which image you’re referencing to build, push, pull, or run it. Every image has a name that consists of an id and a tag.\n\n\n\n\n\n\nNote\n\n\n\nIf you’re using Docker Hub, container IDs take the form &lt;user&gt;/&lt;container name&gt;, so I might have the container alexkgold/my-container. This should look familiar to GitHub users.\nOther registries may enforce similar conventions for IDs, or they may allow IDs in any format they want.\n\n\nTags specify versions and variants of containers and come after the id and :. For example, the official Python Docker image has tags for each version of Python like python:3, variants for different operating systems, and a slim version that saves space by excluding recommended packages.\nSome tags, usually used for versions, are immutable. For example, the rocker/r-ver container is built on Ubuntu and has a version of R built-in. There’s a rocker/r-ver:4.3.1, which is a container with R 4.3.1.\nOther tags are relative to the point in time. If you don’t see a tag on a container name, it’s using the default latest. Other common relative tags refer to the current development state of the software inside, like devel, release, or stable."
  },
  {
    "objectID": "chapters/sec1/1-6-docker.html#running-containers",
    "href": "chapters/sec1/1-6-docker.html#running-containers",
    "title": "6  Demystifying Docker",
    "section": "6.2 Running Containers",
    "text": "6.2 Running Containers\nThe docker run command runs container images as an instance. You can run docker run &lt;image name&gt; to get a running container. However, most things you want to do with your instance require several command line flags.\nThe -name &lt;name&gt; flag names an instance. If you don’t provide a name, each instance gets a random alphanumeric ID on start. Names are useful because they persist across individual instances of a container, so they can be easily remembered or used in code.\nThe -rm flag automatically removes the container after it’s done. If you don’t use the -rm flag, the container will stick around until you clean it up manually with docker rm. The -rm flag can be useful when iterating quickly – especially because you can’t re-use names until you remove the container.\nThe -d flag will run your container in detached mode. This is useful when you want your container to run in the background and not block your terminal session. It’s useful when running containers in production, but you probably don’t want to use it when trying things out and want to see logs streaming out as the container runs.\n\n6.2.1 Getting information in and out\nWhen a container runs, it is isolated from the host. This is a great feature. It means programs running inside the container can address the container’s filesystem and networking without worrying about the host outside. But it also means that using resources on the host requires explicit declarations as part of the docker run command.\nTo get data in or out of a container, you must mount a shared volume (directory) between the container and host with the -v flag. You specify a host directory and a container directory separated by :. Anything in the volume will be available to both the host and the container at the file paths specified.\nFor example, maybe you’ve got a container that runs a job against data it expects in the /data directory. On your host machine, this data lives at /home/alex/data. You could make this happen with\n\n\nTerminal\n\ndocker run -v /home/alex/data:/data\n\nHere’s a diagram of how this works.\n\nSimilarly, if you have a service running in a container on a particular port, you’ll need to map the container port to a host port with the -p flag.\n\n\n6.2.2 Other runtime commands\nIf you want to see your containers, docker ps lists them. This is especially useful to get instance IDs if you didn’t bother with names.\nTo stop a running container docker stop does so nicely and docker kill terminates a container immediately.\nYou can view the logs from a container with docker logs.\nLastly, you can execute a command inside a running container with docker exec. This is most commonly used to access the command line inside the container as if SSH-ing to a server with docker exec -it &lt;container&gt; /bin/bash.\nWhile it’s normal to SSH into a server to poke around, it’s somewhat of an anti-pattern to exec in to fix problems. Generally, you should prefer to review logs and adjust Dockerfiles and run commands."
  },
  {
    "objectID": "chapters/sec1/1-6-docker.html#building-images-from-dockerfiles",
    "href": "chapters/sec1/1-6-docker.html#building-images-from-dockerfiles",
    "title": "6  Demystifying Docker",
    "section": "6.3 Building Images from Dockerfiles",
    "text": "6.3 Building Images from Dockerfiles\nA Dockerfile is a set of instructions to build a Docker image. If you know how to accomplish something from the command line, you shouldn’t have too much trouble building a Dockerfile to do the same.\nOne thing to consider when creating Dockerfiles is that the resulting image is immutable, meaning that anything you build into the image is forever frozen in time. You’ll want to set up the versions of R and Python and install system requirements in your Dockerfile. Depending on the purpose of your container, you may want to copy in code, data, and/or R and Python packages, or you may want to mount those in from a volume at runtime.\nThere are many Dockerfile commands. You can review them all in the Dockerfile documentation, but here are the handful that are enough to build most images.\n\nFROM – specify the base image, usually the first line of the Dockerfile.\nRUN – run any command as if you were sitting at the command line inside the container.\nCOPY – copy a file from the host filesystem into the container.\nCMD - Specify what command to run on the container’s shell when it runs, usually the last line of the Dockerfile.3\n\nEvery Dockerfile command defines a new layer. A great feature of Docker is that it only rebuilds the layers it needs to when you make changes. For example, take the following Dockerfile:\nFROM ubuntu:latest\n\nCOPY my-data.csv /data/data.csv\n\nRUN [\"head\", \"/data/data.csv\"]\nLet’s say I wanted to change the head command to tail. Rebuilding this container would be nearly instantaneous because the container would only start rebuilding after the COPY command.\nOnce you’ve created your Dockerfile, you build it into an image using docker build -t &lt;image name&gt; &lt;build directory&gt;. If you don’t provide a tag, the default tag is latest.\nYou can then push the image to DockerHub or another registry using docker push &lt;image name&gt;."
  },
  {
    "objectID": "chapters/sec1/1-6-docker.html#comprehension-questions",
    "href": "chapters/sec1/1-6-docker.html#comprehension-questions",
    "title": "6  Demystifying Docker",
    "section": "6.4 Comprehension Questions",
    "text": "6.4 Comprehension Questions\n\nDraw a mental map of the relationship between the following: Dockerfile, Docker Image, Docker Registry, Docker Container\nWhen would you want to use each of the following flags for docker run? When wouldn’t you?\n\n-p, --name, -d, --rm, -v\n\nWhat are the most important Dockerfile commands?"
  },
  {
    "objectID": "chapters/sec1/1-6-docker.html#lab-putting-an-api-in-a-container",
    "href": "chapters/sec1/1-6-docker.html#lab-putting-an-api-in-a-container",
    "title": "6  Demystifying Docker",
    "section": "6.5 Lab: Putting an API in a Container",
    "text": "6.5 Lab: Putting an API in a Container\nPutting an API into a container is a popular way to host them. In this lab, we will put the Penguin Model Prediction API from Chapter 2 into a container.\nIf you’ve never used Docker before, start by installing Docker Desktop on your computer.\nFeel free to write your own Dockerfile to put the API in a container. If you want to make it easy, the {vetiver} package, which you’ll remember auto-generated the API for us, can also auto-generate a Dockerfile. Look at the package documentation for details.\nOnce you’ve generated your Dockerfile, take a look at it. Here’s the one for my model:\n\n\nDockerfile\n\n# # Generated by the vetiver package; edit with care\n# start with python base image\nFROM python:3.9\n\n# create directory in container for vetiver files\nWORKDIR /vetiver\n\n# copy  and install requirements\nCOPY vetiver_requirements.txt /vetiver/requirements.txt\n\n#\nRUN pip install --no-cache-dir --upgrade -r /vetiver/requirements.txt\n\n# copy app file\nCOPY app.py /vetiver/app/app.py\n\n# expose port\nEXPOSE 8080\n\n# run vetiver API\nCMD [\"uvicorn\", \"app.app:api\", \"--host\", \"0.0.0.0\", \"--port\", \"8080\"]\n\nThis auto-generated Dockerfile is nicely commented so it’s easy to follow.\n\n\n\n\n\n\nNote\n\n\n\nThis container follows the best practices from Chapter 2. We’d expect the model to be updated much more frequently than the container, so the model isn’t built into the container. Instead, the container knows how to fetch the model using the {pins} package.\n\n\nNow build the container using docker build -t penguin-model ..\nYou can run the container using\ndocker run --rm -d \\\n  -p 8080:8080 \\\n  --name penguin-model \\\n  penguin-model\nIf you go to http://localhost:8080 you’ll find that…it doesn’t work? Why? If you run the container attached (remove the -d from the run command) you’ll get some feedback that might be helpful.\nIn line 15 of the Dockerfile, we copy app.py in to the container. Let’s look at that file to see if we can find any hints.\n\n\napp.py\n\nfrom vetiver import VetiverModel\nimport vetiver\nimport pins\n\n\nb = pins.board_folder('./model', allow_pickle_read=True)\nv = VetiverModel.from_pin(b, 'penguin_model', version = '20230422T102952Z-cb1f9')\n\nvetiver_api = vetiver.VetiverAPI(v)\napi = vetiver_api.app\n\nLook at that (very long) line 6. The API is connecting to a local directory to pull the model. Is your Spidey-Sense tingling? Something about container filesystem vs host filesystem?\nThat’s right: we put our model at /data/model on our host machine. But the API inside the container is looking for /data/model inside the container – which doesn’t exist!\nThis is a case where we need to mount a volume into the container like so:\ndocker run --rm -d \\\n  -p 8080:8080 \\\n  --name penguin-model \\\n  -v /data/model:/data/model \\\n  penguin-model-local\nNow you should be able to get your model up in no time.\n\n6.5.1 Lab Extensions\nRight now, logs from the API stay inside the container instance. But that means that the logs go away when the container does. That’s bad if the container dies because something goes wrong.\nHow might you ensure the container’s logs get written somewhere more permanent?"
  },
  {
    "objectID": "chapters/sec1/1-6-docker.html#footnotes",
    "href": "chapters/sec1/1-6-docker.html#footnotes",
    "title": "6  Demystifying Docker",
    "section": "",
    "text": "This was truer before the introduction of M-series chips for Macs. Chip architecture differences fall below the level that a container captures and many popular containers wouldn’t run on new Macs. These issues are getting better over time and will probably fully disappear relatively soon.↩︎\nThe big three container registries are AWS Elastic Container Registry (ECR), Azure Container Registry, and Google Container Registry.↩︎\nYou may also see ENTRYPOINT, which sets the command CMD runs against. Usually the default /bin/sh -c to run CMD in the shell will be the right choice.↩︎"
  },
  {
    "objectID": "chapters/sec2/2-0-sec-intro.html#getting-and-running-a-server",
    "href": "chapters/sec2/2-0-sec-intro.html#getting-and-running-a-server",
    "title": "IT/Admin for Data Science",
    "section": "Getting and running a server",
    "text": "Getting and running a server\nMany data science tasks require a server and supporting tools like networking and storage. These days, the most common way to set up a data science environment is to rent a server from a cloud provider. That’s why Chapter 7 introduces the cloud and how you might want to use it for data science purposes.\nUnlike your phone or personal computer, you’ll never touch this cloud server you’ve rented. Instead, you’ll administer the server via a virtual interface from your computer. Moreover, servers generally don’t even have the kind of point-and-click interface you’re familiar with from your personal devices.\nInstead, you’ll access and manage your server from the text-only command line. That’s why Chapter 8 is about setting up the command line on your local machine to make it convenient and ergonomic, and how to connect to your server for administration purposes using SSH.\nUnlike the Apple, Windows, or Android operating systems on your personal devices, most servers run the Linux operating system. Chapter 9 will teach you a little about what Linux is and introduce you to the basics of Linux administration, including how to think about files and users on a multi-tenant server.\nBut you’re not just interested in running a Linux server. You want to use it to accomplish data science tasks. In particular, you want to use data science tools like R, Python, RStudio, JupyterHub, and more. You’ll need to learn how to install, run, and configure applications on your server. That’s why Chapter 10 is about application administration.\nWhen your phone or computer gets slow or you run out of storage, it’s probably time for a new one. But, a server is a working machine that can be scaled up or down to accommodate more people or heavier workloads over time. That means you may have to manage the server’s resources more actively than your personal devices. That’s why Chapter 11 is about managing and scaling server resources."
  },
  {
    "objectID": "chapters/sec2/2-0-sec-intro.html#making-it-safely-accessible",
    "href": "chapters/sec2/2-0-sec-intro.html#making-it-safely-accessible",
    "title": "IT/Admin for Data Science",
    "section": "Making it (safely) accessible",
    "text": "Making it (safely) accessible\nUnless you’re doing something very silly, your personal devices aren’t accessible to anyone who isn’t physically touching the device. In contrast, most servers are only useful because they’re addressable on a computer network, perhaps even the open internet.\nMaking a server accessible to people over the internet makes it useful but also introduces risk. Many dastardly plans for your personal devices are thwarted because a villain must physically steal them to get access. For a server, allowing digital access invites many more potential threats to steal data or hijack your computational resources for nefarious ends. Therefore, you’ve got to be careful about how you’re providing access to the machine.\nRisk aside, there’s a lot of depth to computer networking, and just getting it working isn’t trivial. You can probably muddle through by following online tutorials, but that’s a great way to end up with connections that suddenly work and no idea what you did right or how you could break it in the future.\nThe good news is that it’s not magic. Chapter 12 discusses how computers find each other across a network. Once you understand a computer network’s basic structure and operations, you can configure your server’s networking and feel confident that you’ve done it right.\nBut you’re not done once you’ve configured basic connectivity for your server. You will want to take two more steps to make it safe and easy to access. The first is to host your server at a human-friendly URL, which you’ll learn to configure in Chapter 13. The second is to add SSL/TLS to your server to secure the traffic going to and from your server. You’ll learn how to do that in Chapter 14.\nBy the end of these chapters, you will have solid mental models for all the basic tasks you or any other IT/Admin will take on in administering a data science workbench or hosting platform."
  },
  {
    "objectID": "chapters/sec2/2-0-sec-intro.html#labs-in-this-section",
    "href": "chapters/sec2/2-0-sec-intro.html#labs-in-this-section",
    "title": "IT/Admin for Data Science",
    "section": "Labs in this Section",
    "text": "Labs in this Section\nYou created a DevOps-friendly data science project in the book’s first section. The labs in this section will focus on putting that project into production.\nYou’ll start by standing up a server from a cloud provider, configuring your local command line, and connecting to the server via SSH. Once you’ve done that, you’ll learn how to create users on the server and access the server as a particular user.\nYou’ll be ready to transition into data science work at that point. You’ll add R, Python, RStudio Server, and JupyterHub to your server and get them configured. Additionally, you’ll deploy the Shiny App and API you created in the book’s first section onto the server.\nOnce the server is ready, you must configure its networking to make it accessible and secure. You’ll learn how to open the proper ports, set up a proxy to access multiple services on the same server, configure DNS records so your server is available at a real URL, and activate SSL so it can all be done securely.\nBy the time you’ve finished the labs in this section, you’ll be able to use your EC2 instance as a data science workbench and add your penguin mass prediction Shiny App to the Quarto website you created in the book’s first section.\nFor more details on what you’ll do in each chapter, see Appendix C."
  },
  {
    "objectID": "chapters/sec2/2-0-sec-intro.html#footnotes",
    "href": "chapters/sec2/2-0-sec-intro.html#footnotes",
    "title": "IT/Admin for Data Science",
    "section": "",
    "text": "The first car I ever bought was a Honda Civic Hybrid. Great car.↩︎"
  },
  {
    "objectID": "chapters/sec2/2-1-cloud.html#the-cloud-is-rental-servers",
    "href": "chapters/sec2/2-1-cloud.html#the-cloud-is-rental-servers",
    "title": "7  The Cloud",
    "section": "7.1 The cloud is rental servers",
    "text": "7.1 The cloud is rental servers\nAt one time, the only way to get servers was to buy physical machines and hire someone to install and maintain them. This is called running the servers on-prem (short for on-premises).\nThere’s nothing wrong with running servers on-prem. Some organizations, especially those with highly sensitive data, still do. But only those with obviously worthwhile use cases and sophisticated IT/Admin teams have the wherewithal to run on-prem server farms. If your company needs only a little server capacity or isn’t sure about the payoff, hiring someone and buying a bunch of hardware probably isn’t worth it.\nEnter an online bookstore named Amazon. Around 2000, Amazon started centralizing servers across the company so teams who needed capacity could acquire it from this central pool instead of running their own. Over the next few years, Amazon execs (correctly) realized that other companies and organizations would value this ability to rent server capacity. They launched this “rent a server” business as AWS in 2006.\nThe cloud platform business is now enormous – collectively nearly a quarter of a trillion dollars. It’s also highly profitable. AWS was only 13% of Amazon’s revenue in 2021 but was a whopping 74% of the company’s profits for that year.2\nAWS is still the biggest cloud platform by a considerable margin, but it’s far from alone. Approximately 2/3 of the market consists of the big three or the cloud hyper scalers – AWS, Microsoft Azure, and GCP (Google Cloud Platform) – with the final third comprising numerous smaller companies.3"
  },
  {
    "objectID": "chapters/sec2/2-1-cloud.html#real-and-fake-cloud-benefits",
    "href": "chapters/sec2/2-1-cloud.html#real-and-fake-cloud-benefits",
    "title": "7  The Cloud",
    "section": "7.2 Real (and fake) cloud benefits",
    "text": "7.2 Real (and fake) cloud benefits\nThe cloud arrived with an avalanche of marketing fluff. More than a decade after the cloud went mainstream, it’s clear that many of the purported benefits are real, while some are not.\nThe most important cloud benefit is flexibility. Moving to the cloud allows you to get a new server or re-scale an existing one in minutes; you only pay for what you use, often on an hourly basis.4 Because you pay as you go, the risk of incorrectly guessing how much capacity you’ll need is way lower than in an on-prem environment.\nThe other significant benefit of the cloud is that it allows IT/Admin teams to narrow their scope. For most organizations, managing physical servers isn’t part of their core competency and outsourcing that work to a cloud provider is a great choice to promote focus.\n\n\n\n\n\n\nNote\n\n\n\nOne other dynamic is the incentives of individual IT/Admins. As technical professionals, IT/Admins want evidence on their resumes that they have experience with the latest and greatest technologies – generally cloud services – rather than managing physical hardware.\n\n\nAlong with these genuine benefits, the cloud was supposedly going to result in significant savings relative to on-prem operations. For the most part, that hasn’t materialized.\nThe theory was that the cloud would enable organizations to scale their capacity to match needs at any moment. So even if the hourly price were higher, the organization would turn servers off at night or during slow periods and save money.\nBut dynamic server scaling takes a fair amount of engineering effort, and only the most sophisticated IT/Admin organizations have implemented effective autoscaling. And even for the organizations that do autoscale, cloud providers are very good at pricing their products to capture a lot of those savings.\nSome organizations have started doing cloud repatriations – bringing workloads back on-prem for significant cost savings. An a16z study found that, for large organizations with stable workloads, the total cost of repatriated workloads, including staffing, could be only 1/3 to 1/2 the cost of using a cloud provider.5\nThat said, even if the cash savings aren’t meaningful, the cloud is a crucial enabler for many businesses. The ability to start small, focus on what matters, and scale up quickly is worth it.\nSince you’re reading this book, I’m assuming you’re a nerd, and you may be interested in buying a physical server or re-purposing an old computer just for fun. You’re in good company; I’ve run Ubuntu Server on multiple aging laptops. If you mostly want to play, go for it. But if you’re trying to spend more time getting things done and less time playing, acquiring a server from the cloud is the way to go."
  },
  {
    "objectID": "chapters/sec2/2-1-cloud.html#understanding-cloud-services",
    "href": "chapters/sec2/2-1-cloud.html#understanding-cloud-services",
    "title": "7  The Cloud",
    "section": "7.3 Understanding cloud services",
    "text": "7.3 Understanding cloud services\nIn the beginning, cloud providers did just one thing: rent you a server. But they didn’t stop there. Instead, they started building layers and layers of services on top of the rental servers they provide.\nIn the end, all cloud services boil down to “rent me an \\(\\text{X}\\)”. As a data scientist trying to figure out which services you might need, you should start by asking, “What is the \\(\\text{X}\\) for this service?”\nUnfortunately, cloud marketing materials aren’t usually oriented to the data scientist trying to decide whether to use the services; instead, they’re oriented at your boss and your boss’s boss, who wants to hear about the benefits of using the services. That can make it difficult to decode what \\(\\text{X}\\) is.\nIt’s helpful to remember that any service that doesn’t directly rent a server is just renting a server that already has certain software pre-installed and configured.6\n\n\n\n\n\n\nLess of serverless computing\n\n\n\nYou might hear people talking about going serverless. The thing to know about serverless computing is that there is no such thing as serverless computing. Serverless is a marketing term meant to convey that you don’t have to manage the servers. The cloud provider manages them for you, but they’re still there.\n\n\nCloud services are sometimes grouped into three layers to indicate whether you’re renting a basic computing service or something more sophisticated. I will use an analogy to a more familiar layered object to explain cloud service layers. Let’s assume you’re throwing a birthday party for a friend, and you’re responsible for bringing the delicious multi-layer birthday cake.7\n\n\n\n\n\n\nBig Three Service Naming\n\n\n\nIn this next section, I’ll mention services for everyday tasks from the big three. AWS tends to use cutesy names with only a tangential relationship to the task at hand. Azure and GCP name their offerings more literally.\nThis makes AWS names harder to learn, but much more accessible once you’ve learned them. A table of all the services mentioned in this chapter is in Appendix D.\n\n\n\n7.3.1 IaaS Offerings\nInfrastructure as a service (IaaS, pronounced eye-ahzz) is the basic “rent a server” premise from the earliest days of the cloud.\n\n\n\n\n\n\nNote\n\n\n\nWhen you rent a server from a cloud provider, you usually not renting a whole server. Instead, you’re renting a virtualized server or a virtual machine (vm), usually called an instance. What you see as your server is probably just a part of a larger physical server you share with other cloud provider customers.\nUnlike a personal computer, a rented cloud server doesn’t include storage (hard disk), so you’ll acquire that separately and attach it to your instance.\n\n\nFrom a data science perspective, an IaaS offering might look like what we’re doing in the lab in this book, i.e., acquiring a server, networking, and storage from the cloud provider and assembling it into a data science workbench. This is the best and cheapest way to learn how to administer a data science environment, but it’s also the most time-consuming.\nRecalling the cake analogy, IaaS is akin to going to the grocery store, gathering supplies, and baking and decorating your friend’s cake all from scratch.\nSome common IaaS services you’re likely to use include:\n\nRenting a server from AWS with EC2 (Elastic Cloud Compute), Azure with Azure VMs, or GCP with Google Compute Engine Instances.\nAttaching storage with AWS’s EBS (Elastic Block Store), Azure Managed Disk, or Google Persistent Disk.\nCreating and managing the networking where your servers sit with AWS’s VPC (Virtual Private Cloud), Azure’s Virtual Network, and GCP’s Virtual Private Cloud.\nManaging DNS records via AWS’s Route 53, Azure DNS, and Google Cloud DNS. (More on what this means in Chapter 13).\n\nWhile IaaS means the IT/Admins don’t have to be responsible for physical management of servers, they’re responsible for everything else, including keeping the servers updated and secured. For that reason, many organizations are moving away from IaaS toward something more managed.\n\n\n7.3.2 PaaS Offerings\nIn a PaaS (Platform as a Service) solution, you hand off management of the servers and manage your applications via an API specific to the service.\nIn the cake-baking world, PaaS would be like buying a pre-made cake and some frosting and writing “Happy Birthday!” on the cake yourself.\nOne PaaS service that already came up in this book is blob (Binary Large Object) storage. Blob storage allows you to put objects somewhere and recall them to any other machine that has access to the blob store. Many data science artifacts, including machine learning models, are kept in blob stores. The major blob stores are AWS’s S3 (Simple Storage Service), Azure Blob Storage, and Google Cloud Storage.\nYou’ll also likely use cloud-based database, data lake, and data warehouse offerings. I’ve seen RDS or Redshift from AWS, Azure Database or Azure Datalake, and Google Cloud Database and Google BigQuery used most frequently. This category also includes several offerings from outside the big three, most notably Snowflake and Databricks.8\nDepending on your organization, you may also use services that run APIs or applications from containers or machine images like AWS’s ECS (Elastic Container Service), Elastic Beanstalk, or Lambda, Azure’s Container Apps or Functions, or GCP’s App Engine or Cloud Functions.\nIncreasingly, organizations are turning to Kubernetes as to host services. (More on that in Chapter 17.) Most organizations who do so use a cloud provider’s Kubernetes cluster as a service: AWS’s EKS (Elastic Kubernetes Service) or Fargate, Azure’s AKS (Azure Kubernetes Service), or GCP’s GKE (Google Kubernetes Engine).\nMany organizations are moving to PaaS solutions for hosting applications for internal use. It removes the hassle of managing and updating actual servers. On the flipside, these offerings are less flexible than just renting a server, and some applications don’t run well in these environments.\n\n\n7.3.3 SaaS Offerings\nSaaS (Software as a Service) is where you rent the end-user software, often based on seats or usage. You’re already familiar with consumer SaaS software like Gmail, Slack, and Office365.\nThe cake equivalent of SaaS would be heading to a bakery to buy a decorated cake for your friend.\nDepending on your organization, you might use a SaaS data science offering like AWS’s SageMaker, Azure’s Azure ML, or GCP’s Vertex AI or Cloud Workstations.\nThe great thing about SaaS offerings is that you get immediate access to the end-user application and it’s usually trivial (aside from cost) to add more users. IT/Admin configuration is generally limited to hooking up integrations, often authentication and/or data sources.\nThe trade-off for this ease is that they’re generally more expensive and you’re at the provider’s mercy for configuration and upgrades.\n\n\n7.3.4 Cloud Data Stores\nRedshift, Azure Datalake, BigQuery, Databricks, and Snowflake are full-fledged data warehouses that catalog and store data from all across your organization. You might use one of these as a data scientist, but you probably won’t (and shouldn’t) set one up.\nHowever, you’ll likely own one or more databases within the data warehouse and you may have to choose what kind of database to use.\nIn my experience, Postgres is good enough for most things involving rectangular data of moderate size. And if you’re storing non-rectangular data, you can’t go wrong with blob storage. There are more advanced options, but I probably wouldn’t spring for anything more complicated until you’ve tried the combo of a Postgres database and blob storage and found it lacking.\n\n\n7.3.5 Common Services\nRegardless of what you’re trying to do, if you’re working in the cloud, you must ensure that the right people have the correct permissions. To manage these permissions, AWS has IAM (Identity and Access Management), GCP has Identity Access Management, and Azure has Microsoft Entra ID, which was called Azure Active Directory until the summer of 2023. Your organization might integrate these services with a SaaS identity management solution like Okta or OneLogin.\nAdditionally, some cloud services are geographically specific. Each cloud provider has split the world into several geographic areas, which they all call regions.\nSome services are region-specific and can only interact with other services in that region by default. If you’re doing things yourself, I recommend just choosing the region where you live and putting everything there. Costs and service availability vary somewhat across regions, but it shouldn’t be materially different for what you’re trying to do.\nRegions are subdivided into availability zones (AZs), subdivisions of regions. Each AZ is designed to be independent, so an outage affecting one AZ won’t affect other AZs in the region. Some organizations want to run services that span multiple availability zones to protect against outages. If you’re running something sophisticated enough to need multi-AZ configuration, you should be working with a professional IT/Admin."
  },
  {
    "objectID": "chapters/sec2/2-1-cloud.html#comprehension-questions",
    "href": "chapters/sec2/2-1-cloud.html#comprehension-questions",
    "title": "7  The Cloud",
    "section": "7.4 Comprehension Questions",
    "text": "7.4 Comprehension Questions\n\nWhat are two reasons you should consider going to the cloud? What’s one reason you shouldn’t?\nWhat is the difference between PaaS, IaaS, and SaaS? What’s an example of each that you’re familiar with?\nWhat are the names of AWS’s services for: renting a server, file system storage, blob storage?"
  },
  {
    "objectID": "chapters/sec2/2-1-cloud.html#introduction-to-labs",
    "href": "chapters/sec2/2-1-cloud.html#introduction-to-labs",
    "title": "7  The Cloud",
    "section": "7.5 Introduction to Labs",
    "text": "7.5 Introduction to Labs\nWelcome to the lab!\nYou’ll build a functional data science workbench by walking through the labs sequentially. It won’t be sufficient for enterprise-level requirements, but will be secure enough for a hobby project or even a small team.\nFor this lab, we will use services from AWS, as they’re the biggest cloud provider and the one you’re most likely to run into in the real world. Because we’ll be mostly using IaaS services, there are very close analogs from Azure and GCP should you want to use one of them instead."
  },
  {
    "objectID": "chapters/sec2/2-1-cloud.html#lab-getting-started-with-aws",
    "href": "chapters/sec2/2-1-cloud.html#lab-getting-started-with-aws",
    "title": "7  The Cloud",
    "section": "7.6 Lab: Getting started with AWS",
    "text": "7.6 Lab: Getting started with AWS\nIn this first lab, we will get up and running with an AWS account and manage, start, and stop EC2 instances in AWS.\nThe server we’ll stand up will be from AWS’s free tier – so there will be no cost involved as long as you haven’t used up all your AWS free tier credits yet.\n\n\n\n\n\n\nTip\n\n\n\nThroughout the labs, I’ll suggest you name things in specific ways so you can copy commands straight from the book. Feel free to use different names if you prefer.\n\n\nStart by creating a directory for this lab, named do4ds-lab.\n\n7.6.1 Step 1: Login to the AWS Console\nWe’re going to start by logging into AWS at https://aws.amazon.com.\n\n\n\n\n\n\nNote\n\n\n\nAn AWS account is separate from an Amazon account for ordering online and watching movies. You’ll have to create one if you’ve never used AWS before.\n\n\nOnce logged in, you’ll be confronted by the AWS console. You’re now looking at a list of all the different services AWS offers. There are many, and most are irrelevant right now. Poke around if you want and then continue when you’re ready.\n\n\n\n\n\n\nAccessing AWS from code\n\n\n\nAny of the activities in this lab can be done from the AWS Command Line Interface (CLI). Feel free to configure the CLI on your machine if you want to control AWS from the command line.\nThere are also R and Python packages for interacting with AWS services. The most common are Python’s {boto3} package or R’s {paws} and {aws.s3}.\nRegardless of what tooling you’re using, you’ll generally configure your credentials in three environment variables – AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, and AWS_REGION.\nYou can get the access key and secret access key from the AWS console and you should know the region. The region is not a secret, but use proper secrets management for your access ID and key.\n\n\n\n\n7.6.2 Step 2: Stand up an EC2 instance\nThere are five attributes to configure for your EC2 instance. If it’s not mentioned here, stick with the defaults. In particular, stay with the default Security Group. We’ll learn what Security Groups are and how to configure them later.\n\n7.6.2.1 Name + Tags\nInstance name and tags are human-readable labels so you can remember what this instance is. Neither name nor tag is required, but I’d recommend you name the server something like do4ds-lab in case you stand up others later.\nIf you’re doing this at work, there may be tagging policies so the IT/Admin team can figure out who servers belong to later.\n\n\n7.6.2.2 Image\nAn image is a system snapshot that serves as the starting point for your server. AWS’s are called AMIs (Amazon Machine Images). They range from free images of bare operating systems to paid images bundled with software you might want.\nChoose an AMI that’s just the newest LTS Ubuntu operating system. As of this writing, that’s 22.04. It should say free tier eligible.\n\n\n7.6.2.3 Instance Type\nThe instance type identifies the capability of the machine you’re renting. An instance type is a family and a size with a period in between – more on AWS instance types in Chapter 11.\nFor now, I’d recommend getting the largest free tier-eligible server. As of this writing, that’s a t2.micro with 1 CPU and 1 GB of memory.\n\n\n\n\n\n\nServer sizing for the lab\n\n\n\nA t2.micro with 1 CPU and 1 GB of memory is a very small server. For example, your laptop probably has at least 8 CPUs and 16 GB of memory.\nA t2.micro should be sufficient to finish the lab, but you’ll need a substantially larger server to do real data science work.\nLuckily, it’s easy to upgrade cloud server sizes later. More on how, and advice on sizing servers for real data science work in Chapter 11.\n\n\n\n\n7.6.2.4 Keypair\nThe keypair is the skeleton key to your server. We’ll learn how to use and configure it in Chapter 8. For now, create a new keypair. I’d recommend naming it do4ds-lab-key. Download the .pem version and put it in your do4ds-lab directory.\n\n\n7.6.2.5 Storage\nBump up the storage to as much as you can under the free tier, because why not? As of this writing, that’s 30 GB.\n\n\n\n7.6.3 Step 3: Start the Server\nIf you have followed these instructions, you should be looking at a summary that lists the operating system, server type, firewall, and storage. Go ahead and launch your instance.\nIf you go back to the EC2 page and click on Instances you can see your instance as it comes up. When it’s up, it will transition to State: Running.\n\n\n7.6.4 Optional: Stop the Server\nWhenever you’re stopping for the day, you may want to suspend your server so you’re not using up your free tier hours or paying for it. You can suspend an instance in its current state to restart it later. Suspended instances aren’t always free, but they’re very cheap.\nWhenever you want to suspend your instance, go to the EC2 page for your server. Under the Instance State drop-down in the upper right, choose Stop Instance.\nAfter a couple of minutes, the instance will stop. Before you return to the next lab, you’ll need to start the instance back up so it’s ready to go.\nIf you want to delete the instance, you can choose to Terminate Instance from that same Instance State dropdown."
  },
  {
    "objectID": "chapters/sec2/2-1-cloud.html#lab-put-the-penguins-data-and-model-in-s3",
    "href": "chapters/sec2/2-1-cloud.html#lab-put-the-penguins-data-and-model-in-s3",
    "title": "7  The Cloud",
    "section": "7.7 Lab: Put the penguins data and model in S3",
    "text": "7.7 Lab: Put the penguins data and model in S3\nWhether or not you’re hosting your own server, most data scientists working at an organization that uses AWS will run into S3, AWS’s blob store.\nIt is common to store in an ML model S3. We will store the penguin mass prediction model we created in Chapters Chapter 2 and Chapter 3 in an S3 bucket.\n\n7.7.1 Step 1: Create an S3 bucket\nYou’ll have to create a bucket, most commonly from the AWS console. You can also do it from the AWS CLI. I’m naming mine do4ds-lab.\n\n\n7.7.2 Step 2: Push new models to S3\nLet’s change the code in our Quarto doc to push the model into S3 when the model rebuilds, instead of just saving it locally.\nIf your credentials are in the proper environment variables, pushing the model to S3 is easy by changing the {vetiver} board type to board_s3.\nIt’ll look something like this.\n\n\nmodel.qmd\n\nfrom pins import board_s3\nfrom vetiver import vetiver_pin_write\n\nboard = board_s3(\"do4ds-lab\", allow_pickle_read=True)\nvetiver_pin_write(board, v)\n\nUnder the hood, {vetiver} uses standard R and Python tooling to access an S3 bucket.\nInstead of using credentials, you could configure an instance profile using IAM, so the entire EC2 instance can access the S3 bucket without needing credentials. Configuring instance profiles is the kind of thing you should work with a real IT/Admin to do.\n\n\n7.7.3 Step 3: Pull the API model from S3\nYou’ll also have to configure the API to load the model from the S3 bucket. Luckily, this is very easy. Just update the script you used to build your Dockerfile so it pulls from the pin in the S3 bucket rather than the local folder.\nNow, the script to build the Dockerfile looks like this:\n---\ntitle: \"Prepare Dockerfile\"\nformat:\n  html:\n    code-fold: true\n---\n\n## Load Environment\n\n```{python}\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nfrom pins import board_s3\nfrom vetiver import vetiver_prepare_docker\n\nboard = board_s3(\"do4ds-lab\", allow_pickle_read=True)\nvetiver_prepare_docker(board, \"penguin_model\")\n```\n\n\n7.7.4 Step 4: Give GitHub Actions S3 credentials\nWe want our model building to correctly push to S3 even when it’s running in GitHub Actions, but since GitHub doesn’t have our S3 credentials by default, so we’ll need to provide them.\nWe will declare the variables we need in the Render and Publish step of the Action.\nOnce you’re done, that section of the publish.yml should look something like this.\n\n\n.github/workflows/publish.yml\n\non:\n  workflow_dispatch:\n  push:\n    branches: main\n\nname: Quarto Publish\n\njobs:\n  build-deploy:\n    runs-on: ubuntu-latest\n    permissions:\n      contents: write\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v2\n\n      - name: Set up Quarto\n        uses: quarto-dev/quarto-actions/setup@v2\n\n      - name: Install R\n        uses: r-lib/actions/setup-r@v2\n        with:\n          r-version: '4.2.0'\n          use-public-rspm: true\n\n      - name: Setup renv and install packages\n        uses: r-lib/actions/setup-renv@v2\n        with:\n          cache-version: 1\n        env:\n          RENV_CONFIG_REPOS_OVERRIDE: https://packagemanager.rstudio.com/all/latest\n\n      - name: Install Python and Dependencies\n        uses: actions/setup-python@v4\n        with:\n          python-version: '3.10'\n          cache: 'pip'\n      - run: pip install jupyter\n      - run: pip install -r requirements.txt\n\n      - name: Render and Publish\n        uses: quarto-dev/quarto-actions/publish@v2\n        with:\n          target: gh-pages\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}\n          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n          AWS_REGION: us-east-1\n\nNow, unlike the GITHUB_TOKEN secret, which GitHub Actions automatically provides to itself, we’ll have to give these secrets to the GitHub interface.\n\n\n7.7.5 Lab Extensions\nYou might also want to put the actual data you’re using into S3. This can be a great way to separate the data from the project, as recommended in Chapter 2.\nPutting the data in S3 is such a common pattern that DuckDB allows you to directly interface with parquet files stored in S3."
  },
  {
    "objectID": "chapters/sec2/2-1-cloud.html#footnotes",
    "href": "chapters/sec2/2-1-cloud.html#footnotes",
    "title": "7  The Cloud",
    "section": "",
    "text": "Yes, that is a Sound of Music reference.↩︎\nhttps://www.visualcapitalist.com/aws-powering-the-internet-and-amazons-profits/↩︎\nhttps://www.statista.com/chart/18819/worldwide-market-share-of-leading-cloud-infrastructure-service-providers/↩︎\nA huge amount of cloud spending is now done via annual pre-commitments, which AWS calls Savings Plans. The cloud providers offer big discounts for making an up-front commitment, which the organization then spends down over the course of one or more years. If you need to use cloud services, it’s worth investigating whether your organization has committed spend you can tap into.↩︎\nhttps://a16z.com/2021/05/27/cost-of-cloud-paradox-market-cap-cloud-lifecycle-scale-growth-repatriation-optimization/↩︎\nThere are also some wild services that do specific things, like let you rent you satellite ground station infrastructure or do Internet of Things (IoT) workloads. Those services are definitely cool, but are so much outside the scope of this book that I’m going to pretend they don’t exist.↩︎\nIf you’re planning my birthday party, chocolate layer cake with vanilla frosting is the correct cake configuration.↩︎\nSome materials classify Snowflake and Databricks as SaaS. I find the line between PaaS and SaaS to be quite blurry and somewhat immaterial.↩︎"
  },
  {
    "objectID": "chapters/sec2/2-2-cmd-line.html#getting-the-command-line-you-want",
    "href": "chapters/sec2/2-2-cmd-line.html#getting-the-command-line-you-want",
    "title": "8  Using the command line",
    "section": "8.1 Getting the command line you want",
    "text": "8.1 Getting the command line you want\nAs you start on the command line, you’ll soon realize that some customization is in order. Maybe the colors aren’t quite right, or you want shortcuts for commands you often type, or you want more information in the display.\nSome might argue that customizing your command line isn’t the best use of your time and energy. Those people are no fun. A command line that behaves exactly as you like will speed up your work and make you feel like a hacker.\nBut as you get started, you’ll soon find yourself neck-deep in Stack Overflow posts on how to customize your .bashrc. Or wait, is it the .zshrc? Or…\nThe command line you interact with is two or three programs that sit on top of each other. You can mix and match options and configure each in various ways, which makes customization a little confusing.\n\n\n\n\n\n\nNotes on operating systems\n\n\n\nBecause I’ve been using the command line in MacOS for many years, I have strong opinions to share in this chapter.\nI haven’t used a Windows machine in a while. I’ve collected some recommendations, but I can’t personally vouch for them the same way.\nI don’t include Linux recommendations because people who use Linux on their desktops have already gone deep down the customization rabbit hole and don’t need my help wasting their time."
  },
  {
    "objectID": "chapters/sec2/2-2-cmd-line.html#the-terminal",
    "href": "chapters/sec2/2-2-cmd-line.html#the-terminal",
    "title": "8  Using the command line",
    "section": "8.2 The terminal",
    "text": "8.2 The terminal\nThe terminal is the GUI where you’ll type in commands. Your terminal program will dictate the colors and themes available for the window, how tabs and panes work, and the keyboard shortcuts you’ll use to manage them.\nSome integrated development environments (IDEs), like RStudio or VS Code, have terminals built into them. You may not need another if you do all your terminal work from one of these environments. These recommendations are in case you do want one.\n\nMacOSWindows\n\n\nI’d recommend against using the built-in terminal app (called Terminal). It’s okay, but there are better options.\nMy favorite is the free iTerm2, which adds a bunch of niceties like better theming and multiple tabs.\n\n\nThe built-in terminal is the favorite of many users. There are a variety of alternatives you can try, but feel free to stick with the default."
  },
  {
    "objectID": "chapters/sec2/2-2-cmd-line.html#the-shell",
    "href": "chapters/sec2/2-2-cmd-line.html#the-shell",
    "title": "8  Using the command line",
    "section": "8.3 The shell",
    "text": "8.3 The shell\nThe shell takes the commands you type and runs them. It’s what matches the commands you type to actual programs on your system. Your options for plugins and themes will depend on which shell you choose.\nThe shell runs anywhere you’ve got a running operating system, so your computer has one shell, and your server would have a different one. Even a Docker Container has a shell available. That means that if you do a lot of work on a server, you may need to configure your shell twice – locally and on the server.\n\nMacOSWindows\n\n\nThe default shell for MacOS (and Linux) is called bash. I’d advise you to switch it out for zsh, the most popular bash alternative.1 Bash alternatives are programs that extend bash with various bells and whistles.\nRelative to bash, zsh has a few advantages out of the box, like better auto-completion. It also has a huge ecosystem of themes to enhance visual appeal and functionality, and plugins that let your command line do everything from displaying your git status to controlling your Spotify playlist.\nI’d recommend looking up instructions for how to install zsh using Homebrew.\n\n\nWindows comes with two shells built in, the Command Shell (cmd) and the PowerShell.\nThe Command Shell is older and has been superseded by PowerShell. If you’re getting started, you should work with PowerShell. If you’ve been using Command Shell on a Windows machine for a long time, most Command Shell commands work in PowerShell, so it may be worth switching over.\nMany Windows users are switching away from Windows shells entirely in favor of using Windows Subsystem for Linux (WSL), which allows you to run a Linux command line (i.e. bash/zsh) on your Windows machine with minimal configuration, giving you the best of both worlds."
  },
  {
    "objectID": "chapters/sec2/2-2-cmd-line.html#configuration-management",
    "href": "chapters/sec2/2-2-cmd-line.html#configuration-management",
    "title": "8  Using the command line",
    "section": "8.4 Configuration management",
    "text": "8.4 Configuration management\nNow that you’ve installed your shell and terminal, you’ll want to customize them. It is possible to customize both zsh and PowerShell directly. But the best way is to use a configuration manager for your themes and plugins.\n\nMacOSWindows\n\n\nPrezto is my favorite configuration and plugin manager for zsh. OhMyZsh is also popular and very good. Feel free to choose either, but you can only use one.\nOnce you’ve installed Prezto, you’ve got (at least) three places to configure your command line; the iTerm2 preferences, the zsh configuration file .zshrc, and the Prezto configuration file .zpreztorc. I’d recommend leaving .zshrc alone, customizing the look of the window and the tab behavior in the iTerm2 preferences, and customizing the text theme and plugins via Prezto.\nI tend to be pretty light on customization, but I’d recommend looking into git plugins and some advanced auto-completion and command history search functionality.\n\n\nMany people like customizing PowerShell with Oh My Posh."
  },
  {
    "objectID": "chapters/sec2/2-2-cmd-line.html#text-editors",
    "href": "chapters/sec2/2-2-cmd-line.html#text-editors",
    "title": "8  Using the command line",
    "section": "8.5 Text Editors",
    "text": "8.5 Text Editors\nAs you’re working on the command like, you’ll also be using text editors a fair bit. There are many options for text editors and people have strong preferences.\nMac OS’s default text editor is called TextEdit and it’s bad. Don’t use it. Windows users get Notepad, which is somewhat better than TextEdit, but still not the best option.\nYou can always edit text files inside your chosen IDE, like VS Code or RStudio. Others may prefer a standalone text editor. The most popular these days are probably Sublime or Notepad++ (Windows only).\nUnlike with the terminal, there’s no deep configuration here. Install one from the web, configure it as you like, and make sure it’s the default for opening .txt and other files you might want to edit in your system preferences."
  },
  {
    "objectID": "chapters/sec2/2-2-cmd-line.html#secure-server-connections-with-ssh",
    "href": "chapters/sec2/2-2-cmd-line.html#secure-server-connections-with-ssh",
    "title": "8  Using the command line",
    "section": "8.6 Secure server connections with SSH",
    "text": "8.6 Secure server connections with SSH\nOne common IT/Admin task is remotely accessing a server from the command line on your machine. SSH – short for Secure (Socket) Shell – is a tool for making a secure connection to another computer over an unsecured network. It’s most often used to interact with a server’s command line from your computer’s.\nSSH requires invoking the ssh command line interface from a local host (your computer) with a username and the remote host’s (server’s) address. For example, connecting to the server at server.example.com as the user alex would look like\n\n\nTerminal\n\n&gt; ssh alex@server.example.com\n\nOnce you run this command, your terminal will open a session to the server’s terminal.\n\n8.6.1 Understanding SSH keys\nBefore this can work, you’ll have to configure your SSH keys, which come in a set called a keypair. Each keypair consists of a public key and a private key. You’ll register your public key anywhere you’re trying to SSH into, like a server or git host, but your private key must be treated as a precious secret.\nWhen you use the ssh command, your local machine sends a request to open an SSH session to the remote and includes the private key with the request. The remote host verifies the private key with the public key and opens an encrypted connection.\n\nIt can be hard to remember how to configure SSH. So let’s detour into public key cryptography, the underlying technology. Once you’ve built a mental model, you can figure out which key goes where without mechanically remembering.\nPublic key cryptography uses mathematical operations that are simple in one direction but hard to reverse to make it easy to check whether a proffered private key is valid but nearly impossible to fabricate a private key from a public key.\n\n\n\n\n\n\nTip\n\n\n\nUsing the term key for both the public and private keys obscures the differences. I prefer to think of the private key as the key and the public key as the lock. Maybe they should have named them that. But no one asked me.\n\n\nAs a simple example, think of the number \\(91\\) and its prime factors. Do you know what the prime factors of \\(91\\) are offhand? I do not. It will probably take a few minutes to figure out the answer, even if you use a calculator. But if I give you the numbers \\(7\\) and \\(13\\), it takes just a moment to verify that \\(7 * 13 = 91\\).\nIn this example, the number \\(91\\) would be the public key and the prime numbers \\(7\\) and \\(13\\) together would be the private key. This wouldn’t make for very good public key cryptography because it doesn’t take long to figure out that \\(7\\) and \\(13\\) are prime factors of \\(91\\).\nIn real public key cryptography, the mathematical operations are more complex and the numbers much, much bigger. So much so that it’s basically impossible to break public SSH keys by guessing.\nBut that doesn’t make SSH foolproof. While it’s impossible to fabricate a private key, it is possible to steal one. Your private key must be kept secret. The best practice is to never move it from the computer where it was created and never to share it.\nIn summary, do what you want with your public keys, but don’t share your private keys. Don’t share your private keys. Seriously, do not share your private keys."
  },
  {
    "objectID": "chapters/sec2/2-2-cmd-line.html#practical-ssh-usage",
    "href": "chapters/sec2/2-2-cmd-line.html#practical-ssh-usage",
    "title": "8  Using the command line",
    "section": "8.7 Practical SSH usage",
    "text": "8.7 Practical SSH usage\nNow that you understand of how SSH works, the steps should be easy to remember.\n\nCreate an SSH keypair on any machine you’ll be SSH-ing from (local host).\nPut the public key anywhere you’ll be SSH-ing to (remote host).\nUse the ssh command to connect.\n\nIf you’re working on a server, you’ll probably create at least two keypairs. One on your personal computer to SSH to the server, and one on the server to access outside services that use SSH, like Git.\n\n8.7.1 Step 1: Create Keypair\nYou’ll create a keypair on any machine you’re SSH-ing from.\nTo create an SSH keypair, you should follow a tutorial online. The keypair will have two parts. The one that ends in .pub is – you guessed it – the public key.\nYou’ll usually create only one private key on each machine. If you follow standard instructions for creating a key, it will use the default name, probably id_ed25519.2 Sticking with the default name is ideal because the ssh command will automatically use it. You’ll have to specify each time you use your key if you don’t use the default name.\n\n\n\n\n\n\nNote\n\n\n\nRemember, you should never move your private key. If you think the solution to your problem is to move your private key, think again.\nInstead of moving your private key, create a new private key on the machine where you must use SSH and register a second public key on the remote.\n\n\nSome organizations require a unique key for each service you’re using to make it easier to swap keys in case of a breach. If so, you won’t be able to use the default key names.\n\n\n8.7.2 Step 2: Register the public keys\nTo register a public key to SSH into a server, you’ll add the public key to the end of the user’s .ssh/authorized_keys file in their home directory. You’ll have to ensure the permissions on the authorized_keys file are correct – more on that in Chapter 9.\nIf you’re registering with a service like GitHub.com, there’s probably a text box in the GUI to add an SSH key. Google for instructions on how to do it.\n\n\n8.7.3 Step 3: Use SSH\nTo use SSH, type ssh &lt;user&gt;@&lt;host&gt;. Other commands can use SSH under the hood, like git or scp.\n\n\n\n\n\n\nFor Windows users\n\n\n\nWindows didn’t support SSH out of the box for a long time, so SSH-ing from Windows required a separate utility called PuTTY. More recent versions of Windows support using SSH directly in PowerShell or Windows Subsystem for Linux (WSL). If SSH isn’t enabled on your machine, Google for instructions.\n\n\nIf you have multiple SSH keys or didn’t use the default flag, you can specify a particular key with the -i flag.\nI’d recommend setting up an SSH config file if you use SSH a lot. An SSH config file allows you to create aliases that are shortcuts to SSH commands including users, hosts, and other details. If you had a long SSH command like ssh -i my-ssh-key alex@server.example.com, you could shorten it to ssh alex-server or whatever you want.\nOne annoyance about SSH is that it blocks the terminal its using and the connection will break when your computer goes to sleep. Many people like using the tmux command line utility to help solve these issues.\ntmux is a terminal multiplexer, which allows you to manipulate terminal sessions from the command line, including putting sessions into the background and making sessions durable through sleeps and other operations. I’m mentioning tmux because many people love it, but I’ve found the learning curve too steep for me to use it regularly. Your mileage may vary.\nIf you ever run into trouble using SSH, it has one of my favorite debugging modes. Just add a -v to your command for verbose mode. If that’s not enough information, add another v for more verbosity with -vv, and if that’s not enough, add another v for super verbose mode."
  },
  {
    "objectID": "chapters/sec2/2-2-cmd-line.html#comprehension-questions",
    "href": "chapters/sec2/2-2-cmd-line.html#comprehension-questions",
    "title": "8  Using the command line",
    "section": "8.8 Comprehension Questions",
    "text": "8.8 Comprehension Questions\n\nDraw a mental map that includes the following: terminal, shell, theme manager, operating system, my laptop\nUnder what circumstances should you move or share your SSH private key?\nWhat is it about SSH public keys that makes them safe to share?"
  },
  {
    "objectID": "chapters/sec2/2-2-cmd-line.html#lab-log-in-to-the-server",
    "href": "chapters/sec2/2-2-cmd-line.html#lab-log-in-to-the-server",
    "title": "8  Using the command line",
    "section": "8.9 Lab: Log in to the server",
    "text": "8.9 Lab: Log in to the server\nIn the previous chapter, we got your server up and running. In this lab, we’ll use the provided .pem key to log in for the first time.\n\n8.9.1 Step 1: Grab the server address\nFrom the EC2 page, you can click on the instance ID in blue to see all the details about your server.\nCopy the Public IPv4 DNS address, which starts with \\(\\text{ec2-}\\) and ends with \\(\\text{amazonaws.com}\\). You’ll need this address throughout the labs. If you lose it, come back here to get it.\n\n\n\n\n\n\nSet a Server Address Variable\n\n\n\nIn the rest of the labs in this book, I will write the commands using the bash variable SERVER_ADDRESS. If you create that variable, you can copy the commands from the book.\nFor example, as I write this, my server has the address \\(\\text{ec2-54-159-134-39.compute-1.amazonaws.com}\\). So I would set my server address variable on my command line with SERVER_ADDRESS=ec2-54-159-134-39.compute-1.amazonaws.com.\nIf you’re used to R or Python, notice that spaces are not permitted around = when assigning variables in bash.\n\n\n\n\n8.9.2 Step 2: Log on with the .pem key\nThe .pem key you downloaded when you set up the server is the private key for a pre-registered keypair that will let you SSH into your server as the admin user (named ubuntu on a Ubuntu system).\nThe .pem key is just an SSH key, so that you can SSH to your server with\n\n\nTerminal\n\nssh -i do4ds-lab-key.pem \\\n  ubuntu@SERVER_ADDRESS\n\nWhen you first try this, you’re probably going to get an alert that looks something like this:\n\n\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\n\n\\@ WARNING: UNPROTECTED PRIVATE KEY FILE! \\@\n\n\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\\@\n\nPermissions 0644 for 'do4ds-lab-key.pem' are too open.\n\nIt is required that your private key files are NOT accessible by others.\n\nThis private key will be ignored.\n\nLoad key \"do4ds-lab-key.pem\": bad permissions\n\nubuntu@ec2-54-159-134-39.compute-1.amazonaws.com: Permission denied (publickey).\nBecause the keypair is so powerful, AWS requires that you restrict the access. You’ll need to change the permissions before using it to access the server. We’ll get into permissions in Chapter 9. Until then, you can adjust the permissions by navigating to the correct directory with the cd command and running chmod 600 do4ds-lab-key.pem.\nOnce you’ve done that, you can log in to your machine as the root user. You can type exit to exit an SSH session and return to your machine.\n\n\n8.9.3 Step 3: Create your own SSH key\nYou shouldn’t use the AWS-provided .pem key to log in to your server after the first time. It’s too powerful. Create a normal SSH key using the instructions earlier in this chapter. In the next lab, we’ll get that SSH key configured for your user on the server."
  },
  {
    "objectID": "chapters/sec2/2-2-cmd-line.html#footnotes",
    "href": "chapters/sec2/2-2-cmd-line.html#footnotes",
    "title": "8  Using the command line",
    "section": "",
    "text": "zsh is pronounced by just speaking the letters aloud, zee-ess-aitch. Some people might disagree and say it’s zeesh, but they’re not writing this book, are they?↩︎\nThe pattern is id_&lt;encryption type&gt;. ed25519 is the standard SSH key encryption type as of this writing.↩︎"
  },
  {
    "objectID": "chapters/sec2/2-3-linux.html#a-brief-history-of-linux",
    "href": "chapters/sec2/2-3-linux.html#a-brief-history-of-linux",
    "title": "9  Intro to Linux Administration",
    "section": "9.1 A brief history of Linux",
    "text": "9.1 A brief history of Linux\nA computer’s OS defines how applications can interact with the underlying hardware. OSes dictate how files are stored and accessed, how applications are installed, how network connections work, and more.\nBefore the early 1970s, the computer hardware and software market looked nothing like today. Computers of that era had extremely tight linkages between hardware and software. There was no Microsoft Word you could use on a machine from Dell or HP or Apple.\nInstead, every hardware company was also a software company. If Example Corp’s computer did text editing, it was because Example Corp had written (or commissioned) text editing software specifically for their machine. If Example Corp’s machine could run a game, Example Corp had written that game just for their computer.\nThen, in the early 1970s, researchers funded by AT&T’s Bell Labs released Unix – the first operating system. Now, there was a piece of middleware that reduced the need for coordination between computer manufacturers and software developers. Computer manufacturers could design computers that ran Unix and developers could write software that ran on Unix.\nThe one issue (for everyone but Bell Labs) was that they were paying Bell Labs a lot of money for Unix licenses. So, in the 1980s, programmers started writing Unix-like OSes. These so-called Unix clones behaved like Unix but didn’t include any actual Unix code.2\nIn 1991, Linus Torvalds – then a 21-year-old Finnish grad student – released an open-source Unix clone called Linux via a nonchalant newsgroup posting, saying, “I’m doing a (free) operating system (just a hobby, won’t be big and professional like gnu)…Any suggestions are welcome, but I won’t promise I’ll implement them :-).”3\nThe Linux project outgrew that modest newsgroup post. There are over 600 distros (short for distributions) of Linux, which differ by technical attributes and licensing model. The number of distros reflects the natural fragmentation of popular open-source projects and the disparate requirements for systems as varied as your car’s infotainment system, a smartphone, and the controller for your smart thermostat.\nLuckily, you don’t have to know hundreds of distros. Most organizations use one of only a handful on their servers. The most common open-source distros are Ubuntu or CentOS. Red Hat Enterprise Linux (RHEL) is the most common paid distro.4 Many organizations on AWS are using Amazon Linux, which is independently maintained by Amazon but was originally a RHEL derivative.5\nMost individuals who have a choice, including me, prefer Ubuntu. It’s a little simpler and easier to configure than the others.\n\n\n\n\n\n\nA note on Ubuntu Versioning\n\n\n\nUbuntu versions are numbered by the year and month they were released. Most people use the Long Term Support (LTS) releases, released in April of even years.\nUbuntu versions have fun alliterative names, so you’ll hear people refer to releases by name or version. As of this writing, most Ubuntu machines run Focal (20.04, Focal Fossa) or Jammy (22.04, Jammy Jellyfish)."
  },
  {
    "objectID": "chapters/sec2/2-3-linux.html#administering-linux-with-bash",
    "href": "chapters/sec2/2-3-linux.html#administering-linux-with-bash",
    "title": "9  Intro to Linux Administration",
    "section": "9.2 Administering Linux with Bash",
    "text": "9.2 Administering Linux with Bash\nLinux is administered from the command line using bash or a bash alternative like zsh. The philosophy behind bash and its derivatives says that you should be able to accomplish anything you want with small programs invoked via a command. Each command should do just one thing, and complicated things should be performed by composing commands – taking the output from one as the input to the next.\nInvoking a command is done by typing the command on the command line and hitting enter. If you ever find yourself stuck in a situation you can’t seem to exit, ctrl + c will quit in most cases.\nHelpfully, most bash commands are an abbreviation of the word for what the command does. Unhelpfully, the letters often seem somewhat random.\nFor example, the command to list the contents of a directory is ls, which mostly makes sense. Over time, you’ll get very comfortable with the commands you use frequently.\nBash commands can be modified to behave the way you need them to.\nCommand arguments provide details to the command. They come after the command with a space in between. For example, if I want to run ls on the directory /home/alex, I can run ls /home/alex on the command line.\nSome commands have default arguments. For example, the default argument for the ls command is the current directory. So if I’m in /home/alex, I’d get the same thing from either ls or ls /home/alex.\nOptions or flags modify the command’s operation and come between the command and arguments. Flags are denoted by having one or more dashes before them. For example, ls allows the -l flag, which displays the output as a list. So, ls -l /home/alex would get the files in /home/alex as a list.\nSome flags themselves have flag arguments. For example, the -D flag allows you to specify how the datetime from ls -l is displayed. So running ls -l -D %Y-%m-%dT%H:%M:%S /home/alex lists all the files in /home/alex with the date-time of the last update formatted in ISO-8601 format, which is always the correct format for dates.\nBash commands are always formatted as &lt;command&gt; &lt;flags + flag args&gt; &lt;command args&gt;.\nIt’s nice that this structure is standard. It’s not nice that the main argument is at the end because it makes long bash commands hard to read. To make commands more readable, you can break the command over multiple lines and include one flag or argument per line. You can tell bash you want it to continue a command after a line break by ending the line with a space and a \\.\nFor example, here’s that long ls command more nicely formatted:\n\n\nTerminal\n\n&gt; ls -l \\\n  -D %Y-%m-%dT%H:%M:%S \\\n  /home/alex\n\nAll flags and command arguments are found on the program’s man page (short for manual). You can access the man page for any command with man &lt;command&gt;. You can scroll the man page with arrow keys and exit with q.\nBash is a fully functional programming language, so that you can assign variables with &lt;var-name&gt;=&lt;value&gt; (no spaces allowed) and access them $&lt;var-name&gt;. The bash version of print is echo.\nFor example,\n\n\nTerminal\n\n&gt; MSG=\"Hello World!\"\n&gt; echo $MSG\nHello World!\n\nFor the most part, you’ll write commands directly on the command line. You also can write and run bash scripts that include conditionals, loops, and functions. Bash scripts usually end in .sh and are often run with the sh command like sh my-script.sh.\nThe advantage of writing bash scripts is that they can run basically anywhere. The disadvantage of writing bash scripts is that bash is a truly ugly programming language that is hard to debug."
  },
  {
    "objectID": "chapters/sec2/2-3-linux.html#programs-run-on-behalf-of-users",
    "href": "chapters/sec2/2-3-linux.html#programs-run-on-behalf-of-users",
    "title": "9  Intro to Linux Administration",
    "section": "9.3 Programs run on behalf of users",
    "text": "9.3 Programs run on behalf of users\nWhenever a program is running, it is running as a particular user identified by their username.\nOn any Unix-like system, the whoami command returns the username of the active user. When I run whoami on my MacBook, I get:\n\n\nTerminal\n\n&gt; whoami                                                  \nalexkgold\n\nUsernames have to be unique on the system – but they’re not the true identifier for a user. A user is uniquely identified by their user ID (uid), which maps to all the other user attributes like username, password, home directory, groups, and more. The uid for a user is assigned when the user is created and usually don’t need to be changed or specified manually.6\nEach human who accesses a Linux server should have their own account. In addition, many applications create service account users for themselves and run as those users. For example, installing RStudio Server will create a user with username rstudio-server. Then, when RStudio Server goes to do something – start an R session for example – it will do so as rstudio-server.\nUser uids start at 10,000 with those below 10,000 reserved for system processes. There’s also one special user – the admin, root, sudo, or superuser who gets the special uid 0.\nUsers belong to groups, which are collections of one or more users. Each user has exactly one primary group and can be a member of secondary groups. By default, each user’s primary group is the same as their username.\nLike a user has a uid, a group has a gid. User gids start at 100.\nThe id command shows a user’s username, uid, groups, and gid. On my MacBook, I’m a member of several groups, with the primary group staff.\n\n\nTerminal\n\n&gt; id                                                     \nuid=501(alexkgold) gid=20(staff) groups=20(staff),12(everyone),61(localaccounts),79(_appserverusr),80(admin),81(_appserveradm),98(_lpadmin),701(com.apple.sharepoint.group.1),33(_appstore),100(_lpoperator),204(_developer),250(_analyticsusers),395(com.apple.access_ftp),398(com.apple.access_screensharing),400(com.apple.access_remote_ae)\n\nIf you ever need to add users to a server, the easiest way is with the useradd command. Once you have a user, you may need to change the password, which you can do at any time with the passwd command. Both useradd and passwd start interactive prompts, so you don’t need to do more than run those commands.\nIf you ever need to alter a user – the most common task is adding a user to a group, you would use the usermod command with the -aG flag."
  },
  {
    "objectID": "chapters/sec2/2-3-linux.html#permissions-dictate-what-users-can-do",
    "href": "chapters/sec2/2-3-linux.html#permissions-dictate-what-users-can-do",
    "title": "9  Intro to Linux Administration",
    "section": "9.4 Permissions dictate what users can do",
    "text": "9.4 Permissions dictate what users can do\nIn Linux, everything you can interact with is just a file. Every log – file. Every picture – file. Every application – file. Every configuration – file.\nSo whether a user can take an action is determined by whether they have the proper permissions on a particular file.\nBasic Linux permissions (POSIX permissions) consist of a 3x3 matrix of read, write, and execute for the owner, the owning group, and everyone else. Read means the user can see the contents of a file, write means the user can save a changed version, and execute means they can run the file as a program.\n\n\n\n\n\n\nNote\n\n\n\nThere are more complex ways to manage Linux permissions. For example, you might hear about Access Control Lists (ACLs). They’re beyond the scope of this book.\nThere is more information on how organizations manage users and what they can do in Chapter 16, which is all about auth.\n\n\nFor example, here’s a set of permissions that you might have for a program that you wanted anyone to be able to run, group members to inspect, and only the owner to change.\n\nDirectories also have permissions – read allows the user to see what’s in the directory, write allows the user to alter what’s in the directory, and execute allows the user to enter the directory.\nFile permissions and directory permissions don’t have to match. For example, a user could have read permissions on a directory to see the names of the files, but not have read permissions on any of the files so they can’t look at the contents.\nWhen you’re working on the command line, you don’t get a little grid of permissions. Instead, they’re expressed in one of two ways. The first is the string representation, a 10-character string that looks like -rw-r–r--.\nThe first character indicates the type of file: most often - for normal (file) or d for a directory.\nThe following nine characters indicate the three permissions for the user, the group, and everyone else. There will be an r for read, a w for write, and an x for execute or - to indicate that they don’t have the permission.\nSo the permissions in the graphic would be -rwxr-x--x for a file and drwxr-x--x for a directory.\nThe best way to get these permissions is to run the ls -l command.\n\n\nTerminal\n\n&gt; ls -l                                                   \n-rw-r--r--  1 alexkgold  staff     28 Oct 30 11:05 config.py\n-rw-r--r--  1 alexkgold  staff   2330 May  8  2017 credentials.json\n-rw-r--r--  1 alexkgold  staff   1083 May  8  2017 main.py\ndrwxr-xr-x 33 alexkgold  staff   1056 May 24 13:08 tests\n\nEach line starts with the string representation of the permissions followed by the owner and group so you can easily understand who should be able to access that file or directory.\nAll of the files in this directory are owned by alexkgold. Only the owner (alexkgold) has write permission, but everyone has read permission. In addition, there’s a tests directory, with read and execute for everyone and write only for alexkgold.\nYou will probably need to change a file’s permissions when administering a server. You can do so using the chmod command.\nFor chmod, permissions are indicated as a three digit number, like 600, where the first digit is the permission for the user, the second for the group, and the third for everyone else. To get the right number, you sum the permissions as follows: 4 for read, 2 for write, and 1 for execute. You can check for yourself that any set of permissions is uniquely identified by a number between 1 and 7.7\nSo to implement the permissions from the graphic, you’d want the permission set 751 to give the user full permissions (4 + 2 + 1), read and execute (4 + 1) to the group, and execute only (1) to everyone else.\n\n\n\n\n\n\nNote\n\n\n\nIf you spend any time administering a Linux server, you will almost certainly find yourself applying chmod 777 to a directory or file to rule out a permissions issue.\nI can’t tell you not to do this – we’ve all been there. But if it’s something important, change it back once you’ve figured things out.\n\n\nYou might want to change a file’s owner or group. You can change users and groups with the chown command. Users get changed with a username, and groups can be changed with the group name prefixed by a colon.\nSometimes, you might not be the correct user to take a particular action. If you want to change your user, you can use the su (switch user) command. You’ll be prompted for a password to make sure you’re allowed.\nThe admin or root user has full permissions on every file and there are some actions that only the root user can do. When you need to do root-only things, you usually don’t want to su to be the root user. It’s too powerful. And, if you have user-level configuration, it all gets left behind.\nInstead, individual users can be granted the power to temporarily assume root privileges without changing to be the root user. This is accomplished by making them members of the admin group. If a user is a member of the admin group, they can prefix commands with sudo to run those commands with root privileges.\nThe name of the admin group varies by distro. In Ubuntu, the group is called sudo."
  },
  {
    "objectID": "chapters/sec2/2-3-linux.html#the-linux-filesystem-is-a-tree",
    "href": "chapters/sec2/2-3-linux.html#the-linux-filesystem-is-a-tree",
    "title": "9  Intro to Linux Administration",
    "section": "9.5 The Linux Filesystem is a tree",
    "text": "9.5 The Linux Filesystem is a tree\nAll the information available to a computer is indexed by its filesystem, which comprises directories or folders, which are containers for other directories and files.\nYou’re probably used to browsing the filesystem with your mouse on your laptop. Apps completely obscure the filesystem on your phone, but it’s there. On a Linux server, the only way to traverse the filesystem is with written commands. Therefore a good mental model for the filesystem on your server is critical.\nIn Linux, each computer has precisely one filesystem, based at the root directory, /. The rest of the filesystem is a tree (or perhaps an upside-down tree). Every directory is contained in by a parent directory and may contain one or more children or sub-directories.8 A / in between two directories means that it’s a sub-directory.\nEvery directory is a sub-directory of / or a sub-directory of a sub-directory of / or…you get the picture. The /home/alex file path defines a particular location, the alex sub-directory of /home, itself a sub-directory of the root directory, /.\n\n\n\n\n\n\n\nTip\n\n\n\nIt’s never necessary, but viewing the tree-like layout for a directory can sometimes be helpful. The tree utility can show you one. It doesn’t always come pre-installed, so you might have to install it.\n\n\nBecause the entire Linux filesystem is based at /, it doesn’t matter what physical or virtual disks you have attached to your system. They will fall somewhere under the main filesystem (often inside /mnt), but the fact that they’re on separate drives is obscured from the user.\nThis will be familiar to MacOS users because MacOS is based on an operating system called BSD that, like Linux, is a Unix clone. If you’re familiar with Windows, the Linux filesystem may seem strange.\nIn Windows, each physical or logical disk has its own filesystem with its own root. You’re probably familiar with C: as your main filesystem. Your machine may also have a D: drive. If you’ve got network share drives, they’re likely at M: or N: or P:.\nAnother difference is that Windows uses \\ to separate file path elements rather than /. This used to be a big deal, but newer versions of Windows accept file paths using /.\n\n9.5.1 Working with file paths\nWhenever a program runs, it runs at a particular path in the filesystem called the working directory. You can get the absolute path to your working directory with the pwd command, an abbreviation for print working directory.\nWhen you want a program to run the same regardless of where it’s run from, it’s best to use an absolute path, specified relative to the root. Absolute paths are easy to recognize because they start with /.\nSometimes it’s convenient to use a relative file path, which starts at the working directory, denoted by .. For example, if I want to access the data subdirectory of the working directory, that would be available at ./data.\nThe working directory’s parent is at ... You could see everything in the parent directory of your current working directory with ls .. or its parent with ls ../...\nAll accounts representing actual humans should have a home directory, which usually live inside /home.\nThe home directory and all its contents are owned by the user to whom it belongs. The home directory is the user’s space to store what they need, including user-specific configuration. Users can find their home directory at ~.\nYou will need to change your working directory, which you can do with the cd command, short for change directory. You can use either absolute or relative file paths with cd. If you were in /home/alex and wanted to navigate to /home, either cd .. or cd /home would work.\nSome files or directories are hidden so they don’t appear in a normal ls. You know a file or directory is hidden because its name starts with .. Hidden files are usually configuration files you don’t manipulate in normal usage. These aren’t secret or protected in any way; they’re just skipped by ls for convenience. If you want to display all files in a directory, including hidden ones, you can use the -a flag (for all) with ls.\nYou’ve already seen a couple of hidden files in this book – like the .github directory, command line configuration in .zprezto and .zshrc, and Python environmental configuration in .env. You might also be familiar with .gitignore, .Rprofile, and .Renviron."
  },
  {
    "objectID": "chapters/sec2/2-3-linux.html#moving-files-and-directories",
    "href": "chapters/sec2/2-3-linux.html#moving-files-and-directories",
    "title": "9  Intro to Linux Administration",
    "section": "9.6 Moving files and directories",
    "text": "9.6 Moving files and directories\nYou will frequently need to change where files and directories are on your system, including copying, deleting, moving, and more.\nYou can copy a file or directory from one place to another using the cp command. cp leaves the old file or directory behind and duplicates it at the specified location. You can use the -r flag to copy everything in a directory recursively.\nYou can move a file with the mv command, which does not leave the old file behind. If you want to remove a file entirely, you can use the rm command. The -r (recursive) flag can be used with rm to remove everything within a directory and the -f (force) flag can skip rm double checking you really want to do this.\n\n\n\n\n\n\nWarning\n\n\n\nBe very careful with the rm command, especially with -rf.\nThere’s no recycle bin. Things that are deleted are instantly deleted forever.\n\n\nIf you want to make a directory, mkdir makes a file path. It can be used with relative or absolute file paths and can include multiple sub-directories. For example, if you’re in /home/alex, you could mkdir project1/data to make a project1 directory and data sub-directory.\nThe mkdir command throws an error if you try to create a path that includes some existing directories – for example if project1 already existed in the example above. The -p flag can be handy to create only the parts of the path that don’t exist.\nSometimes, it’s useful to operate on every file inside a directory. You can get every file that matches a pattern with the wildcard, *. You can also do partial matching with the wildcard to get all the files that match part of a pattern.\nFor example, let’s say I have a /data directory, and want to put a copy of only the .csv files inside into a new data-copy sub-directory. I could do the following:\n\n\nTerminal\n\n&gt; mkdir -p /data/data-copy\n&gt; cp /data/*.csv /data/data-copy\n\n\n9.6.1 Moving things to and from the server\nIt’s very common to have a file on your server that you want to move to your desktop or vice versa. There are a few different ways to transfer files and directories.\nIf you’re moving multiple files, first combining them into a single object can make things easier. The tar command turns a set of files or a whole directory into a single archive file, usually with the file suffix .tar.gz. Creating an archive also does some compression. The amount depends on the content.\nIn my opinion, tar is a rare failure of bash to provide standalone commands for anything you need to do. tar is used to create and unpack (extract) archive files. Telling it which one requires several flags. You’ll basically never use tar without a bunch of flags, and the incantation is hard to remember. I google it every time I use it. The flags you’ll most often use are in the cheatsheet in Appendix D.\nYou can move files to or from a server with the scp command. scp (short for secure copy) is cp, but with an SSH connection in the middle.9\nSince scp establishes an SSH connection, you need to make the request to somewhere that is accepting SSH connections. That means that whether you’re copying something to or from a server, you’ll run scp from a regular terminal on your laptop, not one already SSH-ed into your server.\nRegular ssh options work with scp, like -i and -v."
  },
  {
    "objectID": "chapters/sec2/2-3-linux.html#pipes-and-redirection",
    "href": "chapters/sec2/2-3-linux.html#pipes-and-redirection",
    "title": "9  Intro to Linux Administration",
    "section": "9.7 Pipes and redirection",
    "text": "9.7 Pipes and redirection\nYou can always copy and paste command outputs or write them to a file, but it can also be helpful to just chain a few commands together. Linux provides a few handy operators that you can use to make this easy.\nThe simplest operator is the pipe |, which takes the output of one command and makes it the input for the subsequent command.\nFor example, you might want to see how many files are in a directory. The wc -l (word count, lines) command counts lines, so you could do ls | wc -l since each file returned by ls is counted as a line.\n\n\n\n\n\n\nCeci n’est pas une pipe?\n\n\n\nThe pipe should feel familiar to R users.\nThe pipe from the {magrittr} package, %&gt;%, was introduced in 2013, and is a popular part of the {tidyverse}.10 The {magrittr} pipe was inspired by pipe operators from Unix (Linux) and the F# programming language.\nDue to its popularity, the pipe |&gt; was formally added to the base R language in R 4.1 in 2021.\n\n\nA few operators write the output of the left-hand side into a file.\nThe &gt; command takes the output of a command on the left and writes it as a new file. If the file you specify already exists, it will be overwritten.\nIf you want to append the new text, rather than overwrite, &gt;&gt; appends to the end of the file. I generally default to &gt;&gt;, because it will create a new file if one doesn’t exist, and I usually don’t mean to overwrite what’s there.\nA common reason you might want to do this is to add something to the end of your .gitignore. For example, if you want to add your .env file to your .gitignore, you could do that with echo .env &gt;&gt; .gitignore.11 Another great use is to add a new public key to your .ssh/authorized_keys file.\nSometimes, you want to create empty files or directories. The touch command makes a blank file at the specified file path. If you touch a preexisting file, it updates the time it was last edited without making any changes. This can be useful because some applications use the file timestamp to see if action is needed."
  },
  {
    "objectID": "chapters/sec2/2-3-linux.html#comprehension-questions",
    "href": "chapters/sec2/2-3-linux.html#comprehension-questions",
    "title": "9  Intro to Linux Administration",
    "section": "9.8 Comprehension Questions",
    "text": "9.8 Comprehension Questions\n\nWhat are the parts of a bash command?\nWhat is the difference between a relative and an absolute path?\nWhat are some ways to direct them to run in a particular place on the filesystem?\nHow can you copy, move, or delete a file? What about to or from a server?\nCreate a mind map of the following terms: Operating System, Windows, MacOS, Unix, Linux, Distro, Ubuntu\nWhat are the 3x3 options for Linux file permissions? How are they indicated in an ls -l command?"
  },
  {
    "objectID": "chapters/sec2/2-3-linux.html#lab-set-up-a-user",
    "href": "chapters/sec2/2-3-linux.html#lab-set-up-a-user",
    "title": "9  Intro to Linux Administration",
    "section": "9.9 Lab: Set up a user",
    "text": "9.9 Lab: Set up a user\nWhen you use your server’s .pem key, you log in as the root user, but that’s too much power to acquire regularly. Additionally, since your server is probably for multiple people, you will want to create users for them.\nIn this lab, you’ll create a regular user for yourself and add an SSH key for them so you can directly log in from your personal computer.\n\n9.9.1 Step 1: Create a non-root user\nLet’s create a user using the adduser command. This will walk us through prompts to create a new user with a home directory and a password. Feel free to add any information you want – or to leave it blank – when prompted.\nI’m going to use the username test-user. If you want to be able to copy/paste commands, I’d advise doing the same. If you were creating users for actual humans, I’d recommend using their names.\n\n\nTerminal\n\n&gt; adduser test-user\n\nWe want this new user to be able to adopt root privileges so let’s add them to the sudo group with\n\n\nTerminal\n\n&gt; usermod -aG sudo test-user\n\n\n\n9.9.2 Step 2: Add an SSH key for your new user\nLet’s register an SSH key for the new user by adding the key from the last lab to the server user’s authorized_users file.\nFirst, you must get your public key to the server using scp.\nFor me, the command looks like this\n\n\nTerminal\n\n&gt; scp -i ~/Documents/do4ds-lab/do4ds-lab-key.pem \\ \n  ~/.ssh/id_ed25519.pub \\\n  ubuntu@$SERVER_ADDRESS:/home/ubuntu\n\n\n\n\n\n\n\nTip\n\n\n\nWe’re copying the public key, but SSH access (argument to -i) is still with the server’s .pem key because there isn’t another one registered yet.\n\n\nThe public key is on the server but in the ubuntu user’s home directory. You’ll need to do the following:\n\nCreate .ssh/authorized_keys in test-user’s home directory.\nCopy the contents of the public key you uploaded into the authorized_keys file (recall &gt;&gt;).\nEnsure the .ssh directory and authorized_keys files are owned by test-user with 700 permissions on .ssh and 600 on authorized_keys.\n\nYou could do this all as the admin user, but I’d recommend switching to being test-user at some point with the su command.\n\n\n\n\n\n\nTip\n\n\n\nIf you run into trouble assuming sudo with your new user, try exiting SSH and returning. Sometimes, these changes aren’t picked up until you restart the shell.\n\n\nOnce you’ve done all this, you should be able to log in from your personal computer with ssh test-user@$SERVER_ADDRESS.\nNow that we’re all set up, you should store the .pem key somewhere safe and never use it to log in again."
  },
  {
    "objectID": "chapters/sec2/2-3-linux.html#footnotes",
    "href": "chapters/sec2/2-3-linux.html#footnotes",
    "title": "9  Intro to Linux Administration",
    "section": "",
    "text": "The remainder are mostly Windows servers. There are a few rarer OSes you might encounter, like Oracle Solaris. There is a product called Mac Server, but it’s just a program for managing Mac desktops and iOS devices, not a server OS.\nThere are also versions on Linux that run on desktop computers. Despite the best efforts of many hopeful nerds, desktop Linux is pretty much only used by professional computer people.↩︎\nOr at least they weren’t supposed to. There’s an interesting history of lawsuits, especially around whether the BSD OS illegally included Unix code.↩︎\nMore in the History of Linux Wikipedia article.\nPedants will scream that the original release of Linux was just the operating system kernel, not a full operating system like Unix. Duly noted, now go away.↩︎\nRHEL and CentOS are related operating systems, but that relationship has changed a lot in the last few years. The details are somewhat complicated, but most people expect less adoption of CentOS in enterprise settings going forward.↩︎\nAs I’m writing this, Amazon Linux 2 is popular, but Amazon Linux 2023 (AL2023) was recently released. I’d expect AL2023 or its successor to be dominant by the time you read this.↩︎\nThe one exception to this is when you’ve got the same user accessing resources across multiple machines. Then the uids have to match. If you’re worrying about this kind of thing, it’s probably time to bring in a professional IT/Admin.↩︎\nClever eyes may realize that this is just the base-10 representation of a three-digit binary number.↩︎\nThe root directory at the base of the tree, /, is its own parent.↩︎\nIt’s worth noting that scp is now considered “insecure and outdated”. The ways it is insecure are rather obscure and not terribly relevant for most people. But if you’re moving a lot of data, you may want something faster. If so, I’d recommend more modern options like sftp and rsync. This isn’t necessary if you’re only occasionally scp-ing small files to or from your server.↩︎\nThe title of this callout box is also the tagline for the {magrittr} package.↩︎\nNote that echo is needed so that the .env gets repeated as a character string. Otherwise .env would be treated as a command.↩︎"
  },
  {
    "objectID": "chapters/sec2/2-4-app-admin.html#linux-app-install-config",
    "href": "chapters/sec2/2-4-app-admin.html#linux-app-install-config",
    "title": "10  Application administration",
    "section": "10.1 Linux app install + config",
    "text": "10.1 Linux app install + config\nThe first step to running applications on a server is installing them. Most software you install will come from system repositories. Your system will have several default repositories; you can add others to install software from non-default repositories.\nFor Ubuntu, the apt command is used for interacting with repositories of .deb files. The yum command is used for installing .rpm files on CentOS and Red Hat.\n\n\n\n\n\n\nNote\n\n\n\nThe examples below are all for Ubuntu, since that’s what we are using in the labs for this book. Conceptually, using yum is very similar, though the exact commands differ somewhat.\n\n\nOn Ubuntu, packages are installed with apt-get install &lt;package&gt;. Depending on your user, you may need to prefix the command with sudo.\nIn addition to installing packages, apt is also the utility for ensuring the lists of available packages you have are up to date with update and that all packages on your system are at their latest version with upgrade. When you find Ubuntu commands online, it’s common to see them prefixed with apt-get update && apt-get upgrade -y to update all system packages to the latest version. The -y flag bypasses a manual confirmation step.\nSome packages may not live in system repositories at all. To install that software, you will download a file on the command line, usually with wget, and then install the software from the file, often with gdebi.\n\n10.1.1 Application configuration\nMost applications require some configuration after they’re installed. Configuration may include connecting to auth soures, setting display and access controls, or configuring networking. You’d probably find the setting in menus on your personal computer. On a server, no such menu exists.\nApplication behavior is usually configured through one or more config files. For applications hosted inside a Docker Container, behavior is often configured with environment variables, sometimes in addition to config files.\nThe application you’re running will have documentation on how to set different configuration options. That documentation is probably dry and boring, but reading it will put you ahead of most people trying to administer the application.\n\n\n10.1.2 Where to find application files\nLinux applications often use several files located in different locations on the filesystem. Here are some of the ones you’ll use most frequently:\n\n/bin, /opt, /usr/local, /usr/bin - installation locations for software.\n/etc - configuration files for applications.\n/var - variable data, most commonly log files in /var/log or /var/lib.\n\nThis means that on a Linux server, the files for a particular application probably don’t all live in the same directory. Instead, you might run the application from the executable in /opt, configure it with files in /etc, and troubleshoot from logs in /var.\n\n\n10.1.3 Configuration with Vim and Nano\nSince application configuration is in text files, you’ll spend a fair bit of time editing text files to administer applications. Unlike on your personal computer, where you click a text file to open and edit it, you’ll need to work with a command line text editor when you’re working on a server.\nThere are two command line text editors you’ll probably encounter: Nano and Vim. While they’re both powerful text editing tools, they can also be intimidating if you’ve never used them.\nYou can open a file by typing nano &lt;filename&gt; or vim &lt;filename&gt;.\n\n\n\n\n\n\nNote\n\n\n\nDepending on your system, you may have Vi in place of Vim. Vi is the original fullscreen text editor for Linux. Vim is its successor (Vim stands for Vi improved). The only difference germane to this section is that you open Vi with vi &lt;filename&gt;.\n\n\nWhen you open Nano, some helpful-looking prompts will be at the bottom of the screen. You’ll see that you can exit with ^x. But should you try to type that, you’ll discover the ^ isn’t the caret character. On Windows, ^ is short for Ctrl; on Mac, it’s for Command (⌘), so Ctrl+x or ⌘+x will exit.\nWhere Nano gives you helpful – if obscure – hints, a first experience with Vim is the stuff of computer nightmares. You’ll type words, and they won’t appear onscreen. Instead, you’ll experience dizzying jumps around the page. Words and entire lines of text will disappear without a trace.\nMany newbie command line users would now be unable to do anything – even to exit and try again. But don’t worry; there’s a way out of this labyrinth. This happens because Vim uses the letter keys to navigate the page, interact with Vim itself, and type words. You see, Vim was created before keyboards uniformly had arrow keys.\nVim is an extremely powerful text editor. Vim includes keyboard shortcuts, called keybindings, that make it fast to move within and between lines and to select and edit text. The learning curve is steep, but I recommend posting a list of keybindings beside your desk and getting comfortable. Most IDEs you might use, including RStudio, JupyterLab, and VSCode, have vim modes. This introduction will be just enough to get you in and out of Vim successfully.\nWhen you enter Vim, you’re in the (now poorly named) normal mode, which is for navigation only. Pressing the i key activates insert mode, which will feel normal for those used to arrow keys. In insert mode, words will appear when you type, and the arrow keys will navigate you around the page.\nOnce you’ve escaped, you may wish never to return to normal mode, but it’s the only way to save files and exit Vim. You can return to normal mode with the escape key.\nTo do file operations, you type a colon, :, followed by the shortcut for what you want to do, and enter. The two most common commands you’ll use are w for save (write) and q for quit. You can combine these to save and quit in one command using :wq.\nSometimes, you may want to exit without saving, or you may have opened and changed a file you don’t have permission to edit. If you’ve made changes and try to exit with :q, you’ll find yourself in an endless loop of warnings that your changes won’t be saved. You can tell Vim you mean it with the exclamation mark, !, and exit using :q!."
  },
  {
    "objectID": "chapters/sec2/2-4-app-admin.html#reading-logs",
    "href": "chapters/sec2/2-4-app-admin.html#reading-logs",
    "title": "10  Application administration",
    "section": "10.2 Reading logs",
    "text": "10.2 Reading logs\nOnce your applications are up and running, you may run into issues. Even if you don’t, you may want to examine how things are going.\nMost applications write their logs somewhere inside the /var directory. Some activities will get logged to the main log at /var/log/syslog. Other things may get logged to /var/log/&lt;application name&gt; or /var/lib/&lt;application name&gt;.\nIt’s essential to get comfortable with the commands to read text files so you can examine logs (and other files). The commands I use most commonly are:\n\ncat prints a whole file, starting at the beginning.\nless prints a file, starting at the beginning, but only a few lines at a time.\nhead prints only the first few lines and exits. It is especially useful to peer at the beginning of a large file, like a csv file – so you can quickly preview the column heads and the first few values.\ntail prints a file going backwards from the end. This is especially useful for log files, as the newest logs are appended to the end of a file. This is such a common practice that “tailing a log file” is a common phrase.\n\nSometimes, you’ll want to use the -f flag (for follow) to tail a file with a live view as it updates.\n\n\nSometimes you want to search around inside a text file. You’re probably familiar with the power and hassle of regular expressions (regex) to search for specific character sequences in text strings. The Linux command grep is the main regex command.\nIn addition to searching in text files, grep is often helpful in combination with other commands. For example, you may want to put the output of ls into grep to search for a particular file in a big directory using the pipe."
  },
  {
    "objectID": "chapters/sec2/2-4-app-admin.html#running-the-right-commands",
    "href": "chapters/sec2/2-4-app-admin.html#running-the-right-commands",
    "title": "10  Application administration",
    "section": "10.3 Running the right commands",
    "text": "10.3 Running the right commands\nLet’s say you want to open Python on your command line. One option would be to type the absolute path to a Python install every time. For example, I’ve got a version of Python in /usr/bin, so /usr/bin/python3 works.\nBut in most cases, it’s nice to type python3 and have the correct version open up.\n\n\nTerminal\n\n&gt; python3\nPython 3.9.6 (default, May  7 2023, 23:32:45) \n[Clang 14.0.3 (clang-1403.0.22.14.1)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; \n\nSometimes you might want to go the other way. Maybe python3 opens Python correctly, but you’re unsure where it’s located. You can use the which command to identify the actual executable for a command. For example this is the result of which python3 on my system.\n\n\nTerminal\n\n&gt; which python3                                                    \n/usr/bin/python3\n\nSometimes, you must make a program available without providing a full path every time. Some applications rely on others, like RStudio Server needing to find R or Jupyter Notebook needing your Python kernels.\nThe operating system knows how to find executables via the path. The path is a set of directories that the system knows to search when it tries to run a program. The path is stored in an environment variable conveniently named PATH.\nYou can check your path at any time with echo $PATH. On my MacBook, this is what it looks like.\n\n\nTerminal\n\n&gt; echo $PATH                                                      \n/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/System/Cryptexes/App/usr/bin:/usr/bin:/bin:/usr/sbin:/sbin\n\nWhen you install a new application, you must add it to the path. Let’s say I installed a new version of Python in /opt/python. That’s not on my PATH, so my system couldn’t find it.\nI can get it on the path in one of two ways. The first option would be to add /opt/python to my PATH every time a terminal session starts, usually via a file in /etc or the .zshrc.\nThe other option is to create a symlink to the new application in a directory already on the PATH. A symlink makes it appear that a copy of a file is in a different location without actually moving it. Symlinks are created with the ln command."
  },
  {
    "objectID": "chapters/sec2/2-4-app-admin.html#running-applications-as-services",
    "href": "chapters/sec2/2-4-app-admin.html#running-applications-as-services",
    "title": "10  Application administration",
    "section": "10.4 Running applications as services",
    "text": "10.4 Running applications as services\nOn your personal computer, you probably have programs that start every time your computer does. Maybe this happens for Slack, Microsoft Teams, or Spotify. Such applications that execute on startup and run in the background, waiting for input, are called a daemon or a service.\nMost server-based applications are configured to run as a service, so users can access them without needing permissions to start them first. For example, on a data science workbench, you’d want JupyterHub and/or RStudio Server to run as a service.\nIn Linux, the tool to turn a regular application into a daemon is called systemd. Some applications automatically configure themselves with systemd when they’re installed. If your application doesn’t, or you want to alter the startup behavior, most applications have their systemd configuration in /etc/systemd/system/&lt;service name&gt;.service.\nDaemonized services are controlled using the systemctl command line tool.\n\n\n\n\n\n\nNote\n\n\n\nBasically all modern Linux distros have coalesced around using systemd and systemctl. Older systems may not have it installed by default and you may have to install it or use a different tool.\n\n\nThe systemctl command has a set of sub-commands that are useful for working with applications. They look like systemctl &lt;subcommand&gt; &lt;application&gt;. Often systemctl has to be run as sudo, since you’re working with an application for all system users.\nThe most useful systemctl commands include start and stop, status for checking whether a program is running, and restart for a stop followed by a start. Many applications also support a reload command, which reloads configuration settings without restartingt the process. Which settings require a restart vs a reload depends on the application.\nIf you’ve changed a service’s systemd configuration, you can load changes with daemon-reload. You also can turn a service on or off for the next time the server starts with enable and disable.\n\n10.4.1 Running Docker Containers as a service\nPeople love Docker Containers because they easily run on most machines. To run a container as a service, you’ll need to make sure Docker itself is daemonized and then ensure the container you care about comes up whenever Docker does by setting a restart policy for the container.\nHowever, many Docker services involve coordinating more than one container. If so, you’ll want to use a purpose-built system for managing multiple containers. The most popular are Docker Compose or Kubernetes.\nDocker Compose is a relatively lightweight system that allows you to write a YAML file describing the containers you need and their relationship. You can then use a single command to launch the entire set of Docker Containers.\nDocker Compose is fantastic for prototyping systems of Docker Containers and for running small-scale Dockerized deployments on a single server. There are many great resources online to learn more about Docker Compose.\nKubernetes is designed for a similar purpose, but instead of running a handful of containers on one server, Kubernetes is a heavy-duty production system designed to schedule up to hundreds or thousands of Docker-based workloads across a cluster of many servers.\nIn general, I’d recommend sticking with Docker Compose for the work you’re doing. If you need the full might of Kubernetes to do what you want, you probably should be working closely with a professional IT/Admin"
  },
  {
    "objectID": "chapters/sec2/2-4-app-admin.html#managing-r-and-python",
    "href": "chapters/sec2/2-4-app-admin.html#managing-r-and-python",
    "title": "10  Application administration",
    "section": "10.5 Managing R and Python",
    "text": "10.5 Managing R and Python\nAs the admin of a data science server, Python and R are probably the most critical applications you’ll manage.\nThe easiest path to making many users happy is having several versions of R and Python installed side-by-side. That way, users can upgrade their version of R or Python as it works for their project, not according to your upgrade schedule.\nIf you just sudo apt-get install python or sudo apt-get install R, you’ll end up with only one version of Python or R, which will get overwritten every time you re-run the command.\n\n10.5.1 Python-specific considerations\nPython is one of the world’s most popular programming languages for general-purpose computing. This makes configuring Python harder. Getting Python up and running is famously frustrating on both servers and your personal computer.1\nAlmost every system comes with a system version of Python. This is the version of Python the operating system uses for various tasks. It’s almost always old, and you don’t want to mess with it.\nTo configure Python for data science, you have to install the versions of Python you want to use, get them on the path, and get the system version of Python off the path.\nInstalling data science Python versions into /opt/python makes this simpler. Managing versions of Python somewhere wholly distinct from the system Python removes some headaches, and adding a single directory to the path is easy.\nMy favorite route (though I’m biased) is to install Python from the pre-built binaries provided by Posit.\n\n\n\n\n\n\nA Note on Conda\n\n\n\nConda is an excellent solution for managing data science Python versions on your personal computer. Conda allows you to install a standalone version of Python in user space. That means you can install and manage versions of Python even if your organization doesn’t let you have admin rights on your computer.\nFor the same reason, Conda isn’t as good for administering a data science server. You’re not a user – you’re an admin. Configuring server-wide data science versions of Python is more straightforward without Conda.\n\n\n\n\n10.5.2 R-specific considerations\nGenerally, people only install R to do data science, so where you install R is usually not a big issue. Using apt-get install is fine if you know you’ll only ever want one version of R.\nIf you want multiple versions, you’ll need to install them manually. I recommend installing into /opt/R with binaries provided by Posit or using rig, a great R installation manager that supports Windows, Mac, and Ubuntu."
  },
  {
    "objectID": "chapters/sec2/2-4-app-admin.html#managing-system-libraries",
    "href": "chapters/sec2/2-4-app-admin.html#managing-system-libraries",
    "title": "10  Application administration",
    "section": "10.6 Managing System Libraries",
    "text": "10.6 Managing System Libraries\nAs an admin, you’ll also have to decide what to do about system packages, which are Linux libraries you install from a Linux repository or the internet.\nMany packages in Python and R don’t do any work themselves. Instead, they’re just language-specific interfaces to system packages. For example, any R or Python library that uses a JDBC database connector must use Java on your system. And many geospatial libraries make use of system packages like GDAL.\nAs the administrator, you must understand the system libraries required for your Python and R packages. You’ll also need to ensure they’re available and on the path.\nFor many of these libraries, it’s not a huge problem. You’ll install the required library using apt or the system package manager for your distro. In some cases (especially Java), more configuration may be necessary to ensure that the system package you need appears on the path when your code runs.\nSome admins with sophisticated requirements around system library versions use Docker Containers or Linux Environment Modules to keep system libraries linked to projects."
  },
  {
    "objectID": "chapters/sec2/2-4-app-admin.html#comprehension-questions",
    "href": "chapters/sec2/2-4-app-admin.html#comprehension-questions",
    "title": "10  Application administration",
    "section": "10.7 Comprehension Questions",
    "text": "10.7 Comprehension Questions\n\nWhat are two different ways to install Linux applications, and what are the commands?\nWhat does it mean to daemonize a Linux application? What programs and commands are used to do so?\nHow do you know if you’ve opened Nano or Vim? How would you exit them if you didn’t mean to?\nWhat are four commands to read text files?\nHow would you create a file called secrets.txt, open it with vim, write something in, close and save it, and make it so that only you can read it?"
  },
  {
    "objectID": "chapters/sec2/2-4-app-admin.html#lab-installing-applications",
    "href": "chapters/sec2/2-4-app-admin.html#lab-installing-applications",
    "title": "10  Application administration",
    "section": "10.8 Lab: Installing Applications",
    "text": "10.8 Lab: Installing Applications\nAs we’ve started to administer our server, we’ve mostly been doing generic server administration tasks. Now, let’s set up the applications we need to run a data science workbench and get our API and Shiny app set up.\n\n10.8.1 Step 1: Install Python\nLet’s start by installing a data science version of Python, so we’re not using the system Python for data science purposes.\nIf you want just one version of Python, you can apt-get install a specific version. As of this writing, Python 3.10 is a relatively new version of Python, so we’ll install that one with\n\n\nTerminal\n\nsudo apt-get install python3.10-venv\n\nOnce you’ve installed Python, you can check that you’ve got the correct version by running\n\n\nTerminal\n\npython3 --version\n\nThis route to installing Python is easy if you only want one version. If you want to enable multiple versions of Python, apt-get install-ing Python isn’t the way to go.\n\n\n10.8.2 Step 2: Install R\nSince we’re using Ubuntu, we can use rig. There are good instructions on downloading rig and using it to install R on the rlib/rig GitHub repo. Use those instructions to install the current R release on your server.\nOnce you’ve installed R on your server, you can check that it’s running by just typing R into the command line. If that works, you can move on to the next step. If not, you’ll need to ensure R got onto the path.\n\n\n10.8.3 Step 3: Install JupyerHub + JupyterLab\nJupyterHub and JupyterLab are Python programs, so we will run them from within a Python virtual environment. I’d recommend putting that virtual environment inside /opt/jupyterhub.\nHere are the commands to create and activate a jupyterhub virtual environment in /opt/jupyterhub:\n\n\nTerminal\n\nsudo python3 -m venv /opt/jupyterhub\nsource /opt/jupyterhub/bin/activate\n\nNow we will get JupyterHub up and running inside the virtual environment we just created. JupyterHub has great docs (Google “JupyterHub quickstart”) to get up and running quickly. If you must stop for any reason, assume sudo and start the JupyterHub virtual environment we created when you return.\nNote that because we’re working inside a virtual environment, you may have to use the jupyterhub-singleuser version of the binary.\n\n\n10.8.4 Step 4: Daemonize JupyterHub\nBecause JupyterHub is a Python process, not a system process, it won’t automatically get daemonized, so we’ll have to do it manually.\nWe don’t need it right now, but it will be easier to manage JupyterHub later on from a config file that’s in /etc/jupyterhub. To do so, activate the jupyterhub virtual environment, create a default JupyterHub config (Google for the command), and move it into /etc/jupyterhub/jupyterhub_config.py.\nNow let’s move on to daemonizing JupyterHub. To start, kill the existing JupyterHub process (consult the cheatsheet in Appendix D if you need help). Since JupyterHub wasn’t automatically daemonized, you must create the systemd file manually.\nHere’s the file I created in /etc/systemd/system/jupyterhub.service.\n\n\n/etc/systemd/system/jupyterhub.service\n\n[Unit]\nDescription=Jupyterhub\nAfter=syslog.target network.target\n\n[Service]\nUser=root\nEnvironment=\"PATH=/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/opt/jupyterhub/bin\"\nExecStart=/opt/jupyterhub/bin/jupyterhub -f /etc/jupyterhub/jupyterhub_config.py\n\n[Install]\nWantedBy=multi-user.target\n\nThere are two things here worth noticing. The first is the Environment line that adds /opt/jupyterhub/bin to the path – that’s where our virtual environment is.\nThe second is ExecStart line, which provides the startup command and specifies that JupyterHub should use the config we just created, specified by -f /etc/jupyterhub/jupyterhub_config.py.\nNow, you’ll need to use systemctl to reload the daemon, start JupyterHub, and enable it.\n\n\n10.8.5 Step 5: Install RStudio Server\nYou can find the commands to install RStudio Server on the Posit website. Make sure to pick the version that matches your operating system. Since you’ve already installed R, skip to the “Install RStudio Server” step.\nUnlike JupyterHub, RStudio Server daemonizes itself right out of the box, so you can check and control the status with systemctl without further work.\n\n\n10.8.6 Step 6: Run the penguin API from Docker\nFirst, you’ll have to ensure that Docker is available. It can be installed from apt using apt-get install docker.io. You may need to adopt sudo privileges to do so.\nOnce Docker is installed, running the API is almost trivially easy using the command we used in Chapter 6 to run our container.\n\n\nTerminal\n\nsudo docker run --rm -d \\\n  -p 8080:8080 \\\n  --name penguin-model \\\n  alexkgold/penguin-model\n\nOnce it’s up, you can check that it’s running with docker ps.\n\n\n10.8.7 Step 7: Put up the Shiny app\nWe will use Shiny Server to host our Shiny app on the server. Start by moving the app code to the server. I put mine in /home/test-user/do4ds-lab/app by cloning the Git repo.\nAfter that, you’ll need to:\n\nOpen R or Python and rebuild the package library with {renv} or {venv}.\nInstall Shiny Server using the instructions from the Shiny Server Admin Guide.\n\nNote that you can skip steps to install R and/or Python, and the {shiny} package since we’ve already done that.\n\nEdit Shiny Server’s configuration file to run the right app.\nStart and enable Shiny Server with systemctl.\n\n\n\n10.8.8 Lab Extensions\nYou might want to consider a few things before moving into the next chapter, where we’ll start working on giving this server a stable public URL.\nFirst, we haven’t daemonized the API. Feel free to try Docker Compose or set a restart policy for the container.\nSecond, neither the API nor the Shiny app will automatically update when we change them. You might want to set up a GitHub Action to do so. For Shiny Server, you’ll need to push the updates to the server and then restart Shiny Server. For the API, you’d need to configure a GitHub action to rebuild the container and push it to a registry. You’d then need to tell Docker on the server to re-pull and restart the container.\nFinally, there’s no authentication in front of our API. The API has limited functionality, so that’s not a huge worry. But if you had an API with more functionality, that might be a problem. Additionally, someone could try to flood your API with requests to make it unusable. The most common way to solve this is to buy a product that hosts the API for you or to put an authenticating proxy in front of the API. We’ll be adding NGINX soon, so you can try adding authentication later."
  },
  {
    "objectID": "chapters/sec2/2-4-app-admin.html#footnotes",
    "href": "chapters/sec2/2-4-app-admin.html#footnotes",
    "title": "10  Application administration",
    "section": "",
    "text": "See, for example, the XKCD comic titled Python Environment.↩︎"
  },
  {
    "objectID": "chapters/sec2/2-5-scale.html#the-briefest-intro-to-computational-theory",
    "href": "chapters/sec2/2-5-scale.html#the-briefest-intro-to-computational-theory",
    "title": "11  Server Resources and Scaling",
    "section": "11.1 The briefest intro to computational theory",
    "text": "11.1 The briefest intro to computational theory\nYou’re probably aware that everything you’ve ever seen on a computer – from this book to your work in R or Python, your favorite internet cat videos, and Minecraft – is just 1s and 0s.\nBut the 1s and 0s aren’t the interesting part. They are just binary representations of integers (whole numbers). The mind-bending part is that the integers represent something meaningful, and the computer only adds these integers.1\nThat means a helpful mental model for a computer is a factory for doing addition problems. Everything you ask your computer to do is turned into an addition problem, then processed and returned, with the results interpreted as meaningful.\nSince a computer is like an addition factory, decisions about server sizing and scaling are akin to optimally designing the conveyor belts in a factory. In this computer as factory analogy, you should consider three main resources: compute, memory, and storage."
  },
  {
    "objectID": "chapters/sec2/2-5-scale.html#how-computers-compute",
    "href": "chapters/sec2/2-5-scale.html#how-computers-compute",
    "title": "11  Server Resources and Scaling",
    "section": "11.2 How computers compute",
    "text": "11.2 How computers compute\nThe addition assembly line – where the work gets done – is called compute. It’s where \\(2+2\\) gets turned into \\(4\\), and where \\(345619912 + 182347910\\) gets turned into \\(527967822\\).\nComputers do most computing in their central processing unit (CPU), which completes addition problems in one or more cores.\nThe CPU’s speed is primarily determined by the number of cores and the speed of those cores.\nThe number of cores is like the number of lines in the factory. These days, most consumer-grade laptops have between 4 and 16 physical cores. Many have software capabilities that effectively double that number, so they can simultaneously do between 4 and 32 addition problems.\nThe baseline speed of an individual core, called single-core clock speed, is how quickly a single core completes a single addition problem. You can think of this as how fast the conveyor belt moves. Clock speeds are measured in operations per second or hertz (Hz). The cores in your laptop probably max out between two and five gigahertz (GHz), which means between two billion and five billion operations per second.\nFor decades, many of the innovations in computing came from increases in single-core clock speed, but those have fallen off in the last few decades. The clock speeds of consumer-grade chips increased by approximately 10x during the 90s, by 2-3x in the 2000s, and somewhere between not at all and 1.5x in the 2010s.\nBut computers have continued getting faster anyway. The improvements mostly come from increases in the number of cores, better usage of software parallelization, and faster loading and unloading of the CPU (called the bus)."
  },
  {
    "objectID": "chapters/sec2/2-5-scale.html#recommendation-1-fewer-faster-cpu-cores",
    "href": "chapters/sec2/2-5-scale.html#recommendation-1-fewer-faster-cpu-cores",
    "title": "11  Server Resources and Scaling",
    "section": "11.3 Recommendation 1: Fewer, faster CPU cores",
    "text": "11.3 Recommendation 1: Fewer, faster CPU cores\nR and Python are single-threaded. This means that unless you’re using special libraries for parallel processing, you’ll max out a single CPU core while the others sit unused.\nTherefore, for most R and Python work, single-core clock speed matters more than the number of cores, and fewer, faster cores are usually preferable to many slower ones.\nYou’re probably not used to thinking about this tradeoff from buying a laptop or phone. Modern CPUs are generally good, and you should buy the one that fits your budget. But, if you’re standing up a cloud server, you often do have an explicit choice between more slower cores and fewer faster ones, determined by the instance family.\nThe number of cores you need for a multi-user data science server can be hard to estimate. If you’re doing non-ML tasks like counts and dashboarding or relatively light-duty machine learning, I might advise the following:\n\\[\n\\text{n cores} = \\text{1 core per user} + 1\n\\]\nThe spare core is for the server to do its own operations apart from the data science usage. On the other hand, if you’re doing heavy-duty machine learning or parallelizing jobs across the CPU, you may need more cores than this rule of thumb."
  },
  {
    "objectID": "chapters/sec2/2-5-scale.html#how-memory-works",
    "href": "chapters/sec2/2-5-scale.html#how-memory-works",
    "title": "11  Server Resources and Scaling",
    "section": "11.4 How memory works",
    "text": "11.4 How memory works\nYour computer’s random access memory (RAM) is its short-term storage. RAM is like the area adjacent to the assembly line where work to be done sits waiting, and completed work is temporarily placed before it gets sent elsewhere.\nYour computer can quickly access objects in RAM, so things stored in RAM are ready to go. The downside is that RAM is temporary. When your computer turns off, the RAM gets wiped.2\n\n\n\n\n\n\nNote\n\n\n\nMemory and storage are measured in bytes with metric prefixes.\nStandard sizes for memory are in gigabytes (billion bytes) and terabytes (trillion bytes). Some enterprise data stores run on the scales of thousands of terabytes (petabytes) or even thousands of petabytes (yotabytes).\n\n\nModern consumer-grade laptops come with somewhere between 4 and 16 GB of memory."
  },
  {
    "objectID": "chapters/sec2/2-5-scale.html#recommendation-2-get-as-much-ram-as-feasible",
    "href": "chapters/sec2/2-5-scale.html#recommendation-2-get-as-much-ram-as-feasible",
    "title": "11  Server Resources and Scaling",
    "section": "11.5 Recommendation 2: Get as much RAM as feasible",
    "text": "11.5 Recommendation 2: Get as much RAM as feasible\nIn most cases, R and Python must load all your data into memory. Thus, the data you can use is limited to your machine’s RAM.\nMost other machine limits will result in work completing slower than you might like, but trying to load too much data into memory will make your session crash.\n\n\n\n\n\n\nNote\n\n\n\nIf you’re facing this limitation, reconsider your project architecture as discussed in Chapter 2. Maybe you can load less data into memory.\n\n\nBecause your computer needs memory for things other than R and Python, and because you’ll often be doing transformations that temporarily increase the size of your data, you need more memory than your largest data set.\nNobody has ever complained about having too much RAM, but a good rule of thumb is that you’ll be happy if:\n\\[\\text{Amount of RAM} \\ge 3 * \\text{max amount of data}\\]\nIf you’re considering running a multi-user server, you’ll need to take a step back to think about how many concurrent users you expect and how much data you anticipate each one to load."
  },
  {
    "objectID": "chapters/sec2/2-5-scale.html#understanding-storage",
    "href": "chapters/sec2/2-5-scale.html#understanding-storage",
    "title": "11  Server Resources and Scaling",
    "section": "11.6 Understanding storage",
    "text": "11.6 Understanding storage\nStorage, or hard disk/drive, is where your computer stores things for the long term. It’s where applications are installed, and you save objects you want to keep.\nRelative to the RAM right next to the factory floor, your computer’s storage is like the warehouse in the next building. Storage is much slower than RAM, often 10x to 100x slower, but storage allows you to save things permanently.\nStorage was even slower until a few years ago when solid-state drives (SSDs) became common. SSDs are collections of flash memory chips up to 15x faster than the hard disk drives (HDDs) that preceded them.\nHDDs consist of spinning magnetic disks with magnetized read/write heads that save and read data from the disks. While HDDs spin very fast – 5,400 and 7,200 RPM are typical speeds – SSDs with no moving parts are much faster."
  },
  {
    "objectID": "chapters/sec2/2-5-scale.html#recommendation-3-get-lots-of-storage-its-cheap",
    "href": "chapters/sec2/2-5-scale.html#recommendation-3-get-lots-of-storage-its-cheap",
    "title": "11  Server Resources and Scaling",
    "section": "11.7 Recommendation 3: Get lots of storage; it’s cheap",
    "text": "11.7 Recommendation 3: Get lots of storage; it’s cheap\nYou should get plenty when configuring storage on your server, but don’t think too hard since storage is cheap and easy to upgrade. It’s almost always more cost-effective to buy additional storage rather than have an expensive professional figure out how to delete things to free up room.\n\n\n\n\n\n\nNote\n\n\n\nIf the IT/Admins at your organization want you to spend a lot of time deleting things from storage, that’s usually a red flag that they aren’t thinking much about how to make the overall organization work more smoothly.\n\n\nIf you’re running a multi-user server, the amount of storage you need depends significantly on your data and workflows. If you’re not saving large data files, the amount of space each person needs on the server is small. Code files are negligible, and it’s rare to see R and Python packages take up more than a few dozen Mb per data scientist. A reasonable rule of thumb is to choose\n\\[\n\\text{Amount of Storage} = \\text{1Gb} * \\text{n users}\n\\]\nOn the other hand, if your workflows save a lot of data to disk, you must consider that. In some organizations, each data scientist will save dozens of flat files of a GB or more for each project.\n\n\n\n\n\n\nNote\n\n\n\nIf you’re working with a professional IT admin, they may be concerned about the storage implications of having package copies for each team member, a best practice for using environments as code as discussed in Chapter 1. I’ve frequently heard this concern from IT/Admins thinking ahead about running their server but rarely encountered a case where it’s actually been a problem.\n\n\nIf you’re operating in the cloud, this isn’t an important choice. As you’ll see in the lab, upgrading the amount of storage you have is a trivial operation, requiring at most a few minutes of downtime. Choose a size you estimate will be adequate and add more if needed."
  },
  {
    "objectID": "chapters/sec2/2-5-scale.html#gpus-are-special-purpose-compute",
    "href": "chapters/sec2/2-5-scale.html#gpus-are-special-purpose-compute",
    "title": "11  Server Resources and Scaling",
    "section": "11.8 GPUs are special-purpose compute",
    "text": "11.8 GPUs are special-purpose compute\nAll computers have a CPU. Some computers have specialized chips where the CPU can offload particular tasks – the most common being the graphical processing unit (GPU). GPUs are architected for tasks like editing photos or videos, rendering video game graphics, some kinds of machine learning, and, yes, Bitcoin mining.3\nA GPU is an addition factory just like a CPU, but with the opposite architecture. CPUs have only a handful of cores, but those cores are fast. A GPU takes the opposite approach with many (relatively) slow cores.\nWhere a consumer-grade CPU has 4-16 cores, mid-range GPUs have 700-4,000 cores, each running at only about 1% to 10% of the speed of a CPU core. For the tasks GPUs are good at, the overwhelming parallelism ends up being more important than the speed of any individual core, and GPU computation can be dramatically faster."
  },
  {
    "objectID": "chapters/sec2/2-5-scale.html#recommendation-4-get-a-gpu-maybe",
    "href": "chapters/sec2/2-5-scale.html#recommendation-4-get-a-gpu-maybe",
    "title": "11  Server Resources and Scaling",
    "section": "11.9 Recommendation 4: Get a GPU, maybe",
    "text": "11.9 Recommendation 4: Get a GPU, maybe\nThe tasks that most benefit from GPU computing include training highly parallel machine learning models like deep learning or tree-based models. If you have one of these use cases, GPU computing can massively speed up your computation – making models trainable in hours instead of days.\nIf you plan to use cloud resources for your computing, large GPU-backed instances are pricey (hundreds of dollars an hour as of this writing). You’ll want to be careful about only putting those machines up when using them.\nBecause GPUs are expensive, I generally wouldn’t bother with GPU-backed computing unless you’ve already tried without and find that it takes too long to be feasible.\nIt’s also worth noting that using a GPU won’t happen automatically. The tooling has gotten good enough that it’s usually easy to set up, but your computer won’t train your XGBoost models on your GPU unless you tell it to do so.\nNow that you’re equipped with some general recommendations about choosing the right amount of resources, let’s learn how to tell whether it might be time to upgrade a system you already have."
  },
  {
    "objectID": "chapters/sec2/2-5-scale.html#assessing-ram-cpu-usage",
    "href": "chapters/sec2/2-5-scale.html#assessing-ram-cpu-usage",
    "title": "11  Server Resources and Scaling",
    "section": "11.10 Assessing RAM + CPU usage",
    "text": "11.10 Assessing RAM + CPU usage\nOnce you’ve chosen your server size and gotten up and running, you’ll want to be able to monitor RAM and CPU for problems.\nA running program is running is called a process. For example, when you type python on the command line to open a REPL, that starts a single Python process. If you were to start a second terminal session and run python again, you’d have a second Python process.\nComplicated programs often involve multiple interlocking processes. For example, running the RStudio IDE involves (at minimum) one process for the IDE itself and one for the R session it uses in the background. The relationships between these processes are mostly hidden from you – the end user.\nAs an admin, you may want to inspect the processes running on your system at any given time. The top command is a good first stop. top shows information about the processes consuming the most CPU in real-time.\nHere’s the top output from my machine as I write this sentence.\n\n\nTerminal\n\nPID    COMMAND      %CPU TIME     #TH    #WQ  #PORT MEM    PURG   CMPRS PGRP\n0      kernel_task  16.1 03:56:53 530/10 0    0     2272K  0B     0B    0\n16329  WindowServer 16.0 01:53:20 23     6    3717  941M-  16M+   124M  16329\n24484  iTerm2       11.3 00:38.20 5      2    266-  71M-   128K   18M-  24484\n29519  top          9.7  00:04.30 1/1    0    36    9729K  0B     0B    29519\n16795  Magnet       3.1  00:39.16 3      1    206   82M    0B     39M   16795\n16934  Arc          1.8  18:18.49 45     6    938   310M   144K   61M   16934\n\nIn most instances, the first three columns are the most useful. The first column is the unique process ID (pid) for that process. You’ve got the name of the process (COMMAND) and how much CPU it’s using. You’ve also got the amount of memory used a few columns over. Right now, nothing is using a lot of CPU.\nThe top command takes over your whole terminal. You can exit with Ctrl + c.\n\n\n\n\n\n\nSo much CPU?\n\n\n\nFor top (and most other commands), CPU is expressed as a percent of single core availability. On a modern machine with multiple cores, it’s very common to see CPU totals well over 100%. Seeing a single process using over 100% of CPU is rare.\n\n\nAnother useful command for finding runaway processes is ps aux. It lists a snapshot of all processes running on the system and how much CPU or RAM they use. You can sort the output with the --sort flag and specify sorting by CPU with --sort -%cpu or by memory with --sort -%mem.\nBecause ps aux returns every running process on the system, you’ll probably want to pipe the output into head. In addition to CPU and Memory usage, ps aux tells you who launched the command and the PID.\nOne of the times you’ll be most interested in the output of top or ps aux is when something is going rogue on your system and using more resources than you intended. If you have some sense of the name or who started it, you may want to pipe the output of ps aux into grep to find the pid.\nFor example, I might run ps aux | grep RStudio to get4\n\n\nTerminal\n\n&gt; ps aux | grep RStudio\nUSER      PID   %CPU %MEM STARTED TIME     COMMAND\nalexkgold 23583 0.9  1.7  Sat09AM 17:15.27 /Applications/RStudio.app/RStudio\nalexkgold 23605 0.5  0.4  Sat09AM  1:58.16 /Applications/RStudio.app/rsession\n\nRStudio is behaving nicely on my machine, but if it were not responsive, I could make a note of its pid and end the process immediately by calling the kill command with the pid."
  },
  {
    "objectID": "chapters/sec2/2-5-scale.html#examining-at-storage-usage",
    "href": "chapters/sec2/2-5-scale.html#examining-at-storage-usage",
    "title": "11  Server Resources and Scaling",
    "section": "11.11 Examining at storage usage",
    "text": "11.11 Examining at storage usage\nA common culprit for weird server behavior is running out of storage space. There are two handy commands for monitoring the amount of storage you’ve got: du and df. These commands are almost always used with the -h flag to put file sizes in human-readable formats.\ndf (disk free) shows the capacity left on the device where the directory sits. For example, here’s the result of running the df command on the chapters directory on my laptop that includes this chapter.\n\n\nTerminal\n\n&gt; df -h chapters\nFilesystem     Size   Used  Avail Capacity iused      ifree %iused  Mounted on\n/dev/disk3s5  926Gi  227Gi  686Gi    25% 1496100 7188673280    0%   /System/Volumes/Data\n\nYou can see that the chapters folder lives on a disk called /dev/disk3s5 that’s a little less than 1Tb and is 25% full – no problem. This can be particularly useful to know on a cloud server because switching a disk out for a bigger one in the same spot is easy.\nIf you’ve figured out that a disk is full, buying a bigger one is usually the most cost-effective. But sometimes something weird happens. Maybe there are a few exceptionally big files, or you think unnecessary copies are being made.\nIf so, the du command (disk usage) gives you the size of individual files inside a directory. It’s particularly useful in combination with the sort command. For example, here’s the result of running du on the chapters directory where the text files for this book live.\n\n\nTerminal\n\n&gt; du -h chapters | sort\n12M chapters\n1.7M    chapters/sec1/images\n1.8M    chapters/sec1\n236K    chapters/images\n488K    chapters/sec2/images-traffic\n5.3M    chapters/sec2/images-networking\n552K    chapters/sec2/images\n6.6M    chapters/sec2\n892K    chapters/append/images\n948K    chapters/append\n\nIf I were thinking about cleaning up this directory, I could see that my sec1/images directory is my biggest single directory. If you need to find big files on your Linux server, it’s worth looking through the options in the help pages for du."
  },
  {
    "objectID": "chapters/sec2/2-5-scale.html#running-out-of-resources",
    "href": "chapters/sec2/2-5-scale.html#running-out-of-resources",
    "title": "11  Server Resources and Scaling",
    "section": "11.12 Running out of Resources",
    "text": "11.12 Running out of Resources\nIf you recognize you’re running out of resources on your current server, you may want to move to something bigger. There are two primary reasons servers run out of room.\nThe first is because people are running big jobs. This can happen at any scale of organization. There are data science teams of one person with use cases that necessitate terabytes of data.\nThe second reason is because you have many people using your server. This is generally a feature of big data science teams, irrespective of workload size.\nEither way, there are two options for how to scale your data science workbench. The first is vertical scaling, which is a fancy way to say get a bigger server. The second option is horizontal scaling, which means running a whole fleet of servers in parallel and spreading the workload across them.\nAs a data scientist, you shouldn’t be shy about vertically scaling if your budget allows it. The complexity of managing a t3.nano with two cores and 0.5 GB of memory is the same as a C5.24xlarge with 96 cores and 192 GB of memory. In fact, the bigger one may be easier to manage since you won’t have to worry about running low on resources.\nThere are limits to the capacity of vertical scaling. As of this writing, AWS’s general-use instance types max out at 96-128 cores. That can quickly get eaten up by 50 data scientists with reasonably heavy computational demands.\nOnce you’re thinking about horizontal scaling, you’ve got a distributed service problem, which is inherently complex. You should almost certainly get an IT/Admin professional involved. See Chapter 17 for more on how to talk to them about it.\n\n11.12.1 AWS Instances for data science\nAWS offers various EC2 instance types split up by family and size. The family is the category of EC2 instance. Different families of instances are optimized for different kinds of workloads.\nHere’s a table of common instance types for data science purposes:\n\n\n\n\n\n\n\nInstance Type\nWhat it is\n\n\n\n\nt3\nThe “standard” configuration. Relatively cheap. Sizes may be limited.\n\n\nC\nCPU-optimized instances, aka faster CPUs.\n\n\nR\nHigher ratio of RAM to CPU relative to t3.\n\n\nP\nGPU instances, very expensive.\n\n\n\nWithin each family, there are different sizes available, ranging from nano to multiples of xl. Instances are denoted by &lt;family&gt;.&lt;size&gt;. For example, when we put our instance originally on a free tier machine, we put it on a t2.micro.\nIn most cases, going up a size doubles the amount of RAM, the number of cores, and the hourly cost. You should do some quick math before you stand up a C5.24xlarge or a GPU-based P instance. If your instance won’t be up for very long, it may be fine, but make sure you take it down when you’re done, lest you rack up a huge bill."
  },
  {
    "objectID": "chapters/sec2/2-5-scale.html#comprehension-questions",
    "href": "chapters/sec2/2-5-scale.html#comprehension-questions",
    "title": "11  Server Resources and Scaling",
    "section": "11.13 Comprehension Questions",
    "text": "11.13 Comprehension Questions\n\nThink about the scenarios below. Which part of your computer would you want to upgrade to solve the problem?\n\nYou try to load a .csv file into {pandas} in Python. It churns for a while and then crashes.\nYou go to build a new ML model on your data. You’d like to re-train the model once a day, but training this model takes 26 hours on your laptop.\nYou design a visualization using the {matplotlib} package and want to create one version of the visualization for each US State. You could do it in a loop, but it would be faster to parallelize the plot creation. Right now, you’re running on a t2.small with 1 CPU.\n\nDraw a mind map of the following: CPU, RAM, Storage, Operations Per Second, Parallel Operations, GPU, Machine Learning\nWhat are the architectural differences between a CPU and a GPU? Why does this make a GPU particularly good for Machine Learning?\nHow would you do the following:\n\nFind all running Jupyter processes that belong to the user alexkgold.\nFind the different disks attached to your server and see how full each one is.\nFind the biggest files in each user’s home directory."
  },
  {
    "objectID": "chapters/sec2/2-5-scale.html#lab-changing-instance-size",
    "href": "chapters/sec2/2-5-scale.html#lab-changing-instance-size",
    "title": "11  Server Resources and Scaling",
    "section": "11.14 Lab: Changing Instance Size",
    "text": "11.14 Lab: Changing Instance Size\nIn this lab, we will upgrade the size of our server. And the best part is that we’re in the cloud, so it will only take a few minutes.\n\n11.14.1 Step 1: Confirm the current server size\nFirst, let’s confirm what we’ve got available. Once you ssh into the server, you can check the number of CPUs you’ve got with lscpu in a terminal. Similarly, you can check the amount of RAM with free -h. This is so that you can prove to yourself later that the instance changed.\n\n\n11.14.2 Step 2: Change the instance type and bring it back\nNow, you can go to the instance page in the AWS console. The first step is to stop (not terminate!) the instance. This means that changing instance type requires some downtime, but it’s brief.\nOnce the instance has stopped, you can change the instance type under Actions &gt; Instance Settings. Then, start the instance. It’ll take a few seconds.\n\n\n11.14.3 Step 3: Confirm the new server size\nFor example, I changed from a t2.micro to a t2.small. Both only have 1 CPU, so I won’t see any difference in lscpu, but running free -h before and after the switch reveals that I’ve got more RAM:\ntest-user@ip-172-31-53-181:~$ free -h\n               total        used        free      shared  buff/cache   available\nMem:           966Mi       412Mi       215Mi       0.0Ki       338Mi       404Mi\nSwap:             0B          0B          0B\ntest-user@ip-172-31-53-181:~$ free -h\n               total        used        free      shared  buff/cache   available\nMem:           1.9Gi       225Mi       1.3Gi       0.0Ki       447Mi       1.6Gi\nSwap:             0B          0B          0B\nThere’s twice as much after the change!\nThere are some rules around being able to change from one instance type to another, but this is a superpower if you’ve got variable workloads or a team that’s growing. Once you’re done with your larger server, it’s just as easy to scale it back down.\n\n\n11.14.4 Step 4: Upgrade storage (maybe)\nIf you want more storage, resizing the EBS volume attached to your server is similarly straightforward.\nI wouldn’t recommend doing it for this lab because you can only automatically adjust volume sizes upwards. That means you’d have to manually transfer your data if you ever wanted to scale back down.\nIf you do resize the volume, you’ll have to let Linux know so it can resize the filesystem with the new space available. AWS has a great walkthrough called Extend a Linux filesystem after resizing the volume that I recommend you follow."
  },
  {
    "objectID": "chapters/sec2/2-5-scale.html#footnotes",
    "href": "chapters/sec2/2-5-scale.html#footnotes",
    "title": "11  Server Resources and Scaling",
    "section": "",
    "text": "This was proved in Alan Turing’s 1936 paper on computability. If you’re interested in learning more, I recommend The Annotated Turing: A Guided Tour Through Alan Turing’s Historic Paper on Computability and the Turing Machine by Charles Petzold for a surprisingly readable walkthrough.↩︎\nYou probably don’t experience this. Modern computers are pretty smart about dumping RAM onto the hard disk before shutting down and bringing it back on startup, so you usually won’t notice this happening.↩︎\nPurpose-built chips are becoming more common for AI/ML tasks, especially doing local inference on large models. These include Tensor Processing Units (TPUs) and Intelligence Processing Units (IPUs).↩︎\nI’ve done a bunch of doctoring to the output to make it easier to read.↩︎"
  },
  {
    "objectID": "chapters/sec2/2-6-networking.html#understanding-digital-addresses",
    "href": "chapters/sec2/2-6-networking.html#understanding-digital-addresses",
    "title": "12  Computer Networks",
    "section": "12.1 Understanding digital addresses",
    "text": "12.1 Understanding digital addresses\nYou already use digital addresses all the time in the form of a URL (Uniform Resource Locator).1 A URL has four parts that fully specify the network location of a resource.2 A full URL looks like this:\n\\[\\overbrace{\\text{https://}}^\\text{protocol}\\underbrace{\\text{google.com}}_\\text{domain}\\overbrace{\\text{:443}}^\\text{port}\\underbrace{\\text{/}}_\\text{path}\\]\nHere’s what each of those four parts are:\n\nThe application layer protocol (often just called the protocol) specifies what type of traffic this is. It’s like agreeing that your letter will be in English or Arabic or Dutch.\nThe domain is a human-readable way of providing the digital street address of the server.\nThe port specifies where on the server to direct the traffic. It’s the digital equivalent of the apartment number.\nThe path is a human-friendly way of specifying who you intend the message to go to. It’s like the addressee’s name on your letter.\n\nThis may look a little strange. You’re probably accustomed to using just the domain and possibly a path in your web browser like \\(\\text{google.com}\\) or \\(\\text{google.com/maps}\\). The reason is that you’re usually fine with the default protocol and port, so you may never have realized they exist."
  },
  {
    "objectID": "chapters/sec2/2-6-networking.html#network-traffic-and-ip-addresses",
    "href": "chapters/sec2/2-6-networking.html#network-traffic-and-ip-addresses",
    "title": "12  Computer Networks",
    "section": "12.2 Network traffic and IP Addresses",
    "text": "12.2 Network traffic and IP Addresses\nA domain is the human-readable way of addressing a resource on the internet. But it’s not actually a digital street address. Instead, server (host) locations are identified with an IP Address. When that IP address is valid across the entire internet, it’s a public IP address. Otherwise, it’s private.\nWhen your computer needs to send a message, it turns that message into one or more packets, addresses them to a target IP Address, and sends them off. Then a system of hardware and software tools called routers is responsible for getting the packets to the right place in a process called packet switching.3 Packet switching is the digital equivalent of getting the mail to the right building.\n\n\n\n\n\n\nNote\n\n\n\nYour computer gets the right IP Address from the domain using the Domain Name Service (DNS), which you’ll learn about in Chapter 13\n\n\nIf you’ve seen an IP Address before, it probably was an IPv4 address. These are four blocks of 8-bit fields (numbers between \\(0\\) and \\(255\\)) with dots in between, so they look like \\(64.56.223.5\\).\nIf you do the math, you’ll realize there are “only” about 4 billion of these. There are so many things on the public internet that we are running out of IPv4 addresses. The good news is that smart people started planning for this a while ago and the adoption of the new IPv6 standard started a few years ago.\nIPv6 addresses are eight blocks of hexadecimal (\\(\\text{0-9}\\) and \\(\\text{a-f}\\)) digits separated by colons with certain rules that allow them to be shortened, so \\(\\text{4b01:0db8:85a3:0000:0000:8a2e:0370:7334}\\) or \\(\\text{3da4:66a::1}\\) are both examples of valid IPv6 addresses. There’s no worry about running out of IPv6 addresses any time soon, because the total quantity of IPv6 addresses is a number with 39 zeroes.\nIPv6 will coexist with IPv4 for a few decades; at some point we’ll switch entirely to IPv6.\nThere are a few special IPv4 addresses it’s worth knowing. You’ll probably see \\(127.0.0.1\\) a lot, which is also known as \\(\\text{localhost}\\) or loopback. \\(\\text{localhost}\\) is how a machine sends traffic back to itself.\nFor example, if you open a Shiny app in RStudio Desktop, the app will pop up in a little window along with a notice that says\n\n\nTerminal\n\nListening on http://127.0.0.1:6311\n\nThat means that the Shiny app is running on the same computer and is available on port \\(6311\\). You can open that location in your browser to view the app as it runs.\nThere are also a few blocks of IPv4 addresses; those that start with \\(192.168\\), \\(172.16\\), and \\(10\\) that are reserved for use on private networks, so they’re never assigned in public."
  },
  {
    "objectID": "chapters/sec2/2-6-networking.html#services-and-ports",
    "href": "chapters/sec2/2-6-networking.html#services-and-ports",
    "title": "12  Computer Networks",
    "section": "12.3 Services and ports",
    "text": "12.3 Services and ports\nOnce the traffic arrives at the server, it must find the right service at the right port. This is the digital equivalent of putting the mail in the right mailbox in the building’s mail room. Every port is uniquely identified by a number. There are over 65,000 ports on every computer. Since you’re probably running no more than a handful of services, the overwhelming majority of the ports are closed at any given time.4\nAs an admin, one of your primary responsibilities is to make sure that incoming traffic gets to the right port for the intended service.\nIf you’ve got just one service, you will probably just move the service to run where the traffic is going anyway. By default, HTTP traffic goes to port \\(80\\) and HTTPS goes to port \\(443\\). So you would configure the application to listen on port \\(80\\) and/or \\(443\\) so incoming web traffic would automatically get to the right service.\nBut sometimes you’ve got multiple services on the server. Since there’s a 1-1 mapping for ports and services, you can’t run multiple services on the same port and you don’t want users to have to remember non-standard ports. In that case, you run each service on a unique port and make the traffic go to that port.\nFor example, maybe you have RStudio and JupyterHub running on the same server. If you run them on their default ports, RStudio will be on port \\(8787\\) and JupyterHub will be on \\(8000\\).5 Somehow you’ve got to smoothly redirect user traffic, which comes in on \\(80\\) or \\(443\\), to those other ports. The most common configuration is to use a piece of technology called a proxy to put each service on a path. For example, you might configure the path \\(\\text{/rstudio}\\) to go to port \\(8787\\) on the server and the \\(\\text{/jupyter}\\) path to go to \\(8000\\).\nThere are free and open-source proxies, like NGINX and Apache. There are also paid proxies like Citrix, Fortinet, Cloudflare, and F5 (they maintain NGINX). Depending on the configuration, the proxy can be on the same server as your services or a different one.\nIn many cases, including using an EC2 instance, simply opening port \\(80\\) or \\(443\\) on the server or proxy still won’t allow you to access the server from the web. That’s because there’s a firewall sitting in front of the server which blocks traffic to all but certain ports. Before accessing anything on the server, you’ll need to configure the firewall to allow traffic on port \\(80\\) or \\(443\\).\nIn addition to blocking traffic to arriving at certain ports, firewalls can be restricted to allow access only from certain IP Addresses. This can be used, for example, to only allow access from your office to a server. Unless a particular server will only ever be accessed by other servers with known IP addresses, this is a brittle way to configure security and I generally don’t recommend it.\n\n\n\n\n\n\nTip\n\n\n\nIn AWS, the default firewall is called the security group. The default security group accepts traffic only on port \\(22\\), which is the default for ssh. If you think you’ve configured a service correctly and you just can’t seem to access it, one of the first things to check is whether you’ve got the port open in the security group.\nOne symptom indicating a possible security group issue is if you try to go to your service and it hangs with no response before eventually timing out.\n\n\n\nWe’ve been talking exclusively about HTTP and HTTPS traffic arriving on \\(80\\) and \\(443\\), because web traffic arrives as a series of HTTP GET requests; there are many other application layer protocols, each with its own default port.\nFor example, in this book, you’ve already seen a lot about SSH which is an application layer protocol for allowing secure login and communication over an unsecured network. SSH defaults to port \\(22\\). Some protocols piggyback on others. For example, SFTP for file transfers secured with SSH also use \\(22\\), and websockets, which are used by Shiny and Streamlit, use standard HTTP(S) ports.\nOther protocols have their own ports, like SMTP for emails (\\(465\\), \\(587\\), or \\(25\\)) and LDAP(S) for auth (\\(389\\) or \\(636\\)). There’s a list of relevant protocols and applications with their standard ports in the cheatsheets in Appendix D."
  },
  {
    "objectID": "chapters/sec2/2-6-networking.html#basic-network-administration",
    "href": "chapters/sec2/2-6-networking.html#basic-network-administration",
    "title": "12  Computer Networks",
    "section": "12.4 Basic network administration",
    "text": "12.4 Basic network administration\nAs you’ve probably gathered, there are many layers to networking which can make it difficult to manage. You might think you’ve configured things correctly, but the traffic just isn’t flowing and it’s not clear where the issue is. Here are some basic tools for network debugging.\n\n12.4.1 Browser devtools\nOne of the most useful tools for debugging networking issues can be found in the menus of your web browser. Your browser has developer tools that allow you to inspect the network traffic going out from your browser and coming back.\nThis can be really handy if things a website is loading slowly or if you’re not sure why a page isn’t loading. By inspecting the status codes of different HTTP calls, the headers on those calls, and the time they take, you can develop a pretty good idea of where things might be getting stuck.\n\n\n12.4.2 SSH tunneling/port forwarding\nWhen you start a new EC2 instance in AWS, the default security group has only port \\(22\\) open, allowing only SSH traffic. So far, you’ve seen SSH used to access the command line on that remote server, but SSH can actually be used to access any port in a process called tunneling or port forwarding.\nWhen you tunnel, you make a port on the remote host available at the same port on \\(\\text{localhost}\\) on your machine. The most common usage is to inspect an HTTP-based service in a browser without configuring the host to accept HTTP traffic on that port.\nYou can create an SSH tunnel to a remote host with\n\n\nTerminal\n\nssh -L &lt;remote port&gt;:localhost:&lt;local port&gt; &lt;user&gt;@&lt;server&gt;\n\nI find that the syntax for port forwarding completely defies my memory and I need to google it every time I use it.6\nSo, for example, if your server were running at \\(64.56.223.5\\) and you have the SSH user test-user, you might forward JupyterHub on port \\(8000\\) with ssh -L 8000:localhost:8000 test-user@64.56.223.5. Once the tunnel was established, you could access JupyerHub in your browser on \\(\\text{localhost:8000}\\).\n\n\n12.4.3 Checking what ports are open\nSometimes you just forget what ports are open on your machine and for what purposes. Or, you want to double check that a configuration change took. In that case, you want to use the netstat command to get the services that are running and their associated ports.\nFor this purpose, netstat is generally most useful with the -tlp flags to show ports that are open and the programs associated.\n\n\n12.4.4 Checking if a host is accessible\nThe ping command can be useful for checking whether your server is reachable at all. For example, the server where this book lives is at \\(185.199.110.153\\). So I can ping that domain to check if it’s accessible.\n\n\nTerminal\n\n&gt; ping -o 185.199.110.153\n\nPING 185.199.110.153 (185.199.110.153): 56 data bytes\n64 bytes from 185.199.110.153: icmp_seq=0 ttl=58 time=23.322 ms\n\n--- 185.199.110.153 ping statistics ---\n1 packets transmitted, 1 packets received, 0.0% packet loss\nround-trip min/avg/max/stddev = 23.322/23.322/23.322/nan ms\n\nThe -o flag tells ping to try just once as opposed to pinging continuously. The fact that I transmitted and received one packet means that everything is working properly.\nSeeing an unreachable host or packet loss would indicate that my networking probably isn’t configured correctly somewhere between me and the server. That means it’s time to check that the server is actually up, followed by firewalls (security groups), and proxies. You can also use ping with a domain, so it can also be used to see if DNS is working properly.\nIf ping succeeds but a particular resource is inaccessible, it’s often helpful to try curl. curl actually attempts to fetch the website at a particular URL. For this purpose, the -I flag, which returns a simple status report rather than the full webpsage, is useful.\nFor example, here’s what I get when I curl the website for this book.\n\n\nTerminal\n\n&gt; curl -I https://do4ds.com                                         \n\nHTTP/2 200\nserver: GitHub.com\ncontent-type: text/html; charset=utf-8\nlast-modified: Tue, 04 Jul 2023 16:23:38 GMT\naccess-control-allow-origin: *\netag: \"64a4478a-79cb\"\n...\n\nThe important thing here is that first line. The server is returning a 200 HTTP status code, which means all is well. If you get something else, take a look at the http code cheatsheet in Appendix D.\nIf ping succeeds, but curl does not, it means that the server is up, but the path or port is incorrect. If you’re running inside a container, you should check that you’ve properly configured the port inside the container to be forwarded to the outside."
  },
  {
    "objectID": "chapters/sec2/2-6-networking.html#comprehension-questions",
    "href": "chapters/sec2/2-6-networking.html#comprehension-questions",
    "title": "12  Computer Networks",
    "section": "12.5 Comprehension Questions",
    "text": "12.5 Comprehension Questions\n\nWhat are the 4 components of a URL? What’s the significance of each?\nWhen you configure a service on a server, how do you get it on the right port?\nDraw a mind map of trying to access the following in your browser. Include the following terms: URL, domain, IP Address, port, path, \\(80\\), \\(443\\), 8000, proxy, server, HTTP, HTTPS, status code, protocol\n\nA Shiny app on a server at \\(\\text{http://my-shiny.com}\\) where Shiny Server is sitting on port \\(80\\).\nJupyterHub on a server at \\(\\text{https://example.com/jupyter}\\) where Jupyter is on port \\(8000\\)."
  },
  {
    "objectID": "chapters/sec2/2-6-networking.html#lab-making-it-accessible-in-one-place",
    "href": "chapters/sec2/2-6-networking.html#lab-making-it-accessible-in-one-place",
    "title": "12  Computer Networks",
    "section": "12.6 Lab: Making it accessible in one place",
    "text": "12.6 Lab: Making it accessible in one place\nIn this lab, we’re going to set up a proxy to be able to access all of our services over HTTP.\nBut first, you might want to try out accessing the various services where they are.\nYou could either try SSH tunneling to them and seeing them on localhost or you could apply custom TCP rules to your security group to temporarily allow access directly to those ports. If you want to try, here’s a reminder of where everything is:\n\n\n\nService\nPort\n\n\n\n\nJupyterHub\n\\(8000\\)\n\n\nRStudio\n\\(8787\\)\n\n\nPenguin model API\n\\(8080\\)\n\n\nShiny App\n\\(3838\\)\n\n\n\nOnce you’re finished playing, make sure to change your security group rules back.\n\n12.6.1 Step 1: Configure NGINX\nConfiguring proxies is an advanced networking topic. In most cases you’d just put one service per server. But if you want to be able to save money by running everything on one server, you’ll need a proxy.\nGetting NGINX is straightforward: you install NGINX, put the configuration file into place, and restart the service to pick up the changes. The hard part is figuring out the right configuration. Configuring proxies can be quite painful as the configuration is very sensitive to seemingly meaningless syntax issues.\nHere are the steps to configure your proxy on your server for JupyterHub and RStudio Server:\n\nSSH into your server.\nInstall NGINX with sudo apt install nginx.\nSave a backup of the default nginx.conf, cp /etc/nginx/nginx.conf /etc/nginx/nginx-backup.conf.7\nEdit the NGINX configuration with sudo vim /etc/nginx/nginx.conf and replace it with:\n\n\n\n/etc/nginx/nginx.conf\n\nuser www-data;\nworker_processes auto;\npid /run/nginx.pid;\ninclude /etc/nginx/modules-enabled/*.conf;\n\nevents {\n\tworker_connections 768;\n\t# multi_accept on;\n}\n\nhttp {\n\n map $http_upgrade $connection_upgrade {\n    \t\tdefault upgrade;\n    \t\t''      close;\n  }\n\n  server {\n    listen 80;\n\n    location /rstudio/ {\n      # Needed only for a custom path prefix of /rstudio\n      rewrite ^/rstudio/(.*)$ /$1 break;\n\n      # Use http here when ssl-enabled=0 is set in rserver.conf\n      proxy_pass http://localhost:8787;\n\n      proxy_http_version 1.1;\n      proxy_set_header Upgrade $http_upgrade;\n      proxy_set_header Connection $connection_upgrade;\n      proxy_read_timeout 20d;\n\n      # Not needed if www-root-path is set in rserver.conf\n      proxy_set_header X-RStudio-Root-Path /rstudio;\n\n      # Optionally, use an explicit hostname and omit the port if using 80/443\n      proxy_set_header Host $host:$server_port;\n    }\n\n    location /jupyter/ {\n      # NOTE important to also set bind url of jupyterhub to /jupyter in its config\n      proxy_pass http://127.0.0.1:8000;\n\n      proxy_redirect   off;\n      proxy_set_header X-Real-IP $remote_addr;\n      proxy_set_header Host $host;\n      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n      proxy_set_header X-Forwarded-Proto $scheme;\n\n      # websocket headers\n      proxy_set_header Upgrade $http_upgrade;\n      proxy_set_header Connection $connection_upgrade;\n    }\n  }\n}\n\n\n\nTest that your configuration is valid sudo nginx -t.\nStart NGINX with sudo systemctl start nginx. If you see nothing all is well.\n\nIf you need to change anything, update the config and then restart with sudo systemctl restart nginx.\n\n\n12.6.2 Step 2: Configure the security group\nIf you try to visit to your your server’s public IP address or DNS in your browser, your browser will spin for a while and nothing will happen. That’s because the AWS security group still only allows SSH access on port \\(22\\). We need to add a rule that will allow HTTP access on port \\(80\\).\nOn the AWS console page for your instance, find the Security section and click into the security group for your instance. You want to add a new inbound HTTP rule that allows access on port \\(80\\) from anywhere. Make sure not to get rid of the rule that allows SSH access on \\(22\\). You still need that one too.\nOnce you do this, you should be able to visit your server address and get the default NGINX landing page.\n\n\n12.6.3 Step 3: Configure your applications\nComplex web apps like RStudio and JupyterHub frequently proxy traffic back to themselves. For example, when you launch a Shiny app in RStudio, you’re just opening a “headless” browser window that gets proxied back into your session.\nThis works by default when those apps are on the root path \\(\\text{/}\\). We’re running RStudio and JupyterHub on subpaths, so we’ve got to let the services know where they’re located.\nConfiguring RStudio Server is already done. The X-RStudio-Root-Path line in the NGINX configuration adds a header to each request coming through the proxy that tells RStudio Server that it’s on the \\(\\text{/rstudio}\\) path.\nJupyterHub needs a configuration update to let it know that it’s on a subpath. Luckily it’s a very simple change. You can edit the Jupyter configuration with\n\n\nTerminal\n\nsudo vim /etc/jupyterhub/jupyterhub_config.py\n\nFind the line that reads # c.JupyterHub.bind_url = 'http://:8000'.\n\n\n\n\n\n\nTip\n\n\n\nYou can search in vim from normal mode with / &lt;thing you're searching for&gt;. Go to the next hit with n.\n\n\nDelete the # to uncomment the line and add the subpath on the end. If you’re using the \\(\\text{/jupyter}\\) subpath and the default \\(8000\\) port, that line will read c.JupyterHub.bind_url = 'http://:8000/jupyter'.\nJupyterHub should pick up the new config when it’s restarted with\n\n\nTerminal\n\nsudo systemctl restart jupyterhub\n\n\n\n12.6.4 Step 4: Try it out!\nNow we should have each service configured on a subpath. RStudio Server at \\(\\text{/rstudio}\\), JupyterHub at \\(\\text{/jupyter}\\). For example, with my server at \\(\\text{64.56.223.5}\\), I can get to RStudio Server at \\(\\text{http://64.56.223.5/rstudio}\\).\nRight now this server is on HTTP, which is not a best practice. In fact, it’s such a bad practice that your browser will probably autocorrect the url to https and you’ll have to manually correct it back to http and ignore some scary warnings. Don’t worry, we’ll fix this in Chapter 14.\n\n\n12.6.5 Lab Extensions\nIf you’ve gone to the root URL for your server, you’ve probably noticed that it’s just the default NGINX landing page which is not very attractive. Consider creating and serving a static landing page at / with links to the other services. Or maybe you want one of the services at \\(\\text{/}\\) and the others at a different subpath. You should have the tools to reconfigure NGINX to accomplish that.\nRight now, neither the penguins model API nor the Shiny app are available from the outside. You might want to add them to the proxy to make them accessible. I’ll leave that as an exercise for you.\n\n\n\n\n\n\nTip\n\n\n\nIt’s very common to put an API and/or a Shiny app behind a proxy. Googling “Shiny app behind nginx” or “FastAPI with nginx” will yield good results.\n\n\nOne thing to consider is whether the model API should be publicly accessible at all. If the only thing calling it is the Shiny app, maybe it shouldn’t be."
  },
  {
    "objectID": "chapters/sec2/2-6-networking.html#footnotes",
    "href": "chapters/sec2/2-6-networking.html#footnotes",
    "title": "12  Computer Networks",
    "section": "",
    "text": "URLs are a subset of a broader category called Uniform Resource Identifiers (URIs), which look like a URL and are used to identify a resource by may not be a valid address. I mention them only because you may run across them in certain contexts, like configuring SSO.↩︎\nDifferent resources divide URLs into somewhere between three and seven parts. I think these four are the most useful for this chapter’s purpose.↩︎\nThe idea is actually pretty clever. Routers are arranged in a tree-like structure. Each router only keeps track of any downstream addresses and a single upstream default address. So the packet gets passed upstream until it hits a router that knows about the target IP address and then back downstream to the right place.↩︎\nPorts are also used for outbound communication. Computers know how to automatically open ports for outbound communication and specify that’s where the response should come; we’re not going any further into the subject of outbound ports here.↩︎\nAuthors of server-based software choose their own default ports. They usually choose a relatively high number to make them unlikely to conflict with anything else.↩︎\nAs you might guess from this complicated syntax, you can do a lot more than this with SSH tunneling; this is what I use it for most frequently.↩︎\nThis is generally a good practice before you start messing with config files. Bad configuration is usually preferable to a service that can’t start at all because you’ve messed up the config so badly. It happens.↩︎"
  },
  {
    "objectID": "chapters/sec2/2-7-dns.html#basics-of-dns-and-domains",
    "href": "chapters/sec2/2-7-dns.html#basics-of-dns-and-domains",
    "title": "13  DNS is for human-readable addresses",
    "section": "13.1 Basics of DNS and domains",
    "text": "13.1 Basics of DNS and domains\nWhen you create or launch a website, you’ll purchase (rent, really) a domain like \\(\\text{do4ds.com}\\) from a domain name registrar. Purchasing a domain gives you the right to attach that domain to an IP Address.\nWhen someone visits your website or server, their computer resolves the IP Address via a DNS lookup against public DNS nameservers. By purchasing a domain, you register the association between your domain and the IP Address with the DNS nameservers so users can look them up.\n\nA complete domain is called a Fully-Qualified Domain Name (FQDN) and consists of three parts:\n\\[\n\\overbrace{\\text{blog}}^{\\text{Subdomain}}.\\overbrace{\\underbrace{\\text{example}}_{\\text{Domain Name}}.\\underbrace{\\text{com}}_{\\text{Top-Level Domain}}}^{\\text{Root Domain}}\n\\]\nWhen you get a domain from a registrar, you are actually renting the root domain. You can choose any root domain you want, as long as it’s not already taken. Domain names are unique only within top-level domains, so you might be able to get \\(\\text{example.fun}\\) even if someone else owns \\(\\text{example.com}\\).\n\n\n\n\n\n\nTop-level domains\n\n\n\nWhen the web first launched, there were only a few top-level domains, such as \\(\\text{.com}\\), \\(\\text{.org}\\), and \\(\\text{.net}\\). ICANN, the group that controls how domains are assigned, controlled them all.\nIn 2013, ICANN decided to allow people and organizations to register their own top-level domains. That’s why, over the last decade or so, there’s been an explosion in websites at top-level domains like \\(\\text{.io}\\), \\(\\text{.ai}\\), and \\(\\text{.fun}\\).\nIf you feel, like me, that it would be fun to own a top-level domain, you’ll be sad to learn that it’s not something to do on a whim. In 2012, the initial application fee was $185,000.\n\n\nSubdomains are a way to specify a part of a domain, usually to signify to users that its for a distinct purpose. You can generally register as many subdomains as you want against a root domain."
  },
  {
    "objectID": "chapters/sec2/2-7-dns.html#configuring-dns",
    "href": "chapters/sec2/2-7-dns.html#configuring-dns",
    "title": "13  DNS is for human-readable addresses",
    "section": "13.2 Configuring DNS",
    "text": "13.2 Configuring DNS\nDNS is configured by giving the proper IP Address information to the domain name registrar where you bought your domain. There’s often some minor configuration on the server side as well to let the server know where it lives.\nThe best way to configure DNS is to google how to configure DNS for wherever you’re hosting your website or server, be it EC2 or GitHub Pages or somewhere else. This section is designed to give you a mental model so what you find in your googling makes sense.\nThere are a variety of domain name registrars. AWS, Azure, and GCP each have their own registrar; there are a number of independent registrars including Namecheap (my personal favorite as it is, indeed, cheap), Cloudflare, and GoDaddy. You can use any registrar to configure a domain irrespective of where the server actually lives.\nCosts for domain names vary widely. Buying a meaningless domain in a less popular top-level domain can cost as little as a few dollars per year. For example, I paid only $1.98 for the domain \\(\\text{do4ds-lab.shop}\\) for a year on Namecheap.\nOn the other hand, buying a \\(\\text{.com}\\) domain that’s a real word or phrase can be thousands of dollars. There are articles every few years about some major company accidentally allowing their domain name to lapse and ransoming it back for ridiculous amounts of money.\nOnce you’ve purchased your domain, you need to configure the public DNS records of your domain to point to the IP Address you want. Configuration of DNS is done by way of records. Records map a path or host to a target.\nRecords fall into a number of categories, but there are only a few you’re likely to see:\n\nA records (or their IPv6 cousin AAAA records) map a domain to an IP Address.\nCNAME records alias subdomains to another record.\nNS records tell the DNS server to forward the request to another namespace server. This is usually only used by big organizations that run their own domain name servers for their subdomains.\n\nWhen you go to configure DNS with your domain name registrar, you’ll configure the records in a record table. Here’s an imaginary DNS record table for the domain \\(\\text{example.com}\\):\n\n\n\nPath/Host\nType\nTarget\n\n\n\n\n\\(\\text{@}\\)\nA\n\\(\\text{64.56.223.5}\\)\n\n\n\\(\\text{www}\\)\nCNAME\n\\(\\text{example.com}\\)\n\n\n\\(\\text{blog}\\)\nA\n\\(\\text{114.13.56.77}\\)\n\n\n\\(\\text{*}\\)\nA\n\\(\\text{64.56.223.5}\\)\n\n\n\nThe first row provides an A record for the special \\(\\text{@}\\) symbol meaning exact match. By this configuration, any traffic to \\(\\text{example.com}\\) will be passed straight through to the specified IP Address.\nThe second row deals with traffic to the \\(\\text{www}\\) subdomain. This CNAME record indicates that traffic to \\(\\text{www.example.com}\\) should be treated exactly like traffic to the bare \\(\\text{example.com}\\). Some domain providers do automatic redirection of \\(\\text{www}\\) traffic making this row unnecessary in some configurations.\nThe next record sends the \\(\\text{blog}\\) subdomain to a completely different IP Address. This is a common configuration when the subdomain might be owned by a completely different group inside the organization or is served from a different server.\nThe last record uses the wildcard symbol (\\(\\text{*}\\)) to send all subdomain traffic that’s not already spoken for back to the main IP Address.\nOther than the \\(\\text{www}\\) subdomain, which stands for world wide web and is generally routed to the same place as the bare root domain, using subdomains and choosing between subdomains and paths is entirely about organizing how users experience your website and what’s easiest for your organization to maintain.\n\n\n\n\n\n\n\\(\\text{www}\\) is just a subdomain\n\n\n\nWhen the internet was first started, it seemed like it might be important to differentiate the \\(\\text{www}\\) subdomain for the website from, for example, the email domain for the organization.\nThat turned out not really to be the case; now \\(\\text{www}\\) and the bare root domain are usually used interchangeably.\n\n\nOnce you’ve configured your DNS records, you will need to wait an annoyingly long time to see if you did it correctly.\nWhen your computer does a DNS lookup, there are often at least three nameservers involved. First, your computer talks to a resolver, which is a server that keeps track of where the top-level domain nameservers are. Then you’re routed to the nameserver for the top-level domain which routes you to the nameserver for your actual domain. And this whole system is duplicated across geographies for redundancy.\nWhen you configure a DNS record, you configure something called the TTL (time to live). Each nameserver, as well as your browser, caches recent DNS lookups to keep things snappy. The TTL defines how long the DNS cache lasts.\nThe upshot is that it can take up to 24 hours for DNS changes to propagate because the change won’t be effective until all the caches end across all of the DNS servers as well as in your browser. If you make a DNS change and it’s not working, you have no idea whether you made a mistake or it just hasn’t propagated yet. It’s very annoying."
  },
  {
    "objectID": "chapters/sec2/2-7-dns.html#comprehension-questions",
    "href": "chapters/sec2/2-7-dns.html#comprehension-questions",
    "title": "13  DNS is for human-readable addresses",
    "section": "13.3 Comprehension Questions",
    "text": "13.3 Comprehension Questions\n\nWhat are the parts of a fully-qualified domain name? How does each of them get created?\nHow does your computer find the IP Address for a domain? Why could it sometimes be wrong?\nWhat are the different kinds of DNS records you’re likely to use?"
  },
  {
    "objectID": "chapters/sec2/2-7-dns.html#lab-configuring-dns-for-your-server",
    "href": "chapters/sec2/2-7-dns.html#lab-configuring-dns-for-your-server",
    "title": "13  DNS is for human-readable addresses",
    "section": "13.4 Lab: Configuring DNS for your server",
    "text": "13.4 Lab: Configuring DNS for your server\nIn the last lab, we configured the server so that all the services were served off of one single port that redirected to various subpaths.\nNow we want to get a real, memorable domain for the server so that you and your users don’t have to remember some random ec2- domain or an IP Address. In this lab, we’ll configure DNS records for our server so it’s available at a real domain.\n\n13.4.1 Step 1: Allocate an Elastic IP\nOur EC2 instance got a public IP when it started. We could just use that IP Address, but that address is released every time the server stops and a new one is assigned when it comes back up. This means you’d have to change your DNS record every time you temporarily take your server offline – no good.\nLuckily, AWS has a service called Elastic IP which gives you a stable public IP Address that you can move from one instance to another as you wish.\n\n\n\n\n\n\nPaying for Elastic IPs\n\n\n\nElastic IPs are free if they’re attached to a running EC2 instance. You pay when they’re not in use to discourage hoarding them.\nIf you do take your server down for a short time, it’s no big deal. As of this writing, it’s 12 cents per day for a single IP. But do make sure to release the Elastic IP if/when you take your server down permanently.\n\n\nTo set up your Elastic IP, find it in the AWS console and allocate an address. Then you will associate your Elastic IP as the default public IP Address for your instance.\nNote that once you make this change, your server will no longer be available at its old IP Address, so you’ll have to SSH in at the new one. If you have SSH terminals open when you make the change, they will break.\n\n\n\n\n\n\nNote\n\n\n\nNext time you stand up a server, you should start by giving it an Elastic IP so you are immediately using its permanent IP Address. In this book, the order of the labs is designed to promote learning, not the right order to configure things.\n\n\n\n\n13.4.2 Step 2: Buy a domain\nYou can buy a domain from any of the domain registrars on the web. This won’t be free, but many domains are very cheap.\nThe easiest place to buy a domain is AWS’s Route53 service, but feel free to use another provider. I usually use Namecheap just because all of the domains I own are there.\n\n\n13.4.3 Step 3: Configure DNS\nOnce you’ve got your domain, you need to configure your DNS. You’ll have to create 2 A records; one each for the \\(\\text{@}\\) host and the \\(\\text{*}\\) host pointing to your IP and one for the CNAME at \\(\\text{www}\\) with the value being your bare domain.\nExactly how you configure this will depend on the domain name provider you choose. In NameCheap, you configure this via a table under Advanced DNS, which looks like this.\n\n\n\nType\nHost\nValue\nTTL\n\n\n\n\nA Record\n\\(\\text{*}\\)\n\\(\\text{64.56.223.5}\\)\nAutomatic\n\n\nCNAME Record\n\\(\\text{www}\\)\n\\(\\text{do4ds-lab.shop}\\)\nAutomatic\n\n\nA Record\n\\(\\text{@}\\)\n\\(\\text{64.56.223.5}\\)\nAutomatic\n\n\n\nI would recommend sticking with the default for TTL.\n\n\n13.4.4 Step 4: Wait an annoyingly long time\nNow you just have to be patient. Unfortunately, DNS takes time to propagate. After a few minutes (or hours?), your server should be reachable at your domain.\nIf it’s not (yet) reachable, see if an incognito browser works because that sidesteps the browser level of caching. If it doesn’t, wait some more. When you run out of patience, try reconfiguring everything and check if it works.\n\n\n\n\n\n\nTip\n\n\n\nWe still haven’t configured HTTPS, so you’ll need to manually input the URL as \\(\\text{http://}\\), because your browser will otherwise assume it’s HTTPS.\n\n\n\n\n13.4.5 Step 5: Add the Shiny app to your site\nNow that the Shiny app is at a stable URL, let’s put it on our site so people can look at our penguin size prediction model. I put the app at the subpath \\(\\text{/penguins}\\), so it’s now at \\(\\text{http://do4ds-lab.shop/penguins}\\).\nWe’re going to use something called an iFrame, which lets you embed one website inside another. An iFrame is a basic HTML construct and it’s easy to put one in a Quarto site.\n\n\n\n\n\n\nNote\n\n\n\nOnce you change your website to go over HTTPS in the next section, you’ll have to adjust the iFrame URL as well.\n\n\nIn Quarto, you can just add an html block to any document and it will get loaded in automatically. I want the app on the landing page of my site, index.qmd. So I’ve added a block that looks like:\n\n\nindex.qmd\n\n&lt;iframe width=\"780\" height=\"500\" src=\"http://do4ds-lab.shop/penguins/\" title=\"Penguin Model Explorer\"&gt;&lt;/iframe&gt;"
  },
  {
    "objectID": "chapters/sec2/2-7-dns.html#footnotes",
    "href": "chapters/sec2/2-7-dns.html#footnotes",
    "title": "13  DNS is for human-readable addresses",
    "section": "",
    "text": "This is a very shallow intro to DNS. If you want to go a little deeper, I highly recommend Julia Evans’s zines on a variety of technical topics, including DNS. You can find them at wizardzines.com.↩︎"
  },
  {
    "objectID": "chapters/sec2/2-8-ssl.html#what-https-does",
    "href": "chapters/sec2/2-8-ssl.html#what-https-does",
    "title": "14  You should use SSL/HTTPS",
    "section": "14.1 What HTTPS does",
    "text": "14.1 What HTTPS does\nHTTPS is the same as HTTP, but secured with a technology called SSL/TLS (secure sockets layer/transport layer security). SSL/TLS works by configuring the resource to provide an SSL certificate upon demand; the certificate is then used to verify the site’s identity and establish an encrypted session.\n\n\n\n\n\n\nSSL vs TLS\n\n\n\nThese days, TLS is actually what’s in use, but you’ll mostly talk about SSL. That’s because SSL has been around for a long time and people got used to talking about configuring SSL certificates. TLS is configured exactly the same, so most people still talk in terms of SSL when they really mean TLS.\n\n\nYou use HTTPS constantly. Go to a website in your browser and look for a little lock icon near the search bar. That little lock indicates that the domain is secured using HTTPS. If you click on it, you can get more information about the site’s SSL certificate.\nIf you’re of a certain age, you may recall warnings that you shouldn’t use the WiFi at your neighborhood Starbucks. The issue was twofold.\nFirst, HTTP has no way to verify that the website you think you’re interacting with is, in fact, that website. A bad actor could put up a fake WiFi network that resolves \\(\\text{bankofamerica.com}\\) to a look-alike website that captures your banking login That’s called a man-in-the-middle attack.\nAnd even if they didn’t use that trick specifically, there were tools to read the unencrypted HTTP traffic going back and forth in what’s called a packet sniffing attack.\nIn 2015, Google Chrome began the process of marking any site using plain HTTP as insecure, which led to the nearly complete adoption of HTTPS across the internet. Both man-in-the-middle and packet sniffing attacks have been neutered. Because of HTTPS it’s now safe to use any random WiFi network you want.\nAs a website administrator, securing your website or server with HTTPS is one of the most basic things you can do to make sure your website traffic is safe. You should always configure HTTPS for a public website – full stop.\nThis SSL/TLS security can be applied to different application layer protocols, including (S)FTP and LDAP(S). You may run across these depending on your organization. In any case, the SSL/TLS part works the same – all that changes is what’s inside the secure digital envelope."
  },
  {
    "objectID": "chapters/sec2/2-8-ssl.html#how-ssltls-works",
    "href": "chapters/sec2/2-8-ssl.html#how-ssltls-works",
    "title": "14  You should use SSL/HTTPS",
    "section": "14.2 How SSL/TLS works",
    "text": "14.2 How SSL/TLS works\nSSL/TLS uses public key encryption (remember, we learned about that in Chapter 8) to do two things: validate that the site you’re visiting is the site you intend and encrypt the traffic back and forth to the site.\nTo set up SSL for a website, you create or acquire an SSL certificate, which has a public and a private component (sound familiar?).1 Then, verify the public certificate with a trusted Certificate Authority (CA) and put the private certificate in the right place on the website.\nWhen you go to access that resource, your machine asks for a signature. The service uses its private key to generate a signature and your machine verifies the signature against its internal trusted CA store.2 Now your machine knows it’s communicating with the right host on the other side and you’re not falling victim to a man-in-the-middle attack.\nOnce the verification process is done, your machine and the remote on the other side create temporary session keys to establish encryption with the website on the other end.3 Only then does it start sending real data, now encrypted securely inside a digital envelope."
  },
  {
    "objectID": "chapters/sec2/2-8-ssl.html#getting-and-using-ssl-certificates",
    "href": "chapters/sec2/2-8-ssl.html#getting-and-using-ssl-certificates",
    "title": "14  You should use SSL/HTTPS",
    "section": "14.3 Getting and using SSL certificates",
    "text": "14.3 Getting and using SSL certificates\nWhen you buy a computer, it comes configured out of the box to trust a small number of official CAs. If you want to get a certificate for a website, it’s easiest to get it from one of those CAs.\nIn the past, this process was painful. The CAs charged to issue certificates. While it was only $10 per year to for a basic SSL certificate, they typically would only cover a single subdomain. A wildcard certificate to cover all the subdomains of a root domain was expensive enough to discourage hobbyists.\nIf you wanted a free certificate, your only option was to use a self-signed certificate. Creating a self-signed certificate is as easy as creating an SSH key. But using a self-signed certificate is a pain because you have to manually add the public certificate to the CA store of every machine that accesses the site, and then re-add it when the certificate expires.4\nLuckily there’s now another option. For most small organizations or hobbyists, I recommend getting a free SSL certificate from the nonprofit CA Let’s Encrypt. They even have some nice tooling that makes it easy to create and configure your certificate right on your server.\nMost organizations, use a public CA to get SSL on public-facing resources and use plain HTTP with no SSL/TLS inside their private networks. Some large organizations want to encrypt their private traffic as well and run their own private CA. If this is the case, your organization’s policies will make it clear. This can be a pain point, because you’ve got to make sure every host inside the network trusts the private CA.\nOnce you’ve configured SSL/TLS, you generally want to only allow HTTPS traffic to your site. You’ll accomplish this by redirecting all HTTP traffic on port \\(80\\) to come in via HTTPS on port \\(443\\).\nSome web applications support configuring a certificate directly, while others only accept HTTP traffic, meaning you’d need to do SSL termination with a proxy in front of the application."
  },
  {
    "objectID": "chapters/sec2/2-8-ssl.html#comprehension-questions",
    "href": "chapters/sec2/2-8-ssl.html#comprehension-questions",
    "title": "14  You should use SSL/HTTPS",
    "section": "14.4 Comprehension Questions",
    "text": "14.4 Comprehension Questions\n\nWhat are the two risks of using plain HTTP and how does HTTPS mitigate them?\nWrite down a mental map of how SSL secures your web traffic. Include the following: public certificate, private certificate, certificate authority, encrypted traffic, port 80, port 443"
  },
  {
    "objectID": "chapters/sec2/2-8-ssl.html#lab-configure-ssl",
    "href": "chapters/sec2/2-8-ssl.html#lab-configure-ssl",
    "title": "14  You should use SSL/HTTPS",
    "section": "14.5 Lab: Configure SSL",
    "text": "14.5 Lab: Configure SSL\nWe’re going to use Let’s Encrypt’s certbot utility to automatically generate an SSL certificate, share it with the CA, install it on the server, and even update the NGINX configuration.\nIf you’ve never had to manually configure SSL in the past, let me tell you, this is magical!\n\n14.5.1 Step 1: Follow instructions to add SSL for NGINX\nUsing Let’s Encrypt to add an SSL certificate to NGINX configuration is a very common task. As of this writing, there’s a great blog post entitled Using Free Let’s Encrypt SSL/TLS Certificates with NGINX. I’d encourage you to look for that article (or something similar) and follow the steps there.\nAt a high level, what you’ll do is\n\nConfigure the NGINX configuration to know what domain it’s on.\nInstall certbot on the system.\nRun certbot to get the certificate, apply it to the server, and update the NGINX configuration.\n\nBefore you move along, I’d recommend you take a moment and inspect the /etc/nginx/nginx.conf file to see what certbot added.\nRelative to the old version, you’ll notice two things. First, the line that read listen 80 is gone from the server block because we no longer listen for HTTP traffic. In its place, there’s now a listen 443 along with lines that tell NGINX where to find the certificate on the server.\nScrolling down a little, there’s a new server block that is listening on \\(80\\). This block returns a 301 status code (permanent redirect) and sends traffic to HTTPS on \\(443\\).\n\n\n14.5.2 Step 2: Let RStudio Server know it’s on HTTPS\nBefore we exit and test it out, let’s do one more thing. As mentioned when we configured NGINX the first time, RStudio Server does a bunch of proxying traffic back to itself, so it needs to know that it’s on HTTPS.\nYou can add a header to all traffic letting RStudio Server know the protocol is HTTPS by adding this line to your nginx.conf:\n\n\n/etc/nginx/nginx.conf\n\nproxy_set_header X-Forwarded-Proto https;\n\nOk, now try to visit RStudio Server at your URL and you’ll find that…it’s broken again.\nBefore you read along, think for just a moment. Why is it broken?\n\n\n14.5.3 Step 3: Configure security groups\nIf your thoughts went to something involving ports and AWS security groups, you’re right!\nBy default, our server was open to SSH traffic on port \\(22\\). Since then, we may have opened or closed port \\(80\\), \\(8000\\), \\(8080\\), \\(8787\\), and/or \\(3838\\).\nBut now the proxy is exclusively receiving HTTPS traffic on \\(443\\) and redirecting or refusing all other traffic. You have to adjust the security group so there are only 2 rules – one that allows SSH traffic on \\(22\\) and one that allows HTTPS traffic on \\(443\\).\nIt’s up to you whether you want to leave port \\(80\\) open. If you do, it will redirect people to HTTPS on \\(443\\). If you close it entirely, people who come to port \\(80\\) will be blocked and will eventually get a timeout. If people are already coming to the server via HTTP, it might be nice to leave \\(80\\) open so they get a smooth redirect experience instead of getting confusingly blocked.\n\n\n14.5.4 Step 4: We did it!\nThis is the end of the labs in this book.\nAt this point, your server is fully configured. You have three real data science services available on a server at a domain of your choosing, all protected by HTTPS.\nTake a moment to celebrate. It’s very cool to be able to stand up and administer your own data science workbench. Whether you’re working at a small organization or you’re a hobbyist, you can really use this server to do real data science work.\nHowever, this server isn’t enterprise-ready. If you work at a large organization or one with stringent security or privacy rules, your IT/Admin group is going to have concerns. The final section of the book will introduce you to the (valid) reasons why your organization may not be happy with the data science workbench you’ve configured."
  },
  {
    "objectID": "chapters/sec2/2-8-ssl.html#footnotes",
    "href": "chapters/sec2/2-8-ssl.html#footnotes",
    "title": "14  You should use SSL/HTTPS",
    "section": "",
    "text": "Like with SSL, this makes more sense if you think “key” where you see private and “lock” where you see public.↩︎\nYour machine doesn’t actually keep information on individual websites. Instead, it keeps public certificates for CAs. The CA verifies the certificate for an individual website with a signature signing it. When your machine gets an SSL certificate for an individual website, it can validate that the CA actually endorses this certificate as valid.↩︎\nUnlike the asymmetric encryption used by SSL and SSH for the public key encryption, the session keys are symmetric, so they work the same in both directions.↩︎\nYou also could skip that step, in which case you got the session encryption benefits of SSL/TLS, but not the verification.↩︎"
  },
  {
    "objectID": "chapters/sec3/3-0-sec-intro.html#creating-a-devops-culture",
    "href": "chapters/sec3/3-0-sec-intro.html#creating-a-devops-culture",
    "title": "Enterprise-grade data science",
    "section": "Creating a DevOps culture",
    "text": "Creating a DevOps culture\nAs a data scientist, your primary concern about your data science environment is its usefulness. You want the latest versions of Python or R, abundant access to packages, and data at your fingertips.\nGreat IT/Admin teams also care about the system’s usefulness to users (that’s you), but it’s usually a distant third to their primary concerns of security and stability. And that focus benefits you. Minute-to-minute, you may be primarily focused on getting data science work done, but an insecure or unstable data science platform is not useful to anyone.\nThere’s a reason why these concerns primarily arise in an enterprise context. If you’re a small team of three data scientists sitting next to each other, accidentally crashing your workbench server is a chance for a coffee break and an opportunity to learn something new about how servers work.\nBut if you’re working on an enterprise-wide data science workbench supported by a central IT/Admin function, it’s infuriating if someone three teams over can disturb your work. And you don’t want to work in an environment where you must consider every action’s security implications.\nBalancing security, stability, and usefulness is always about tradeoffs. The only way to be 100% sure that private data will never leak is never to give anyone access at all. Organizations that do DevOps right embrace this tension and are constantly figuring out the proper stance for the whole organization.\nIt is an unfortunate truth that many IT/Admin teams don’t act as partners. They act as gatekeepers to the resources you need to do your job, which can be incredibly frustrating. While it’s unfair, you stand to lose more if they don’t give you the access you need, so you’ll have to learn what matters to those teams, communicate what matters to you, and reach acceptable organizational outcomes."
  },
  {
    "objectID": "chapters/sec3/3-0-sec-intro.html#a-hierarchy-of-itadmin-concerns",
    "href": "chapters/sec3/3-0-sec-intro.html#a-hierarchy-of-itadmin-concerns",
    "title": "Enterprise-grade data science",
    "section": "A hierarchy of IT/Admin concerns",
    "text": "A hierarchy of IT/Admin concerns\nThe primary concerns of IT/Admins are security and stability. A secure and stable system gives valid users access to the systems they need to get work done and withholds access from people who shouldn’t have it. If you understand and communicate with IT/Admins about the risks they perceive, you can generate buy-in by taking their concerns seriously.\nThe worst outcome for a supposedly secure data science platform would be an unauthorized person gaining access and stealing data. In the most extreme form, this is someone entirely outside the organization (outsider threat). But it also could be someone inside the organization who is disgruntled or seeking personal gain (insider threat). And even if data isn’t stolen, it’s bad if someone hijacks your computational resources for nefarious ends like crypto mining or virtual DDOS attacks on Turkish banks.1\nSomewhat less concerning, but still the stuff of IT/Admin nightmares is platform instability that results in the erasure of important data, called data loss. And even if data isn’t permanently lost, instability that results in lost time for platform users is also bad.\nIT/Admins may have some stake in ensuring that the environment doesn’t include error-ridden software that results in incorrect work. And last, way down the list, is that users don’t have a bad experience using the environment."
  },
  {
    "objectID": "chapters/sec3/3-0-sec-intro.html#enterprise-tools-and-techniques",
    "href": "chapters/sec3/3-0-sec-intro.html#enterprise-tools-and-techniques",
    "title": "Enterprise-grade data science",
    "section": "Enterprise tools and techniques",
    "text": "Enterprise tools and techniques\nConceptually, enterprise IT/Admins always try to implement security in layers. This means an application or server has multiple kinds of protection, making an accidental or intentional breach less likely.\nAt every layer, sophisticated organizations try to implement the principle of least privilege. The idea is to give people the permissions needed to complete their work – and no more. For example, you might want root access on your data science workbench, but you are not getting it if you work in an enterprise because you shouldn’t need it in your day-to-day work.\nThere is no one-size-fits-all (or even most) way to implement these ideas. Your organization should choose a solution that balances the value of universal availability of information versus the risk of breach or disclosure.\n\n\n\n\n\n\nBuild or Buy?\n\n\n\nOne big question any enterprise IT/Admin faces when creating a data science environment is whether to build or buy one. Some IT/Admin organizations prefer to build data science platforms straight from open-source tools, like JupyterHub and RStudio Server. Conversely, some want to buy seats on a SaaS solution.\nI am admittedly biased on this question, as I work for a company that sells software to create data science platforms. But in my experience, only enterprises with extraordinarily competent IT/Admin teams can be better off building.\nI have seen many organizations decline to buy Posit’s Pro Products in favor of attempting to build a platform. Many come back 6 or 12 months later, having discovered that DIY-ing an 80% solution is easy, but creating a fully enterprise-ready data science environment from open-source components is hard.\n\n\nNetworking is the first line of defense for keeping bad actors out of private systems. If the network is secure, it’s hard for bad actors to infiltrate and hard for insiders to accidentally or intentionally exfiltrate valuable data. That’s why Chapter 15 discusses how enterprises approach networking to create highly secure environments.\nIf you work in a small organization, everyone likely has access to nearly everything. For larger or more security-conscious organizations, it is a higher priority that people have access to the systems they need – and only the systems they need. Sophisticated approaches are required to manage the access of many users to many systems and complex rules that govern who has access to what. In Chapter 16, you’ll learn the basics of how enterprises think about providing a way to log in to different systems and how you can make use of those tools in a data science context.\nOnce IT/Admins feel that the platform is secure, their concerns turn to ensuring it has sufficient horsepower to support all the users that need it and implementing ongoing upgrades and changes with minimal interruption to users. Chapter 17 discusses how enterprises manage computational resources to ensure stability, especially when a lot are required.\nLastly, there are your concerns as a data scientist. In particular, using open-source R and Python packages can be complicated in an enterprise context. That’s why Chapter 18 is all about the difficulties I’ve observed for organizations using open-source packages and the solutions I’ve seen work for those environments.\nBy the time you’ve finished these chapters, I hope you’ll be able to articulate precisely the needs of your enterprise data science environment and be a great partner to IT/Admin when issues, tension, or problems arise."
  },
  {
    "objectID": "chapters/sec3/3-0-sec-intro.html#no-labs-in-this-section",
    "href": "chapters/sec3/3-0-sec-intro.html#no-labs-in-this-section",
    "title": "Enterprise-grade data science",
    "section": "(No) labs in this section",
    "text": "(No) labs in this section\nAs this section is about developing mental models and language to collaborate with IT/Admins, there are no labs in this section of the book. There are a lot of pictures and comprehension questions to ensure you’ve grasped the material."
  },
  {
    "objectID": "chapters/sec3/3-0-sec-intro.html#footnotes",
    "href": "chapters/sec3/3-0-sec-intro.html#footnotes",
    "title": "Enterprise-grade data science",
    "section": "",
    "text": "Yes, both things I’ve actually seen happen.↩︎"
  },
  {
    "objectID": "chapters/sec3/3-1-ent-networks.html#enterprise-networks-are-private",
    "href": "chapters/sec3/3-1-ent-networks.html#enterprise-networks-are-private",
    "title": "15  Enterprise Networking",
    "section": "15.1 Enterprise networks are private",
    "text": "15.1 Enterprise networks are private\nAn enterprise network houses dozens or hundreds of servers, each with its own connection requirements. Some are accessible to the outside, while others are only accessible to other servers inside the network.\nFor that reason, the servers controlled by an enterprise live inside one or more private networks. The good news is that private networks look like public ones, which you learned about in Chapter 12. Every host in a private network has an IP Address. It’s just a private IP address handed out by the router that governs the network and is valid only inside the private network. And, like you can get a domain for a human-friendly way to address a server on a public network, many private networks use private hostnames to have human-friendly ways to talk about servers.\n\n\n\n\n\n\nWhere’s my private network?\n\n\n\nIn AWS, every server lives inside a private network called a virtual private cloud (VPC). When we set up our data science workbench throughout Section 2, we ignored the VPC and assigned the instance a public IP address – which explains why this is the first time you’ve heard about it.\nIn an enterprise context, this kind of configuration would be a no-go.\n\n\nWith so many services running inside a network, connection requirements can get byzantine. For example, you probably want to set up a data science workbench and a data science hosting environment.\nThe servers in the data science environment should:\n\nBe reachable from users’ laptops.\nReach one or more databases that are only accessible from inside the private network and may also connect to data sources.\nAccess a package repository.\n\nAnd, to comply with the principle of least privilege, you don’t want any of these servers to be more available than needed. Providing precisely the right level of networking access isn’t a trivial undertaking.\n\nThose are just the servers for actually doing work. Enterprise networks also include various devices that control the network traffic itself. When you’re working in a data science environment and run into trouble, you should start by asking whether the issue could be with network traffic struggling to get into, out of, or across the private network."
  },
  {
    "objectID": "chapters/sec3/3-1-ent-networks.html#the-shape-of-an-enterprise-network",
    "href": "chapters/sec3/3-1-ent-networks.html#the-shape-of-an-enterprise-network",
    "title": "15  Enterprise Networking",
    "section": "15.2 The shape of an enterprise network",
    "text": "15.2 The shape of an enterprise network\nWhen you access something important inside a private network, the IP Address is rarely the server doing the work. Instead, it’s usually the address of a proxy server or proxy, an entire server that exists just to run proxy software that routes traffic around the network.\nRouting all traffic through the proxy ensures that the work servers only get traffic from other servers that the organization controls, decreasing the number of attack vectors to the work servers. Proxy servers may also do other tasks like terminate SSL or authentication.\n\n\n\n\n\n\nVPN vs VPC?\n\n\n\nYou may have to log in to a VPN (Virtual Private Network) for work or school. Where a VPC is a private network inside a cloud environment, a VPN is a private network for remote access to a shared network. You generally don’t directly log in to an enterprise VPC (or on-prem private network), but you might log in to an adjacent VPN ensuring that anyone who accesses the network is coming from an authenticated machine.\n\n\nEnterprise networks are almost always subdivided into subnets, which are logically separate partitions of the private network.1 In most cases, the private network is divided between the public subnet or demilitarized zone (DMZ) where the proxies live and the private subnet where all the important servers live.2\n\nAside from the security benefits, putting the important servers in the private subnet is also more convenient because the IT/Admins can use private hostnames and IP Addresses without worrying about uniqueness on the public internet. For example, they could use the hostname \\(\\text{google.com}\\) for a server because it only needs to be valid inside the private network. But, that’s confusing and I wouldn’t recommend it."
  },
  {
    "objectID": "chapters/sec3/3-1-ent-networks.html#networking-pain-follows-proxies",
    "href": "chapters/sec3/3-1-ent-networks.html#networking-pain-follows-proxies",
    "title": "15  Enterprise Networking",
    "section": "15.3 Networking pain follows proxies",
    "text": "15.3 Networking pain follows proxies\nThe most straightforward networking issue is that a connection doesn’t exist where one is needed. This is usually clear when using tools like ping and curl and can be solved by working with your IT/Admin team.\nDifficulties tend to be more subtle when proxies are involved and enterprise networks feature proxies all over the place. Much like the watertight bulkheads between every room on a naval ship, proxies show up between any two parts of the network that you might want to seal off at some point. And where a proxy exists, it can cause you trouble.\nIn fact, two different proxies might mess with any given leg of the journey. There could be a proxy that handles the traffic as it leaves one server (outbound) as well as one that intercepts traffic arriving at the destination (inbound).\n\n\n\n\n\n\nNote\n\n\n\nInbound and outbound are terms I’ve chosen.\nTraditionally, proxies are classified as reverse or forward as if you’re a host inside the private network, with inbound proxies called reverse and outbound ones called forward. I found that nearly impossible to remember and started using inbound and outbound. I find it much easier to remember and IT/Admins always understand what I mean.\n\n\n\nThe first step in debugging networking issues is to ask whether one or more proxies might be in the middle. You can jumpstart that discussion by clearly describing where the traffic originates, where it’s going, the protocol it’s using, and the port it’s targeting.\nPeople often get tripped up on where the traffic originates and terminates, especially when using their laptop to access a server. When you’re accessing a data science project running on a server, the only inbound traffic to the private network is the connection from your laptop to the server. Code that runs on the server can only generate outbound traffic. So nearly all the traffic you care about is outbound, including package installation, making API calls in your code with {requests} or {httr}, connecting to a Git repo, or connecting to data sources.\n\n15.3.1 Issues with inbound proxies\nAlmost all private networks feature inbound proxies that handle traffic coming in from the internet. This can cause problems in a data science environment if everything isn’t configured correctly.\n\n\n\n\n\n\nWhat ports do I need?\n\n\n\nOne of the first questions IT/Admins ask is what ports must be open in the proxy.\nDatabase traffic often runs using non-HTTP traffic to special ports. For example, Postgres runs on port \\(5432\\). However, your database traffic should probably all occur inside the private network so this won’t be an issue.\nAlmost other traffic, including package downloads, is standard HTTP(S) traffic, so it can happily run over \\(80\\) or \\(443\\).\n\n\nInbound redirection issues can be hairy to debug. Very often, these issues arise because the application you’re using (say, JupyterHub or RStudio) expects to be able to redirect you back to itself. If the proxy isn’t configured correctly, the service will start and run just fine, but certain user interactions won’t work the way you expect.3 This will likely surface upon starting new sessions or launching something (like an app or doc) inside the platform. Your application should have an admin guide with directions on hosting it behind a proxy. Confirm with your admin that those steps have been followed.\nProxies also often impose file size limits and/or session duration timeouts. If weird things happen during file uploads or downloads, or sessions end unexpectedly, start by checking on the inbound proxy settings.\nSome data science app frameworks, including Shiny and Streamlit, use a technology called websockets to maintain the connection between the user and the app session. Most modern proxies support websockets, but since some older on-prem proxies don’t, you may have to figure out a workaround if you can’t get websockets enabled on your proxy."
  },
  {
    "objectID": "chapters/sec3/3-1-ent-networks.html#air-gapping-with-outbound-proxies",
    "href": "chapters/sec3/3-1-ent-networks.html#air-gapping-with-outbound-proxies",
    "title": "15  Enterprise Networking",
    "section": "15.4 Air gapping with outbound proxies",
    "text": "15.4 Air gapping with outbound proxies\nUnlike inbound proxies, which appear in virtually every enterprise private network, outbound proxies are only used when there is a need to restrict traffic from leaving the private network. This can be necessary to avoid data exfiltration or to ensure that users only acquire resources that have been explicitly allowed into the environment.\nEnvironments with limited outbound access are called offline or air gapped. The term air gap originates from before ubiquitous wireless connections and referred to a literal gap of air where a network cable might otherwise be. These days, truly air gapped networks are very rare and air gapping is usually accomplished by routing outbound traffic to an outbound proxy that disallows (nearly) all connections.\nThe biggest issue in an air gapped environment is that you can’t access anything outside the network, including public repositories of Python and R packages. You will be to make sure your IT/Admin understands that you cannot do your job without a way to work with packages. There’s more on managing packages in an air gapped environment in Chapter 18.\nYour IT/Admin must also determine how to manage operating system updates, system library installations, and licensing of any paid software inside the environment.4 They likely already have solutions that include a data transfer system, internal repositories, and/or temporarily opening the firewall."
  },
  {
    "objectID": "chapters/sec3/3-1-ent-networks.html#comprehension-questions",
    "href": "chapters/sec3/3-1-ent-networks.html#comprehension-questions",
    "title": "15  Enterprise Networking",
    "section": "15.5 Comprehension Questions",
    "text": "15.5 Comprehension Questions\n\nWhat is the advantage of adopting a more complex networking setup over a server deployed directly on the internet? Are there advantages other than security?\nDraw a mental map with the following entities: inbound traffic, outbound traffic, proxy, private subnet, public subnet, VPC\nLet’s say you’ve got a private VPC that hosts an instance of RStudio Server, JupyterHub, and Shiny Server with an app deployed. Here are a few examples of traffic. Are they outbound, inbound, or within the network?\n\nSomeone connecting to and starting a session on RStudio Server.\nSomeone SFTP-ing an app and packages from RStudio Server to Shiny Server.\nSomeone installing a package to the Shiny Server.\nSomeone uploading a file to JupyterHub.\nA call in a Shiny app using httr2 or requests to a public API that hosts data.\nAccessing a private corporate database from a Shiny for Python app using sqlalchemy.\n\nWhat are the most likely pain points for running a data science workbench that is fully offline/airgapped?"
  },
  {
    "objectID": "chapters/sec3/3-1-ent-networks.html#footnotes",
    "href": "chapters/sec3/3-1-ent-networks.html#footnotes",
    "title": "15  Enterprise Networking",
    "section": "",
    "text": "Subnets are defined as a range of IP addresses by something called a CIDR (Classless Inter-Domain Routing) block.\nEach CIDR block is defined by a starting address and a suffix that indicates the size of the range. For example, the \\(10.33.0.0/26\\) CIDR block is the 64 addresses from \\(10.33.0.0\\) to \\(10.33.0.63\\).\nEach CIDR number is half the size of the prior block, so the \\(10.33.0.0/26\\) CIDR can be split into the \\(10.33.0.0/27\\) block of 32 addresses from \\(10.33.0.0\\) to \\(10.33.0.31\\) and the \\(10.33.0.32/27\\) block for \\(10.33.0.32\\) through \\(10.33.0.63\\).\nDon’t try to remember this. There are online CIDR block calculators if you ever need to create them.↩︎\nThe public subnet usually hosts at least two proxies – one to handle regular HTTP(S) traffic and one just to route SSH traffic to hosts in the private network. The SSH proxy is often called a bastion host or jump box.\nThere are also network infrastructure devices to translate public and private IP addresses back and forth that go alongside the proxies. Private subnets have a device that only allows outbound traffic called a NAT (Network Address Translation) Gateway by AWS. Public subnets have a two-way device called an Internet Gateway by AWS.\nIt’s also very common to have 4 subnets and duplicate the public/private subnet configuration across two availability zones to be resilient to failures in one availability zone.↩︎\nFor example, remember those headers we had to add to traffic to RStudio Server in Chapter 12 and Chapters 14 so it knew it was on a subpath and on HTTPS.\nThis can be particularly gnarly if your proxy also does authentication. If your proxy expects that every request has credentials attached, but your application doesn’t realize it has to go through the proxy, weird behavior can ensue when your application calls itself via HTTP.↩︎\nIn online neworks, licenses are often applied by reaching out to a license server owned by the software vendor.↩︎"
  },
  {
    "objectID": "chapters/sec3/3-2-auth.html#a-gentle-introduction-to-auth",
    "href": "chapters/sec3/3-2-auth.html#a-gentle-introduction-to-auth",
    "title": "16  Auth in Enterprise",
    "section": "16.1 A gentle introduction to auth",
    "text": "16.1 A gentle introduction to auth\nConsider all of the services in an enterprise that require auth: email, databases, servers, social media accounts, HR systems, and more. Let’s picture each of those services as a room in a building.\nNow, imagine yourself as the person who’s in charge of auth. Your job is to give everyone access to the rooms they need and only the rooms they need.\nFirst, you need a way to ascertain and validate the identity of anyone trying to enter a room; to do that, you’ll need to issue and verify credentials. The most common computer credentials are a username and password, but they are relatively insecure, and more secure alternatives are on the rise. These include passkeys, biometrics like fingerprints and facial identification, multi-factor codes, push notifications on your phone, and ID cards. The process of verifying credentials is called authentication (authn).\nBut in an enterprise context, just knowing that someone has valid credentials is insufficient. Remember, not every person gets to access every system or every feature of every system. You’ll also need a way to check their permissions, which is the binary choice of whether they can take an action like accessing a service. The permissions checking process is called authorization (authz). The combination of authn and authz comprise auth.\n\nMany organizations start simply. They add services one at a time and allow each to use built-in functionality to issue service-specific usernames and passwords to users. This would be similar to posting a guard at each room’s door to create a unique credential for each user.\n\nThis quickly becomes a mess for everyone. It’s bad for users because they either need to keep many credentials straight or reuse the same ones, which is insecure. And as an IT/Admin, adding, removing, or changing permissions is cumbersome, because each system has to be changed individually."
  },
  {
    "objectID": "chapters/sec3/3-2-auth.html#centralizing-user-management-with-ldapad",
    "href": "chapters/sec3/3-2-auth.html#centralizing-user-management-with-ldapad",
    "title": "16  Auth in Enterprise",
    "section": "16.2 Centralizing user management with LDAP/AD",
    "text": "16.2 Centralizing user management with LDAP/AD\nIn the mid-1990s, an open protocol called LDAP (Lightweight Directory Access Protocol, pronounced ell-dapp) became popular. LDAP allows services to collect usernames and passwords and send them to a central LDAP database for verification. The LDAP server sends back information on the user, often including their username and groups, which the service can use to authorize the user. Microsoft implemented LDAP as a piece of software called Active Directory (AD) that became so synonymous with LDAP that the technology is often called LDAP/AD.\nSwitching to LDAP/AD is like changing the door guarding process so the guard will radio in the user’s credentials and you’ll radio back if those credentials are valid. This is a vast improvement as having only one set of credentials makes life easier for users. We know now that all the rooms are using a similar level of security and if credentials are compromised, it’s easy to swap them out.\n\nLDAP/AD also provides a straightforward way to create Linux users with a home directory, so it’s often used in data science workbench contexts that have that requirement.\nLDAP/AD is being phased out in many organizations. LDAP/AD predates the rise of the cloud and SaaS services. It is usually used on-prem and has limited features relative to SSO providers.\nAdditionally, LDAP/AD requires that the service request the user credentials and pass them along, which is a potential security risk. It also means it’s usually impossible to incorporate now-common requirements like multi-factor authentication (MFA). Many organizations are getting rid of their LDAP/AD implementations and are adopting a smoother user and admin experience with cloud-friendly technologies.\n\n\n\n\n\n\nNote\n\n\n\nIt’s worth noting that LDAP/AD isn’t an auth technology at all. It’s a type of database that happens to be particularly well-suited to managing users in an organization. So even as many organizations are switching to more modern systems, they may be wrappers around user data stored in an existing LDAP/AD system."
  },
  {
    "objectID": "chapters/sec3/3-2-auth.html#the-rise-of-single-sign-on-sso",
    "href": "chapters/sec3/3-2-auth.html#the-rise-of-single-sign-on-sso",
    "title": "16  Auth in Enterprise",
    "section": "16.3 The rise of Single Sign On (SSO)",
    "text": "16.3 The rise of Single Sign On (SSO)\nSingle Sign On (SSO) is when you log in once to a standalone identity provider at the start of your workday. The identity provider gives you a secure SSO token that is responsible for granting access to various services you might use.1 Usually, the acquisition and management of SSO tokens are hidden from the user, resulting in a pleasant experience where accessing different services throughout the day “just works”.\nAdvantages of SSO include centrally managing authorization at the identity provider and making it easier to implement more sophisticated credentials, like MFA, because they only need to be implemented by the identity provider. For many organizations, especially enterprise ones, SSO is a requirement for any new service.\n\n\n\n\n\n\nNote\n\n\n\nThe term SSO is somewhat ill-defined. It usually means the experience described here, but sometimes just means the centralized user and credential management in an LDAP/AD system.\n\n\nSSO is analogous to a process where users exchange their credentials for an access pass upon entering the building. Each room has a machine to send a request to the central security office, where the room can be remotely unlocked if the request is approved.\n\nSSO isn’t a technology. It describes a user and admin experience almost always accomplished through a standalone identity provider like Okta, OneLogin, Ping, or Microsoft Entra ID.2 These providers most often use one of two technologies – SAML (Security Assertion Markup Language) or OIDC/OAuth (Open Identity Connect/OAuth2.0).3 Your organization’s IT/Admins will likely use OAuth for external SaaS services and may prefer SAML for services inside your organization’s firewall. The experience of using either is very similar as a user.\nAs I write this in 2023, there’s a transition underway from on-prem systems like LDAP/AD to cloud-friendly SSO systems and the enhanced security they enable. In particular, the use of non-password credentials like passkeys and the use of OAuth to do sophisticated authorization management inside enterprises are now cutting-edge, but will be standard practices within a few years."
  },
  {
    "objectID": "chapters/sec3/3-2-auth.html#connecting-to-data-sources",
    "href": "chapters/sec3/3-2-auth.html#connecting-to-data-sources",
    "title": "16  Auth in Enterprise",
    "section": "16.4 Connecting to data sources",
    "text": "16.4 Connecting to data sources\nWhether you’re working directly on a data science workbench or deploying a project to a hosting platform, you’re almost certainly connecting to a database, storage bucket, or data API along the way.\nIt used to be the case that most data sources had simple username and password auth, so you could authenticate by just passing those credentials along to the data source, preferably via environment variables (see Chapter 3). This is still the easiest way to connect to data sources.\nOrganizations are increasingly turning to modern technologies, like OAuth and IAM, to secure access to data sources, including databases, APIs, and cloud services. Sometimes you’ll have to manually navigate the token exchange process in your Python or R code. For example, you’ve likely acquired and dispatched an OAuth token to access a Google Sheet or a modern data API.\nIncreasingly, IT/Admins want users to have the experience of logging in and automatically accessing data sources. This situation is sometimes termed passthrough auth or identity federation. This is a great user experience and is highly secure because there are never any credentials in the data science environment, only secure cryptographic tokens.\nHowever, this experience is more complicated than it appears. From what we’ve discussed so far, it seems like you could use the SSO token that got you into the data science environment to access the data source. But that doesn’t work. Instead, each service has its own token that can’t be transformed into one for a different service for security reasons. That means “passthrough” is a misnomer, and a much more complicated exchange occurs.\n\nOAuth and IAM are quickly becoming industry standards for accessing data sources, but automated handoffs for every combination of SSO technology, data science platform, and service token aren’t fully implemented. I expect broader adoption in the next few years. For now, you’ll need to talk to your IT/Admin team about whether there’s an integration to access a data source seamlessly when you log in or if you’ll have to continue using a username and password for a little longer.\nAnother technology you may encounter when accessing data sources is an old, but very secure, technology called Kerberos that uses a Kerberos Ticket to connect to databases and file shares. Kerberos is most often used in on-prem Windows environments with LDAP/AD."
  },
  {
    "objectID": "chapters/sec3/3-2-auth.html#managing-permissions",
    "href": "chapters/sec3/3-2-auth.html#managing-permissions",
    "title": "16  Auth in Enterprise",
    "section": "16.5 Managing permissions",
    "text": "16.5 Managing permissions\nIrrespective of the technology used, your organization will have policies about managing permissions that you must incorporate or adopt.\nThere are meaningful differences in how LDAP, SAML, and OAuth communicate to services about permissions. That’s a level of detail beyond this chapter – more in Appendix A if you’re interested.\nIf your organization has a policy that you’re going to need to be able to enforce inside the data science environment, it’s most likely a Role Based Access Control (RBAC) policy. In RBAC, permissions are assigned to an abstraction called a role. Users and groups are then given roles depending on their needs.\nFor example, there might be a manager role that should have access to specific permissions in the HR software. This role would be applied to anyone in the data-science-manager group as well as the data-engineering-manager group.\nThere are a few issues with RBAC. Most importantly, if there are many idiosyncratic permissions, creating tons of special roles is often simpler than figuring out how to harmonize them into a system.\nMany organizations haven’t reached the complexity of adopting RBAC. They often use simple Access Control Lists (ACLs) of who can access each service.4 ACLs have the advantage of being conceptually simple, but maintaining individual lists for each service is a lot of work with many services and users.\nSome organizations are moving toward even more granular techniques than RBAC and are adopting Attribute Based Access Control (ABAC). In ABAC, permissions are granted based on an interaction of user-level and object-level attributes and a rules engine.\nFor example, you can imagine three distinct attributes a user could have: data-science, data-engineer, and manager. You could create a rules engine that provides access to different resources based on the combinations of these attributes.\nRelative to RBAC, ABAC is a more robust system that allows for more granular permissions, but it’s a much bigger lift to configure initially. You’ve already encountered an ABAC system in the AWS IAM system. You were probably completely befuddled if you tried to configure anything in IAM. You can thank the power and complexity of ABAC."
  },
  {
    "objectID": "chapters/sec3/3-2-auth.html#comprehension-questions",
    "href": "chapters/sec3/3-2-auth.html#comprehension-questions",
    "title": "16  Auth in Enterprise",
    "section": "16.6 Comprehension Questions",
    "text": "16.6 Comprehension Questions\n\nWhat is the difference between authentication and authorization?\nWhat are some advantages of token-based auth? Why are most organizations adopting it? Are there any drawbacks?\nFor each of the following, is it a username + password method or a token method? PAM, LDAP, Kerberos, SAML, ODIC/OAuth\nWhat are some different ways to manage permissions? What are the advantages and drawbacks of each?"
  },
  {
    "objectID": "chapters/sec3/3-2-auth.html#footnotes",
    "href": "chapters/sec3/3-2-auth.html#footnotes",
    "title": "16  Auth in Enterprise",
    "section": "",
    "text": "I’m using the term token colloquially. The actual name for this token depends on the underlying technology and may be called a token, ticket, or assertion.↩︎\nUntil recently, Microsoft Entra ID was called Azure Active Directory, which confusingly was for SSO, not Active Directory. That’s probably why they changed the name.↩︎\nOIDC is an authentication standard based on the much broader OAuth authorization standard. As a user, you’ll never know the difference.\nThere is a technology called Kerberos that some organizations use to accomplish SSO with LDAP/AD, though this is rare.↩︎\nStandard Linux permissions (POSIX permissons) that were discussed in Chapter 9 are a special case of ACLs. ACLs allow setting individual-level permissions for any number of users and groups, as opposed to the one owner, one group, and everyone else permissions set for POSIX.\nLinux distros now have support for ACLs on top of the standard POSIX permissions.↩︎"
  },
  {
    "objectID": "chapters/sec3/3-3-ent-scale.html#devops-best-practices",
    "href": "chapters/sec3/3-3-ent-scale.html#devops-best-practices",
    "title": "17  Compute at enterprise scale",
    "section": "17.1 DevOps best practices",
    "text": "17.1 DevOps best practices\nThe process of standing up an environment comes in two stages. First the servers and networking need to be provisioned, which involves creating all the required resources. Once that’s done, they must be configured, which means installing and activating applications like Python, R, JupyterHub, and RStudio Server.\nThe IT/Admin group manages anywhere from dozens to thousands of servers in an enterprise. In many enterprises, provisioning and configuration are done by separate groups. There is often one central team that provisions servers, another central function that manages networking, and another team that does configuration and application administration.\nTo keep that many servers manageable, the IT/Admin group tries to make them standardized and interchangeable. This idea is often encompassed in the adage that “servers should be cattle, not pets”. That means that you almost certainly won’t be allowed to SSH in and make whatever changes you might want as a data scientist.\n\n\n\n\n\n\nNote\n\n\n\nAvoiding this complexity is why many organizations are moving away from directly managing servers. Instead, they outsource server management by acquiring PaaS or SaaS software from cloud providers.\n\n\nIndeed, in many organizations, no one is allowed to SSH in and make changes. Instead, all changes must go through a robust change management process and are deployed via Infrastructure as Code (IaC) tools so the environment can always be torn down and replaced quickly.\nThere are many different IaC tools that your IT/Admins may use. These include Terraform, Ansible, CloudFormation (AWS’s IaC tool), Chef, Puppet, and Pulumi. Most of these tools can do both provisioning and configuration; however, since most specialize in one or the other, many organizations use a pair of them together.\n\n\n\n\n\n\nNo Code IaC\n\n\n\nSome enterprises manage servers without IaC. These usually involve writing extensive run books to tell another person how to configure the servers. If your spidey sense is tingling, you’re correct that this probably isn’t nearly as good as IaC. Learning that your enterprise IT/Admin organization doesn’t use IaC tooling is a red flag.\n\n\nAlong with making deployments via IaC, organizations that follow DevOps best practices use a Dev/Test/Prod setup for making changes to servers and applications. The Dev and Test environments, often called lower environments, are solely for testing changes to the environment itself. To differentiate these environments from the data scientist’s Dev and Test environments, I often refer to this as Staging.1\nGenerally, you won’t have access to the Staging environment at all, except for potentially doing user acceptance testing for changes there.\nIn this setup, promotion is a two-dimensional grid, with IT/Admins working on changes to the environment in Staging and data scientists working on data science projects in Dev and Test within the Prod IT/Admin environment. Ultimately, the goal is to create an extremely reliable Prod-Prod environment.\n\nIn enterprises, moves from staging to prod, including upgrades to applications or operating systems or adding system libraries have rules around them. They may need to be validated or approved by security. In some highly regulated environments, the IT/Admin group may only be able to make changes during specific times. This can cause tension between a data scientist who wants a new library or version now and an IT/Admin who can’t move that fast.\nIn addition to changes from staging to prod, enterprises sometimes undergo a complete rebuild of their environments. Many of those rebuilds result from a move to the cloud, which can be a multi-year affair."
  },
  {
    "objectID": "chapters/sec3/3-3-ent-scale.html#compute-for-many-users",
    "href": "chapters/sec3/3-3-ent-scale.html#compute-for-many-users",
    "title": "17  Compute at enterprise scale",
    "section": "17.2 Compute for many users",
    "text": "17.2 Compute for many users\nWith many data scientists, you outstrip the ability of any one server – even a big one – to accommodate all the work that needs to get done. How many data scientists it takes to overtax a single server depends entirely on what data scientists do at your organization. You may hit it with only one person if you do intensive simulation work or deep learning. On the other hand, I’ve seen organizations that primarily work on small data sets that can comfortably fit 50 concurrent users on a single server.\nOnce you need multiple servers to support the data science team(s), you must horizontally scale. There is a simple way to horizontally scale, giving every user or group a standalone server. In some organizations this can work very well. The downside is that this results in a lot of environments. That can be a hassle for the IT/Admin to manage or they delegate server management to the individual teams.\nMany enterprises don’t want a profusion of team-level data science environments. Instead, they want to run one centralized service that handles much of the organization’s data science needs. Managing just one environment simplifies operations in some ways but also drastically increases the cost of downtime. For example, one hour of downtime for a platform that supports 500 data scientists wastes over $25,000.2\n\n\n\n\n\n\nMeasuring Uptime\n\n\n\nOrganizations often introduce Service Level Agreements (SLAs) or Operating Level Agreements (OLAs) about how much downtime is allowed. These limits are usually measured in nines of uptime, which refers to the proportion of the time that the service is guaranteed to be online. So, a one-nine service is guaranteed to be up 90% of the time, allowing 36 days of downtime a year. A five-nine service is guaranteed up 99.999% of the time, allowing only 5 1/4 minutes of annual downtime.\n\n\nTherefore, organizations that support enterprise data science platforms focus seriously on avoiding downtime. Most have a disaster recovery policy. Sometimes, that policy dictates maintaining frequent (often nightly) snapshots so they can roll back to a known good state in case of failure. Sometimes, it means a copy of the environment is waiting on standby in case of an issue with the primary environment.\nOther times, there are stiffer requirements that the environment does not experience downtime at all. This requirement for limited cluster downtime is often called high availability."
  },
  {
    "objectID": "chapters/sec3/3-3-ent-scale.html#computing-in-clusters",
    "href": "chapters/sec3/3-3-ent-scale.html#computing-in-clusters",
    "title": "17  Compute at enterprise scale",
    "section": "17.3 Computing in clusters",
    "text": "17.3 Computing in clusters\nWhether for horizontal scaling or high availability reasons, most enterprises run their data science environments in a load-balanced cluster, a set of servers (nodes) that operate together as one unit. Ideally, working in a cluster feels the same as working in a single-server environment, but there are multiple servers to add computational power or provide resilience if one server fails.\nFor a cluster to operate as a single environment, the IT/Admin need to solve two problems. First, they want to provide a single front door that routes users to a node in the cluster, preferably without them being aware of it happening. This is accomplished with a load balancer, a kind of proxy server.\nSecond, they must ensure that the user can save things (state) on the server and access that state even if they end up on a different node later. This is accomplished by setting up storage so persistent data doesn’t stay on the nodes. Instead, it lives in separate storage, most often a database and/or file share, that is symmetrically accessible to all the nodes in the cluster.\n\nIf you are a solo data scientist reading this, please do not try to run a load-balanced data science cluster. When you undertake load balancing, you’ve tackled a distributed systems problem, which is inherently difficult.\nIt’s worth noting that load balancing doesn’t eliminate the single points of failure anathema to a high-availability setup. In fact, it’s possible to make your system less stable by carelessly load balancing several nodes. For example, what if your load balancer were to fail? How will the system be resilient to bad performance in the state storage? Sophisticated IT/Admin organizations have answers to these questions and standard ways they implement high availability.\n\n\n\n\n\n\nNote\n\n\n\nFor technical information on how load balancers work and different types of configuration, see Appendix B."
  },
  {
    "objectID": "chapters/sec3/3-3-ent-scale.html#docker-in-enterprise-kubernetes",
    "href": "chapters/sec3/3-3-ent-scale.html#docker-in-enterprise-kubernetes",
    "title": "17  Compute at enterprise scale",
    "section": "17.4 Docker in enterprise = Kubernetes",
    "text": "17.4 Docker in enterprise = Kubernetes\nInitially created at Google and released in 2014, the open-source Kubernetes (K8S, pronounced koo-ber-net-ees or kates for the abbreviation) is the way to run production services out of Docker containers.3 Many organizations are moving toward running most or all of their production work in Kubernetes.\n\n\n\n\n\n\nNote\n\n\n\nApparently, Kubernetes is an ancient Greek word for “helmsman”.\n\n\nIn a load-balanced cluster of servers, the IT/Admin must provision each node and then configure applications on it. In Kubernetes, the IT/Admin creates and registers a cluster of worker nodes. The only requirement for the worker nodes is that they run Kubernetes. No application-level configuration occurs on the nodes.\nTo run applications in a Kubernetes cluster, an IT/Admin tells the cluster’s control plane to run a set of Docker containers with a certain amount of computational power allocated to each one. These Docker Containers running in Kubernetes are called pods.\nThe elegance of Kubernetes is that the IT/Admin doesn’t have to think about where each pod goes. The control plane schedules the pods on the nodes without a human having to consider networking or application requirements.\n\nFrom the IT/Admin’s perspective, this is wonderful because they ensure the cluster has sufficient horsepower and all the app requirements come in the container, which makes node configuration trivial. The app’s authors are often responsible for building the Docker Containers, removing one thing from the IT/Admin’s to-do list.\nFor production purposes, pod deployments are usually managed with Helm charts, the standard IaC way to declare what pods you want, how many you need, and their relationships.\nAlmost any IT/Admin running Kubernetes can add nodes with a few clicks because they’re using a cloud provider’s managed service: AWS’s EKS (Elastic Kubernetes Service, Azure’s AKS (Azure Kubernetes Service), or GCP’s GKE (Google Kubernetes Engine).4\nKubernetes is a powerful tool, and there are many reasons to like it. But like most powerful tools, it’s also complicated. Becoming a proficient Kubernetes admin is a skill unto itself. These days, many IT/Admins are trying to add Kubernetes to their list of skills. If you have a competent Kubernetes admin, you should be in good shape, but you should be careful that setting up your data science environment doesn’t become someone else’s chance to learn Kubernetes."
  },
  {
    "objectID": "chapters/sec3/3-3-ent-scale.html#sizing-and-scaling-clusters",
    "href": "chapters/sec3/3-3-ent-scale.html#sizing-and-scaling-clusters",
    "title": "17  Compute at enterprise scale",
    "section": "17.5 Sizing and scaling clusters",
    "text": "17.5 Sizing and scaling clusters\nYour IT/Admin team probably knows a lot about how to run a cluster but not much about how data scientists use computational resources. To run a cluster for your work, they will need your help deciding how many machines you need and how to size and configure those machines. Here are some mental models I’ve found helpful to communicate with IT/Admin teams about sizing and scaling an enterprise cluster.\nGenerally, organizations run the workbench in one cluster and the deployment platform in a second. That’s because the usage patterns are very different between the two. From a resource perspective, a deployment platform looks a lot like any other application hosting platform. Your IT/Admin team will probably want help figuring out the size, but they’ll be comfortable with the basic idea.\nA data science workbench, on the other hand, probably looks nothing like the platforms your IT/Admin team is most used to supporting. A workbench supports a relatively small number of professionals doing computationally-intensive tasks, but in patterns that may be unusual for the IT/Admin team to support.\nUsually, a data science platform has a baseline load. For the workbench, this is the average number of data professionals who visit on a daily basis and the scale of the jobs they’re running. For the deployment platform, it’s the number of jobs and app and API sessions that run on a normal day. Aside from the total load for this work, the IT/Admins will also need to know the minimum node size to support a single session. For example, some organizations have a large cluster, but each job could comfortably fit on a machine with 16 GB of RAM, but that would be woefully inadequate for others.\nSecond, there’s burst usage. This is extra capacity across both the workbench and deployment environment that is sometimes needed to support additional users, jobs, or applications. For example, your organization might occasionally do workshops that pull in more users than usual, or you’re launching a new data science app that will add load to the deployment cluster.\nLast, there’s the need for special-purpose computational resources. These needs for very large or GPU-backed instances are usually transient for particular projects with very large data or while training specific machine learning models. These are common needs for a workbench environment but are less likely in the deployment environment unless you’re re-training machine learning models on a schedule in the deployment environment.\nThe IT/Admin team will be pleased if you can help them figure out a reasonable estimate for each of the following:\n\nBaseline number of CPU cores, amount of RAM, and the largest jobs you’ll have to accommodate daily for the workbench and deployment environment.\nBurst usage in terms of frequency, duration of burst demand, and amount of CPU cores and RAM as best you can estimate.\nSpecial-purpose needs in frequency, duration, job sizes, and GPU needs.\n\nMost teams will find it unaffordable to keep all this capacity continuously available, and your IT/Admin team will want to devise a scaling strategy.\nUsually, the IT/Admin team will target some always-on capacity, often matching the baseline load. Ideally, this can be run on just one machine. If not, running the appropriate number of approximately laptop-sized machines is common. In AWS, this often means a certain number of t3.xlarges with four CPU cores and 16 GB of RAM or t3.2xlarges with twice that capacity.\nWhen burst usage or special purpose machines are needed for a discrete time (i.e., a day or longer), it’s often easiest for an IT/Admin to manually stand up more machines with IaC. That means creating a temporary standalone environment for special-purpose projects or manually expanding the size of an existing cluster.\nHowever, manual adjustment doesn’t work for some organizations. Organizations with complex data source and authentication interactions may be unable to create new clusters quickly. In others, organizational rules and requirements might make it hard to stand up new nodes.\nThese other organizations often want to run only one cluster and may want that cluster to scale itself automatically. Some very competent IT/Admin teams can create a single autoscaling data science cluster, but it’s hard. I’ve often found that organizations trying for a single cluster are besotted with the elegance of the idea and underestimate the difficulty of making it work. You may have to impress upon your IT/Admins that running heterogeneous workloads in one cluster isn’t trivial and that a data science workbench is a particularly awkward fit for the autoscaling frameworks they probably prefer.\nAutoscaling to make a cluster bigger is never a problem, but autoscaling a workbench cluster back down is tricky. Most autoscaling frameworks choose the cluster size based on actual resource usage (usually CPU). But CPU usage is very uneven for a workbench cluster. Most activities in a data science workbench, like typing code, debugging, or examining data, use minuscule amounts of CPU. Your brain is cranking, but the CPU is not.\nThat means a CPU-based autoscaling framework will register that node as unused and think it’s ok to autoscale away. As a cluster user, you will experience this as a frustratingly unstable system. Autoscaling frameworks that work well for a data science workbench need to have a notion of active sessions that isn’t based on moment-to-moment resource usage.\nThese days, many IT/Admins automatically assume autoscaling means Kubernetes, as it has powerful tools for managing cluster size. Unfortunately, Kubernetes can be an awkward fit for a data science workbench depending on your needs. First, Kubernetes pods are generally limited to the size of the underlying nodes. So if you’ve got a cluster of t3.xlarges, pods are usually limited to 16 GB of RAM or less. Second, Kubernetes autoscaling is usually CPU-based and you must trick the Kubernetes scheduling engine to work well for a data science workbench. Last, Kubernetes allows for heterogeneous nodes (e.g., large or GPU-backed) in one cluster, but it’s not a core use case and can be awkward to configure.5\nHigh performance computing (HPC) frameworks like Slurm are often a better fit for a data science workbench. Most people think about HPC in the context of supercomputing. That’s because HPC allows users to address a cluster of many machines as one, so you can have thousands of CPU cores working on one job. But HPC also works quite well at a more modest size, for example, to run a workload requiring 10 cores in a cluster with only t3.xlarges with 4 cores each.\nEven if you don’t have any node sizing issues, HPC can be an excellent fit for autoscaling or a cluster with heterogeneous nodes. HPC frameworks are generally quite session-aware, making them a good choice for autoscaling a data science workbench. Most also support different categories of work (called queues in Slurm) right out of the box. If you’re interested in trying out Slurm, AWS has a service called ParallelCluster that allows users to set up an HPC cluster with no additional cost beyond the EC2 instances in the cluster.\nThe upshot is that running a single data science cluster that autoscales is hard. It’s usually easier to create one-off data science environments or manually scale a cluster. If your organization does want a single environment, most IT/Admins will use a standard autoscaling framework or Kubernetes, but HPC may be a better solution."
  },
  {
    "objectID": "chapters/sec3/3-3-ent-scale.html#comprehension-questions",
    "href": "chapters/sec3/3-3-ent-scale.html#comprehension-questions",
    "title": "17  Compute at enterprise scale",
    "section": "17.6 Comprehension Questions",
    "text": "17.6 Comprehension Questions\n\nWhat is the difference between horizontal and vertical scaling? For each of the following examples, which one would be more appropriate?\n\nYou’re the only person using your data science workbench and run out of RAM because you’re working with very large data sets in memory.\nYour company doubles the size of the team working on your data science workbench. Each person will be working with reasonably small data, but there will be a lot more of them.\nYou have a big modeling project that’s too large for your existing machine. The modeling you’re doing is highly parallelizable.\n\nWhat is the role of the load balancer in horizontal scaling? When do you need a load balancer and when can you go without?\nWhat are the biggest strengths of Kubernetes as a scaling tool? What are some drawbacks? What are some alternatives? When is HPC a better fit?"
  },
  {
    "objectID": "chapters/sec3/3-3-ent-scale.html#footnotes",
    "href": "chapters/sec3/3-3-ent-scale.html#footnotes",
    "title": "17  Compute at enterprise scale",
    "section": "",
    "text": "You’ll have to fight out who gets to claim the title Dev/Test/Prod for their environments with the IT/Admins at your organization. Be nice, they probably had the idea long before you did.↩︎\nAssuming a (probably too low) fully-loaded cost of $100,000 and 2,000 working hours per year.↩︎\nIf you are pedantic, there are other tools for deploying Docker containers like Docker Swarm and Kubernetes is not limited to Docker containers. But for all practical purposes, production Docker = Kubernetes.↩︎\nIt’s rare, but some organizations do run an on-prem Kubernetes cluster with Oracle’s OpenShift.↩︎\nYou, or your IT/Admin, will have to work with taints, tolerations, node pool selectors, node affinities, and more.↩︎"
  },
  {
    "objectID": "chapters/sec3/3-4-ent-pm.html#ensuring-packages-are-safe",
    "href": "chapters/sec3/3-4-ent-pm.html#ensuring-packages-are-safe",
    "title": "18  Package Management in the Enterprise",
    "section": "18.1 Ensuring packages are safe",
    "text": "18.1 Ensuring packages are safe\nIT/Admins’s biggest concern about packages is that they might be unsafe. For example, a package could introduce an exploitable bug in your code, be a trojan horse that exfiltrates data when activated, or include incorrect code that will yield numerically bad results.\nSome of these concerns are lessened because most data science projects run entirely inside a private environment. For example, worries about exploitable Javascript code are significantly reduced when the only people with access to the application are already staff at your organization. Similarly, a package that maliciously grabs data from your system and exports it will be ineffective in an airgapped environment.\nIT/Admins help create “validated” environments with trusted packages in some industries. Those validated environments may also be locked to particular projects. This is particularly common in highly regulated industries with longstanding statistical practices, like pharmaceuticals.\nA basic, but effective, form of package security is to limit the allowable packages to popular packages, packages from known organizations and authors, or packages that outside groups have validated as safe. Some organizations pay for paid products or services to do this validation. Increasingly, industry groups are creating lists of packages that meet quality and security standards. For example, the R Validation Hub is a pharmaceutical industry group creating lists of R packages that meet quality standards.\nEven IT/Admin organizations that don’t help validate packages for quality may want to check incoming packages for known security vulnerabilities.\nEvery day, software security vulnerabilities are identified and publicized. These vulnerabilities are maintained in the CVE (Common Vulnerabilities and Exposures) system. Each CVE is assigned an identifier and a severity score that ranges from None to Critical.\nThese CVEs can get into your organization when they are in code, which is a component of the software you’re using directly. For example, a CVE in Javascript might show up in the version of Javascript used by Jupyter Notebook, RStudio, Shiny, or Streamlit. Many companies disallow using software with Critical CVEs and only temporarily allow software with a few High CVEs.\nBeyond checking for known vulnerabilities, some organizations try to ensure that packages aren’t introducing novel security issues via a code scanner. This software runs incoming code through an engine to detect potential security risks – like insecure encryption libraries or calls to external web services or databases.\nCode scanners are almost always paid tools. I believe that the creators of these tools often overstate the potential benefits and that a lot of code scanning is security theater. This is particularly true because of the languages used by data scientists. Javascript is extremely popular, is the front-end of public websites, and has reasonably well-developed scanning software. But it’s rarely used in data science. Python is very popular but is rarely on the front end of websites and, therefore, has fewer scanners. R is far less prevalent than either Python or Javascript, is never in a website front end, and has no scanners I know of.\nUnfortunately, your organization may require running Python and R packages through a code scanner, even if there’s little value from the activity."
  },
  {
    "objectID": "chapters/sec3/3-4-ent-pm.html#open-source-licensing-issues",
    "href": "chapters/sec3/3-4-ent-pm.html#open-source-licensing-issues",
    "title": "18  Package Management in the Enterprise",
    "section": "18.2 Open-source licensing issues",
    "text": "18.2 Open-source licensing issues\nIn addition to security issues, some organizations are concerned about the legal implications of using free and open-source software (FOSS) in their environment. These organizations, most often those selling software, want to limit the use of specific FOSS licenses in their environment.\n\n\n\n\n\n\nNote\n\n\n\nI am not a lawyer, and this should not be taken as legal advice; hopefully, this is helpful context on the legal issues with FOSS.\n\n\nWhen someone releases software, they can choose a license, a legal document explaining what consumers can do with the software.\nThe type of license you’re probably most familiar with is a copyright. A copyright gives the owner exclusivity to distribute the software and charge for it. For example, if you buy a copy of Microsoft Word, you have a limited license to use the software, but you’re not allowed to inspect the source code of Microsoft Word or to share the software.\nIn 1985, the Free Software Foundation (FSF) was created to support the creation of free software. They wanted to facilitate using, reusing, and sharing software. In particular, the FSF supported four freedoms for software:1\n\nRun the program however you wish for any purpose.\nStudy the source code of the program and change it as you wish.\nRedistribute the software as you wish to others.\nDistribute copies of the software once you’ve made changes so everyone can benefit.\n\nSomeone could try to grant these freedoms by simply not applying a copyright to their software. But then there would be no guarantee that they wouldn’t show up later, claiming they deserved a cut of software built on theirs. FOSS licensing made it clear what was permitted.\n\n\n\n\n\n\nWhat does “free” mean?\n\n\n\nIt’s expensive to create and maintain FOSS. For that reason, the free in FOSS is about freedom, not zero cost. As a common saying goes, it means free as in free speech, not free as in free beer.\nOrganizations have attempted to support FOSS with different business models to varying degrees of success. These models include pay-what-you-want models, donations or foundation support, paid features or products, advertising or integrations, and paid support, services, or hosting.\n\n\nThere isn’t just one FOSS license; instead, there are dozens. Permissive licenses allow you to do whatever you want with the FOSS software. For example, the permissive MIT license allows you to, “use, copy, modify, merge, publish, distribute, sublicense, and/or sell” MIT-licensed software without attribution. Most organizations have no concerns using software with a permissive open-source license.\nThe bigger concern is software with a copyleft or viral FOSS license. Copyleft software requires that any derivative works be released under a similar license. The idea is that open-source software should beget more open-source software and not be used by big companies to make megabucks.\nThe concern enterprises have with copyleft licenses is that they might legally propagate into private work inside the organization. For example, what if a court ruled that Apple or Google had to suddenly open-source all their software because of developers’ use of copyleft licenses?\nMuch of the concern centers around what it means for software to be a derivative work of another. Most people agree that artifacts created with copyleft-licensed software – like your plots, reports, and apps – are not themselves derivative works. But the treatment of software that incorporates copyleft-licensed software is murky. The reality is that since there have been no court cases on this topic, nobody knows how it would shake out if it does someday get to court, leading some organizations to err on the side of caution.\nThese concerns are less relevant for Python than for R. Python is released under a permissive Python Software Foundation (PSF) license and Jupyter Notebook under a permissive modified BSD. R is released under the copyleft GPL license and RStudio under a copyleft AGPL.\nHowever, every single package author can choose a license for themselves. In an enterprise context, these discussions focus on knowing – and potentially blocking – the use of packages under copyleft licenses inside the enterprise."
  },
  {
    "objectID": "chapters/sec3/3-4-ent-pm.html#controlling-package-flows",
    "href": "chapters/sec3/3-4-ent-pm.html#controlling-package-flows",
    "title": "18  Package Management in the Enterprise",
    "section": "18.3 Controlling package flows",
    "text": "18.3 Controlling package flows\nWhether your organization is trying to limit CVE exposure, run a code scanner, limit copyleft exposure, or stick to a known list of good packages, it needs a way to restrict the packages available inside the environment.\nIf you’ve given someone access to Python or R, you can’t remove the ability to run pip install or install.packages. That’s one reason why many enterprise environments are airgapped – it’s the only way to ensure data scientists can’t install packages from outside.\nMost IT/Admins understand that airgapping is the best way to stop unauthorized package installs. The next bit – that they do need to provide you a way to install packages – is the part that may require some convincing.\nMany enterprises run package repository software inside their firewall to govern package ingestion and availability. Most package repository products are paid because enterprises primarily need them. Common ones include Jfrog Artifactory, Sonatype Nexus, Anaconda Business, and Posit Package Manager.\nArtifactory and Nexus are generalized library and package management solutions for all sorts of software, while Anaconda and Posit Package Manager are more narrowly tailored for data science use cases. I’d suggest working with your IT/Admins to get data science focused repository software. Often these repositories can run alongside general-purpose repository software if you already have it.\nDepending on your repository software, it may connect to an outside sync service or support manual file transfers for package updates. In many airgapped environments, IT/Admins are comfortable having narrow exceptions so the package repository can download packages.\n\nThis tends to work best when the IT/Admin is the one who controls which packages are allowed into the repository and when. Then you, as the data scientist, have the freedom to install those packages into any individual project and manage them there using environment as code tooling, as discussed in Chapter 1."
  },
  {
    "objectID": "chapters/sec3/3-4-ent-pm.html#comprehension-questions",
    "href": "chapters/sec3/3-4-ent-pm.html#comprehension-questions",
    "title": "18  Package Management in the Enterprise",
    "section": "18.4 Comprehension Questions",
    "text": "18.4 Comprehension Questions\n\nWhat are the concerns IT/Admins have about packages in an enterprise context?\nWhat are three tools IT/Admins might use to ensure packages are safe?\nWhat is the difference between permissive and copyleft open-source licenses? Why are some organizations concerned about using code that includes copyleft licenses?"
  },
  {
    "objectID": "chapters/sec3/3-4-ent-pm.html#footnotes",
    "href": "chapters/sec3/3-4-ent-pm.html#footnotes",
    "title": "18  Package Management in the Enterprise",
    "section": "",
    "text": "They’re numbered 1-4 here, but like many numbered computer science lists, the official numbering actually begins with 0.↩︎"
  },
  {
    "objectID": "chapters/append/auth.html#service-based-auth",
    "href": "chapters/append/auth.html#service-based-auth",
    "title": "Appendix A — Technical Detail: Auth Technologies",
    "section": "A.1 Service-based auth",
    "text": "A.1 Service-based auth\nMany pieces of software come with integrated authentication. When you use those systems, the service stores encrypted username and password pairs in its own database. If you’re administering a single service, this is really simple. You just set up individual users on the service.\nBut once you have multiple services, everything has to be managed service-by-service. And the system is only as secure as what the service has implemented. Almost any organization with an IT/Admin group will prefer not to use service-based auth."
  },
  {
    "objectID": "chapters/append/auth.html#system-linux-accounts",
    "href": "chapters/append/auth.html#system-linux-accounts",
    "title": "Appendix A — Technical Detail: Auth Technologies",
    "section": "A.2 System (Linux) Accounts",
    "text": "A.2 System (Linux) Accounts\nMany pieces of software – especially data science workbenches – can look at the server it’s sitting on and authenticate against the user accounts and groups on the server.\nOn a Linux server, Pluggable Authentication Modules (PAM) allow a service to make use of the Linux host’s users and groups. As of this writing, PAM is the default authentication method for RStudio Server and JupyterHub.\nAs the name suggests, PAM includes modules that allow it to authenticate against different systems. The most common is to authenticate against the underlying Linux server, but it can also use LDAP/AD (common) or Kerberos tickets (uncommon).\n\nPAM can also be used to do things when users log in. The most common of these is initializing Kerberos tickets to connect with databases or connecting with shared drives.\nWhen PAM is used in concert with LDAP/AD, the Linux users are usually created automatically on the system using SSSD (System Security Services Daemon). This process is called joining the domain.\nThough conceptually simple, the syntax of PAM modules is confusing, and reading, writing, and managing PAM modules is onerous. Additionally, as more services move to the cloud, there isn’t necessarily an underlying Linux host where identities live and PAM can’t be used at all."
  },
  {
    "objectID": "chapters/append/auth.html#ldapad",
    "href": "chapters/append/auth.html#ldapad",
    "title": "Appendix A — Technical Detail: Auth Technologies",
    "section": "A.3 LDAP/AD",
    "text": "A.3 LDAP/AD\nFor many years, Microsoft’s Lightweight Directory Access Protocol (LDAP) implementation called Active Directory (AD) was the standard in enterprise auth. It is increasingly being retired in favor of token-based systems like SAML and OAuth2.0.\nSome services can use LDAP/AD indirectly via PAM, while others may be directly configured to talk to LDAP/AD.\n\n\n\n\n\n\nNote\n\n\n\nLDAP is an application-layer protocol, like HTTP. And like HTTP, there is an SSL-secured version called LDAPS. Because LDAP is almost always used only inside a private network, adoption of LDAPS is uneven. The default port for LDAP is \\(389\\) and for LDAPS it’s \\(636\\).\n\n\nStrictly speaking, LDAP/AD isn’t an authentication tool. It’s a hierarchical tree database that is good for storing organizational entities. Doing authentication with LDAP/AD consists of sending a search for the provided username/password combination to the LDAP/AD database using the ldapsearch command.\nWhen you configure LDAP/AD in an application, you’ll configure a search base, which is the subtree to look for users inside. Additionally, you may configure LDAP/AD with bind credentials of a service account to authenticate to the LDAP/AD server itself.\n\n\n\n\n\n\n\nNote\n\n\n\nDepending on your application and LDAP/AD configuration, it may be possible to skip the bind credentials look up the user with their own credentials in single-bind mode, as opposed to double-bind when there are bind credentials. Single-bind is inferior and shouldn’t be used unless you can’t get bind credentials.\n\n\nAn ldapsearch returns the distinguished name (DN) of the entity that you are looking for – assuming it’s found.\nHere’s what my entry in a corporate LDAP directory might look like this:\ncn: Alex Gold\nmail: alex.gold@example.com\nmail: alex.gold@example.org\ndepartment: solutions\nmobile: 555-555-5555\nobjectClass: Person\nThis is helpful information, but you’ll note that there’s no direct information about authorization. Instead, you configure the service to authorize certain users or groups. This is time-consuming and error-prone, as each service needs to be configured separately."
  },
  {
    "objectID": "chapters/append/auth.html#sec-kerberos",
    "href": "chapters/append/auth.html#sec-kerberos",
    "title": "Appendix A — Technical Detail: Auth Technologies",
    "section": "A.4 Kerberos Tickets",
    "text": "A.4 Kerberos Tickets\nKerberos is a relatively old, but very secure, token-based auth technology for use inside a private network. In Kerberos, encrypted tokens called Kerberos tickets are passed between the servers in the system. A system that is designed to authenticate against a Kerberos ticket is called kerberized.\nKerberos was widely adopted along with Active Directory, and it’s used almost exclusively in places that are running a lot of Microsoft products. The most frequent use of Kerberos tickets is to establish connections to Microsoft databases.\nIn a Kerberos-based system, users often store their credentials in a secure keytab, which is a file on disk. They can manually initialize a ticket using the kinit command or via a PAM session that automatically fetches a ticket upon user login.\nWhen a Kerberos session is initialized, the service sends the user’s credentials off to the central Kerberos Domain Controller (KDC) and requests the Ticket Granting Ticket (TGT) from the KDC. Like most token authentication, TGTs have a set expiration period and must be reacquired when they expire.\nWhen the user wants to access a service, they send the TGT back to the KDC again along with the service they’re trying to access and get a session key (sometimes referred to as a service ticket) that allows access to a particular service.\n\nKerberos is only used inside a corporate network and is tightly linked to the underlying servers. That makes it very secure. Even if someone stole a Kerberos ticket, it would be very hard for them to use it.\nOn the other hand, because Kerberos is so tightly tied to servers, it is a difficult fit alongside cloud technologies and services."
  },
  {
    "objectID": "chapters/append/auth.html#oauth-saml",
    "href": "chapters/append/auth.html#oauth-saml",
    "title": "Appendix A — Technical Detail: Auth Technologies",
    "section": "A.5 Modern systems: OAuth + SAML",
    "text": "A.5 Modern systems: OAuth + SAML\nMost organizations are now quickly moving toward implementing a modern token-based authentication system through SAML and/or OAuth2.0.\nWhen you go to log in to a service that uses SAML or OAuth, you are redirected to the SAML/OAuth identity provider to seek a token that will let you in. Assuming all goes well, you’re granted a token and go back to the service to do your work.\nBoth OAuth and SAML rely on plain HTTP traffic, making them easier to configure than LDAP/AD or Kerberos from a networking standpoint.\n\nA.5.1 SAML\nThe current SAML 2.0 standard was finalized in 2005, roughly coinciding with the beginning of the web’s modern era, with Facebook launching just the prior year.\nSAML was invented to be a successor to enterprise auth methods like LDAP/AD and Kerberos. SAML uses encrypted and cryptographically signed XML tokens that are generated through a browser redirect flow.\nIn SAML, the service you’re accessing is called the service provider (SP) and the entity issuing the token is the SAML identity provider (IdP). Most SAML tooling allows you start at either the IdP or the SP.\nIf you start at the SP, you’ll get redirected to the IdP. The IdP will verify your credentials. If the credentials are valid, the IdP will put a SAML token in your browser, which the SP will use to authenticate you.3\n\nA SAML token contains several claims, which usually include a username and may include groups or other attributes. Whoever controls the IdP can configure what claims appear on the token at the IdP. The SAML standard itself is for authentication, not authorization, but it’s very common for an application to have required or optional claims that it can interpret to do authorization.\n\n\nA.5.2 OAuth\nOAuth was started in 2006 and the current 2.0 standard was finalized in 2013. OAuth 2.1 is under development as of 2023.\nOAuth was designed to be used with different services across the web from the beginning. Any time you’ve used a Log in with Google/Facebook/Twitter/GitHub flow – that’s OAuth.\nOAuth relies on passing around cryptographically signed JSON Web Tokens (JWTs). This makes OAuth more straightforward to debug than SAML because the JWT is plain-text JSON with a signature that proves it’s valid.\nUnlike a SAML token that always lives in a browser cache, JWTs can go anywhere. They can live in the browser cache, but they also can pass from one server to another to do authorization or can be saved in a user’s home directory. For example, if you’ve accessed Google Sheets or another Google service from R or Python, you may have manually handled the resulting OAuth token in your home directory.\nOAuth is an authorization scheme, so the contents of a JWT are about the permissions of the bearer of the token. A related standard called OpenID Connect (OIDC) can be used to do authentication with OAuth tokens. Over the next few years, I fully expect all data access to move toward using OAuth tokens.\nIn OAuth, the service you’re trying to visit is called the resource server and the token issuer is the authorization server. When you try to access a service, the service knows to look for a JWT that includes specific claims against a set of scopes. If you don’t have a JWT, you must seek it from the authorization server.\nFor example, if you want to read my Google Calendar, you need a JWT that includes a claim granting read access against the scope of events on Alex’s calendar.\n\nUnlike in SAML where action occurs via browser redirects, OAuth makes no assumptions about how this flow happens. The process of requesting and getting a token can happen in several different ways, including browser redirects and caches, but could be done entirely in R or Python."
  },
  {
    "objectID": "chapters/append/auth.html#user-provisioning",
    "href": "chapters/append/auth.html#user-provisioning",
    "title": "Appendix A — Technical Detail: Auth Technologies",
    "section": "A.6 User Provisioning",
    "text": "A.6 User Provisioning\nWhen you’re using a service, users often need to be created (provisioned) in that system. Sometimes, the users will be provisioned the first time they log in. In other cases, you may want the ability to provision them beforehand.\nLDAP/AD is very good for user provisioning. You can often configure your application to provision everyone who comes back from a particular ldapsearch. In contrast, token-based systems don’t know anything about you until you show up for the first time with a valid token.\nThere is a SAML-based provisioning system called SCIM (System for Cross-Domain Identity Management) that is slowly being adopted by many IdPs and SPs."
  },
  {
    "objectID": "chapters/append/auth.html#footnotes",
    "href": "chapters/append/auth.html#footnotes",
    "title": "Appendix A — Technical Detail: Auth Technologies",
    "section": "",
    "text": "As in Chapter 16, I’m using the term token as a summary, but Kerberos, SAML, and OAuth all have different names – more on that below.↩︎\nTo be precise, possible if integrated with Kerberos, but unlikely.↩︎\nThe diagram assumes you don’t already have a token in your browser. If the user has a token already, steps 2-5 get skipped.↩︎"
  },
  {
    "objectID": "chapters/append/lb.html#load-balancer-settings",
    "href": "chapters/append/lb.html#load-balancer-settings",
    "title": "Appendix B — Technical Detail: Load balancers",
    "section": "B.1 Load balancer settings",
    "text": "B.1 Load balancer settings\nRegardless of which load balancer you’re using, a basic requirement is that it knows what nodes are accepting traffic. This is accomplished by configuring a health check/heartbeat for the application on the node. A health check is an application feature that responds to periodic pings from the load balancer. If no response comes back, the load balancer treats that node as unhealthy and doesn’t send traffic there.\nFor applications that maintain user state, like Shiny apps, you want to get back to the same node in the cluster so you can resume a previous session. This can be enabled with sticky sessions or sticky cookies. In most load balancers, this is simply an option you can activate."
  },
  {
    "objectID": "chapters/append/lb.html#ways-to-configure-load-balancing",
    "href": "chapters/append/lb.html#ways-to-configure-load-balancing",
    "title": "Appendix B — Technical Detail: Load balancers",
    "section": "B.2 Ways to configure load balancing",
    "text": "B.2 Ways to configure load balancing\nThe simplest form of load balancing is to rotate traffic to each healthy node in a round-robin configuration. Depending on the capabilities of the load balancer and what metrics are emitted by the application, it may also be possible or desirable to do more sophisticated load balancing that routes sessions according to how loaded each node is.\nUsually, load balancers are configured to send traffic to all the nodes in the cluster in an active/active configuration. It is also possible to configure the load balancer in an active/passive configuration to send traffic to only some of the nodes, with the rest remaining inert until they are switched on – usually in the event of a failure in the active ones. This is sometimes called a blue/green or red/black configuration when it’s used to diminish downtime in upgrades and migrations."
  },
  {
    "objectID": "chapters/append/lb.html#shared-state",
    "href": "chapters/append/lb.html#shared-state",
    "title": "Appendix B — Technical Detail: Load balancers",
    "section": "B.3 Shared state",
    "text": "B.3 Shared state\nAside from the load balancer, the nodes need to be able to share state so users can have the same experience on each node. The requirements for that shared state depend on the software.\nOften the shared state takes the form of a database (often Postgres) and/or Network Attached Storage (NAS, pronounced naahz) for things that get stored in a filesystem.\nIf your NAS is exclusively for Linux, it would use NFS (Network File System). If Windows is involved, you’d use SMB (Server Message Block) or Samba to connect SMB to a Linux server. There’s also an outdated Windows NAS called CIFS (Common Internet File System) that you might see in older systems.\nEach of the cloud providers has a NAS offering. AWS has EFS (Elastic File System) and FSx. Azure has Azure File, and GCP has Filestore."
  },
  {
    "objectID": "chapters/append/lb.html#upgrades-in-ha",
    "href": "chapters/append/lb.html#upgrades-in-ha",
    "title": "Appendix B — Technical Detail: Load balancers",
    "section": "B.4 Upgrades in HA",
    "text": "B.4 Upgrades in HA\nSometimes IT/Admins want to upgrade the software running in the cluster without taking the service offline. This is called a zero-downtime upgrade. In a zero-downtime upgrade, you take some nodes offline, upgrade them, put them back online, and then upgrade the remainder.\nTo accomplish this feat, there are two features the application needs to support. If it doesn’t support both, you’ll need to endure some downtime to do an upgrade.\nThe first is node draining. If you just naively removed a node, you might kill someone’s active session. Instead, you’d want to configure the node so that it doesn’t kill any existing sessions but also doesn’t accept any new ones. As the current sessions end, the node empties and you can safely take it offline when all the sessions are gone.\nThe second is rolling upgrade, which is the ability to support mixed software versions in the same cluster. When you upgrade a piece of software, there are often changes to how the data in the shared state is stored. That means the creators would need to undertake painstaking work to avoid conflicts during the upgrade process. Because it’s tricky to support active sessions in a cluster with mixed versions, it’s relatively uncommon.\nIf your application doesn’t support zero downtime upgrades, some organizations like to get close by building a second copy of the server and its applications, getting it almost live, and then taking downtime solely to switch the networking over. That’s generally much faster than building the whole thing during downtime."
  },
  {
    "objectID": "chapters/append/lab-map.html",
    "href": "chapters/append/lab-map.html",
    "title": "Appendix C — Lab Map",
    "section": "",
    "text": "This section aims to clarify the relationship between the assets you’ll make in each portfolio exercise and labs in this book.\n\n\n\n\n\n\n\nChapter\nLab Activity\n\n\n\n\nChapter 1: Environments as Code\nCreate a Quarto site that uses {renv} and {venv} to create standalone R and Python virtual environments. Add an R EDA page and Python modeling.\n\n\nChapter 2: Project Architecture\nCreate an API that serves a Python machine-learning model using {vetiver} and {fastAPI}. Call that API from a Shiny App in both R and Python.\n\n\nChapter 3: Data Architecture\nMove data into a DuckDB database and serve model predictions from an API.\n\n\nChapter 4: Logging and Monitoring\nAdd logging to the app from Chapter 2.\n\n\nChapter 5: Deployments\nPut a static Quarto site up on GitHub Pages using GitHub Actions that renders the project.\n\n\nChapter 6: Docker\nPut API from Chapter 2 into Docker Container.\n\n\nChapter 7: Cloud\nStand up an EC2 instance.\nPut the model into S3.\n\n\nChapter 8: Command Line\nLog into the server with .pem key and create SSH key.\n\n\nChapter 9: Linux Admin\nCreate a user on the server and add SSH key.\n\n\nChapter 10: Application Admin\nAdd R, Python, RStudio Server, JupyterHub, API, and App to EC2 instance from Chapter 7.\n\n\nChapter 11: Scaling\nResize the server.\n\n\nChapter 12: Networking\nAdd proxy (NGINX) to reach all services from the web.\n\n\nChapter 13: DNS\nAdd a URL to the EC2 instance. Put the Shiny app into an iFrame on the Quarto site.\n\n\nChapter 14: SSL\nAdd SSL/HTTPS to the EC2 instance."
  },
  {
    "objectID": "chapters/append/cheatsheets.html#environments-as-code",
    "href": "chapters/append/cheatsheets.html#environments-as-code",
    "title": "Appendix D — Cheatsheets",
    "section": "D.1 Environments as code",
    "text": "D.1 Environments as code\n\nD.1.1 Checking library + repository status\n\n\n\n\n\n\n\n\nStep\nR Command\nPython Command\n\n\n\n\nCheck whether library is in sync with lockfile.\nre nv::status()\nNone\n\n\n\n\n\nD.1.2 Creating and using a standalone project library\nMake sure you’re in a standalone project library.\n\n\n\n\n\n\n\n\n\n\n\nStep\n\n\n\n\nR Command\n\n\n\n\nPython Command\n\n\n\n\n\n\nCreate a standalone library.\n\n\n\n\nrenv::init()\n\n\nTip: get {renv} w/ install.p ackages(“renv”)\n\n\n\n\np ython -m venv &lt;dir&gt;\n\n\nConvention: use.venv for &lt;dir&gt;\n\n\nTip: {venv} included w/ Python 3.5+\n\n\n\n\n\n\nActivate project library.\n\n\n\n\nr env::activate()\n\n\nHappens automatically if in RStudio project.\n\n\n\n\nsource &lt;dir&gt; /bin/activate\n\n\n\n\n\n\nInstall packages as normal.\n\n\n\n\ninstall.pa ckages(“&lt;pkg&gt;”)\n\n\n\n\npython - m pip install &lt;pkg&gt;\n\n\n\n\n\n\nSnapshot package state.\n\n\n\n\nr env::snapshot()\n\n\n\n\npip freeze &gt; requirements.txt\n\n\n\n\n\n\nExit project environment.\n\n\n\n\nLeave R project or re n v::deactivate()\n\n\n\n\ndeactivate\n\n\n\n\n\n\n\n\nD.1.3 Collaborating on someone else’s project\nStart by downloading the project into a directory on your machine.\n\n\n\n\n\n\n\n\n\n\n\nStep\n\n\n\n\nR Command\n\n\n\n\nPython Command\n\n\n\n\n\n\nMove into project directory.\n\n\n\n\nset wd (“&lt; p roject-dir&gt;”)\n\n\nOr open project in RStudio.\n\n\n\n\ncd &lt;project-dir&gt;\n\n\n\n\n\n\nCreate project environment.\n\n\n\n\nrenv::init()\n\n\n\n\npython -m venv &lt;dir&gt;\n\n\nRecommend: use .venv for &lt;dir&gt;\n\n\n\n\n\n\nEnter project environment.\n\n\n\n\nHappens automatically or ren v::activate()\n\n\n\n\nsource &lt;dir&gt; /bin/activate\n\n\n\n\n\n\nRestore packages.\n\n\n\n\nHappens automatically or re nv::restore()\n\n\n\n\npip install -r requirements.txt"
  },
  {
    "objectID": "chapters/append/cheatsheets.html#cheat-http",
    "href": "chapters/append/cheatsheets.html#cheat-http",
    "title": "Appendix D — Cheatsheets",
    "section": "D.2 HTTP code cheatsheet",
    "text": "D.2 HTTP code cheatsheet\nAs you work with HTTP traffic, you’ll learn some of the common codes. Here’s are some of those used most frequently.\n\n\n\n\n\n\n\nCode\nMeaning\n\n\n\n\n\\(200\\)\nEveryone’s favorite, a successful response.\n\n\n\\(\\text{3xx}\\)\nYour query was redirected somewhere else, usually ok.\n\n\n\\(\\text{4xx}\\)\nErrors with the request\n\n\n\\(400\\)\nBad request. This isn’t a request the server can understand.\n\n\n\\(401\\)/\\(403\\)\nUnauthorized or forbidden. Required authentication hasn’t been provided.\n\n\n\\(404\\)\nNot found. There isn’t any content to access here.\n\n\n\\(\\text{5xx}\\)\nErrors with the server once your request got there.\n\n\n\\(500\\)\nGeneric server-side error. Your request was received, but there was an error processing it.\n\n\n\\(504\\)\nGateway timeout. This means that a proxy or gateway between you and the server you’re trying to access timed out before it got a response from the server."
  },
  {
    "objectID": "chapters/append/cheatsheets.html#cheat-git",
    "href": "chapters/append/cheatsheets.html#cheat-git",
    "title": "Appendix D — Cheatsheets",
    "section": "D.3 Git",
    "text": "D.3 Git\n\n\n\n\n\n\n\nCommand\nWhat it does\n\n\ngit clone &lt;remote&gt;\nClone a remote repo – make sure you’re using SSH URL.\n\n\ngit add &lt;files/dir&gt;\nAdd files/directory to staging area.\n\n\ngit commit -m &lt;message&gt;\nCommit staging area.\n\n\ngit push origin &lt;branch&gt;\nPush to a remote.\n\n\ngit pull origin &lt;branch&gt;\nPull from a remote.\n\n\ngit checkout &lt;branch name&gt;\nCheckout a branch.\n\n\ngit checkout -b &lt;branch name&gt;\nCreate and checkout a branch.\n\n\ngit branch -d &lt;branch name&gt;\nDelete a branch."
  },
  {
    "objectID": "chapters/append/cheatsheets.html#cheat-docker",
    "href": "chapters/append/cheatsheets.html#cheat-docker",
    "title": "Appendix D — Cheatsheets",
    "section": "D.4 Docker",
    "text": "D.4 Docker\n\nD.4.1 Docker CLI commands\n\n\n\n\n\n\n\n\n\n\n\n\nStage\n\n\n\n\nCommand\n\n\n\n\nWhat it does\n\n\n\n\nNotes and helpful options\n\n\n\n\n\n\nBuild\n\n\n\n\ndocker build &lt;directory&gt;\n\n\n\n\nBuilds a directory into an image.\n\n\n\n\n-t &lt;name:tag&gt; provides a name to the container.\n\n\ntag is optional, defaults to latest.\n\n\n\n\n\n\nMove\n\n\n\n\ndocker push &lt;image&gt;\n\n\n\n\nPush a container to a registry.\n\n\n\n\n\n\n\n\nMove\n\n\n\n\ndocker pull &lt;image&gt;\n\n\n\n\nPull a container from a registry.\n\n\n\n\nRarely needed because run pulls the container if needed.\n\n\n\n\n\n\nRun\n\n\n\n\ndocker run &lt;image&gt;\n\n\n\n\nRun a container.\n\n\n\n\nSee flags in next table.\n\n\n\n\n\n\nRun\n\n\n\n\ndocker stop &lt;container&gt;\n\n\n\n\nStop a running container.\n\n\n\n\ndocker kill can be used if stop fails.\n\n\n\n\n\n\nRun\n\n\n\n\ndocker ps\n\n\n\n\nList running containers.\n\n\n\n\nUseful to get container id to do things to it.\n\n\n\n\n\n\nRun\n\n\n\n\ndocker exec &lt;cont aine r&gt; &lt;command&gt;\n\n\n\n\nRun a command inside a running container.\n\n\n\n\nBasically always used to open a shell with d ocker exec -it &lt;co ntainer&gt; /bin/bash\n\n\n\n\n\n\nRun\n\n\n\n\ndocker logs &lt;container&gt;\n\n\n\n\nViews logs for a container.\n\n\n\n\n\n\n\n\n\n\nD.4.2 Flags for docker run\n\n\n\n\n\n\n\n\nFlag\nEffect\nNotes\n\n\n--name &lt;name&gt;\nGive a name to container.\nOptional. Auto-assigned if not provided\n\n\n--rm\nRemove container when its stopped.\nDon’t use in production. You probably want to inspect failed containers.\n\n\n-d\nDetach container (don’t block the terminal).\nAlmost always used in production.\n\n\n-p &lt;port&gt;:&lt;port&gt;\nPublish port from inside running container to outside.\nNeeded if you want to access an app or API inside the container.\n\n\n-v &lt;dir&gt;:&lt;dir&gt;\nMount volume into the container.\n\n\n\n\nReminder: Order for -p and -v is &lt;host&gt;:&lt;container&gt;\n\n\nD.4.3 Dockerfile commands\nThese are the commands that go in a Dockerfile when you’re building it.\n\n\n\n\n\n\n\n\nCommand\nPurpose\nExample\n\n\nFROM\nIndicate base container.\nFROM rocker/r-ver:4.1.0\n\n\nRUN\nRun a command when building.\nRUN apt-get update\n\n\nCOPY\nCopy from build directory into the container.\nCOPY . /app/\n\n\nCMD\nSpecify the command to run when the container starts.\nCMD quarto render ."
  },
  {
    "objectID": "chapters/append/cheatsheets.html#cloud-services",
    "href": "chapters/append/cheatsheets.html#cloud-services",
    "title": "Appendix D — Cheatsheets",
    "section": "D.5 Cloud services",
    "text": "D.5 Cloud services\n\n\n\n\n\n\n\n\n\nService\nAWS\nAzure\nGCP\n\n\nKubernetes cluster\nEKS or Fargate\nAKS\nGKE\n\n\nRun a container or application\nECS or Elastic Beanstalk\nAzure Container Apps\nGoogle App Engine\n\n\nRun an API\nLambda\nAzure Functions\nGoogle Cloud Functions\n\n\nDatabase\nRDS\nAzure SQL\nGoogle Cloud Database\n\n\nData Warehouse\nRedshift\nDataLake\nBigQuery\n\n\nML Platform\nSageMaker\nAzure ML\nVertex AI\n\n\nNAS\nEFS or FSx\nAzure File\nFilestore"
  },
  {
    "objectID": "chapters/append/cheatsheets.html#cheat-cli",
    "href": "chapters/append/cheatsheets.html#cheat-cli",
    "title": "Appendix D — Cheatsheets",
    "section": "D.6 Command line",
    "text": "D.6 Command line\n\nD.6.1 General command line\n\n\n\nSymbol\nWhat it is\n\n\n\n\nman &lt;command&gt;\nOpen manual for command.\n\n\nq\nQuit the current screen.\n\n\n\\\nContinue bash command on new line.\n\n\nctrl + c\nQuit current execution.\n\n\necho &lt;string&gt;\nPrint string (useful for piping).\n\n\n\n\n\nD.6.2 Linux filesystem navigation\n\n\n\n\n\n\n\n\n\n\n\nCommand\n\n\n\n\nWhat it does/is\n\n\n\n\nNotes + Helpful options\n\n\n\n\n\n\n\n\n/\n\n\n\n\nSystem root or file path separator.\n\n\n\n\n\n\n\n\n.\n\n\n\n\nCurrent working directory.\n\n\n\n\n\n\n\n\n..\n\n\n\n\nParent of working directory.\n\n\n\n\n\n\n\n\n~\n\n\n\n\nHome directory of the current user.\n\n\n\n\n\n\n\n\nls &lt;dir&gt;\n\n\n\n\nList objects in a directory.\n\n\n\n\n-l - format as list\n\n\n-a - all (include hidden files that start with .)\n\n\n\n\n\n\npwd\n\n\n\n\nPrint working directory.\n\n\n\n\n\n\n\n\ncd &lt;dir&gt;\n\n\n\n\nChange directory.\n\n\n\n\nCan use relative or absolute paths.\n\n\n\n\n\n\n\n\nD.6.3 Reading text files\n\n\n\n\n\n\n\n\n\n\n\nCommand\n\n\n\n\nWhat it does\n\n\n\n\nNotes + Helpful options\n\n\n\n\n\n\ncat &lt;file&gt;\n\n\n\n\nPrint a file from the top.\n\n\n\n\n\n\n\n\nless &lt;file&gt;\n\n\n\n\nPrint a file, but just a little.\n\n\n\n\nCan be very helpful to look at a few rows of csv.\n\n\nLazily reads lines, so can be much faster than cat for big files.\n\n\n\n\n\n\nhead &lt;file&gt;\n\n\n\n\nLook at the beginning of a file.\n\n\n\n\nDefaults to 10 lines, can specify a different number with -n &lt;n&gt;.\n\n\n\n\n\n\ntail &lt;file&gt;\n\n\n\n\nLook at the end of a file.\n\n\n\n\nUseful for logs where the newest part is last.\n\n\nThe -f flag is useful to follow for a live view.\n\n\n\n\n\n\ngrep &lt;expression&gt;\n\n\n\n\nSearch a file using regex.\n\n\n\n\nWriting regex can be a pain. I suggest testing on .\n\n\nOften useful in combination with the pipe.\n\n\n\n\n\n\n|\n\n\n\n\nThe pipe.\n\n\n\n\n\n\n\n\nwc &lt;file&gt;\n\n\n\n\nCount words in a file.\n\n\n\n\nUse -l to count lines, useful for .csv files.\n\n\n\n\n\n\n\n\nD.6.4 Manipulating files\n\n\n\n\n\n\n\n\n\n\n\nCommand\n\n\n\n\nWhat it does\n\n\n\n\nNotes + Helpful Options\n\n\n\n\n\n\nrm &lt;path&gt;\n\n\n\n\nRemove.\n\n\n\n\n-r - recursively remove everything below a file path\n\n\n-f - force - dont ask for each file\n\n\nBe very careful, its permanent!\n\n\n\n\n\n\ncp &lt;from&gt; &lt;to&gt;\n\n\n\n\nCopy.\n\n\n\n\n\n\n\n\nmv &lt;from&gt; &lt;to&gt;\n\n\n\n\nMove.\n\n\n\n\n\n\n\n\n*\n\n\n\n\nWildcard.\n\n\n\n\n\n\n\n\nmkdir/rmdir\n\n\n\n\nMake/remove directory.\n\n\n\n\n-p - create any parts of path that dont exist\n\n\n\n\n\n\n\n\nD.6.5 Move things to/from server\n\n\n\n\n\n\n\n\n\n\n\nCommand\n\n\n\n\nWhat it does\n\n\n\n\nNotes + Helpful Options\n\n\n\n\n\n\ntar\n\n\n\n\nCreate/extract archive file.\n\n\n\n\nAlmost always used with flags.\n\n\nCreate is usually tar -czf &lt;archive name&gt; &lt;file(s)&gt;\n\n\nExtract is usually tar -xfv &lt;archive name&gt;\n\n\n\n\n\n\nscp\n\n\n\n\nSecure copy via ssh.\n\n\n\n\nRun on laptop to server.\n\n\nCan use most ssh flags (like -i and -v).\n\n\n\n\n\n\n\n\nD.6.6 Write files from the command line\n\n\n\n\n\n\n\n\nCommand\nWhat it does\nNotes\n\n\n\n\ntouch\nCreates file if doesn’t already exist.\nUpdates last updated to current time if it does exist.\n\n\n&gt;\nOverwrite file contents.\nCreates a new file if it doesn’t exist.\n\n\n&gt;&gt;\nConcatenate to end of file.\nCreates a new file if it doesn’t exist.\n\n\n\n\n\nD.6.7 Command line text editors (Vim + Nano)\n\n\n\n\n\n\n\n\nCommand\nWhat it does\nNotes + Helpful options\n\n\n\n\n^\nPrefix for file command in nano editor.\nIt’s the ⌘ or Ctrl key, not the caret symbol.\n\n\ni\nEnter insert mode (able to type) in vim.\n\n\n\nescape\nEnter normal mode (navigation) in vim.\n\n\n\n:w\nWrite the current file in vim (from normal mode).\nCan be combined to save and quit in one, :wq.\n\n\n:q\nQuit vim (from normal mode).\n:q! quit without saving."
  },
  {
    "objectID": "chapters/append/cheatsheets.html#cheat-ssh",
    "href": "chapters/append/cheatsheets.html#cheat-ssh",
    "title": "Appendix D — Cheatsheets",
    "section": "D.7 SSH",
    "text": "D.7 SSH\nGeneral usage:\nssh &lt;user&gt;@&lt;host&gt;\n\n\n\n\n\n\n\n\nFlag\nWhat it does\nNotes\n\n\n\n\n-v\nVerbose, good for debugging.\nAdd more vs as you please, -vv or -vvv.\n\n\n-i\nChoose identity file (private key).\nNot necessary with default key names."
  },
  {
    "objectID": "chapters/append/cheatsheets.html#linux-admin",
    "href": "chapters/append/cheatsheets.html#linux-admin",
    "title": "Appendix D — Cheatsheets",
    "section": "D.8 Linux admin",
    "text": "D.8 Linux admin\n\nD.8.1 Users\n\n\n\n\n\n\n\n\nCommand\nWhat it does\nHelpful options + notes\n\n\nsu &lt;username&gt;\nChange to be a different user.\n\n\n\nwhoami\nGet username of current user.\n\n\n\nid\nGet full user + group info on current user.\n\n\n\npasswd\nChange password.\n\n\n\nuseradd\nAdd a new user.\n\n\n\nusermo  d &lt;username&gt;\nModify user username.\n-aG &lt;group&gt; adds to a group (e.g.,sudo)\n\n\n\n\n\nD.8.2 Permissions\n\n\n\n\n\n\n\n\nCommand\nWhat it does\nHelpful options + notes\n\n\n\n\nchmod &lt;permissions&gt; &lt;file&gt;\nModifies permissions on a file or directory.\nNumber indicates permissions for user, group, others: add 4 for read, 2 for write, 1 for execute, 0 for nothing, e.g.,644.\n\n\nchown &lt;user/group&gt; &lt;file&gt;\nChange the owner of a file or directory.\nCan be used for user or group, e.g.,:my-group.\n\n\nsudo &lt;command&gt;\nAdopt root permissions for the following command.\n\n\n\n\n\n\nD.8.3 Install applications (Ubuntu)\n\n\n\n\n\n\n\nCommand\nWhat it does\n\n\napt-get update && apt-get upgrade -y\nFetch and install upgrades to system packages\n\n\napt-get install &lt;package&gt;\nInstall a system package.\n\n\nwget\nDownload a file from a URL.\n\n\ngdebi\nInstall local .deb file.\n\n\n\n\n\nD.8.4 Storage\n\n\n\n\n\n\n\n\n\n\n\nCommand\n\n\n\n\nWhat it does\n\n\n\n\nHelpful options\n\n\n\n\n\n\n\n\ndf\n\n\n\n\nCheck storage space on device.\n\n\n\n\n-h for human readable file sizes.\n\n\n\n\n\n\ndu\n\n\n\n\nCheck size of files.\n\n\n\n\nMost likely to be used as du -h &lt;dir&gt; | sort -h\n\n\nAlso useful to combine with head.\n\n\n\n\n\n\n\n\nD.8.5 Processes\n\n\n\n\n\n\n\n\nCommand\nWhat it does\nHelpful options\n\n\n\n\ntop\nSee what’s running on the system.\n\n\n\nps aux\nSee all system processes.\nConsider using --sort and pipe into head or grep.\n\n\nkill\nKill a system process.\n-9 to force kill immediately\n\n\n\n\n\nD.8.6 Networking\n\n\n\n\n\n\n\n\n\n\n\nCommand\n\n\n\n\nWhat it does\n\n\n\n\nHelpful Options\n\n\n\n\n\n\nnetstat\n\n\n\n\nSee ports and services using them.\n\n\n\n\nUsually used with -tlp, for tcp listening applications, including pid.\n\n\n\n\n\n\nssh -L &lt;port&gt;:&lt;i p&gt;:&lt;port&gt;:&lt;host&gt;\n\n\n\n\nPort forwards a remote port on remote host to local.\n\n\n\n\nRemote ip is usually localhost.\n\n\nChoose local port to match remote port.\n\n\n\n\n\n\n\n\nD.8.7 The path\n\n\n\n\n\n\n\nCommand\nWhat it does\n\n\nwhich &lt;command&gt;\nFinds the location of the binary that runs when you run command.\n\n\nln -s &lt;linked location&gt;:&lt;where to put symlink&gt;\nCreates a symlink from file/directory at linked location to where to put symlink.\n\n\n\n\n\nD.8.8 systemd\nDaemonizing services is accomplished by configuring them in /etc/systemd/system/&lt;service name&gt;.service.\nThe format of all commands is systemctl &lt;command&gt; &lt;application&gt;.\n\n\n\n\n\n\n\nCommand\nNotes/Tips\n\n\n\n\nstatus\nReport status.\n\n\nstart\n\n\n\nstop\n\n\n\nrestart\nstop then start.\n\n\nreload\nReload configuration that doesn’t require restart (depends on service).\n\n\nenable\nDaemonize the service.\n\n\ndisable\nUn-daemonize the service."
  },
  {
    "objectID": "chapters/append/cheatsheets.html#cheat-ports",
    "href": "chapters/append/cheatsheets.html#cheat-ports",
    "title": "Appendix D — Cheatsheets",
    "section": "D.9 IP Addresses and ports",
    "text": "D.9 IP Addresses and ports\n\nD.9.1 Special IP Addresses\n\n\n\n\n\n\n\n\n\n\nAddress\n\n\n\n\nMeaning\n\n\n\n\n\n\n\n\n\n\n\n\nor loopback – the machine that originated the request.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProtected address blocks used for private IP addresses.\n\n\n\n\n\n\n\n\nD.9.2 Special ports\nAll ports below \\(1024\\) are reserved for server tasks and cannot be assigned to admin-controlled services.\n\n\n\nProtocol/application\nDefault port\n\n\n\n\nHTTP\n\\(80\\)\n\n\nHTTPS\n\\(443\\)\n\n\nSSH\n\\(22\\)\n\n\nPostgreSQL\n\\(5432\\)\n\n\nRStudio Server\n\\(8787\\)\n\n\nShiny Server\n\\(3939\\)\n\n\nJupyterHub\n\\(8000\\)"
  }
]