# Basic Linux SysAdmin {#sec-linux-admin}

It's pretty hard to live in today's world without administering a
computer. You've got to manage the apps and storage on your phone. Your
laptop needs an update or you have to go click that annoying box before
you can share your screen in Zoom.

Through all that clicking, you probably have some sense of what's going
on under the hood and how to manage things. But now, you're about to
join the big leagues.

Where administering your personal computer is an unavoidable chore,
administering a server is a career. The main reason for this is that a
server is the 18-wheeler to your Honda Civic.[^3-2-linux-admin-1] It's
bigger and badder. Handling it is totally possible, but you have to
think about what you're doing differently.

[^3-2-linux-admin-1]: The first car I ever bought was a 2009 Honda Civic
    hybrid. Loved that car.

There are three big things that differentiate what you have to think
about administering a server vs a personal computer or phone.

First, no graphical interface. In Chapters [@sec-cmd-line] and
[@sec-cmd-line-use], you learned a lot about how to interact with a
computer via the command line. You're going to need it to administer a
server. Feel free to skim back through those chapters or to flip back as
a reference.

Second, a server is almost always a multi-user environment. This means
you need to be thinking more carefully about who has access to different
files and folders, both for security and to make sure people know where
their stuff is. Also, you'll need to make sure that Bob the resource hog
doesn't overwhelm the server, leading to a bad experience for everyone
else.

Last, when your phone or computer gets slow or you run out of storage,
it's probably time for a new one. But on a server, load can be much more
variable. Maybe there are 5 people using your app this week and 50 next
week. That means that you may have to manage the server's resources much
more actively.

In this chapter, you'll learn about the basics of Linux System
Administration. Now, that's a whole career unto itself, so this is
really just going to be the basics. But once you've completed this
chapter, you should feel confident doing basic adminstrative tasks like
installing applications, managing users, and some basic troubleshooting.

Regardless of which distro you're using, there are a handful of
different things you'll have to manage when you administer a Linux
server. Here are the most important:

-   **System resources** Each server has a certain amount of CPU, RAM,
    and storage available. You'll need to keep track of how much you've
    got of these things and make sure you've got enough for your server
    to run well.

-   **Networking** Your server is only valuable if you and others can
    connect to it, so you'll need to manage how your server can connect
    to the local network and/or the Internet.

-   **Permissions** Most servers are used by multiple users at the same
    time. You'll frequently be called on to create and manage users and
    what they're allowed to access and do.

-   **Applications** If you want to do data science on your server,
    you'll need data science applications, like R, Python, RStudio,
    JupyterHub, and more. So you'll need to install and run applications
    and adjust them along the way.

I think this lab is the most exciting in the whole book. In the lab for
this chapter, we're going to take the EC2 instance we stood up in the
[the last lab](@sec-aws-intro) and turn it into a data science
workbench. We're going to configure R and Python, get both RStudio
Server and JupyterHub running, and get our Shiny App and FastAPI going.

Now, it won't be accessible from the web until we deal with networking
concerns throughout the rest of this section, but you'll be able to use
it for real data science tasks should you want.

And at the end of the chapter, there's [a cheatsheet](#cheat-admin) of
all the commands from throughout the chapter.

## Servers run on Linux

One of the first differences you'll notice between your server and the
computers you're more used to is the operating system (OS). You're
probably already comfortable with at least two OSes -- either MacOS or
Windows for your computer and Android or iOS on your phone.

In order to successfully administer a server, you're going to have to
learn a little about Linux -- the OS that you may not have heard about,
but is all around you.

Let's start with what an OS does. A computer's OS defines how
applications like Microsoft Word or RStudio or Minecraft can interact
with the underlying hardware. OSes define how files are stored and
accessed, how applications are installed and can connect to networks,
how printers, monitors, and mice can connect, and more.

While Windows and MacOS are quite dominant in desktop computing, Linux
is dominant...basically everywhere else. Android, which runs on the
majority of the world's phones, is a flavor of Linux. A huge majority of
the world's servers run on Linux. And basically every embedded system
like your car's infotainment system and the software on an ATM runs on
some version of Linux.[^3-2-linux-admin-2]

[^3-2-linux-admin-2]: There are also versions on Linux that run on
    desktop computers. But only nerds like me, and probably you, put
    Linux on their desktop computers.

As you've probably already intuited, there's something different about
Linux. In the previous paragraph, I kept saying "flavors of Linux" or
"versions of Linux". That's really different from Windows or MacOS. They
just have one flavor and your computer is either up-to-date, or it's
not.

Making sense of the different flavors of Linux requires a little trip
back into the history of computing.

Before the early 1970s, the market for computer hardware and software
looked nothing like it does now. Computers of that era had extremely
tight linkages between hardware and software, so there was no Microsoft
Word you could use on machines manufactured by different manufacturers.
Every hardware company was also a software company. If their computer
did text editing, they wrote text editing software that only worked on
their machine. If their computer could run a game, they wrote a game
that only worked on their machine.

Then, in the early 1970s, some researchers funded by AT&T's Bell Labs
released *Unix* -- the first operating system.

Now, there was a piece of middleware that sat between the computer
hardware and the software you actually wanted to use, so you could
actually have the same software on computers manufactured by two
different companies.

Once that happened, the computer market started looking a lot more
familiar to 2020s eyes. Hardware manufacturers would build machines that
ran Unix and software companies could write applications that ran on
Unix. The fact that those applications would run on *any* Unix machine
was a game-changer.

The one issue was that everyone was paying Bell Labs a lot of money for
the priviledge because they owned the license for Unix. So, in the
1980s, programmers wanted to be able to work with Unix themselves, but
didn't want to pay Bell Labs, so they started writing Unix-like OSes,
usually called *Unix clones*. Unix clones behaved just like Unix, but
didn't actually include any code from Unix itself.[^3-2-linux-admin-3]

[^3-2-linux-admin-3]: Or at least they weren't supposed to. BSD is a
    Unix clone that was the predecessor of MacOS that has an interesting
    history of lawsuits over whether it illegally included Unix code.

In 1991, Linus Torvalds -- then a 21 year-old Finnish grad student --
released Linux, an open source Unix clone via a amusingly nonchalant
newsgroup posting, saying, "I'm doing a (free) operating system (just a
hobby, won't be big and professional like gnu)...Any suggestions are
welcome, but I won't promise I'll implement themÂ :-)."
[^3-2-linux-admin-4]

[^3-2-linux-admin-4]: More in the [History of Linux Wikipedia
    article](https://en.wikipedia.org/wiki/History_of_Linux).

    Pedants will scream that the original release of Linux was just the
    operating system *kernel*, not a full operating system like Unix.
    Duly noted, now go away.

Since then, Linux has seen tremendous adoption including a large
majority of the world's servers run on Linux.[^3-2-linux-admin-5] Almost
all of the world's embedded computers -- in ATMs, cars and planes, TVs,
smart thermostats, and most other gadgets and gizmos -- Linux. If you
have an Android phone or a Chromebook -- that's Linux. Basically all of
the world's supercomputers use Linux.

[^3-2-linux-admin-5]: The remainder are almost entirely Windows servers.
    There are a few other Unix-like systems that you might encounter,
    like Oracle Solaris. There is no MacOS server. There is a product
    called *Mac Server*, but it's just a program for managing Mac
    desktops and iOS devices.

So now we get back to different Linux flavors. As you might imagine,
your car's infotainment system has very different OS requirements from a
smartphone versus or the controller for your smart thermostat.

As a result, there are many different kinds of Linux called
distributions (*distros* for short) of Linux. Linux distros differ
widely in both technical attributes as well as the licensing model.

Many organizations have a standard Linux distro they use on their
servers. These days the most common open source distros are Ubuntu or
CentOS and Red Hat Enterprise Linux (RHEL) is the most common paid
one.[^3-2-linux-admin-6] Many organizations on AWS are using Amazon
Linux, which is independently maintained by Amazon but was originally a
RHEL derivative.[^3-2-linux-admin-7]

[^3-2-linux-admin-6]: RHEL and CentOS are related operating systems, but
    that relationship has changed a lot in the last few years. The
    details are somewhat complicated, but most people expect less
    adoption of CentOS in enterprise settings going forward.

[^3-2-linux-admin-7]: As I'm writing this, Amazon Linux 2 is popular,
    but Amazon Linux 2023 (AL2023) was recently released. I'd expect
    AL2023 or it's successor to be dominant by the time you read this.

Most individuals who have a choice in the matter prefer Ubuntu. It's
definitely my personal preference. It's a little simpler and easier to
configure than the others.

::: callout-note
## A note on Ubuntu Versioning

Most other distros just have numbered releases. Ubuntu versions are
numbered by the year and month they were released. Most people use the
Long Term Support (LTS) releases, which are released in April of even
numbered years.

Ubuntu versions have fun alliterative names, so you'll hear people refer
to releases by name or version. As of this writing, most Ubuntu machines
are running Bionic (20.04, Bionic Beaver) or Jammy (22.04, Jammy
Jellyfish).
:::

## The Linux Filesystem is a tree

Regardless of which Linux distro you're running, understanding where to
find things on your system is crucial.

All of the information available to a computer is indexed by its
*filesystem*. The filesystem is made up of *directories* or *folders*,
which are containers for other directories and files.

On your laptop, you're probably used to browsing the filesystem with
your mouse. On your phone, the filesystem is completely obscured by
apps, but it's there.

When using the command line, the only way to traverse the filesystem is
with written commands. Having a good mental model for what the
filesystem looks like is, therefore, really important.

On Linux, the entire filesystem is a tree (or perhaps an upside-down
tree). Every directory is contained in by a *parent directory*, and may
contain one or more *children* or *sub-directories*. The *root
directory, `/`* is the base of the tree and is its own parent. A `/` in
between two directories means that it's a sub-directory.

![](../sec2/images/directories.png){fig-alt="A tree of directories. / is the root, /home is a sub directory, /home/alex is a sub-sub-directory, and /etc is another sub-directory."
width="450"}

Every directory is a sub-directory of `/` or a sub-directory of a
sub-directory of `/` or...you get the picture. So the `/home/alex` *file
path* defines a particular location, which is the `alex` sub-directory
of `/home`, which is a sub-directory of the root directory, `/`.

::: callout-note
## Linux filesystems in comparison

Each Linux computer has exactly one filesystem based at the root. On
Windows you can have multiple filesystems on the same machine, one for
each physical or logical disk. You're probably familiar with `C:` as
your main filesystem. Your machine may also have a `D:` drive.

In Linux, network shares are mounted somewhere on the filesystem, often
at `/mnt`, but the fact that they're on separate drives is obscured from
the user. On Windows, network share drives are mounted as a separate
filesystem, often at `M:` or `N:` or `P:`.

One other difference is that Windows uses `\` to separate file path
elements rather than `/`. This used to be a big deal, but newer versions
of Windows accept file paths using `/`.

MacOS is based on an operating system called BSD that, like Linux, is a
Unix clone. So the Mac filesystem is very similar to a Linux one. It's
got a single root and other drives are mounted in -- though the
graphical file interface works pretty hard to obscure this fact.
:::

The Linux `/` contains a number of directories that are used for certain
things. Here are some of the ones you'll use most frequently:

-   `/home` - user home directories.

-   `/bin`, `/opt`, `/usr/local/`, `/usr/bin`- places you're most likely
    to install software.

-   `/etc` - configuration files for applications.

-   `/var` - variable data, most commonly log files.

These distinctions aren't super strict, and the guide for whatever
you're trying to do will tell you where to put important things. But
it's worth realizing that you might have an application that's installed
in `/opt`, is configured in `/etc`, and writes logs to `/var`.

## Everything is on behalf of users

Whenever a program is running in Linux, it is running as a particular
user. Many of the users on a Linux server correspond to actual humans,
but there are more users than that. Most programs that run on a Linux
server run as a *service account* that represent the set of permissions
allowed to that program.

On any Unix-like system, the `whoami` command returns the *username* of
the active user.

So when I run `whoami` on my MacBook, I get:

``` bash
â¯ whoami                                                       
alexkgold
```

Usernames have to be unique on the system -- but they're not the true
identifier for a Linux user. A user is uniquely identified by their
*user id (`uid`)*, which maps to all the other user attributes like
username, password, home directory, groups, and more.

Many applications create users for themselves and run as those users.
For example, installing RStudio Server will create a user with username
`rstudio-server`. Then, when RStudio Server goes to do something --
start an `R` session for example -- it will do so as `rstudio-server`.

There's also one special user -- called the admin, root, sudo, or super
user. They get the ultra-cool `uid` 0.

::: callout-note
## A few details on UIDs

`uid`s are just numbers from 0 to over 2,000,000,000. `uid`s are
assigned by the system at the time the user is created and usually don't
need to be changed manually -- unless you need the same user to exist
across multiple machines. Then the `uid`s need to match.

If you are manually assigning `uid`s, you should choose a number above
10,000 and below 2,000,000. Everything below 10,000 is reserved for
predefined system accounts or application accounts and some applications
can't deal with `uid`s bigger than 2,000,000.
:::

In addition to users, Linux has a notion of groups. A group is a
collection of one or more users. Each user has exactly one *primary*
group and can be a member of secondary groups.[^3-2-linux-admin-8] By
default, each user's primary group is the same as their username.

[^3-2-linux-admin-8]: Depending on your version of Linux, there *may* be
    a limit of 16 groups per user.

Like a user has a `uid` a group has a `gid`. User `gid`s start at 100.

You can see a user's username, `uid`, groups, and `gid` with the `id`
command. On my MacBook, I'm a member of a number of different groups,
with the primary group `staff`.

``` bash
 â¯ id                                                                
uid=501(alexkgold) gid=20(staff) groups=20(staff),12(everyone),61(localaccounts),79(_appserverusr),80(admin),81(_appserveradm),98(_lpadmin),701(com.apple.sharepoint.group.1),33(_appstore),100(_lpoperator),204(_developer),250(_analyticsusers),395(com.apple.access_ftp),398(com.apple.access_screensharing),400(com.apple.access_remote_ae)
```

If you ever need to add users to a server, the easiest way is with the
`useradd` command. Once you have a user, you may need to change the
password, which you can do at any time with the `passwd` command. Both
`useradd` and `passwd` start interactive prompts, so you don't need to
do much more than run those commands.

If you ever need to alter a user -- the most common task being to add a
user to a group, you would use the `usermod` command with the `-aG`
flag.

## Permissions manage what users can do

Every object in Linux is just a file. Every log -- file. Every picture
-- file. Every program -- file. Every system setting -- file.

For every file, there are three possible permissions a user can have:
read, write, and execute. Read means you're allowed to see the contents
of a file, write means you can save a changed version of a file, and
execute means you're allowed to run the file as a program.

So determining whether a user can take a particular action is really a
question of whether they have the right permissions on a particular
file.

The basic Linux authorization scheme is that each file has an owner, an
owning group, and then there's everyone else and permissions are
assigned to each of those three entities.

::: callout-note
There are more complex ways to manage Linux permissions. For example,
you might hear about Access Control Lists (ACLs). That's beyond the
scope of this book.

There is more information on different ways organizations manage users
and what they're allowed to do in [Chapter @sec-auth], which is all
about auth.
:::

So you can think of permissions in Linux as being assigned in a 3x3
grid. The owner, the owning group, and everyone else can have
permissions to read, write, or execute the file.

So, for example, here's a set of permissions that you might have for a
program if you wanted anyone to be able to run it, group members to
inspect it, and only the owner to change it.

![](./images/perms-ex.png){fig-alt="A 3x3 grid read, write, execute, on one side and owner, owning group, and everyone else at the top. Green checks in all of the execute, write for the owner, and read for owner and owning group. Red xs everywhere else."}

Directories also have permissions -- read allows the user see what's in
the directory, write allows the user to alter what's in the directory,
and execute allows the user to enter the directory. File permissions and
directory permissions don't have to match. For example, you could have
read permissions on a directory, allowing you to see the names of the
files in a directory, but not actually have read permissions on any of
the files, so you can't look at the contents.

When you're working on the command line, you don't get a little grid of
permissions. Instead permissions are expressed in one of two ways.

So, for example, here's the output of running `ls -l`, which shows the
contents of a directory along with the permissions, on a python project
I have.

``` bash
â¯ ls -l                                                           
-rw-r--r--  1 alexkgold  staff     28 Oct 30 11:05 config.py
-rw-r--r--  1 alexkgold  staff   2330 May  8  2017 credentials.json
-rw-r--r--  1 alexkgold  staff   1083 May  8  2017 main.py
drwxr-xr-x 33 alexkgold  staff   1056 May 24 13:08 tests
```

The stuff that's important to understanding permissions are the first
ten characters on the left, and then the two words after the number.

Those two words are the owner and group. In this directory, I,
`alexkgold`, own all the files, and the group of all the files is my
group, `staff`.

Those ten-characters on the left are the permissions file permission.

The first character indicates the type of file: most often `-` for
normal or `d` for a directory.

The next nine characters are indicators for the three permissions -- `r`
for read, `w` for write, and `x` for execute (or `-` for in place of any
of those for not) -- first for the user, then for the group, then for
any other user on the system.

So, for example, my `config.py` file with permissions of `rw-r--r--`
indicates the user (`alexkgold`) can read and write the file, and
everyone else -- including in the file's group `staff` -- has read-only
permission.

In the course of administering a server, you will probably need to
change a file's permissions. You can do so using the `chmod` command.

For `chmod`, permissions are indicated as a three digit number, where
the first digit is the permission for the user, the second for the
group, and the third for everyone else. The way you get the right digit
is pretty clever -- you just sum up the permissions as follows: 4 for
read, 2 for write, and 1 for execute. You can check for yourself that
any set of permissions is uniquely identified by a number between 1 and
7.[^3-2-linux-admin-9]

[^3-2-linux-admin-9]: Clever eyes may realize that this is just the
    base-10 representation of a three-digit binary number.

So to implement the permissions in the graphic above, you'd want the
permission set `751` to give the user full permissions (4 + 2 + 1), read
and execute (4 + 1) to the group, and execute only (1) to everyone else.

::: callout-note
If you spend any time administering a Linux server, you almost certainly
will at some point find yourself running into a problem and applying
`chmod 777` out of frustration to rule out a permissions issue.

I can't in good faith tell you not to do this -- we've all been there.
But if it's something important, be sure you change it back once you're
finished figuring out what's going on.
:::

In some cases you might actually want to change the owner or group of a
file. You can change users and groups with either names or ids. You can
do so using the `chown` command. Changing users just uses the username
and changing groups get prefixed with a colon.

In some cases, you might not be the correct user to take a particular
action. If you want to change the user you are, you can use the `su`
command to switch users. You'll be prompted for a password to make sure
you're allowed.

Every system has an admin user, who has full permissions on every file.
That's a lot of power. Some actions are also reserved for the admin
user. For example, here's a config file.

``` bash
 â¯ ls -l /etc/config/my-config                      
-rw-r--r--  1 root  system  4954 Dec  2 06:37 config.conf
```

As you can see, all users can read this file to check the configuration
settings. But this file is owned by `root`, and only the owner has
`write` permissions. So I could run `cat config.conf` to see it. Or I
could go into it with `vim config.conf`, but I'd find myself stuck if I
wanted to make changes.

A regular user could be given permission to access this file by adding
them to the admin group, which would allow them to temporarily assume
admin powers by prefixing commands with `sudo`, like
`sudo vim config.conf`.

The particular name of the admin user and group vary by distro.

## Installing applications from the command line

There are several different ways to install programs for Linux, and
you'll see a few of them throughout this book.

Just as CRAN and PyPI are repositories for R and Python packages and you
have R and Python commands `install.packages` and `pip install` to
install packages, Linux distros also have their own repositories and
install commands.

For Ubuntu, the `apt` command is used for accessing and installing
`.deb` files from the Ubuntu repositories. For CentOS and RedHat, the
`yum` command is used for installing `.rpm` files.

::: callout-note
The examples below are all for Ubuntu, since that's what we use in the
lab for this book. Conceptually, using `yum` is very similar, though the
exact commands differ somewhat.
:::

When you're installing packages in Ubuntu, you'll often see commands
prefixed with `apt-get update && apt-get upgrade -y`. This command makes
your machine update the list of available packages it knows about on the
server and upgrade everything to the latest version. The `-y` flag
bypasses a manual confirmation step, which can be convenient.

Packages are installed with `apt-get install <package>`. Depending on
which user you are, you may need to prefix the command with `sudo`.

You can also install packages that aren't from the central package
repository. Doing that will generally involve downloading a file
directly from a URL -- usually with `wget` and then installing it from
the file you've downloaded -- often with the `gdebi` command.

## Debugging and troubleshooting

If things are going poorly on your server, here are some steps you might
take to try to troubleshoot.

### Check storage usage

A common culprit for weird server behavior is running out of storage
space. There are two handy commands for monitoring the amount of storage
you've got -- `du` and `df`. These commands are almost always used with
the `-h` flag to put file sizes in human-readable formats.

`df`, for disk free, shows the capacity left on the device where the
directory sits.

For example, here's the result of running the `df` command on the
chapters directory on my laptop that includes this chapter.

``` bash
 â¯ df -h chapters                                                    
Filesystem     Size   Used  Avail Capacity iused      ifree %iused  Mounted on
/dev/disk3s5  926Gi  163Gi  750Gi    18% 1205880 7863468480    0%   /System/Volumes/Data
```

So you can see that the chapters folder lives on a disk called
`/dev/disk3s5` that's a little less than 1Tb and is 18% full -- no
problem. On a server this can be really useful to know, because it's
quite easy to switch a disk out for a bigger one in the same spot.

If you've figured out that a disk is full and need to figure out where
the culprits are, the `du` can be useful. `du`, short for disk usage,
gives you the size of individual files inside a directory. It's
particularly useful in combination with the `sort` command.

For example, here's the result of running `du` on the `chapters`
directory where the text files for this book live.

``` bash
 â¯ du -h chapters | sort -h                                      
 44K    chapters/sec2/images-servers
124K    chapters/sec3/images-scaling
156K    chapters/sec2/images
428K    chapters/sec2/images-traffic
656K    chapters/sec1/images-code-promotion
664K    chapters/sec1/images-docker
1.9M    chapters/sec1/images-repro
3.4M    chapters/sec1
3.9M    chapters/sec3/images-auth
4.1M    chapters/sec3
4.5M    chapters/sec2/images-networking
5.3M    chapters/sec2
 13M    chapters
```

So if I were thinking about cleaning up this directory, I could see that
my `images-networking` directory in `sec2` is the biggest single
bottom-level directory. If you find yourself needing to find big files
on your Linux server, it's worth spending some time with the help pages
for `du`. There are lots of really useful options.

`du` is useful for identifying large files and directories on a server.

### Look at what's running

A running program is a *process*. For example, when you type `python` on
the command line to open a REPL, that starts a single Python process. If
you were to start a second terminal session and run `python` again,
you'd have a second Python process.

Complicated programs often involves multiple than one process. For
example, running the RStudio IDE involves (at minimum) one process for
the IDE itself and one for the R session that it uses in the background.
The relationships between these different processes is mostly hidden
from you -- the end user.

As a server admin, finding runaway processes, killing them, and figuring
out how to prevent the them from happening again is a pretty common
task. Runaway processes usually misbehave by using up the entire CPU or
filling up the entire machine's RAM.

Like users and groups have ids, each process has a numeric process id
(`pid`). Each process also has an owner -- this can be either a service
account or a real user. If you've got a rogue process, the pattern is to
try to find the process and make note of its `pid`. Then you can
immediately end the process by `pid` with the `kill` command.

So, how do you find a troublesome process?

The `top` command is a good first stop. `top` shows the top
CPU-consuming processes in real time. Here's the `top` output from my
machine as I write this sentence.

``` bash
PID    COMMAND      %CPU TIME     #TH    #WQ  #PORT MEM    PURG   CMPRS PGRP
0      kernel_task  16.1 03:56:53 530/10 0    0     2272K  0B     0B    0
16329  WindowServer 16.0 01:53:20 23     6    3717  941M-  16M+   124M  16329
24484  iTerm2       11.3 00:38.20 5      2    266-  71M-   128K   18M-  24484
29519  top          9.7  00:04.30 1/1    0    36    9729K  0B     0B    29519
16795  Magnet       3.1  00:39.16 3      1    206   82M    0B     39M   16795
16934  Arc          1.8  18:18.49 45     6    938   310M   144K   61M   16934
16456  Messages     1.7  06:58.27 4      1    603   138M   2752K  63M   16456
1      launchd      1.7  13:41.03 4/1    3/1  3394+ 29M    0B     6080K 1
573    diagnosticd  1.4  04:31.97 3      2    49    2417K  0B     816K  573
16459  zoom.us      1.3  66:38.37 30     3    2148  214M   384K   125M  16459
16575  UniversalCon 1.3  01:15.89 2      1    131   12M    0B     2704K 16575
```

In most instances, the first three columns are the most useful. You've
got the name of the process (`COMMAND`) and how much CPU its using.
Right now, nothing is using very much CPU. If I were to find something
concerning -- perhaps an R process that is using 500% of CPU -- I would
want to take notice of its `pid` to kill it with `kill`.

::: callout-note
### So much CPU?

For `top` (and most other commands), CPU is expressed as a percent of
*single core* availability. So, on a modern machine with multiple cores,
it's very common to see CPU totals well over 100%. Seeing a single
process using over 100% of CPU is rarer.
:::

The `top` command takes over your whole terminal. You can exit with
`Ctrl + c`.

Another useful command for finding runaway processes is `ps aux`. It
lists a snapshot of all processes running on the system, along with how
much CPU and RAM they're using. You can sort the output with the
`--sort` flag and specify sorting by cpu with `--sort -%cpu` or by
memory with `--sort -%mem`.

Because `ps aux` returns *every* running process on the system, you'll
probably want to pipe the output into `head`.

Another useful way to use `ps aux` is in combination with `grep`. If you
pretty much know what the problem is -- often this might be a runaway R
or Python process -- `ps aux | grep <name>` can be super useful to get
the `pid`.

For example, here are the RStudio processes currently running on my
system.[^3-2-linux-admin-10]

[^3-2-linux-admin-10]: This command actually cuts off the header line of
    the table. What I actually ran was `ps aux | grep "RStudio\|USER"`.

``` bash
 > ps aux | grep "RStudio"                                                                                      [10:21:18]
USER               PID  %CPU %MEM      VSZ    RSS   TT  STAT STARTED      TIME COMMAND
alexkgold        23583   0.9  1.7 37513368 564880   ??  S    Sat09AM  17:15.27 /Applications/RStudio.app/Contents/MacOS/RStudio
alexkgold        23605   0.5  0.4 36134976 150828   ??  S    Sat09AM   1:58.16 /Applications/RStudio.app/Contents/MacOS/rsession --config-file none --program-mode desktop 
```

### Make sure the right ports are open

Networking is a complicated topic, which we'll approach with great
detail in [Chapter @sec-basic-networks]. Most often, you'll want to
check that the application you're running is actually accessible to the
outside world, assuming you want it to be.

The main command to help you see what ports are being used and by what
services is the `netstat` command. `netstat` returns the services that
are running and their associated ports. `netstat` is generally most
useful with the `-tlp` flags to show programs that are listening and the
programs associated.

Sometimes you *know* you've got a service running on your machine, but
you just can't seem to get the networking working. It can be useful to
access the service directly without having to deal with networking.

SSH port forwarding allows you to take the output of a port on a remote
server, route it through SSH, and display it as if it were on a local
port. For example, let's say you've got RStudio Server running on my
server but the web interface isn't working. If you've got SSH working
properly, you can double check that the service is working and the issue
really is networking.

I find that the syntax for port forwarding completely defies my memory
and I have to google it every time I use it. For the kind of port
forwarding you'll use most often in debugging, you'll use the `-L` flag
to get a remote port locally.

`ssh -L <local port>:<remote ip>:<remote port> <ssh hostname>`

The local would be your laptop and the remote your server, so if you had
RStudio Server running on a server on port `3939`. Then you could run
`ssh -L 3939:localhost:3939 my-user@my-ds-workbench.com`. To get
whatever is at port `3939` on the server (hopefully RStudio Workbench!)
by going to `localhost:3939` in the laptop's browser.

Most often, you'll use the same port locally and on the remote and the
remote ip will be `localhost`.

### Check your path

Let's say you want to open Python on your command line. One option would
be to type the complete path to a Python install every time. For
example, I've got a version of Python in `/usr/bin`, so
`/usr/bin/python3` works.

But in most cases, it's nice to just type `python3` and have the right
version open up.

``` bash
$ python3
Python 3.9.6 (default, May  7 2023, 23:32:45) 
[Clang 14.0.3 (clang-1403.0.22.14.1)] on darwin
Type "help", "copyright", "credits" or "license" for more information.
>>> 
```

In some cases, this isn't optional. Certain applications will rely on
others being available, like RStudio needing to find R. Or Jupyter
Notebook finding your Python kernels.

So how does the operating system know where to find those applications
and files?

If you ever want to check which actual executable is being used by a
command, you can use the `which` command. For example, on my system this
is the result of `which python3`.

``` bash
 â¯ which python3                                                    
/usr/bin/python3
```

The operating system knows how to find the actual runnable programs on
your system via the *path*. The path is a set of directories that the
system knows to search when it tries to run a path. The path is stored
in an environment variable conveniently named `PATH`.

You can check your path at any time by echoing the `PATH` environment
variable with `echo $PATH`. On my MacBook, this is what the path looks
like.

``` bash
 â¯ echo $PATH                                                      
/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/System/Cryptexes/App/usr/bin:/usr/bin:/bin:/usr/sbin:/sbin
```

When you install a new piece of software you'll need to add it two the
path. Say I was to install a new version of Python in `/opt/python`.
That's not on my `PATH`, so my system wouldn't be able to find it.

I can get it on the path in one of two ways -- the first option would be
to add `/opt/python` to my `PATH` every time a terminal session starts
usually via a file in `/etc` or the `.zshrc`. The other option is to
create a *symlink* to the new application in a directory already on the
`PATH`. A symlink does what it sounds like -- creates a way to link to a
file from a different directory without moving it.

Symlinks are created with the `ln` command.

### Installing R and Python

As the admin of a data science server, R and Python are some of the most
critical software you'll manage.

People pretty much only ever install R to do data science so it's
generally not a huge deal where you install R and get it on the path.

In contrast, Python is one of the world's most popular programming
languages for general purpose computing. This actually makes configuring
Python **harder** than configuring R.

That's because you have to install versions of Python elsewhere on the
system, get them on the path, and get the system version of Python off
the path. This is the source of much frustration trying to get Python up
and running, both on servers and your personal computer.

## Comprehension Questions

1.  Create a mind map of the following terms: Operating System, Windows,
    MacOS, Unix, Linux, Distro, Ubuntu
2.  What are the 3x3 options for Linux file permissions? How are they
    indicated in an `ls -l` command?
3.  How would you do the following?
    1.  Find and kill the process IDs for all running `rstudio-server`
        processes.

    2.  Figure out which port `JupyterHub` is running on.

    3.  Create a file called `secrets.txt`, open it with vim, write
        something in, close and save it, and make it so that only you
        can read it.

## Lab: A working Data Science Workbench

In the last chapter's lab, we set up a bare Ubuntu server in on an EC2
instance. In this chapter, we're going to make that server into a data
science workbench. By the end of this chapter, you'll have a functional
data science workbench - though it won't yet be accessible to the
outside world.

#### Step 1: Log on with the `.pem` key

The `.pem` key you downloaded when you set up the server is the skeleton
key -- it will let you SSH into your server as the admin user (named
`ubuntu` on a Ubuntu system).

The `.pem` key is just an SSH key, so you can SSH to your server with

``` {.bash filename="Terminal"}
ssh -i do4ds-lab-key.pem \
  ubuntu@SERVER_ADDRESS
```

When you first try this, you're probably going to get an alert that
looks something like this:

``` bash

\@\@\@\@\@\@\@\@\@\@\@\@\@\@\@\@\@\@\@\@\@\@\@\@\@\@\@\@\@\@\@\@\@\@\@\@\@\@\@\@

\@ WARNING: UNPROTECTED PRIVATE KEY FILE! \@

\@\@\@\@\@\@\@\@\@\@\@\@\@\@\@\@\@\@\@\@\@\@\@\@\@\@\@\@\@\@\@\@\@\@\@\@\@\@\@\@

Permissions 0644 for 'do4ds-lab-key.pem' are too open.

It is required that your private key files are NOT accessible by others.

This private key will be ignored.

Load key "do4ds-lab-key.pem": bad permissions

ubuntu@ec2-54-159-134-39.compute-1.amazonaws.com: Permission denied (publickey).
```

Because the keypair is so powerful, AWS requires that you restrict the
access pretty severely. Before you can use it to open the server, you'll
need to change the permissions to `600` with `chmod` so only the owner
can read and write the file and no one else can access.

Once you've done that, try SSH-ing in again.

### Step 2: Create a non-root user

We want to create normal users for the humans who are going to log into
the server.

I'm going to use the username `test-user`. If you want to be able to
copy/paste commands, I'd advise doing the same. If you were creating
users based on real humans, I'd advise using their names.

Let's create a user using the `adduser` command. This will walk us
through a set of prompts to create a new user with a home directory and
a password. Feel free to add any information you want -- or to leave it
blank -- when prompted.

``` {.bash filename="Terminal"}
$ adduser test-user
```

We want this new user to be able to adopt root privileges. Remember that
the way that is determined is whether the user is part of the `sudo`
group. Here's the command you'll need to make it so.

``` {.bash filename="Terminal"}
usermod -aG sudo test-user
```

### Step 3: Register an SSH Key for your user

We're going to register an SSH key for our new server user by adding the
key to the server user's `authorized_users` file.

First, you need to get your public key to the server using
`scp`.[^3-2-linux-admin-11]

[^3-2-linux-admin-11]: You could alternatively open the file on the
    server in a terminal just copy/paste the contents of your public key
    in there. Either way works.

For me, the command looks like this

``` {.bash filename="Terminal"}
scp -i ~/Documents/do4ds-lab/do4ds-lab-key.pem ~/.ssh/id_ed25519.pub ubuntu@$SERVER_ADDRESS:/home/ubuntu 
```

Note that I'm still using the `.pem` key to SSH in, because I don't have
another key registered yet.

You may need to change the file paths to either your server's `pem` key
or your SSH key if they're not in the same locations as mine.

Now the public key is on the server, but it's in the `ubuntu` user's
home directory. You'll need to do the following: 1. Create
`.ssh/authorized_keys` in `test-user`'s home directory. 2. Copy the
contents of the public key you uploaded into the `authorized_keys` file.
3. Make sure the `.ssh` directory and `authorized_keys` files are owned
by `test-user` with `700` permissions on `.ssh` and `600` on
`authorized_keys`.

You could do this all as the admin user, or you may want to switch to
being `test-user` at some point with the `su` command.

Once you've done all this, you should be able to log in from your
personal computer with `ssh test-user@$SERVER_ADDRESS`.

Now that we're all set up, you should store the `pem` key somewhere safe
and never use it to log in again.

When you ever want to exit SSH and get back to your machine, you can
just type `exit`.

::: callout-tip
If you run into trouble assuming `sudo` with your new user, try exiting
SSH and coming back. Sometimes these changes aren't picked up until you
restart the shell.
:::

### Step 4: Install R

Everything until now has been generic server administration. Now let's
get into some data-science-specific work -- setting up R and Python.

There are a number of ways to install R on your server including
installing it from source, from the system repository, or using
R-specific tooling.

You can use the system repository version of R, but then you just get
whatever version of R happens to be current when you run
`sudo apt-get install R`. My preferred option is to use `rig`, which is
an R-specific installation manager.

::: callout-note
As of this writing, `rig` only supports Ubuntu. If you want to install
on a different Linux distro, you will have to install R a different way.
:::

There are good instructions on downloading `rig` and using it to install
R on the [`rlib/rig` GitHub repo](https://github.com/r-lib/rig). Use
those instructions to install the current R release on your AWS server.

Once you've installed R on your server, you can check that it's running
by just typing `R` into the command line. If that works, you're good to
move on to the next step.

### Step 5: Install Python

It's very likely that the version of Python on your system is old.
Generally we're going to want to install a newer Python for doing data
science work, so let's start there. As of this writing, Python 3.10 is a
relatively new version of Python, so we'll install that one.

Let's start by actually installing Python 3.10 on our system. We can do
that with apt.

``` {.bash filename="Terminal"}
sudo apt-get install python3.10-venv
```

Once you've installed Python, you can check that you've got the right
version by running

``` {.bash filename="Terminal"}
python3 --version
```

### Step 5: Installing RStudio Server

Once R is installed, let's download and install RStudio Server.

I'm not going to reproduce the commands here because the RStudio Server
version numbers change frequently and you probably want the most recent
one.

You can find the exact commands on the [Posit
website](https://posit.co/download/rstudio-server/). Make sure to pick
the version that matches your operating system. Since you've already
installed R, you can skip down to the "Install RStudio Server" step.

Once you've installed, RStudio Server is running as a system process.
You can check the status with the `systemctl` (system control) utility
with `sudo systemctl status rstudio-server`. If there's a line that says
`Active: active (running)`, you're good to go!

### Step 6: Installing JupyerHub + JupyterLab

Unlike RStudio, which calls R but is not an R program, JupyterHub and
JupyterLab **are** Python programs. We're going to create standalone
virtual environment for running JupyterHub.

Here are the commands to create and activate a `jupyterhub` virtual
environment

``` {.bash filename="Terminal"}
sudo python3 -m venv /opt/jupyterhub
source /opt/jupyterhub/bin/activate
```

Now we're going to actually get JupyterHub up and running inside the
virtual environment we just created. JupyterHub produces [docs that you
can use](https://jupyterhub.readthedocs.io/en/stable/quickstart.html) to
get up and running very quickly. If you have to stop for any reason,
make sure to come back, assume sudo, and start the JupyterHub virtual
environment we created.

Here were the installation steps that worked for me:

``` {.bash filename="Terminal"}
sudo su
apt install npm nodejs
npm install -g configurable-http-proxy
python3 -m pip install jupyterhub jupyterlab notebook

ln -s /opt/jupyterhub/bin/jupyterhub-singleuser /usr/local/bin/jupyterhub-singleuser # symlink in singleuser server, necessary because we're using virtual environment

jupyterhub
```

If all went well, you'll now have JupyterHub up and running on port
`8000`!

#### Running JupyterHub as a service

JupyterHub is a Python process, not a system process. This is ok, but it
means that we've got to remember the command to start it if we have to
restart it, and that it won't auto restart if it were to fail for any
reason.

A program that runs in the background on a machine, starting
automatically, and controlled by `systemctl` is called a *daemon*. Since
we want JupyterHub to be a daemon, we're got to add it as a system
daemon, which isn't hard.

We don't need it right now, but it'll be easier to manage JupyterHub
later on from a config file that's in `/etc/jupyterhub`.

Let's create a default config file and move it into the right place
using

``` {.bash filename="Terminal"}
sudo su
source /opt/jupyterhub/bin/activate
jupyterhub --generate-config
mkdir -p /etc/jupyterhub
mv jupyterhub_config.py /etc/jupyterhub
```

Now we've got to daemon-ize JupterHub. There are two steps -- create a
file describing the service for the server's daemon, and then start the
service.

To start with, end the existing JupyterHub process. If you've still got
that terminal open, you can do so with `ctrl` + `c`. If not, you can use
your `ps aux` and `grep` skills to find and kill the JupyterHub
processes.

On Ubuntu, adding a daemon file uses a tool called `systemd` and is
straightforward.

First, add the following to `/etc/systemd/system/jupyterhub.service`.

``` {.yaml filename="/etc/systemd/system/jupyterhub.service"}
[Unit]
Description=Jupyterhub
After=syslog.target network.target

[Service]
User=root
Environment="PATH=/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/opt/jupyterhub/bin"
ExecStart=/opt/jupyterhub/bin/jupyterhub -f /etc/jupyterhub/jupyterhub_config.py

[Install]
WantedBy=multi-user.target
```

Hopefully this file is pretty easy to parse. Two things to notice -- the
`Environment` line adds `/opt/jupyterhub/bin` to the path -- that's
where our virtual environment is.

Second, the `ExecStart` line is the startup command and includes our
`-f /etc/jupyterhub/jupyterhub_config.py` -- this is the command to
start JupyterHub with the config we created a few seconds ago.

Now we just need to reload the daemon tool so it picks up the new
service it has available and start JupyterHub!

``` {.bash filename="Terminal"}
systemctl daemon-reload
systemctl start jupyterhub
```

You should now be able to see that JupyterHub is running using
`systemctl status jupyterhub` and can see it again by tunneling to it.

To set JupyterHub to automatically restart when the server restarts, run
`systemctl enable jupyterhub`.

### Step 7: Running a Plumber API in a Container

In addition to running a development workbench on your server, you might
want to run a data science project. If you're running a Shiny app, Shiny
Server is easy to configure along the lines of RStudio Server.

However, if you put an API in a container like we did in [Chapter
@sec-docker], you might want to deploy that somewhere on your server as
a running container.

The first step is to install docker on your system with
`sudo apt-get install docker.io`. You can check that you can run docker
with `docker ps`. You may need to adopt `sudo` privileges to do so.

Once we have docker installed, getting the API running is almost
trivially easy using the command we used back in [Chapter @sec-docker]
to run our container.

``` {.bash filename="Terminal"}
sudo docker run --rm -d \
  -p 8080:8080 \
  --name penguin-model \
  alexkgold/penguin-model
```

The one change you might note is that I've changed the port on the
server to be 8080, since we already have JupyterHub running on 8000.

Once it's up, you can check that it's running with `docker ps`.

This is why people love docker -- it's wildly easy to get something
simple running. But getting Docker hardened for production takes a bit
more work.

The first thing to notice is that there's no auth on our API. Anyone can
hit this API as much as they want if they have the URL. Needless to say,
this is not a security best practice unless you intend to host a public
service.

The other issue is that we haven't daemonized the API. If we restart the
server or the container dies for any reason, it won't auto-restart.

It's not generally a best practice to daemon-ize a docker container by
just putting the `run` command into `systemd`.

Instead, you should use a container management system that is designed
specifically to manage running containers, like Docker Compose or
Kubernetes. Getting deeper into those systems is beyond the scope of
this book.

You also might want to configure GitHub Actions to build your container
afresh when there's a push to the repo.

### Step 8: Putting up the Shiny app

Start by putting the Shiny app on the server. I put mine in
`/home/test-user/do4ds-lab/app`.

You can pull down the git repo where you have the app code manually. If
you're feeling fancy, you could set up a GitHub Action to push the new
code whenever you update the repo.

Once you've done that, you'll want to open up R or Python and rebuild
the package library with `{renv}` or `{venv}`.

Once you've done that, you'll want to install Shiny Server using the
instructions from the [Admin
Guide](https://docs.posit.co/shiny-server/#getting-started). Note that
you can skip steps to install R and/or Python, as well as the `{shiny}`
package as we've already done that.

Once you've got the app on the server, you need to edit
`/etc/shiny-server/shiny-server.conf` to run the right app.

``` {.bash include="../../_labs/server-config/shiny-server.conf" filename="/etc/shiny-server/shiny-server.conf"}
```

Once that's configured you can start Shiny Server with
`sudo systemctl start shiny-server`.

### Step 8: Check it all out

Just knowing that all of our services are running isn't nearly as fun as
actually trying them out.

We don't have a stable public URL for the server yet, so we can't just
access it from our browser. This is a perfect use case for an SSH
tunnel.

If you recall, the command for an SSH tunnel from a remote server to
`localhost` is to do
`ssh -L <remote port>:localhost:<local port> <user>@<server>`.

We've got three services running on our server, RStudio Server at
`8787`, JupyterHub on `8000`, Shiny Server on `3838`, and our Plumber
API on `8080`. You can try each of them out by subbing those in for the
remote port and putting them at a local port. I'd recommend just using
the same one.

For example, by running
`ssh -L 8787:localhost:8787 test-user@$SERVER_ADDRESS`, I can visit
RStudio Server in my browser at `localhost:8787` and login with the
username `test-user` and password I set on the server.

## Cheatsheet: Linux Admin Commands {#cheat-admin}

### Users

+------------------+---------------------------------------------------+
| Command          | What it does                                      |
+==================+===================================================+
| `su <username>`  | Change to be a different user.                    |
+------------------+---------------------------------------------------+
| `whoami`         | Get username of current user.                     |
+------------------+---------------------------------------------------+
| `id`             | Get full user + group info on current user.       |
+------------------+---------------------------------------------------+
| `passwd`         | Change password.                                  |
+------------------+---------------------------------------------------+
| `useradd`        | Add a new user.                                   |
+------------------+---------------------------------------------------+
| `use             | Modify user `username`. `-aG <group>` adds to a   |
| rmod <username>` | group                                             |
+------------------+---------------------------------------------------+

### Permissions

+------------------+------------------+-------------------------+
| Command          | What it does     | Helpful options + notes |
+==================+==================+=========================+
| `chmod <per m    | Modifies         | Number indicates        |
| issions> <file>` | permissions on a | permissions for user,   |
|                  | file.            | group, others: add 4    |
|                  |                  | for read, 2 for write,  |
|                  |                  | 1 for execute, 0 for    |
|                  |                  | nothing.                |
+------------------+------------------+-------------------------+
| `chown <us e     | Change the owner | Can be used for user or |
| r/group> <file>` | of a file.       | group, e.g.             |
|                  |                  | `:my-group`.            |
+------------------+------------------+-------------------------+
| `su <username>`  | Change active    |                         |
|                  | user to          |                         |
|                  | `username`.      |                         |
+------------------+------------------+-------------------------+
| `sudo <command>` | Adopt super user |                         |
|                  | permissions for  |                         |
|                  | the following    |                         |
|                  | command.         |                         |
+------------------+------------------+-------------------------+

### Install applications (Ubuntu)

+-------------------------------+--------------------------------+
| **Command**                   | **What it does**               |
+-------------------------------+--------------------------------+
| `apt-get                      | Fetch and install upgrades to  |
| update && apt-get upgrade -y` | system packages                |
+-------------------------------+--------------------------------+
| `apt-get install <package>`   | Install a system package.      |
+-------------------------------+--------------------------------+
| `wget`                        | Download a file from a URL.    |
+-------------------------------+--------------------------------+
| `gdebi`                       | Install local `.deb` file.     |
+-------------------------------+--------------------------------+

### Storage

+----------------+--------------------+-------------------------+
| Command        | What it does       | Helpful options         |
+================+====================+=========================+
| `df`           | Check storage      | `-h` for human readable |
|                | space on device.   | file sizes.             |
+----------------+--------------------+-------------------------+
| `du`           | Check size of      | Most likely to be used  |
|                | files.             | as                      |
|                |                    | `du -h <dir> | sort -h` |
|                |                    |                         |
|                |                    | Also useful to combine  |
|                |                    | with `head`.            |
+----------------+--------------------+-------------------------+

### Processes

+----------------+----------------+-------------------------------+
| Command        | What it does   | Helpful options               |
+================+================+===============================+
| `top`          | See what's     |                               |
|                | running on the |                               |
|                | system.        |                               |
+----------------+----------------+-------------------------------+
| `ps aux`       | See all system | Consider using `--sort` and   |
|                | processes.     | pipe into `head` or `grep`    |
+----------------+----------------+-------------------------------+
| `kill`         | Kill a system  | `-9` to force kill            |
|                | process.       | immediately                   |
+----------------+----------------+-------------------------------+

### Networking

+----------------+-----------------------------+------------------+
| Command        | What it does                | Helpful options  |
+================+=============================+==================+
| `netstat`      | See ports and services      | Usually used     |
|                | using them.                 | with `-tlp`, for |
|                |                             | tcp listening    |
|                |                             | applications,    |
|                |                             | including `pid`  |
+----------------+-----------------------------+------------------+
| `ssh - L       | Port forwards a remote port | Choose local     |
|  <port>:<ip>:  | on host to local.           | port to match    |
| <port> <host>` |                             | remote port.     |
|                |                             | Remote `ip` is   |
|                |                             | usually          |
|                |                             | `localhost`.     |
+----------------+-----------------------------+------------------+

### The path

+-----------------------+------------------------------------------+
| Command               | What it does                             |
+=======================+==========================================+
| `which <command>`     | Finds the location of the binary that    |
|                       | runs when you run `command`.             |
+-----------------------+------------------------------------------+
| `ln -s                | Creates a symlink from file at           |
| <location to link>: < | `location to link` to                    |
| location of symlink>` | `location of symlink`.                   |
+-----------------------+------------------------------------------+
