# Compute at enterprise scale {#sec-ent-scale}

Managing compute is no mean feat at enterprise scales. Patterns that
might work at smaller scales start to break down when the environment
needs to support 100 or 500 or even 1,000 data scientists working on the
same platform.

Supporting that many users involves much more complexity and expense
because you absolutely have to have multiple -- perhaps many -- servers.
With so many users, you're probably supporting a variety of different
types of work, which adds complexity, and downtime becomes very
expensive, so the stability of the system becomes paramount.

This chapter will help you understand how enterprise IT/Admins think
about managing a lot of servers and how to communicate with them about
your needs as a data scientist.

## DevOps best practices

In an enterprise, the IT/Admin group is managing dozens or hundreds or
thousands of servers. In order to keep that many servers manageable, the
IT/Admin group tries to keep those servers very standardized and
interchangeable. This idea is often encompassed in the adage that
"servers should be cattle, not pets". That means that you almost
certaintly won't be allowed to SSH in and just make changes that you
might want as a data scientist.

Indeed, in many organizations, **no one** is allowed to SSH in and make
changes. Instead, all changes have to go through a robust change
management process and are deployed via *Infrastructure as Code (IaC)*
tools so the environment can always be torn down and replaced easily.

::: callout-note
Avoiding this complexity is the major reason many organizations are
moving away from directly managing servers at all. Instead, they're
outsourcing server patches and management by acquiring PaaS or SaaS
software from cloud providers.
:::

There are actually two parts to standing up an environment. First the
servers and networking need to be stood up, called *provisioning*. Once
that's done, they need to be *configured*, which installing and
activating applications like Python, R, JupyterHub, RStudio Server. In
many enterprise organizations, provisioning and configuration are
actually done by separate groups. They may be called the server group
and the application administration group.

There are many different IaC tools including Terraform, Ansible,
CloudFormation (AWS's IaC tool), Chef, Puppet, and Pulumi. Most of these
tools can do both provisioning and configuration, but most specialize in
one or the other, so many organizations use a pair.

::: callout-note
### No Code IaC

Some enterprises manage servers without IaC. These usually involve
writing extensive *run books* to tell another person how to configure
the servers. If your spidey sense is tingling that this probably isn't
nearly as good as code, you're right. Enterprise IT/Admin organizations
that don't use IaC tooling is definitely a red flag.
:::

Along with making deployments via IaC, organizations that follow DevOps
best practices use a Dev/Test/Prod setup for making changes to servers
and applications. In this setup, the IT/Admin group has its own set of
Dev and Test environments that are solely for testing changes to the
environment itself. In order to differentiate this environment from the
data scientist's Dev and Test environments, I often refer to this as
*staging*.[^3-3-ent-scale-1]

[^3-3-ent-scale-1]: You'll have to work out who gets to use
    Dev/Test/Prod with your IT/Admin group.

Moves from staging to prod, including upgrades to applications, adding
system libraries, or operating system upgrades often have rules around
them. They may need to be validated or approved by security. In some
highly-regulated environments, the IT/Admin group may only be able to
make changes during certain windows of time. This can be a source of
tension between a data scientist who needs a new library or version now
and an IT/Admin who isn't allowed to move fast.

In this kind of setup, Dev/Test/Prod is a two-dimensional grid, with
IT/Admins working on changes to the environment in staging and data
scientists working in Dev and Test within the Prod IT/Admin environment.
The ultimate goal of all of this is to create a bulletproof prod-prod
environment that is extremely reliable.

![](images/dev-test-prod.png){fig-alt="The IT/Admin promotes the complete staging environment, then the data scientist or IT/Admin promote within Prod."
width="600"}

In addition to changes that go from staging to prod, enterprises also
sometimes undergo a complete rebuild of their environments. These days,
many of those rebuilds are the result of a move to the cloud, which can
be a multi-year affair.

## Compute for many users

At some number of data scientists, you outstrip the ability of any one
server -- even a big one -- to accommodate all of the data science work
that needs to get done. How quickly that happens depends entirely on
what data scientists do at your organization. If you're doing highly
intensive simulation work or deep learning, you may hit it with only one
person. On the other hand, I've seen organizations that mostly work on
small data sets who can comfortably fit 50 concurrent users on a single
server.

Once you need multiple servers to support the data science team(s), you
have to *horizontally scale*. There is a simple way to horizontally
scale, which is just to give every user or every group their own
disconnected server to use. In some organizations this can work very
well. The downside is that this either leaves a lot of hassle for the
IT/Admin to manage or they just delegate server management to data
scientists, which generally isn't ideal.

In enterprises, this usually isn't the way things get done. Most
enterprises want to run one centralized service that everyone in the
company -- or at least across a large group -- come to. Managing just
one environment makes things simpler in some ways, but it can be hard to
manage different use cases in one environment, and it also increases the
cost of downtime. For example, one hour of downtime for a platform that
supports 500 data scientists wastes over \$25,000.[^3-3-ent-scale-2]

[^3-3-ent-scale-2]: Assuming a (probably too low), fully-loaded cost of
    \$100,000 and 2,000 working hours per year.

::: callout-note
### Measuring Uptime

Organizations often introduce internal *Service Level Agreements (SLAs)*
about how much downtime is allowed. These limits are often measured in
*nines of uptime*, which refers to the proportion of the time that the
service is guaranteed to be online. So a one-nine service is guaranteed
to be up 90% of the time, allowing for 36 days of downtime a year. A
five-nine service is up for 99.999% of the time, allowing for only about
5 1/4 minutes of downtime a year.
:::

Enterprise IT/Admin organizations take avoiding downtime very seriously.
Most have some sort of *disaster recovery* policy. Some organizations
maintain frequent snapshots of the state of the environment (often
nightly) so they can roll back to a known good state in the event of a
failure. Sometimes it means that there is actually a copy of the
environment waiting on standby in the event of an issue with the main
environment.

Other times, there are stiffer requirements such that nodes in the
cluster could fail and the users wouldn't be meaningfully affected. This
requirement for limited cluster downtime is often called *high
availability*. High availability is not a description of a particular
technology or technical approach. Instead, it's a description of a
desired outcome for a cluster.

## Computing in clusters

Whether it's for horizontal scaling or high availability reasons, most
enterprises run their data science environments in a *cluster*, which is
a set of servers (*nodes*) that operate together as one unit. The
cluster should feel like working in a single server environment, but on
the back end there are multiple servers to add computational power,
provide different kinds of computational environments, and/or to add
resilience if one server were to fail.

In order to have a cluster operate as a single environment, you need to
solve two problems. First, you want to provide a single front door that
routes users to a node in the cluster, preferably without them being
aware of it happening. This is accomplished with a *load-balancer*,
which is a kind of proxy server.

Second, you need to make sure that the user is able to persist things
(*state*) on the server even if they end up on a different node later.
This is accomplished by setting up storage so that persistent data
doesn't actually live on any of the nodes. Instead, it lives in separate
storage that is symmetrically accessible to all the nodes in the
cluster.

![](images/lb-cluster.png){fig-alt="Users come to the load balancer, which sends them to the nodes, which connect to the state."}

If you are a solo data scientist reading this, please do not try to run
your own horizontally-scaled data science cluster. When you undertake
load balancing, you've taken on a distributed systems problem and those
are **inherently difficult**.

High-availability is often phrased as "no single point of failure". It's
worth noting that adding computational nodes still leaves single points
of failure. In fact, it's totally possible to make your system **less
stable** by just doing horizontal scaling in one spot without thinking
through the implications. For example, what if your load-balancer were
to fail? Or the place where the state is stored? Sophisticated IT/Admin
organizations have answers to these questions and standard ways they
implement high availability.

::: callout-note
For technical information on how load-balancers work and different types
of configuration, see [Appendix @sec-append-lb].
:::

## Docker in enterprise = Kubernetes

Originally created at Google and released in 2014, the open source
*Kubernetes (K8S, pronounced koo-ber-net-ees or kates for the
abbreviation)* is the way to run production services out of Docker
containers.[^3-3-ent-scale-3] Many organizations are moving towards
running much or all of their production work in Kubernetes.

[^3-3-ent-scale-3]: If you are pedantic, there are other tools for
    deploying Docker containers like Docker Swarm and Kubernetes is not
    limited to Docker containers. But for all practical purposes,
    $\text{production Docker = Kubernetes}$.

::: callout-note
Apparently Kubernetes is an ancient Greek word for "helmsman".
:::

Relative to managing individual nodes in a load-balanced cluster,
Kubernetes makes things way easier because it completely separates
provisioning and configuration. In Kubernetes, you create a cluster of
worker nodes. Then, you tell the cluster's *control plane* that you want
it to run a certain set of Docker containers as *pods* along with how
much computational power you want to grant to each pod.

The elegance of Kubernetes is that you don't have to think at all about
where each pod goes. The control plane *schedules* the pods on the nodes
without you haveing to worry about the underlying nodes at all.

![](images/k8s.png){fig-alt="Image of a Kubernetes cluster with 3 nodes and 6 pods of various sizes arranged across the nodes."
width="600"}

From the perspective of the IT/Admin, this is extremely powerful because
you just make sure you've got enough horsepower in the cluster and then
all the app requirements go in the container. This makes node
configuration trivial.

These days, almost everyone is using a cloud provider's managed
Kubernetes service -- AWS's *EKS (Elastic Kubernetes Service*, Azure's
*AKS (Azure Kubernetes Service)*, or GCP's *GKE (Google Kubernetes
Engine)*.[^3-3-ent-scale-4] One really nice thing about using these
Kubernetes clusters as a service is that adding more nodes is just a few
clicks away.

[^3-3-ent-scale-4]: It's rare, but some organizations do run an on-prem
    Kubernetes cluster with Oracle's OpenShift.

For production purposes, pod deployments are usually managed with *Helm
charts*, which are the standard IaC way to declare what pods, how many
of each, and their relationship inside the cluster.

Kubernetes is an extremely powerful tool and there are many reasons to
like it. But like most powerful tools, it's also complicated. and
becoming a proficient Kubernetes admin is a skill unto itself. These
days, many IT/Admins are trying to add Kubernetes to their list of
skills. You should be careful that your data science environment doesn't
become someone's first Kubernetes project.

## Variable use cases in one cluster

As you add more people, you also are likely to add more variety in
requirements. For example, you might want to be able to incorporate jobs
that require very large nodes. Or maybe you want to run GPU-backed
nodes. Or maybe you want to have "burst-able" capacity for particularly
busy days or times.

It's unlikely that your organization wants to be running all of these
nodes, especially expensive large or GPU nodes, all the time. By far the
simplest way to manage this complexity is to run a "main" cluster for
everyday kind of workloads and stand up additional special environments
as needed. If the need is just for burst capacity over a relatively long
time scales (e.g. a full day), manually adding or removing nodes from a
cluster is easy.

Some organizations really want to run a single environment that contains
different kinds of nodes or that can autoscale to accommodate quick
bursts of needed capacity. This is much harder.

Often, the best option for doing these kinds of activities is a
high-performance computing (HPC) framework. HPC is particularly
appropriate when you need very large machines. For example, Slurm is an
HPC framework that supports multiple *queues* for different sets of
machines and allows you to treat an entire cluster as if it were one
machine with many -- even thousands -- of CPU cores. AWS has a service
called *ParallelCluster* that allows users to easily set up a Slurm
cluster -- and with no additional cost relative to the cost of the
underlying AWS hardware.

Some organizations want to accomplish this kind of approach in
Kubernetes. That is generally more difficult. It is not as easy to
manage multiple kinds of nodes in one cluster with Kubernetes, though it
is possible. It's also generally not possible to schedule pods larger
than the size of the actual nodes in the cluster. In contrast, most HPC
frameworks allow you to combine an arbitrary number of nodes into what
acts like a single machine with thousands or tens of thousands of nodes.

Autoscaling is also easier in many HPC frameworks compared to
Kubernetes. Regardless of the framework, scaling up is always easy. You
just add more nodes and you can schedule more work. But it's scaling
down that's hard.

Because of its history, Kubernetes takes a faiassumes that pods can
easily be moved from one node to another and it wouldn't be a problem to
shut down a node on someone and just move their pod to another node.
Obviously, if what is being autoscaled is a data science workbench,
that's a bad assumption. Relative to Kubernetes, HPC is much more
"session-aware" and often does a better job scaling down the kinds of
workloads that happen in a data science environment.

The upshot is that most IT/Admins will reach for Kubernetes to solve the
problem of multiple use cases in one cluster or to autoscale it, but HPC
may actually be a better solution in a data science context.

## Comprehension Questions

1.  What is the difference between horizontal and vertical scaling? For
    each of the following examples, which one would be more appropriate?
    a.  You're the only person using your data science workbench and run
        out of RAM because you're working with very large data sets in
        memory.

    b.  Your company doubles the size of the team that will be working
        in your data science workbench. Each person will be working with
        reasonably small data, but there's going to be a lot more of
        them.

    c.  You have a big modeling project that's too large for your
        existing machine. The modeling you're doing is highly
        parallelizable.
2.  What is the role of the load balancer in horizontal scaling? When do
    you really need a load balancer and when can you go without?
3.  What are the biggest strengths of Kubernetes as a scaling tool? What
    are some drawbacks? What are some alternatives?
