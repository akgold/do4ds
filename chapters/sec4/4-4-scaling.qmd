# Enterprise Scale Data Science {#sec-scale}

Data science environments need to scale for two reasons. The first
reason that people think of most often is because people are running big
jobs. This can happen at any scale of organization. There are data
science teams of one who have use cases that necessitate terrabytes of
data.

But this isn't an enterprise problem with scaling. The problem in the
enterprise is having many concurrent users of the system. Additionally,
in an enterprise context, the stability of the platform becomes much
more important. If you've got a data science team of four people who sit
together, someone accidentally knocking the RStudio Server offline isn't
a huge deal (been there, done that). But if the data science team is 100
people, the cost of downtime is much higher and more aggressive measures
should be taken to ensure it doesn't happen.

If you additionally have high resource requirements -- or highly
variable requirements -- that just adds a level of complexity onto the
need to figure out how you might scale up your data science environment.

There are two main ways to scale server-based resources.

The first is *vertical scaling*, which is just a fancy way of saying
"use a bigger server". As discussed in [Chapter @sec-servers], if you're
in the cloud, vertical scaling is really easy.

If you have resource-intensive jobs, vertical scaling should be your
first move. This is usually cost effective as well. AWS resource costs
generally scale linearly within a server family. But there are limits.
As of this writing, AWS's general-use instance types max out at 96-128
cores these days. That's probably sufficient for many workloads, but if
you've got an RStudio Server with 50 concurrent users doing reasonably
heavy compute loads, that can quickly get eaten up.

*Horizontal scaling* or *load-balancing* is the other way to scale and
means distributing the workload across multiple machines. Many
organizations (in my opinion) opt for premature horizontal scaling. It
is usually worthwhile to exhaust the ability to vertically scale
*before* undertaking horizontal scaling.

Distributed system problems are inherently difficult

This is because configuring horizontal scaling is usually a non-trivial
task. Going from managing something that lives on a single server to
managing a single entity hosted multiple places introduces all kinds of
complications.

Sometimes horizontal scaling is undertaken for pure scaling purposes,
but sometimes it's undertaken for cluster resilience purposes. For
example, you might want the cluster to be resilient to a node randomly
failing, or being taken down for maintenance. In this context,
horizontal scaling is often called *high availability*. Like many other
things, high availability is a squishy term, and different organizations
have very different definitions of what it means.

For example, in one organization, high availability might just mean that
there's a robust disaster recovery plan so servers can be brought back
online with little data loss. In another organization, high availability
might mean having duplicate servers that aren't physically colocated to
avoid potential outages due to server issues or natural disasters. In
other contexts, it might be a commitment to a particular amount of
uptime.[^4-4-scaling-1]

[^4-4-scaling-1]: Often called nines of uptime, referring to the
    proportion of the time that the service is guaranteed to be online
    out of a year. So a one-nine service is up 90% of the time, allowing
    for 36 days of downtime a year. a five-nine service is up for
    99.999% of the time, allowing for only about 5 1/4 minutes of
    downtime a year.

Spannning Multiple AZs

As the requirements for high availability get steeper, the engineering
cost to make sure the service really is that resilient rise
exponentially...so be careful how much uptime you're trying to achieve.

### How Horizontal Scaling Works

Since vertical scaling is so conceptually simple, we're not really going
to get much more into it. Horizontal scaling/load balancing is much more
complicated, so we're going to get into it here.

There are two reasons horizontal scaling is hard. The first is that
you're going to want to create one front door to your cluster. You can
always just stand up different servers for different people or groups.
This is a crude form of horizontal scaling, but it can be effective.

If you want people to have one front door, this is where things start to
get complicated. In this case, you want the load-balancing to be
invisible to the user, which means two things have to happen -- you have
to have one front door and you have to make sure the user's state
appears the same on all of the machines.

![](images-scaling/lb-cluster.png){width="484"}

Making there be one front door for the cluster is pretty
straightforward. You put a special kind of proxy (a load balancer) in
front of your instances. Your load balancer will route sessions among
the nodes in your cluster. Some load balancers can be smart about where
they route people, but at a minimum, your load balancer will need to be
configured with a health check/heartbeat endpoint for your product.

This is a specific endpoint in the product that returns a statement that
the node is actually healthy and accepting traffic. In the absence of a
heartbeat/health check, the load balancer should not route traffic to
that node. Some heartbeats include additional information you might want
to use to do the load balancing. All of the major cloud providers have
*load balancers as a service*.

::: callout-note
## AWS Load Balancer Types

In AWS, the modern load balancer options are an Application Load
Balancer (ALB) and a Network Load Balancer (NLB). For most data science
environment applications, you'd want an ALB.
:::

The other thing that gets difficult about invisibly load balancing a
service is that state has to be symmetrically available to all of the
nodes in the cluster. For example, if I'm working in RStudio Server, I
expect that any session I bring up will have my data and code available
to me without furthre configuration.

The details of how this is accomplished varies by the individual app,
but the basic idea is to separate the server state into a backend, often
some combination of a database and file storage, and allow each of the
nodes in the cluster to symmetrically read and write from the state.

### Load-Balancing configurations

One feature that is likely to be a requirement of your data science
environment is *sticky sessions* or *sticky cookies*. This means that
when you come through the load balancer, a cookie is added to your
browser so that the load balancer can keep track of which node you were
routed to. This is important in cases where the app state lives on the
server -- for example, you want to get back to your same instance of a
shiny app in an RStudio Connect cluster. In most load balancers, this is
a simple option you can just turn on.

There are a few different ways to configure load balancing for servers.
The first is called active/active. This just means that all of the
servers are online all of the time. So if I have two RStudio Server
instances, they're both accepting traffic all the time.[^4-4-scaling-2]

[^4-4-scaling-2]: If the service you're configuring does internal
    routing, like RStudio Workbench, you also may need to select whether
    you have one or several primary nodes. A primary node is one that
    accepts traffic from the load balancer and routes it internally. You
    could have just one node in the cluster serve as primary or several.
    Unless there's a good reason to prefer one, it's usually
    advantageous to do several, because it adds one layer of resilience
    to your cluster.

\[Graphic: network diagram of lb-config -- active/active vs
active/passive\]

In active/passive configurations, you have two or more servers, with one
set accepting traffing all the time, and the other set remaining inert
until or unless the first set goes offline. This is sometimes called a
blue/green or red/black configuration.[^4-4-scaling-3] People often
really like this configuration if they have high requirements for
uptime, and want to be able to do upgrades to the system behind the
scenes and then just cut the traffic over at some point without an
interruption in service. It is a nice idea. It is often very hard to
pull off.

[^4-4-scaling-3]: Blue/Green and Red/Black aren't the same things, but
    the differences are deep in the weeds...and not even consistently
    used.

Disaster recovery is not really a load-balancing method, but it's
closely related. In a disaster recovery configuration, you have a server
that isn't accepting traffic, but is getting syncs of the main server's
data on a regular basis. In the event the real server were to go down,
the disaster recovery server could be activated and brought in to
receive traffic more quickly than the original server could be revived.

## High Performance Computing

## Container Deployment + Orchestration {#k8s}

One tool that comes up increasingly frequently when talking about
scaling is Kubernetes (sometimes abbreviated as K8S).[^4-4-scaling-4]
Kubernetes is the way people orchestrate Docker containers in production
settings.[^4-4-scaling-5] So basically that it's the way to put
containers into production when you want more than one to interact --
say you've got an app that separately has a database and a front end in
different containers, or, like in this chapter, multiple load-balanced
instances of the same containers.

[^4-4-scaling-4]: Apparently, Kubernetes is an ancient Greek word for
    "helmsman", so cutesy nautical puns abound. See, for example, the
    Kubernetes logo, which is a ship wheel.

[^4-4-scaling-5]: Pedants will be delighted in all the ways this is
    technically incorrect. Some people use other tools like Docker Swarm
    in development, but *rarely* in production. Kubernetes also is not
    limited to *Docker* containers. As of this writing in 2022, those
    are irrelevant details for most people.

While the operational details of Kubernetes are very different from the
horizontal scaling patterns we've discussed so far in this chapter, the
conceptual requirements are the same.

TODO: Diagram of K8S

Many people like Kubernetes because of its declarative nature. If you
recall from the section on [Infrastructure as Code](), declarative code
allows you to make a statement about what the thing is you want and just
get it, instead of specifying the details of how to get there.

Of course, in operation this all can get much more complicated, but once
you've got the right containers, Kubernetes makes it easy to say, "Ok, I
want one instance of my load balancer container connected to three
instances of my compute container with the same volume connected to all
three."

::: callout-warning
## Kubernetes Tripwire!

If you're reading this and are extremely excited about Kubernetes --
that's great! Kubernetes does make a lot of things easy that used to be
hard. Just know, networking configuration is the place you're likely to
get tripped up. You've got to deal with networking into the cluster,
networking among the containers inside the cluster, and then networking
within each container.

Complicated kubernetes networking configurations are not for the faint
of heart.
:::

For individual data scientists, Kubernetes is usually overkill for the
type of work you're doing. If you find yourself in this territory, it's
likely you should try to work with you organization's IT/Admin group.

One of the nice abstraction layers Kubernetes provides is that in
Kubernetes, you provide declarative statements of the containers you
want to run, and any requirements you have. You separately register
actual hardware with the cluster, and Kubernetes takes care of placing
the conatiners onto the hardware depending on what you've got available.

In practice, unless you're part of a very sophisticated IT organization,
you'll almost certainly use Kubernetes via one of the cloud providers'
*Kubernetes clusters as a service*. AWS's is called Elastic Kubernetes
Service (EKS).[^4-4-scaling-6]

[^4-4-scaling-6]: If you aren't using EKS, Azures AKS, or Google's GKE,
    the main other competitor is Oracle's OpenShift, which some
    organizations have running in on-prem Kubernetes clusters.

One really nice thing about using these Kubernetes clusters as a service
is that adding more compute power to your cluster is generally as easy
as a few button clicks. On the other hand, that also makes it dangerous
from a cost perspective.

It is possible to define a Kubernetes cluster "on the fly" and deploy
things to a cluster in an ad hoc way. I wouldn't recommend this for any
production system. *Helm* is the standard tool for defining kubernetes
deployments in code, and *Helmfile* is a templating system for Helm.

So, for example, if you had a standard "Shiny Server" that was one load
balancer containers, two containers each running a Shiny app, and a
volume mounted to both, you would define that cluster in Helm. If you
wanted to be able to template that Helm code for different clusters,
you'd use Helmfile.

## Comprehension Questions

1.  What is the difference between horizontal and vertical scaling? For
    each of the following examples, which one would be more appropriate?
    a.  You're the only person using your data science workbench and run
        out of RAM because you're working with very large data sets in
        memory.

    b.  Your company doubles the size of the team that will be working
        in your data science workbench. Each person will be working with
        reasonably small data, but there's going to be a lot more of
        them.

    c.  You have a big modeling project that's too large for your
        existing machine. The modeling you're doing is highly
        parallelizable.
2.  What is the role of the load balancer in horizontal scaling? When do
    you really need a load balancer and when can you go without?
3.  What are the biggest strengths of Kubernetes as a scaling tool? What
    are some drawbacks?

