# Enterprise Server Management {#sec-ent-servers}

If you work at a small organization that just runs a few servers, you
can run them individually. But as the number of servers, the complexity
of the interactions across services, and security requirements gets
higher, different patterns to manage servers are required.

The common DevOps adage to treat servers as "cattle, not pets" really
comes to the fore. When you're running a few servers, it's ok if each is
a little special. But if you're running 40 or 400, it's too
time-consuming to have to consider individual servers.

There are three main ways that enterprises manage servers very
differently than smaller organizations. The first is trying to
centralize the management of servers for the organization. In some
cases, this means that the IT/Admin team is the only entity that can
stand up servers. In other cases, it means creating a division of labor
so there's a subteam that controls actually provisioning the servers the
organization needs. In other cases, it involves setting up platforms
where individual teams or users can set requisition servers.

As those services and servers get centralized, scaling becomes a bigger
issue. In a large organization with many different data science groups,
trying to centralize all those groups on one data science platform means
that the platform needs to scale.

The first reason that people think of most often is because people are
running big jobs. This can happen at any scale of organization. There
are data science teams of one who have use cases that necessitate
terrabytes of data.

But this isn't an enterprise problem with scaling. The problem in the
enterprise is having many concurrent users of the system. Additionally,
in an enterprise context, the stability of the platform becomes much
more important. If you've got a data science team of four people who sit
together, someone accidentally knocking the RStudio Server offline isn't
a huge deal (been there, done that). But if the data science team is 100
people, the cost of downtime is much higher and more aggressive measures
should be taken to ensure it doesn't happen.

If you additionally have high resource requirements -- or highly
variable requirements -- that just adds a level of complexity onto the
need to figure out how you might scale up your data science environment.

Lastly, enterprise IT/Admins are always thinking about how to make the
process of managing all these servers easier and simpler.
Infrastructure-as-code is the best way to accomplish this.

## Scaling in enterprise

The simplest method of scaling is *vertical scaling*, which is just a
fancy way of saying *use a bigger machine*. Often, moving to a server is
the first way to do vertical scaling. As discussed in [Chapter
@sec-servers], if you're in the cloud, vertical scaling is really easy.

If you have resource-intensive jobs, vertical scaling should be your
first move. This is usually cost effective as well. AWS resource costs
generally scale linearly within a server family. But there are limits.
As of this writing, AWS's general-use instance types max out at 96-128
cores these days. That's probably sufficient for many workloads, but if
you've got an RStudio Server with 50 concurrent users doing reasonably
heavy compute loads, that can quickly get eaten up.

Vertical scaling is very conceptually simple, so we're not going to get
much more into it. If you want a reminder of how to scale up a server in
AWS, look back at the lab in [Chapter @sec-servers].

*Horizontal scaling* or *load-balancing* means distributing the workload
across multiple machines. So instead of a data science workbench that's
just a single server, you can have two or ten or 100 servers available
to you.

There are two main reasons to scale horizontally -- to get more
horsepower behind in your cluster or to provide a *high availability*
experience with more reliable uptime guarantees.

Sometimes people also undertake horizontal scaling to be able to do
autoscaling. The idea here is that the organization could maintain a
small amount of "always-on" capacity and scale out other capacity as
needed to maintain costs. This is something that's possible -- but it
requires nontrivial engineering work.

Anyone who tells you that configuring horizontal scaling will be trivial
is selling you something or lying or both. Distributed system problems
are inherently difficult computer science problems. And by choosing
horizontal scaling, you've given yourself a distributed system problem.

When done for scaling reasons, it is almost always worthwhile to exhaust
the part of vertical scaling where costs grow linearly with compute
before undertaking horizontal scaling.

There is one very simple version of horizontal scaling, which is just
having a bunch of different servers. Each person or each team gets their
own server. This can work well, but involves a lot more server
management and usually involves the team or individual needing
permission and knowledge to spin up servers on their own.

In enterprises, this usually isn't the way things get done. In
enterprises, the plan is usually to run one centralized service that
everyone in the company -- or at least across a large group -- come to.

If you want people to have one front door, this is where things start to
get complicated. In this case, you want the load-balancing to be
invisible to the user, which means two things have to happen -- you have
to have one front door and you have to make sure all of the servers can
serve the same thing.

![](images-scaling/lb-cluster.png){width="484"}

TODO -- add state to this diagram

Making there be one front door for the cluster is pretty
straightforward. You put a special kind of proxy (a load balancer) in
front of your instances. Your load balancer will route sessions among
the nodes in your cluster. Some load balancers can be smart about where
they route people, but at a minimum, your load balancer will need to be
configured with a health check/heartbeat endpoint for your product.

This is a specific endpoint in the product that returns a statement that
the node is actually healthy and accepting traffic. In the absence of a
heartbeat/health check, the load balancer should not route traffic to
that node. Some heartbeats include additional information you might want
to use to do the load balancing. All of the major cloud providers have
*load balancers as a service*.

::: callout-note
### AWS Load Balancer Types

In AWS, the modern load balancer options are an Application Load
Balancer (ALB) and a Network Load Balancer (NLB). For most data science
environment applications, you'd want an ALB.
:::

The other thing that gets difficult about invisibly load balancing a
service is that state has to be symmetrically available to all of the
nodes in the cluster. For example, if I'm working in RStudio Server, I
expect that any session I bring up will have my data and code available
to me without furthre configuration.

One feature that is likely to be a requirement of your data science
environment is *sticky sessions* or *sticky cookies*. This means that
when you come through the load balancer, a cookie is added to your
browser so that the load balancer can keep track of which node you were
routed to. This is important in cases where the app state lives on the
server -- for example, you want to get back to your same instance of a
shiny app in an RStudio Connect cluster. In most load balancers, this is
a simple option you can just turn on.

The details of how this is accomplished varies by the individual app,
but the basic idea is to separate the server state into a backend, often
some combination of a database and file storage, and allow each of the
nodes in the cluster to symmetrically read and write from the state.

Moreover a data science workbench is a particularly difficult thing to
autoscale because it's inherently *stateful*. In computer science terms,
*state* is the configuration of the system at any given point in time.
So if you're typing code in an IDE, your state includes the last line of
code you types. If you're in a Shiny app, state includes the history of
all the interaction you've had with the app.

That's in contrast to a *stateless* application, where every interaction
is completely standalone. The line between statefulness and
statelessness are pretty blurry -- but working inside an IDE is about as
stateful as it gets. You have long-term state like the files you need
and your preferences and short-term state like the recent commands
you've typed and the packages loaded into your R and Python environment.

People love statelessness these days. Containers (and Kubernetes) were
originally conceived to be stateless. As they've come into wider usage,
people have built more statefulness into container usage, but there's
still some awkwardness there.

Many autoscaling routines assume you can scale down and just move
someone around from one machine to another. This is a bad assumption for
a data science workbench and autoscaling a data science workbench
downwards is a difficult challenge.

### Horizontal Scaling for High Availability

Another reason people to horizontal scaling is for resilience. For
example, you might want the cluster to be resilient to a node randomly
failing or being taken down for maintenance. In this context, horizontal
scaling is often called *high availability*. Like many other things,
high availability is a squishy term, and different organizations have
very different definitions of what it means.

For example, in one organization, high availability might just mean that
there's a robust disaster recovery plan so servers can be brought back
online with little data loss. In another organization, high availability
might mean having duplicate servers that aren't physically colocated to
avoid potential outages due to server issues or natural disasters. In
other contexts, it might be a commitment to a particular amount of
uptime.[^4-4-scaling-1]

[^4-4-scaling-1]: Often called nines of uptime, referring to the
    proportion of the time that the service is guaranteed to be online
    out of a year. So a one-nine service is up 90% of the time, allowing
    for 36 days of downtime a year. a five-nine service is up for
    99.999% of the time, allowing for only about 5 1/4 minutes of
    downtime a year.

It's worth repeating that when you undertake horizontal scaling, you've
taken on a distributed systems problem and those are *inherently
difficult*. As the requirements for high availability get steeper, the
engineering cost to make sure the service really is that resilient rise
exponentially...so be careful how much uptime you're trying to achieve.

In fact, it's totally possible to make your system *less stable* by just
doing horizontal scaling in one spot without thinking through the
implications.

Your organization may have requirements that services be configured in a
load-balanced or high-availability mode. If that's the case, you should
figure out what it'll take to do it right and make sure that gets done.

### Specific load-balancing configurations

There are a few different ways to configure load balancing for servers.
The first is called active/active. This just means that all of the
servers are online all of the time. So if I have two RStudio Server
instances, they're both accepting traffic all the time.[^4-4-scaling-2]

[^4-4-scaling-2]: If the service you're configuring does internal
    routing, like RStudio Workbench, you also may need to select whether
    you have one or several primary nodes. A primary node is one that
    accepts traffic from the load balancer and routes it internally. You
    could have just one node in the cluster serve as primary or several.
    Unless there's a good reason to prefer one, it's usually
    advantageous to do several, because it adds one layer of resilience
    to your cluster.

\[Graphic: network diagram of lb-config -- active/active vs
active/passive\]

In active/passive configurations, you have two or more servers, with one
set accepting traffing all the time, and the other set remaining inert
until or unless the first set goes offline. This is sometimes called a
blue/green or red/black configuration.[^4-4-scaling-3]

[^4-4-scaling-3]: Blue/Green and Red/Black aren't the same things, but
    the differences are deep in the weeds...and not even consistently
    used.

People often really like this configuration if they have high
requirements for uptime, and want to be able to do upgrades to the
system behind the scenes and then just cut the traffic over at some
point without an interruption in service. It is a nice idea. It is often
very hard to pull off.

Disaster recovery is not really a load-balancing method, but it's
closely related. In a disaster recovery configuration, you have a server
that isn't accepting traffic, but is getting syncs of the main server's
data on a regular basis. In the event the real server were to go down,
the disaster recovery server could be activated and brought in to
receive traffic more quickly than the original server could be revived.

## Kubernetes in Enterprise

Kubernetes (K8S) is the way people use Docker containers to run
production services.[^4-4-scaling-4] Kubernetes solves all three of the
key enterprise IT/Admin challenges with running servers because you
manage a single cluster, you can scale rather effortlessly, and
app-level administration is extremely simple.

[^4-4-scaling-4]: Apparently, Kubernetes is an ancient Greek word for
    "helmsman", so cutesy nautical puns abound. See, for example, the
    Kubernetes logo, which is a ship wheel.

    Pedants will be delighted in all the ways this is technically
    incorrect. Some people use other tools like Docker Swarm in
    development, but *rarely* in production. Kubernetes also is not
    limited to *Docker* containers. As of this writing in 2022, those
    are irrelevant details for most people.

So basically that it's the way to put containers into production when
you want more than one to interact -- say you've got an app that
separately has a database and a front end in different containers, or,
like in this chapter, multiple load-balanced instances of the same
containers.

While the operational details of Kubernetes are very different from the
horizontal scaling patterns we've discussed so far in this chapter, the
conceptual requirements are the same.

At a conceptual level, people like Kubernetes because you get to think
about the applications you want running completely separately from the
set of servers running underneath.

In Kubernetes, a running Docker container is called a *pod*. When you
want to run one or more pods in Kubernetes, you declare to Kubernetes
that you want a certain number of instances of a certain Docker
container with a certain amount of horsepower behind each pod.

Kubernetes takes care of scheduling the pods on the *nodes* underlying
the cluster. This is amazing, because unlike running services on a
regular VM, you just make sure you've got enough horsepower in the
cluster and then all the app-level requirements go in the container.
Then when you declare how many pods you want, you don't have to worry
about what's going on with each of the nodes in the cluster because it
is just running Kubernetes and Docker.

This really is extremely powerful -- it's pretty easy to tell
Kubernetes, "I want one instance of my load balancer container connected
to three instances of my Workbench container with the same storage
volume connected to all three."

However, Kubernetes is still a complex tool and a data science workbench
is a particuarly difficult fit for Kubernetes. In particular, Kubernetes
was originally designed to be for stateless use cases. For example,
let's imagine a search engine -- every time you put in a search, it
spins up a pod on Kubernetes, does your search, and then spins down.

If you come back in a minute or two, it can just spin up another pod.
Maybe it's saved your last query and remembers that, but the state
maintenance is quite simple. If that pod is on a different node, no big
deal!

That's not the case with a data science workbench. Imagine if Kubernetes
tried to autoscale the node you're on out from under you while you're in
the midst of working on some code.

Networking in Kubernetes can also be quite complicated. For anything
that lives fully in Kubernetes, like your Workbench nodes and
load-balancer, it's quite simple. But getting things into and out of the
Kubernetes cluster -- like the filesystem you probably need to mount in
and accessing databases is a real challenge.

All this to say -- these are solvable problems for an experienced
Kubernetes admin. But they will probably need some guidance around
specific requirements for a data science workbench.

In practice, unless you're part of a very sophisticated IT organization,
you'll almost certainly use Kubernetes via one of the cloud providers'
*Kubernetes clusters as a service*. AWS's is called Elastic Kubernetes
Service (EKS).[^4-4-scaling-5]

[^4-4-scaling-5]: If you aren't using EKS, Azures AKS, or Google's GKE,
    the main other competitor is Oracle's OpenShift, which some
    organizations have running in on-prem Kubernetes clusters.

One really nice thing about using these Kubernetes clusters as a service
is that adding more compute power to your cluster is generally as easy
as a few button clicks. On the other hand, that also makes it dangerous
from a cost perspective.

It is possible to define a Kubernetes cluster "on the fly" and deploy
things to a cluster in an ad hoc way. I wouldn't recommend this for any
production system. *Helm* is the standard tool for defining Kubernetes
deployments in code.

## Comprehension Questions

1.  What is the difference between horizontal and vertical scaling? For
    each of the following examples, which one would be more appropriate?
    a.  You're the only person using your data science workbench and run
        out of RAM because you're working with very large data sets in
        memory.

    b.  Your company doubles the size of the team that will be working
        in your data science workbench. Each person will be working with
        reasonably small data, but there's going to be a lot more of
        them.

    c.  You have a big modeling project that's too large for your
        existing machine. The modeling you're doing is highly
        parallelizable.
2.  What is the role of the load balancer in horizontal scaling? When do
    you really need a load balancer and when can you go without?
3.  What are the biggest strengths of Kubernetes as a scaling tool? What
    are some drawbacks?
