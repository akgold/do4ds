# DevOps Lessons for Data Science

The field of DevOps grew out of the advent of the Agile software movement. In particular, Agile's focus on quick iteration via frequent delivery of small chunks and immediate feedback proved completely incompatible with a pattern where developers completed software and hurled it over an organizational the wall to somehow be put into production by an IT/Admin team.

DevOps was developed as a portmanteau reflecting the two halves of the delivery of software it is meant to bridge -- **dev**elopment and **op**eration**s**. There are a few particular problems that DevOps attempted to solve -- problems that will probably feel familiar if you've ever tried to put a data science asset into production. 

1. Works on my machine
2. Breaks on deployment
3. Services don't scale/perform

Basically every resource on DevOps lists a different set of core principles and frameworks, so I'm going to give my best summary here.

There are a few core tenets of almost any DevOps strategy:

1.  Code should be well-tested and tests should be automated.
2.  Updates should be frequent and low-risk.
3.  Security concerns should be considered up front as part of architecture.
4.  Production systems should have monitoring and logging.
5.  Frequent opportunities for review, change, and updating should be built into the system -- both culturally and technically.

In the DevOps world, there four technical best practices that support these tenets.[^sec-intro-1]

[^sec-intro-1]: Resources you see online have anywhere between three and seven of these. I think these four cover the core of almost any list you'll see.

1.  **CI/CD** Continuous Integration/Continuous Delivery/Continuous Deployment (CI/CD) is the notion that there should be a central repository of code where changes are merged. Once these changes are merged, the code should be tested, built, and delivered/deployed in an automated way.
2.  **Infrastructure as Code** The underlying infrastructure for development and deployment should be reproducible using code so it can be updated and replaced with minimal fuss or disruption.
3.  **Microservices** Any large application should be decomposed into numerous smaller services that is as atomic and lightweight as possible. This makes large projects easier to reason about and makes it safer to update a single component as interfaces are clear.
4.  **Monitoring and Logging** Application metrics and logs are essential for understanding the usage and performance of production services, and should be leveraged as much as possible to have a holistic picture at all times.

Most DevOps frameworks also include communication, collaboration, and review practices as part of their framework, as the technical best practices of DevOps exist to support the work of the people who use them.

## What's in this section

Throughout my experience as a data scientist and working with numerous customers putting data science into production at RStudio, I've come to understand that while 1-1 transcription of these practices isn't appropriate, there are analogs to each of these in the data science world.

This section is going to be about the data science analogs to each of these DevOps best practices.

| DevOps Tenet           | Data Science Analog             |
|------------------------|---------------------------------|
| CI/CD                  | Code Promotion and Integration  |
| Infrastructure as Code | Environments as Code            |
| Microservices          | Data Science Project Components |
| Monitoring and Logging | Monitoring and Logging          |

The first four chapters of this section will be one each on the data science analogs to the DevOps tenets. The last chapter of the section will be an introduction to one of the tools that underlies many DevOps practices -- Docker.

The first chapter on code promotion and integration will help you think about how to structure your app or report so that you can feel secure moving an app into production and updating it later. This chapter will also include an introduction to real CI/CD tools, so that you can get started using them in your own work.

The second chapter is about environments as code. This chapter will help you think about how to create a reproducible and secure project-level data science environment so you can be confident it can be used, secured, and resurrected later (or somewhere else) as need be.

The third chapter is about data science project components. This chapter will help you think about what the various components of your projects are and how to split them up for painless and simple updating and atom-izing.

The fourth chapter in this section is on monitoring and logging, which is -- honestly -- in its infancy in the data science world, but deserves more love and attention.

And in the fifth chapter, we'll learn about Docker -- a tool that has become so common in DevOps practices that it deserves some discussion all on its own. In this section, you'll get a general intro to what Docker is and how it works -- as well as a hands-on intro to using Docker yourself.
