# Code deployment and promotion {#sec-code-promotion}

It's well known that the most dangerous moments of any plane flight are
takeoff and landing. Things can go wrong when you're at altitude, but
the process of getting a plane into and out of the air turns out to be
by far the most dangerous part of the process of flying
somewhere.[^1-1-code-promotion-1]

[^1-1-code-promotion-1]: It's worth noting that commercial air travel is
    exceptionally safe -- including takeoff and landing. But we're
    talking relative risk here.

It turns out that it's the same with software. If it's properly tested,
software that's running smoothly tends -- for the most part -- to
continue running smoothly. But it's the getting it there that gets you.

That's why *deployment* is one of the primary concerns of DevOps best
practices*.*

Deployment is the process of taking code that isn't in production right
now -- whether it's an entirely new piece of software or an update to
something existing -- and turn it into something in production. That
moment when your Shiny app goes live or you run your report or job for
real for the first time, or when you add a new feature to your model --
that's a deployment process.

A good deployment process has three major components:

1.  A separation between production and non-production environments.
2.  An explicit process by which code is deployed into production.
3.  Automation to smoothly execute the deployment.

In this chapter, we'll explore how to apply these three components to a
data science workflow so you can safely develop and test your app before
going to production and how to design and execute a promotion workflow
that makes your deployment completely seamless.

By the end of this chapter, I hope to accomplish three things --
convince you that you need to create a code promotion workflow, equip
you to explain how a data science promotion workflow is different from a
general purpose software one, and illuminate some of the common tooling
you can use to build your own promotion workflows.

## You need a prod

One of the primary risks to production code is that something gets put
into the production environment before it was supposed to.

One precondition for keeping things out of the production environment
until their supposed to be there is actually having a separate
production environment -- usually called *prod*. In order to keep prod
separate from other environments, you'll also need a *promotion process*
that dictates how things get into prod.

![](images-code-promotion/dev-test-prod.png){fig-alt="An app moving through dev, test, and prod environments."}

You can choose the level of separation that's appropriate for the prod
environment for your use case, but some attributes that are common among
prod environments and promotion processes:

-   Nothing in prod is changed manually. You don't get to go in and just
    check out what's happening in prod. If something breaks, you use the
    logs (see [Chapter @sec-monitor-log]) to figure out what went wrong
    and re-create that issue in a lower environment.

-   Only certain people are authorized to approve software into prod.
    The approval process is well-understood and things can only go to
    prod after the proper approvals are gained.

-   The only way for things to get into prod is via promotion from a
    lower environment. Nothing gets tried for the first time in prod.
    It's tried and tested in a lower environment before ever touching
    the prod environment.

In most cases, the right tooling to implement these rules is a
Continuous Integration/Continuous Deployment (CI/CD) pipeline that runs
based on pushes and merges to a git repo. Most often, the git repo is
where the permissions management lives.

## Dev and test environments

Everything that's not prod is a "lower environment". There are often at
least two: dev and test. There are sometimes multiple testing
environments for different kinds of testing.

For data science purposes, *dev* is a sandbox for exploration and
experimentation. Your dev environment is for exploratory data analysis,
testing out new model features or types, and creating a new plot to add
to the dashboard. Dev is probably a Jupyter Notebook or an IDE with data
science capabilities like RStudio, Spyder, VSCode, or PyCharm.

*Test* is an environment that looks exactly like prod, but it's not.
Depending on what you're creating, you might do testing for correctness,
security, performance, or user acceptance testing (UAT). Relative to
general purpose software development, testing a data science project is
a little more difficult. In addition to testing functional aspects of
the software (does this button do what I think?), you may also need to
test for correctness -- does the model output the *right* output, for
some meaning of the word right.

For software that is read-only, you may not really need a completely
standalone environment for testing. You'll want to have a separate
instance of your project that can be tested and adjusted without
affecting the prod instance, but it may be possible to have the two
sitting side-by-side in the same environment. That really depends on
what exactly you're building and the standards of your organization.

This is really different than the general purpose software development
notion of a dev environment. A general purpose software engineer is like
an architect -- they have a stated purpose, and they're designing a
piece of software to fulfill that purpose. They need some space to get
things dusty and to think through exactly the right materials and how
all the pieces fit together and stay on budget -- but they pretty much
know what they're trying to build.

In contrast, a data scientist is much more like an archaeologist. You
don't know what exactly you're going to need to do in dev, because you
need to spend time exploring the data before you can figure out what
you're going to build -- or even if you can build anything of value at
all. The path to finished data science software is usually much more
meandering than general purpose software engineering. is often
meandering -- and it's usually not clear whether it's even a possible
one when you start.

For general purpose software engineering, the big requirements of a
lower environment is that the shape of the inputs look exactly like
prod, but the actual content doesn't matter. For example, if you're
building an online store, you need dev and test environments where the
API calls from the sales system are in exactly the same format as the
real data -- but you don't actually care if it's real data. In fact, you
probably want to create some odd-looking cases for testing purposes.

The requirements for a data science dev and test environment is
different. In a dev or test environment, you don't want write access to
anything sensitive. You absolutely do not want to overwrite your
organization's real data. But for data science, a dev environment
without read access to the real data is basically useless.

::: callout-note
As most data science software consumes much more data than it produces,
read-only access may mean that dev and prod are very similar. If your
software *writes* data, you can use configuration switches so that the
software writes to a fake location in dev and test and to a real
location in prod. More on that below.
:::

This is often a point of friction with IT/Admins who might believe
access to fake data is fine. Convincing them that you need *read-only*
access in a dev environment can often help allay these concerns.

You also may have much more freedom to try out and install different R
and Python packages in dev and test. You'll need a process to get free
reign over packages in dev and test and to lock down package access in
prod.

STOPPED HERE

## Taking data science to production with CI/CD

The common term for the mechanics of code promotion is Continuous
Integration/Continuous Deployment (CI/CD).

CI/CD is about moving your data science projects, including code, data,
and environment, into successively more formal states of production.
While it's not a requirement to use source control to make this happen,
the *mechanics* of CI/CD is usually tightly linked to source control
procedures.

### A Rough Intro to Git

If you're not already familiar, I'd suggest spending some time learning
git. If you're just starting, you're in for a bit of a ride.

People who say git is easy are either lying to look smarter or learned
so long ago that they have forgotten how easy it is to mess up your
entire workflow at any moment.

I'm not going to get into the mechanics of git in this book-- what it
means to add, commit, push, pull, merge, and more. There are lots of
great resources out there that I'm not aiming to reproduce.

::: rmdinfo
If you don't already know git and want to learn, I'd recommend
[HappyGitWithR](https://happygitwithr.com/index.html) by Jenny Bryan.
It's a great on-ramp to learn git.

Even if you're a Python user, the sections on getting started with git,
on basic git concepts, and on workflows will be useful since they
approach git from a data science perspective.
:::

There's a git [cheatsheet](#cheat-git) at the end of the chapter, and I
will talk about some git *strategies* that match well with using git to
execute a data science code promotion strategy.

For production data science assets, I generally recommend long-running
dev (or test) and prod branches, with feature branches for developing
new things. The way this works is that new features are developed in a
feature branch, merged into dev for testing, and then promoted to prod
when you're confident it's ready.

For example, if you had two new plots you were adding to an existing
dashboard, your git commit graph might look like this:

![](images-code-promotion/git-branches.png){width="474"}

CI/CD adds a layer on top of this. CI/CD allows you to integrate
functional testing by automatically running those tests whenever you do
something in git. These jobs can run when a merge request is made, and
are useful for tasks like spellchecking, linting, and running tests.

For the purposes of CI/CD, the most interesting jobs are those that do
something after there's a commit or a completed merge, often deploying
the relevant asset to its designated location.

A CI/CD integration using the same git graph as above would have
released 3 new test versions of the app and 2 new prod versions. Note
that in this case, the second test release revealed a bug, which was
fixed and tested in the test version of the app before a prod release
was completed.

In years past, the two most popular CI/CD tools were called Travis and
Jenkins. By all accounts, these tools were somewhat unwieldy and
difficult to get set up. More recently, GitHub -- the foremost git
server -- released GitHub Actions (GHA), which is CI/CD tooling directly
integrated into GitHub that's free for public repositories and free up
to some limits for private ones.

It's safe to say GHA is eating the world of
CI/CD.[^1-1-code-promotion-2]

[^1-1-code-promotion-2]: It's worth remarking that GitLab has long
    offered a very good CI/CD integration. I'm going to talk about GHA,
    since it's the most popular, but you can accomplish these same
    things with GitLab CI/CD.

    Lots of organizations have Azure DevOps, formerly known as Visual
    Studio Team Services. Microsoft, who owns Azure, acquired GitHub in
    2018, so I'd expect Azure DevOps and GitHub to converge at some
    point...but who knows?

For example, if you're reading this book online, it was deployed to the
website you're currently viewing using GHA. I'm not going to get deep
into the guts of GHA, but instead talk generally about the pattern for
deploying data science assets, and then go through how I set up this
book on GHA.

### Using CI/CD to deploy data science assets

In general, using a CI/CD tool to deploy a data science asset is pretty
straightforward. The mental model to have is that the CI/CD tool stands
up a completely empty server for you, and runs some code on it.

That means that if you're just doing something simple like
spellchecking, you can probably just specify to run spellcheck. If
you're doing something more complicated, like rendering an R Markdown
document or Jupyter Notebook and then pushing it to a server, you'll
have to take a few extra steps to be sure the right version of R or
Python is on the CI/CD server, that your package environment is properly
reproduced, and that you have the right code to render your document.

## Per-Environment Configuration {config-swap=""}

Sometimes you want a little more flexibility -- for example the option
to switch many the environment variables depending on the environment.

In R, the standard way to do this is using the
[`config`](https://cran.r-project.org/web/packages/config/index.html)
package. There are many options for managing runtime configuration in
Python, including a package called config.

For example, let's consider this shiny app. In this app, every time I
press the button, the app sends a `POST` request to an external service
indicating that the button has been pressed.

```{r}
#| eval: false

library(shiny)

# UI that's just a button
ui <- fluidPage(
  actionButton("button", "Press Me!")
)

# Do something on button press
server <- function(input, output) {
  observeEvent(
    input$button, 
    httr::POST(
      "www.my-external-system.com", 
      body = list(button_pressed = TRUE)
    )
  )
}

# Run the application 
shinyApp(ui = ui, server = server)
```

With the URL hardcoded like this, it's really hard to imagine doing this
in a Dev or Test environment.

However, with R's config package, you can create a config.yml file that
looks like this:

```{default}
dev:
  url: "www.test-system.com"
  
prod:
  url: "www.my-external-system.com"
```

Then you can switch to the correct config and apply that configuration
inside the app.

```{r}
#| eval: false

library(shiny)
config <- config::get("prod")

# UI that's just a button
ui <- fluidPage(
  actionButton("button", "Press Me!")
)

# Do something on button press
server <- function(input, output) {
  observeEvent(
    input$button, 
    httr::POST(
      config$url, 
      body = list(button_pressed = TRUE)
    )
  )
}

# Run the application 
shinyApp(ui = ui, server = server)
```

## The power of environment variables {#sec-env-vars}

Environment variables are an extremely powerful way to manage the
context in which your code is running.

Whenever your code is running, your system maintains a number of
different values to be used by processes running on the system. These
variables are not specific to the R or Python process on the system --
but they are usable by R or Python to powerful ends.

It is convention to make environment variable names in all caps, words
separated by underscores. The values are always simple character values,
though these can be cast to some other type inside R or Python.

In R, you can set environment variables in an `.Renviron` file or using
`Sys.setenv(VAR_NAME = VALUE)` in a `.Rprofile`. These files are sourced
on process startup, so the values are always available inside the
running process with `Sys.getenv("VAR_NAME")`. You can have user and/or
project level files and even server-level ones with `.Renviron.site` and
`.Rprofile.site`.

In Python, environment variables are accessible one-by-one using the
`os.getenv` function or as a dictionary in `os.environ`. If you're using
a lot of environment variables, it's common to import them using a
statement like `from os import environ as env`.

There is no equivalent of the `.Rprofile` that's sourced before startup
in Python, but it's common to use the `python-dotenv` package to
manually load the contents of a `.env` file into the environment as one
of the first few lines in your code.

You should not commit these files to git, but should manually move them
across environments, so they never appear anywhere centrally accessible.

Once you're using environment variables, they're extremely powerful.
It's very common to set the default value of a function argument to be
the value of an environment variable. For example, in the example using
the R `{config}` package, `config::get()` uses the value of the
`R_CONFIG_ACTIVE` environment variable to choose which configuration to
use.

That means that switching from the dev to the prod version of the app is
as easy as making sure you've got the correct environment variable set
on your system.[^1-1-code-promotion-3]

[^1-1-code-promotion-3]: If you're deploying to Posit Connect,
    `R_CONFIG_ACTIVE` is locked to `rsconnect`. If you've got multiple
    deployments to Connect that you want to differentiate, you may want
    to use a different environment variable, which you would specify
    manually like `config::get(config = Sys.getenv("WHICH_CONFIG"))` .

It can also be a great way to safely store secrets on your system. If
you've got a function that, for example, creates a database connection,
it's a good idea to have the `username` and `password` arguments of your
function default to reading from an environment variable so you don't
ever have to expose them in your code.

That might look something like this in R:

```{r}
#| eval: false

my_db_con <- function(user = Sys.getenv("DB_USER"), pw = Sys.getenv("DB_PW")) {
  make_db_con(user, pw)
}
```

and in Python

TODO: Check python version works

```{python}
#| eval: false

def my_db_con(user = os.getenv("db_user"), pw = os.getenv("db_pw")):
  make_db_con(user, pw)

```

## Comprehension Questions

1.  Write down a mental map of the relationship between the three
    environments for data science.
2.  What are the options for protecting production data in a dev or test
    environment?
3.  Why is git so important to a good code promotion strategy? Can you
    have a code promotion strategy without git?
4.  What is the relationship between git and CI/CD? What's the benefit
    of using git and CI/CD together?

## Lab 1: Host a Website on GitHub with Automatic Updates {#lab1}

The goal for this lab is to set up a [GitHub
Pages](https://pages.github.com/) website that automatically updates
when the code changes using [GitHub
Actions](https://github.com/features/actions).

We're going to create the website using [Quarto](https://quarto.org) for
our website. Quarto is a open source scientific and technical publishing
system that makes it easy to render R and Python code into beautiful
documents, websites, reports, and presentations.

I'm not going to give you a complete step-by-step walkthrough, but here
is the outline:

1.  Create an empty public git repo on GitHub and clone it locally.
2.  Create a Quarto website in the repo.
3.  Publish the website to GitHub Pages.
4.  Configure GitHub Actions to re-render the website and push updates
    when the GitHub repo is updated.

If you need to learn more about using git or GitHub, I'd recommend
checking out [Happy Git with R](https://happygitwithr.com/). There are
tons of great git references online. This one is nicely oriented towards
the needs of a data professional. It's obviously a little more
R-oriented than Python, but I think it's a great resource regardless of
your language of choice.

Once you've got the git repo created and a basic Quarto website created,
the Quarto website has [great instructions to configure publishing with
GitHub
Actions](https://quarto.org/docs/publishing/github-pages.html#github-action).

In those instructions, you'll set up GitHub Pages to serve your website
off a long-running standalone branch called `gh-pages`. Along the way,
you'll generate an `_publish.yml`, which is a Quarto-specific file for
configuring publishing locations.

By default, your website will be available at
`https://<username>.github.io/<repo-name>`. If you want to use a custom
domain, GitHub has [good
instructions](https://docs.github.com/en/pages/configuring-a-custom-domain-for-your-github-pages-site/about-custom-domains-and-github-pages)
for how to set that up. While you can just follow those instructions,
I'd recommend you wait until you get to [Chapter @sec-dns] to understand
a little better how it all works.

You'll also generate the GitHub Action, which will live at
`.github/workflows/publish.yml`. Since the main point here is to learn
about CI/CD like GitHub Actions, let's take a minute to make sure you
understand what the GitHub Action does.

``` {.yaml include="../../_labs/gha/publish-basic.yml" filename=".github/workflows/publish.yml"}
```

If you've never looked at YAML before, it should be reasonably
human-readable. In this YAML file, there are three high-level sections
-- `on`, `name`, and `jobs`. This is a pretty minimal GitHub Actions
workflow. Every workflow needs an `on` and `jobs`. The `name` is
technically optional, but it's a good idea to keep track of what job is
running.

While this particular syntax is unique to GitHub Actions, the basic idea
is universal to any CI/CD system. You have to define when something
occurs and what that something is.

The `on` section defines *when* the workflow occurs. In this case, we've
configured the workflow only to trigger on a push to the `main` branch.
Note that a merge into main counts. This is basically the simplest
trigger you can have. GitHub Actions supports a ton of different ways to
trigger a workflow. You can see more [in the
documentation](https://docs.github.com/en/actions/using-workflows/triggering-a-workflow).

The `jobs` section defines what happens when the job is triggered. The
most common way to define what happens is to use preexisting GitHub
Actions steps with `uses`. Recall that part of the value of CI/CD is
that it occurs in a completely isolated environment, so you'll first
have to set up the environment for the work to occur, then actually do
the work.

As you can see from the `runs-on` field, we're starting with a very
basic Ubuntu setup. After that, the first two workflow steps set up our
environment, with our website's GitHub repository checked out into the
Actions environment and Quarto installed.

In the third step, we're actually rendering the website and then pushing
it back to the repository. This third step makes use of arguments in
GitHub Actions. In GitHub Actions, `with` defines variables used by the
workflow, and `env` defines values in the environment -- often secrets
used by the workflow, like in this case.

At this point, you can just take the pre-defined workflow and use it.

Once you've done that, you're website should be good to go. If you want
to check, make a change to your website and push to the main branch. If
you click on the `Actions` tab on GitHub you'll be able to see the
Action running. Once it finishes, you should be able to see your change
reflected on your website.

## Cheatsheet: Git {#cheat-git}

| Command                         | What it Does                                           |
|---------------------------|---------------------------------------------|
| `git clone <remote>`            | Clone a remote repo -- make sure you're using SSH URL. |
| `git add <files/dir>`           | Add files/dir to staging area.                         |
| `git commit -m <message>`       | Commit your staging area.                              |
| `git push origin <branch>`      | Push to a remote.                                      |
| `git pull origin <branch>`      | Pull from a remote.                                    |
| `git checkout <branch name>`    | Checkout a branch.                                     |
| `git checkout -b <branch name>` | Create and checkout a branch.                          |
| `git branch -d <branch name>`   | Delete a branch.                                       |

