# Code deployment and promotion {#sec-code-promotion}

It's well known that the most dangerous moments of any plane flight are
takeoff and landing. Things can go wrong when you're at altitude, but
the process of getting a plane into and out of the air turns out to be
by far the most dangerous part of the process of flying
somewhere.[^1-1-code-promotion-1]

[^1-1-code-promotion-1]: It's worth noting that commercial air travel is
    exceptionally safe -- including takeoff and landing. But we're
    talking relative risk here.

It turns out that it's the same with software. If it's properly tested,
software that's running smoothly tends -- for the most part -- to
continue running smoothly. But it's the getting it there that gets you.

That's why *deployment* is one of the primary concerns of DevOps best
practices.

Deployment is the process of taking code that isn't in production right
now -- whether it's an entirely new piece of software or an update to
something existing -- and turn it into something in production. That
moment when your Shiny app goes live or you run your report or job for
real for the first time, or when you add a new feature to your model --
that's a deployment process.

In DevOps speak, good deployment processes are referred to as Continuous
Integration and Continuous Deployment (CI/CD). The idea is that
deployment to production is very low friction (preferably automated), so
deployments are incremental, frequent, and low drama.

A good deployment process has three major components:

1.  A separation between production and non-production environments.
2.  An explicit process by which code is deployed into production.
3.  Automation to smoothly execute the deployment.

In this chapter, we'll explore how to apply these three components to a
data science workflow so you can safely develop and test your app before
going to production and how to design and execute a promotion workflow
that makes your deployment completely seamless.

By the end of this chapter, I hope to accomplish three things --
convince you that you need to create a code promotion workflow, equip
you to explain how a data science promotion workflow is different from a
general purpose software one, and illuminate some of the common tooling
you can use to build your own promotion workflows.

## You need a prod

One of the primary risks to production code is that something gets put
into the production environment before it was supposed to.

One precondition for keeping things out of the production environment
until their supposed to be there is actually having a separate
production environment -- usually called *prod*. In order to keep prod
separate from other environments, you'll also need a *promotion process*
that dictates how things get into prod.

![](images-code-promotion/dev-test-prod.png){fig-alt="An app moving through dev, test, and prod environments."}

You can choose the level of separation that's appropriate for the prod
environment for your use case, but some attributes that are common among
prod environments and promotion processes:

-   Nothing in prod is changed manually. You don't get to go in and just
    check out what's happening in prod. If something breaks, you use the
    logs (see [Chapter @sec-monitor-log]) to figure out what went wrong
    and re-create that issue in a lower environment.

-   Only certain people are authorized to approve software into prod.
    The approval process is well-understood and things can only go to
    prod after the proper approvals are gained.

-   The only way for things to get into prod is via promotion from a
    lower environment. Nothing gets tried for the first time in prod.
    It's tried and tested in a lower environment before ever touching
    the prod environment.

## Dev and test environments

Everything that's not prod is a "lower environment". There are often at
least two: dev and test. There are sometimes multiple testing
environments for different kinds of testing.

For data science purposes, *dev* is a sandbox for exploration and
experimentation. Your dev environment is for exploratory data analysis,
testing out new model features or types, and creating a new plot to add
to the dashboard. Dev is probably a Jupyter Notebook or an IDE with data
science capabilities like RStudio, Spyder, VSCode, or PyCharm.

*Test* is an environment that looks exactly like prod, but it's not.
Depending on what you're creating, you might do testing for correctness,
security, performance, or user acceptance testing (UAT). Relative to
general purpose software development, testing a data science project is
a little more difficult. In addition to testing functional aspects of
the software (does this button do what I think?), you may also need to
test for correctness -- does the model output the *right* output, for
some meaning of the word right.

For software that is read-only, you may not really need a completely
standalone environment for testing. You'll want to have a separate
instance of your project that can be tested and adjusted without
affecting the prod instance, but it may be possible to have the two
sitting side-by-side in the same environment. That really depends on
what exactly you're building and the standards of your organization.

This is really different than the general purpose software development
notion of a dev environment. A general purpose software engineer is like
an architect -- they have a stated purpose, and they're designing a
piece of software to fulfill that purpose. They need some space to get
things dusty and to think through exactly the right materials and how
all the pieces fit together and stay on budget -- but they pretty much
know what they're trying to build.

In contrast, a data scientist is much more like an archaeologist. You
don't know what exactly you're going to need to do in dev, because you
need to spend time exploring the data before you can figure out what
you're going to build -- or even if you can build anything of value at
all. The path to finished data science software is usually much more
meandering than general purpose software engineering. is often
meandering -- and it's usually not clear whether it's even a possible
one when you start.

For general purpose software engineering, the big requirements of a
lower environment is that the shape of the inputs look exactly like
prod, but the actual content doesn't matter. For example, if you're
building an online store, you need dev and test environments where the
API calls from the sales system are in exactly the same format as the
real data -- but you don't actually care if it's real data. In fact, you
probably want to create some odd-looking cases for testing purposes.

The requirements for a data science dev and test environment is
different. In a dev or test environment, you don't want write access to
anything sensitive. You absolutely do not want to overwrite your
organization's real data. But for data science, a dev environment
without read access to the real data is basically useless.

::: callout-note
As most data science software consumes much more data than it produces,
read-only access may mean that dev and prod are very similar. If your
software *writes* data, you can use configuration switches so that the
software writes to a fake location in dev and test and to a real
location in prod. More on that below.
:::

This is often a point of friction with IT/Admins who might believe
access to fake data is fine. Convincing them that you need *read-only*
access in a dev environment can often help allay these concerns.

You also may have much more freedom to try out and install different R
and Python packages in dev and test. You'll need a process to get free
reign over packages in dev and test and to lock down package access in
prod.

The rest of this chapter will explain the actual procedures and tooling
that comprise a good code promotion process.

## Environment Variables  {#sec-env-vars}

Environment variables are extremely important for code promotion process
because they provide a way to manage secrets across multiple
environments, as well as a way to change behavior of your project in
different environments.

An environment variable is a value that is set in the environment. That
means unlike a regular R or Python variable that is expected to change
throughout the runtime, environment variables are set at the outset of
your code to change the behavior throughout the runtime.

Inside your code, you'll use the *name* of the environment variable, so
your app can use secrets and have different behavior without your code
changing at all from environment to environment.

## Environment Variables to inject secrets

When you're moving your code across environments, one of the most
delicate parts is managing secrets like passwords and secret access
keys. Let's start with the wrong way to do this. *Do not hardcode your
secrets in your code*.

Putting secrets directly in your code is a great way to end up with them
leaking on GitHub or to people who aren't supposed to have them.
Instead, the best way to manage secrets is to reference them by name in
your code. When you manage secrets by name, it makes it safe to share
your code freely and make secrets management a separate concern.

::: callout-note
It is convention to make environment variable names in all caps with
words separated by underscores. The values are always simple character
values, though these can be cast to some other type inside R or Python.
:::

In Python, you can read environment variables from the `os.environ`
dictionary or by using `os.getenv("<VAR_NAME>")`. In R, you can get and
set environment variables with `Sys.getenv("<VAR_NAME>")`.

In your Dev environment, you'll probably load in secrets manually. Most
often this is done in plain text from a file on disk, though
organizations are increasingly moving to secrets management tools so you
get these secrets via an API of some sort.

If you are loading in secrets from a plain text file, in Python,
environment variables are usually set by explicitly reading a `.env`
file into your Python session. The [`{python-dotenv}`
package](https://pypi.org/project/python-dotenv/) is a good choice for
doing this.

R automatically reads the `.Renviron` file as environment variables and
also sources the `.Rprofile` file, where you can set environment
variables with `Sys.setenv()`. I personally prefer putting everything in
`.Rprofile` for simplicity -- but that's not a universal opinion.

Depending on the tooling you're using, your Test and Prod environment
may well come with integrated secrets management. The exact
configuration will depend on that environment, but you should use those
tools when they're available.

## Different behavior per environment

Let's think about the world from the perspective of your data science
project as it moves through Dev, Test, and Prod. If your it's very
simple -- perhaps a Shiny app that just reads a local CSV file -- then
the world will look exactly the same in the three environments.

But in many cases, something will actually look different to the app.
You might want to switch data sources from a Dev database to a prod one,
or you want to write to a real output location, or you want to use a
different level of logging.

That means your project needs the ability to know which environment it's
in and then to respond accordingly.

-   **Environment Names** Very often, you want your code to know where
    it is. Having a variable that specifies the name of the environment
    (Dev, Test, or Prod) can be useful.

-   **Behavioral Configuration** If you've got a Dev, Test, Prod setup,
    you may want specific behavior -- like whether something actually
    writes or logging levels to be different. You can use an environment
    variable to let your code know what to do.

-   **Data Access Parameters** The most common difference across
    environments is the location of data. Putting these values into
    environment variables that change across environments is a great way
    to store them.

The basic idea is that you set up a config file with named sections.
There's a default section with overrides for higher values. Then, you
tell your code which environment it's in so that it picks up the right
section.

There are a variety of ways to do this in Python -- many of which
involve creating a custom class for each of the environments you want to
use and then selecting the right class based on the value of an
environment variable.

In R, the `{config}` package is the standard way to load an
environmental configuration. You use the `config::get()` uses the value
of the `R_CONFIG_ACTIVE` environment variable to choose which
configuration to use.

That means that switching from the dev to the prod version of the app is
as easy as making sure you've got the correct environment variable set
on your system.

## Automated Promotion

It's possible to manually promote code from one environment to another.
Some organizations write procedures for how code gets promoted. The
issue with this approach is that humans aren't great at following
procedures. Environments often drift apart, and your rigorous code
promotion strategy is suddenly coming undone.

Computers, on the other hand, are great at following procedures. The
best way to automate CI/CD is to attach it to version control. These
days, [git](https://git-scm.com/) is the industry standard.

### A Rough Intro to Git

If you're not already familiar, I'd suggest spending some time learning
git. If you're just starting, you're in for a bit of a ride.

People who say git is easy are either lying to look smarter or learned
so long ago that they have forgotten how easy it is to mess up your
entire workflow at any moment.

I'm not going to get into the mechanics of git in this book-- what it
means to add, commit, push, pull, merge, and more. There are lots of
great resources out there that I'm not aiming to reproduce, though there
is a git [cheatsheet](#cheat-git) at the end of the chapter.

::: rmdinfo
If you don't already know git and want to learn, I'd recommend
[HappyGitWithR](https://happygitwithr.com/index.html) by Jenny Bryan.
It's a great on-ramp to learn git.

Even if you're a Python user, the sections on getting started with git,
on basic git concepts, and on workflows will be useful since they
approach git from a data science perspective.
:::

Instead, I want to talk about git *strategies* that match well with
using git to execute a data science code promotion strategy.

For production data science assets, I generally recommend long-running
dev (and/or test) and prod branches, with feature branches for
developing new things. The way this works is that new features are
developed in a feature branch, merged into dev for testing, and then
promoted to prod when you're confident it's ready.

For example, if you had two new plots you were adding to an existing
dashboard, your git commit graph might look like this:

![](images-code-promotion/git-branches.png){width="474"}

CI/CD adds a layer on top of this. CI/CD allows you to integrate
functional testing by automatically running those tests whenever you do
something in git. These jobs can run when a merge request is made, and
are useful for tasks like spell checking, linting, and running tests.

For the purposes of CI/CD, the most interesting jobs are those that do
something after there's a commit or a completed merge, often deploying
the relevant asset to its designated location.

A CI/CD integration using the same git graph as above would have
released 3 new test versions of the app and 2 new prod versions. Note
that in this case, the second test release revealed a bug, which was
fixed and tested in the test version of the app before a prod release
was completed.

In years past, the two most popular CI/CD tools were called Travis and
Jenkins. By all accounts, these tools were somewhat unwieldy and
difficult to get set up. More recently, GitHub -- the foremost git
server -- released GitHub Actions (GHA), which is CI/CD tooling directly
integrated into GitHub that's free for public repositories and free up
to some limits for private ones.

It's safe to say GHA is eating the world of
CI/CD.[^1-1-code-promotion-2]

[^1-1-code-promotion-2]: It's worth remarking that GitLab has long
    offered a very good CI/CD integration. I'm going to talk about GHA,
    since it's the most popular, but you can accomplish these same
    things with GitLab CI/CD.

    Lots of organizations have Azure DevOps, formerly known as Visual
    Studio Team Services. Microsoft, who owns Azure, acquired GitHub in
    2018, so I'd expect Azure DevOps and GitHub to converge at some
    point...but who knows?

For example, if you're reading this book online, it was deployed to the
website you're currently viewing using GHA. I'm not going to get deep
into the guts of GHA, but instead talk generally about the pattern for
deploying data science assets, and then go through how I set up this
book on GHA.

## Comprehension Questions

1.  Write down a mental map of the relationship between the three
    environments for data science.
2.  What are the options for protecting production data in a dev or test
    environment?
3.  Why is git so important to a good code promotion strategy? Can you
    have a code promotion strategy without git?
4.  What is the relationship between git and CI/CD? What's the benefit
    of using git and CI/CD together?

## Lab 1: Host a Website on GitHub with Automatic Updates {#lab1}

The goal for this lab is to set up a [GitHub
Pages](https://pages.github.com/) website that automatically updates
when the code changes using [GitHub
Actions](https://github.com/features/actions).

We're going to create the website using [Quarto](https://quarto.org) for
our website. Quarto is a open source scientific and technical publishing
system that makes it easy to render R and Python code into beautiful
documents, websites, reports, and presentations.

I'm not going to give you a complete step-by-step walkthrough, but here
is the outline:

1.  Create an empty public git repo on GitHub and clone it locally.
2.  Create a Quarto website in the repo.
3.  Publish the website to GitHub Pages.
4.  Configure GitHub Actions to re-render the website and push updates
    when the GitHub repo is updated.

If you need to learn more about using git or GitHub, I'd recommend
checking out [Happy Git with R](https://happygitwithr.com/). There are
tons of great git references online. This one is nicely oriented towards
the needs of a data professional. It's obviously a little more
R-oriented than Python, but I think it's a great resource regardless of
your language of choice.

Once you've got the git repo created and a basic Quarto website created,
the Quarto website has [great instructions to configure publishing with
GitHub
Actions](https://quarto.org/docs/publishing/github-pages.html#github-action).

In those instructions, you'll set up GitHub Pages to serve your website
off a long-running standalone branch called `gh-pages`. Along the way,
you'll generate an `_publish.yml`, which is a Quarto-specific file for
configuring publishing locations.

By default, your website will be available at
`https://<username>.github.io/<repo-name>`. If you want to use a custom
domain, GitHub has [good
instructions](https://docs.github.com/en/pages/configuring-a-custom-domain-for-your-github-pages-site/about-custom-domains-and-github-pages)
for how to set that up. While you can just follow those instructions,
I'd recommend you wait until you get to [Chapter @sec-dns] to understand
a little better how it all works.

You'll also generate the GitHub Action, which will live at
`.github/workflows/publish.yml`. Since the main point here is to learn
about CI/CD like GitHub Actions, let's take a minute to make sure you
understand what the GitHub Action does.

``` {.yaml include="../../_labs/gha/publish-basic.yml" filename=".github/workflows/publish.yml"}
```

If you've never looked at YAML before, it should be reasonably
human-readable. In this YAML file, there are three high-level sections
-- `on`, `name`, and `jobs`. This is a pretty minimal GitHub Actions
workflow. Every workflow needs an `on` and `jobs`. The `name` is
technically optional, but it's a good idea to keep track of what job is
running.

While this particular syntax is unique to GitHub Actions, the basic idea
is universal to any CI/CD system. You have to define when something
occurs and what that something is.

The `on` section defines *when* the workflow occurs. In this case, we've
configured the workflow only to trigger on a push to the `main` branch.
Note that a merge into main counts. This is basically the simplest
trigger you can have. GitHub Actions supports a ton of different ways to
trigger a workflow. You can see more [in the
documentation](https://docs.github.com/en/actions/using-workflows/triggering-a-workflow).

The `jobs` section defines what happens when the job is triggered. The
most common way to define what happens is to use preexisting GitHub
Actions steps with `uses`. Recall that part of the value of CI/CD is
that it occurs in a completely isolated environment, so you'll first
have to set up the environment for the work to occur, then actually do
the work.

As you can see from the `runs-on` field, we're starting with a very
basic Ubuntu setup. After that, the first two workflow steps set up our
environment, with our website's GitHub repository checked out into the
Actions environment and Quarto installed.

In the third step, we're actually rendering the website and then pushing
it back to the repository. This third step makes use of arguments in
GitHub Actions. In GitHub Actions, `with` defines variables used by the
workflow, and `env` defines values in the environment -- often secrets
used by the workflow, like in this case.

At this point, you can just take the pre-defined workflow and use it.

Once you've done that, you're website should be good to go. If you want
to check, make a change to your website and push to the main branch. If
you click on the `Actions` tab on GitHub you'll be able to see the
Action running. Once it finishes, you should be able to see your change
reflected on your website.

## Cheatsheet: Git {#cheat-git}

| Command                         | What it Does                                           |
|---------------------------|---------------------------------------------|
| `git clone <remote>`            | Clone a remote repo -- make sure you're using SSH URL. |
| `git add <files/dir>`           | Add files/dir to staging area.                         |
| `git commit -m <message>`       | Commit your staging area.                              |
| `git push origin <branch>`      | Push to a remote.                                      |
| `git pull origin <branch>`      | Pull from a remote.                                    |
| `git checkout <branch name>`    | Checkout a branch.                                     |
| `git checkout -b <branch name>` | Create and checkout a branch.                          |
| `git branch -d <branch name>`   | Delete a branch.                                       |
