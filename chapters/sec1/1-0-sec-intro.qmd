# DevOps Lessons for Data Science {#sec-1-intro}

You are a software developer.

It might not be your title, which is probably something like data
scientist or statistician or data engineer. But if you're writing R or
Python code for production, you're developing software.

DevOps principles aim to create software that builds scalability,
security, and stability into the software from the very beginning. The
whole idea is to avoid creating software that works locally, but doesn't
work well in collaboration or production.

You could just take general purpose DevOps principles and apply them to
data science. If you talk to a software engineer or IT/Admin who doesn't
know about data science, they'll probably encourage you to do so.

But the specifics of those principles are squishy. Basically every
resource on DevOps lists a different set of core principles and
frameworks.[^1-0-sec-intro-1] And for data scientists, that's
exacerbated by the profusion of data science adjacent *xOps* terms like
DataOps, MLOps, and more.

[^1-0-sec-intro-1]: If you enjoy this introduction, I strongly recommend
    *The Phoenix Project* by Gene Kim, Kevin Behr, and George Spafford.
    It's a novel about implementing DevOps principles. A good friend
    described it as, "a trashy romance novel about DevOps". It's a very
    fun read.

Moreover, the type of software you're creating as a data scientist isn't
general purpose software.

A general purpose software engineer is like an architect -- they have a
known spot to build something that needs to meet certain specifications.
That's a very different kind of job than trying to ferret out and share
a needle of signal in a haystack of noise. General purpose software
engineers get to dream up data structures and data flows from scratch,
designing them to work optimally for their systems.

As a data professional, your job is to take data generated elsewhere --
by a business, social, or natural process -- derive some sort of signal
from it, and make that signal available to the systems and people that
need it.

That means that if the sofware developer is an architecture, you as a
data scientist are an archaeologist. You're pointed at a spot where
there's some data and told to figure out what -- if anything -- can be
built to deliver value from it. The path to finished data science
software is usually much more meandering than general purpose software
engineering.

That means the software you write is software for data science. And the
tooling and best practices for a resilient production experience are
different than for general purpose software development.

In the chapters in this section, we'll explore the best practices and
choices you can make as a data scientist to make your apps and
environments as robust as possible.

### Managing environments

One of the core issues DevOps addresses is the dreaded "works on my
machine" phenomenon. If you've ever collaborated on a data science
project, you've almost certainly reached a point where something worked
on your laptop but not for your colleague, and *you just don't know
why.*

The issue is that the code you're writing relies on the environment in
which it runs. and you've shared the code but not the environment.The
DevOps solution is to create explicit linkages between the code and the
environment so you can share both together, which is what [Chapter
@sec-env-as-code] is all about.

### App Architecture

Despite the fact that you're more archaeologist than architect, you do
have some space to play architect as you take your work to production.
At that point you should know what you've unearthed and you're trying to
figure out how to best share it.

Software development best practices and DevOps have a lot to say about
app architecture patterns that work well. One of the key concepts is of
*app layers*. In traditional software engineering, the most common
architecture is a *three-layer app*. In a three layer app, the
*front-end* or *presentation layer*, is divided up from the *back-end*
or the *application layer* and *data layer*.

But there are two big differences between general purpose software and
data science software.

First, the software you create is much more likely to *consume* data
than to *produce* it. And you're probably stuck with existing data flows
that were designed by someone who wasn't thinking about the needs of
data science at all.

That's in stark contrast to general purpose software, which generally
are produce at least as much data as they consume -- and the data they
consume is usually data the software produces for itself. On net,
Microsoft Word, electronic health records, and Twitter all produce much
more data then they consume, and most of the data they previously
produced.

That means that instead of designing a database layer for your app,
you're probably using a preexisting data source. [Chapter
@sec-data-access] is all about how to connect to securely connect to
data sources from your data science projects.

[Chapter @sec-proj-arch] is all about how to take DevOps and Software
Engineering best practices and apply them to the layers of your app you
**can** control -- the processing and presentation layers.

### Monitoring and Logging

DevOps addresses the "what's happening in there" issue. It's bad to find
out from someone else that your app was down or that your model was
producing bad results. DevOps practices aim to make the what's happening
inside the system visible during and after the code runs. [Chapter
@sec-log-monitor] addresses how to build monitoring and logging into
your data science projects.

### Deployments

DevOps also addresses the "breaks on deployment" issue. Perhaps your
code was thoroughly tested, but only locally, or perhaps you don't test
your code. DevOps patterns are designed to increase the likelihood your
code will deploy right *the first time*. [Chapter @sec-code-promotion]
gets into how to design a deployment and promotion system that is
robust.

### Docker for Data Science

Docker is an increasingly popular tool in the software development and
data science world that allows for the easy capture and sharing of the
environment around code. While Docker itself doesn't solve these
problems, it's increasingly popular to use Docker in a data science
context, which is why [Chapter @sec-docker] is a basic introduction to
what Docker is and how to use it.

## Labs in this section

Each chapter in this section has a lab so you can get hands-on
experience implementing the best practices I propose.

If you complete the labs, you'll have stood up your Palmer Penguins
website to explore the relationship between penguin bill length and
mass. Your website will include pages on exploratory data analysis and
model building. This website will automatically build and deploy based
on changes in a git repo.

By the end of the section, you'll also create a Shiny app that
visualizes model predictions and an API that hosts the model and
provides real-time predictions to the app. Additionally, you'll get to
practice putting that API inside a Docker Container to see how using
Docker can make your life easier when moving code around.

For more details on exactly what you'll do in each chapter, see
[Appendix @sec-append-lab].

