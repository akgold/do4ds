# DevOps Lessons for Data Science {#sec-1-intro}

Despite the fact that you're a data scientist, not a software developer,
the problems DevOps addresses are probably familiar to you.

The first issue is the dreaded "works on my machine" phenomenon. If
you've ever collaborated on a data science project, you've almost
certainly reached a point where something worked on your laptop but not
for your colleague, and *you just don't know why.*

At its root, works on my machine occurs when there's a disconnect
between the code you're writing and the environment in which it runs.
The solution from a DevOps perspective is to create explicit linkages
between the code and the environment to maximize the likelihood that it
will match at some point in the future.

Another issue DevOps addresses is the "breaks on deployment" issue.
Perhaps your code was thoroughly tested, but just locally, or perhaps
you don't really have patterns around testing code in your organization.
Even if you tested thoroughly, you might not know if something breaks
when its deployed. DevOps is designed to reduce the risk of deploying
code that won't function as intended *the first time* it's deployed.

Finally, DevOps addresses the "what's happening in there" issue. If
you've ever found out that your app was down or that your model was
producing bad results second-hand, you've experienced this issue. DevOps
is designed to reduce the risk that you'll hear about your code running
amok or being insecure only from someone else.

In this chapter, we're going to talk a little about the history and
insights of DevOps and how they apply to data science at a high level.
It's going to tee up the chapters in the rest of this section, which are
all about how to architect a data science project to be ready to go to
production.

## Core principles and best practices of DevOps

DevOps is designed to incorporate ideas about scaling, security, and
stability into the genesis of a piece of software software, helping
avoid software that works fine locally, but doesn't work well in
production.

As I've mentioned, the term DevOps is squishy. So squishy that there
isn't even agreeement on what the basic tenets of DevOps are. And it's
just made more squishy by *xOps* terms like DataOps, MLOps. Basically
every resource on DevOps lists a different set of core principles and
frameworks.[^1-0-sec-intro-1]

[^1-0-sec-intro-1]: If you want to think more about the philosophy of
    DevOps, I strongly recommend *The Phoenix Project* by Gene Kim,
    Kevin Behr, and George Spafford. It's a book on DevOps formatted as
    a novel, so it's not very technical but is very fun. A good friend
    described it as, "like a trashy romance novel about DevOps".

Here's a list of some important core principles of DevOps. Some lists of
DevOps have more components, and some fewer, but here's my attempt.

1.  **Code** should be **well-tested** and tests should be
    **automated**.
2.  **Code updates** should be **frequent** and **low-risk**.
3.  **Security** concerns should be considered **up front** as part of
    architecture.
4.  Production **systems** should have **monitoring** and **logging**.
5.  Frequent opportunities for reviewing, changing, and **updating
    process** should be **built into the system** -- both culturally and
    technically.

Now, exactly how to pull off these five things is a much harder
question. Before we get into that, let's think about the kind of
software you're writing as a data scientist.

You might be tempted to say, "Well DevOps has figured this all out! I
don't need this book."

And that's fair, you can figure this all our yourself. But in my
experience, the type of software data scientists write is fundamentally
different than general purpose software, and that changes some of the
particularities of DevOps workflows.

::: callout-note
It's also nice that a huge majority of data science work is done in just
two programming languages, R and Python (and SQL). That means I can get
pretty specific about tooling in those languages relative to a general
purpose book on DevOps.
:::

Traditional pieces of software generally are *producers* of data.
Microsoft Word, electronic health records, and Twitter all produce tons
of data. And where they do consume data, it's in a mostly symmetric
manner.

In contrast, data science software almost exclusively consumes data.
You're taking data generated outside -- by a business, social, or
natural process -- deriving some sort of signal from that data flow, and
making it available to other people or other software.

And it turns out that this difference in the direction of data flow has
major implications for how the corresponding software is written.

Software engineers get to dream up data structures and data flows from
scratch, designing them to work optimally for their systems. In
contrast, you are stuck with the way the data flows into your system --
most likely designed by someone who wasn't thinking about the needs of
data science at all.

One other difference is the number of times the software is run. Most
"normal" software is designed to run a lot. And you might have a
scheduled job that runs every day or a report that runs once a month --
but that's still very infrequent relative to lots of regular software.

To get a little clarity about how we might apply DevOps to Data Science,
let's think a little about what exactly the software you're developing
is.

## Data science software types

You are a software developer.

It might not be your title, which is probably something like data
scientist or statistician or data engineer. But if you're writing R or
Python code for production, you're developing software.

In my time working with many different organizations, most data science
software falls into one of four categories.

The first type of software is a *job*. A job is something you care about
because of its side effects. It might move data around in an ETL job,
build a model, or produce plots, graphs, or numbers to be used in a
Microsoft Office report.

Frequently, these steps are done in some sort of SQL-based pipelining
tool. *dbt* is quickly rising in popularity. If you're doing these tasks
in R or Python, they're most commonly done in a `.R` or `.py`
script.[^1-0-sec-intro-2]

[^1-0-sec-intro-2]: Though I'll argue in [Chapter @sec-log-monitor] that
    you should always use a literate programming tool like Quarto, R
    Markdown, or Jupyter Notebook.

The second type of data science software is a *report*. I use report as
a general category to refer to anything where you are rendering a
document and care about the output. The actual form of the output might
be a document, a book, a presentation, or a website.

In these cases, you're taking an R Markdown doc, Quarto doc, or Jupyter
Notebook and rendering it into a final documents for people to consume
-- either on the web, as a live presentation, or by emailing them. I
generally Depending on what you're doing, these docs may be completely
static (this book is a Quarto doc) or they may have some interactive
elements.

The third type of data science software is an interactive web *app*.
These apps, created in frameworks like Shiny (R or Python), Dash
(Python), or Streamlit (Python) are for people to click around. Most
often they are used to explore data sets and to provide non-data
scientists an interface to data insights.

Exactly how much interactivity turns a report into an app is completely
subjective, especially since you can use a report framework like Quarto
to lay out the skeleton for a Shiny app.

The fourth type of data science software is an API (application
programming interface). An API is for machine-to-machine communication.
It facilitates cross-language communication as well as making it easy to
separate different kinds of logic in a data science project.

## What's in this section

This section consists of five chapters that are my best effort to apply
the lessons of DevOps to the practice of doing data science.

Each chapter addresses one important aspect of making your code more
robust and resilient. Additionally, there's a lab in each chapter so you
can get some hands-on experience implementing those practices.

If you follow along with the labs, you'll have a website with automated
deployment from a git repository by the end of the chapter.

You'll also build an API that serves a machine learning model in R or
Python and Shiny app in R or Python using that API.

For more details on what you'll do in each chapter, see [Appendix
@sec-append-lab].
