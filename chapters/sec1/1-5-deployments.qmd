# Deployments and code promotion {#sec-deployments}

Data science work doesn't count if it just sits around on your computer.
Your goal is to be useful, and the only way your work becomes useful is
if you share it with the people and systems that matter. That requires
putting it into production.

Putting code into production might sound like an ordeal, and it can be.
But it would really be nice if putting code into production was a
low-drama affair. That's why *deployment* is one of the primary concerns
of DevOps best practices.

The DevOps way to deploy code is called *CI/CD*, which is short for
Continuous Integration and Continuous Deployment and Continuous Delivery
(yes, the CD stands for two different things). CI/CD is a set of
principles to ease the deployment process through a combination of good
workflows and automation.

A few of the principles of CI/CD workflows include:

-   Central availability of source code that allows you to build the
    project from scratch -- almost always in version control.

-   Frequent and incremental additions to the production version of the
    code.

-   Automation for carrying out the actual deployment.

-   Automated testing on a pre-deployment version of the code.

In this chapter, I'll talk about how to create a code promotion process
that allows you to incorporate CI/CD principles into your data science
deployment projects.

## Separate the prod environment

The bedrock of a good CI/CD process is a production environment that's
actually separate from non-production environments.

CI/CD is all about easily promoting code into production, but if the
boundaries of production are a mushy mess, it's all too easily to
accidentally mess up code that's in production.

In order to structure a smooth pathway to production, software
environments are separated into three -- *dev*, *test*, and *prod*. Dev
is the development environment where new work is produced, test is where
the code is tested for performance, usability, and feature completeness,
and prod is the production environment. Depending on your organization
you might have just a dev and prod or you might have more environments
between dev and prod.

![](images/dev-test-prod.png){fig-alt="An app moving through dev, test, and prod environments."}

The number and configuration of lower environments will vary according
to your organization and its needs. But like Tolstoy said about happy
families, all prod environments are alike.

Some criteria that all good prod environments meet:

1.  The environment is created using code. For data science, that
    managing R and Python packages using environments as code tooling,
    as discussed in [Chapter @sec-env-as-code].

2.  Changes happen via a promotion process. The process combines human
    approvals that the code is ready for production and automations to
    run tests and do the actual deployment.

3.  Changes **only** happen via the promotion process. This means no
    manual changes to the environment or the code.

Rules 1 and 2 are pretty straightforward to follow. But the first time
something breaks in your prod environment, you will be sorely tempted to
violate rule 3. Don't do it.

If you want to run a data science project that becomes critical to your
organization, keeping a pristine prod environment that you can rely on
is critical. Re-create the issue in a lower environment to figure out
what's wrong and push changes through your promotion process.

### Dev and test environments

These guidelines for a prod environment look almost identical to
guidelines for general purpose software engineering. It's in the
composition of lower environments that the needs of data scientists
diverge from general purpose software engineers.

As a data scientist, dev means working in a lab environment like
RStudio, Spyder, VSCode, or PyCharm and experimenting with the data.
You're slicing the data this way or that to see if anything meaningful
emerges, creating plots to see if they are the right way to show off a
finding, and checking whether certain features improve model
performance. All this means that it's basically impossible to do work
without real data.

"Duh", you say, "Of course it's silly to do data science on fake data."

This may be obvious to you, but doing dev data science on real data is a
very common source of friction with IT/Admins.

That's because this need is unique to data scientists. For general
purpose software engineering, a lower environment needs data that is
*formatted* like the real data, but the actual content doesn't matter.

For example, if you're building an online store, you need dev and test
environments where the API calls from the sales system are in the same
format as the real data -- but you don't actually care if it's real
data. In fact, you probably want to create some odd-looking cases for
testing purposes.

One way to help allay these concerns is to create a *data science
sandbox*. A great data science sandbox provides:

-   Read-only access to real data for experimentation.

-   Places to write mock data to test out things you'll write for real
    in prod.

-   Broad access to R and Python packages to experiment with before
    things go to prod.

Working with your IT/Admin team to get these things isn't always easy.
One thing to point out is that creating this environment actually makes
things *more secure*. It gives you a place to do development without any
fear that you might actually damage production data or services.

## Version control implements code promotion

You need a way to actually operationalize your code promotion process.
If your process says that your code needs testing and review before it's
pushed to prod, you need a place to actually do that. *Version control*
is the tool to make your code promotion process real.

Version control is software that allows you to keep the prod version of
your code safe, gives contributors their own copy to work on, and hosts
tools to manage merging changes back together. These days,
[git](https://git-scm.com/) is the industry standard for version
control.

Git is a system for tracking changes in computer files in a project
called a *repository*. Git is open source and freely available. There
are a number of different companies that host git repositories. Many of
them allow you to host public -- and some private - repositories for
free and have enterprise products that your organization may pay for.
GitHub is by far the most popular git host, but GitLab, Bitbucket, and
Azure DevOps are also common.

This is not a book on git. If you're not already comfortable with using
local and remote repositories, branching, and merging, the rest of this
chapter is going to be completely useless. I recommend you take a break
from this book and spend some time learning git.

::: callout-note
## Hints on Learning Git

People who say git is easy to learn are either lying or have forgotten.
I am sorry our industry has standardized on a tool with such terrible
ergonomics, but it's really worth it to learn.

Whether you're an R or Python user, I'd recommend starting with a
resource designed to teach git to a data science user. My recommendation
is to check out [HappyGitWithR](https://happygitwithr.com/) by Jenny
Bryan.

If you're a Python user, some of the specific tooling suggestions won't
apply, but the general principles will be exactly the same.
:::

If you understand git and just need a reminder of some common commands,
there is a [cheatsheet](#cheat-git) at the end of the chapter.

The precise contours of your code promotion process -- and therefore
your git policies -- are up to you and your organization's needs. Do you
need multiple rounds of review? Can anyone promote something to prod, or
just certain people? Is automated testing required?

You should make these decisions as part of designing your code promotion
process, which you can then enshrine in the configuration of your
project's git repository.

One important decision you'll make is on how to configure the branches
of your git repository. Here's how I'd suggest you do it for production
data science projects:

1.  Maintain two long running branches -- `main` is the prod version of
    your project, and `test` is a long-running pre-prod version.
2.  Code can only be promoted to `main` via a merge from `test`. Direct
    pushes to `main` are not allowed.
3.  New functionality is developed in short-lived *feature branches*
    that are merged into `test` when you think they're ready to go. Once
    sufficient approvals are granted, the feature branch changes in
    `test` are merged into `main`.

This framework helps maintain a reliable prod version on the `main`
branch, while also leaving sufficient flexibility to accomplish
basically any set of approvals and testing you might want.

Here's an example of how this might work. Let's say you were working on
a dashboard and were trying to add a new plot.

You would create a new feature branch, perhaps called `new_plot` to work
on the plot. When you were happy with how it looked you would merge the
feature branch to `test`. Depending on your organization's process, you
might be able to merge to `test` yourself or you might require approval.

If your testing turned up a bug, you'd fix the bug in the feature
branch, merge the bug fix into `test`, re-test, and merge to `main` once
you were satisfied.

Here's what the git graph for that sequence of events might look like:

![](images/git-branches.png){fig-alt="Diagram showing branching strategy. A feature branch called new plot is created from and then merged back to test. A bug is revealed, so another commit fixing the bug is merged into test and then into main."}

One of the tenets of a good CI/CD practice is that changes are merged
frequently and incrementally into production.

A good rule of thumb is that you want your merges to be the smallest
meaningful change that can be incorporated into main in a standalone
way.

Creating feature branches for every word of text you might change is
clearly too small. Completely rewriting the dashboard in one merge
request is also probably too big.

There's no hard and fast rules here. Knowing the appropriate scope for a
single merge is an art -- one that can take years to develop. Your best
resource here is more senior team members who've already figured it out.

## CI/CD automates git operations

The role of git is to make your code promotion process happen. Git
allows you to configure requirements for whatever approvals and testing
you might need. Your CI/CD tool sits on top of that so that all this
merging and branching actually **does** something.[^1-5-deployments-1]

[^1-5-deployments-1]: Strictly speaking, this is not true. There are a
    lot of different ways to kick off CI/CD jobs. But the right way to
    do it is to base it on git operations.

To be more precise a *CI/CD pipeline* for a project watches the git
repository and does something when certain triggers are met, like a
merge to a particular branch or a pull request opening.

The most common CI/CD operations are *pre-merge checks* like spell
checking, code linting, and automated testing and *post-merge
deployments*.

There are a variety of different CI/CD tools available. Because of the
tight linkage between GitHub repos and CI/CD, CI/CD pipelines built
right into git providers are very popular.

GitHub Actions (GHA) was released a few years ago and is eating the
world of CI/CD. Depending on your organization and the age of your CI/CD
pipeline, you might also see Jenkins, Travis, Azure DevOps, or GitLab.

If you're curious how exactly this works, you'll get your hands dirty in
the lab at the end of the chapter.

### Configuring per-environment behavior

As you promote an app from dev to test and prod, you probably want
behavior to look different across the environments. For example, you
might want to switch data sources from a dev database to a prod one, or
switch a read-only app into write mode, or use a different level of
logging.

The easiest way to create per-environment behavior is to write code that
behaves differently based on on the value of an environment variable and
to set that environment variable in each environment.

My recommendation is to use a *config file* to store the values you want
for your environment variables for each environment. My preference is to
use YAML to store configuration, but there are different ways it can be
done.

::: callout-note
Only non-secret configuration settings should go in a config file.
Secrets should always be configured using secrets management settings in
the tooling you're using so they don't appear in plain text.
:::

For example, you could write a project that knows whether to write or
not based on the value of the config's `write` and which database using
the config's `db-path`. Then you could use the YAML below to specify
which environments write and which ones use which database:

```{yaml filename="config.yml"}
dev:
  write: false
  db-path: dev-db
test
  write: true
prod:
  write: true
  db-path: prod-db
```

You would set a relevant environment variable so your code pulls the
`dev` configuration in dev, `test` in test, and `prod` in prod.

In Python there are many different ways to set and read in your a
per-environment configuration. If you want to use YAML like in the
example above, you could save it as `config.yml` and use the `{yaml}`
package to read it in as a dictionary, and choose which part of the
dictionary to at the start of your script.

In R, the `{config}` package is the standard way to load an
environmental configuration from a YAML file. The `config::get()`
function uses the value of the `R_CONFIG_ACTIVE` environment variable to
choose which configuration to use. That means that switching from the
dev to the prod version of the app is as easy as making sure you've got
the correct environment variable set on your system.

## Comprehension Questions

1.  Write down a mental map of the relationship between the three
    environments for data science?
2.  Why is git so important to a good code promotion strategy? Can you
    have a code promotion strategy without git?
3.  What is the relationship between git and CI/CD? What's the benefit
    of using git and CI/CD together?
4.  Write out a mental map of the relationship of the following terms:
    git, GitHub, CI/CD, GitHub Actions, Version Control

## Lab 5: Host a website with automatic updates {#lab5}

In labs 1 through 4, you've created a Quarto website for the penguin
model. You've got sections on EDA and model building. But it's still
just on your computer.

In this lab, we're going to actually deploy that website to a public
site on GitHub and and set up GitHub Actions as CI/CD so the EDA and
modeling steps re-render every time we make changes.

Before we get into the meat of the lab, there are a few things you have
to do on your own. If you don't know how to do these things, there are
plenty of great tutorials online.

1.  Create an empty public git repo on GitHub.
2.  Configure the repo as the remote for your Quarto project directory.

Once you've got the GitHub repo connected to your project, you need to
set up the Quarto project to publish via GitHub Actions. There are great
directions on how to get that configured on the [Quarto
website](https://quarto.org/docs/publishing/github-pages.html#github-action).

Following those instructions will accomplish three things for you:

1.  Generate a `_publish.yml`, which is a Quarto-specific file for
    configuring publishing locations.
2.  Configure GitHub Pages to serve your website off a long-running
    standalone branch called `gh-pages`.
3.  Generate a GitHub Actions workflow file, which will live at
    `.github/workflows/publish.yml`.

Here's the basic GitHub Actions file (or close to it) that the process
will auto-generate for you.

``` {.yaml include="../../_labs/gha/publish-basic.yml" filename=".github/workflows/publish.yml"}
```

One of the reasons GitHub Actions has gotten so popular is that the
actions defined in a very human-readable YAML file and it's very likely
you can read and understand this without much editorializing. But let's
still go through it in some detail.

This particular syntax is unique to GitHub Actions, but the idea is
universal to all CI/CD systems -- you define a trigger and a job to do
when it's triggered.

In GitHub Actions, the `on` section defines **when** the workflow
occurs. In this case, we've configured the workflow only to trigger on a
push to the `main` branch.[^1-5-deployments-2] Another common case would
be to trigger on a pull request to `main` or another branch.

[^1-5-deployments-2]: A completed merge counts as a push.

The `jobs` section defines **what** happens.

When your action starts up, it's in a completely standalone environment.
This is actually a great thing -- if you can easily specify how to start
from zero and get your code running in GitHub actions, you can bet it'll
do the same in prod.

The `runs-on` field specifies exactly where we start, which in this case
is the latest version of the Ubuntu and not much else.

Once that environment is up, each step in `jobs` runs sequentially.

The most common way to define a step is with `uses`, which calls a
preexisting GitHub Actions step that someone else has written. In some
cases, you'll want to specify variable values using `with` or
environment variables with `env`.

Take a close look at how this action uses the `GITHUB_TOKEN`. That's an
environment secret that's auto-provisioned for an action. By using it as
a variable here, it's easy to see what happens, but the value is still
totally secret.

Now, if you try to run this, it probably won't work.

That's because the CI/CD process occurs in a completely isolated
environment. This auto-generated action doesn't including setting up
versions of R and Python or the packages to run our EDA and modeling
scripts. We have to get that configured before this action will work.

::: callout-note
If you read the Quarto documentation, they recommend freezing your
computations. Freezing is very useful if you want to render your R or
Python code only once and just update the text of your document. You
wouldn't need to set up R or Python in CI/CD and the document would
render faster.

That said, freezing isn't an option if you intend the R or Python code
to re-run because it's a job you care about.

Because the main point here is to learn about getting environments as
code working in CI/CD you **should not** freeze your environment.
:::

First, add the commands to install R, `{renv}`, and the packages for
your content to the GitHub Actions workflow.

``` {.yml filename=".github/workflows/publish.yml" include="../../_labs/gha/publish-r-py.yml" start-line="20" end-line="31"}
```

::: callout-note
If you're having slow package installs in CI/CD for R, I'd strongly
recommend using a repos override like in the example above.

The issue is that CRAN doesn't serve binary packages for Linux, which
means really slow installs. You've got to direct `{renv}` to install
from Public Posit Package Manager, which does have Linux binaries.
:::

You'll also need to add a workflow to GitHub Actions to install Python
and the necessary Python packages from the `requirements.txt`.

``` {.yml filename=".github/workflows/publish.yml" include="../../_labs/gha/publish-r-py.yml" start-line="33" end-line="39"}
```

Note that in this case, we run the Python environment restore commands
with `run` rather than `uses`. Where `uses` takes an existing GitHub
Action and runs it, `run` just runs the shell command natively.

Once you've made those changes, try pushing or merging your project to
`main`. If you click on the `Actions` tab on GitHub you'll be able to
see the Action running.

In all honesty, it will probably fail the first time or five. You will
almost never get your Actions correct on the first try. Just breathe
deeply and know we've all been there. You'll figure it out.

Once it finishes, you should be able to see your change reflected on
your website.

Once it's up, your website will be available at
`https://<username>.github.io/<repo-name>`.

## Cheatsheet: Git {#cheat-git}

| Command                         | What it Does                                           |
|---------------------------------|--------------------------------------------------------|
| `git clone <remote>`            | Clone a remote repo -- make sure you're using SSH URL. |
| `git add <files/dir>`           | Add files/dir to staging area.                         |
| `git commit -m <message>`       | Commit your staging area.                              |
| `git push origin <branch>`      | Push to a remote.                                      |
| `git pull origin <branch>`      | Pull from a remote.                                    |
| `git checkout <branch name>`    | Checkout a branch.                                     |
| `git checkout -b <branch name>` | Create and checkout a branch.                          |
| `git branch -d <branch name>`   | Delete a branch.                                       |
