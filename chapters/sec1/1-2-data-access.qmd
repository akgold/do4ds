# Accessing data sources {#sec-data-access}

For most data science projects, you're going to have preexisting data
sources you have to work with. This chapter is all about how to access
and work with those data sources.

There are really just two key questions you need to know about those
data sources that dictate your design pattern:

1.  What type are they?
2.  Is there auth needed? If so, how does it work?

In this chapter, we'll get into how to determine the answers to these
three questions for any data source you've got and how you'll need to
think about architecting your project as a result.

::: callout-note
One thing you'll note *isn't* here is data size. In general, the size of
your input data is irrelevant. What matters is how much data actually
makes its way into your project, which we'll get into in [Chapter
@sec-proj-arch].
:::

## Data source type dictates connection method

Data sources come in three basic types:

-   **Flat files** are files that live on disk that you directly load
    into your R or Python session.

-   **Databases** are external entities that are query-able from your R
    and Python code that directly return data.

-   **Data APIs** are external entities query-able from your R and
    Python code that return data in the body of an HTTP return.

The main implications of the data source type is how you connect to it.

### Connecting to flat files

For flat files, you'll just read them into memory in R or Python using
standard file read methods.

You may need to mount drives in.

If you have flat files that are too big to read into memory all at once,
you'll have to process them first - more on that in [Chapter
@sec-proj-arch].

### Connecting to Databases

In both R and Python, there are two general ways to connect to a
database.

The first option is to use a direct connector to connect to the
database. These connectors are most often thin R or Python wrappers
around a bunch of C++ code. If this exists for the database you're
using, it's generally the most optimized for your database and your
should probably use it.

If your database is a Spark cluster, you may not be using it to its
utmost if you just treat it as a database. You can access more advanced
computational methods with `{pyspark}` in Python and `{sparklyr` in R.

::: callout-tip
If you're just getting started with Spark, I strongly recommend the book
[Mastering Spark with R](https://therinspark.com) (free online version,
like this book!) for getting started. Even if you're a Python user, the
conceptual introduction is fantastic.
:::

The second option is to connect to the database using a system-level
driver and then connect to that database from R or Python. Many
organizations like these because IT/Admins can configure them on behalf
of users and can be agnostic about whether users are using them from R,
Python, or something else entirely.

\<TODO: image of direct connection vs through driver\>

In this case, you'll have to get the driver from your database provider,
configure it on the system, and connect to it from R or Python. There
are two main standards for system drivers -- Java Database Connectivity
(JDBC) and Open Database Connectivity (ODBC).

In general, I recommend using ODBC over JDBC if available. If you're
using JDBC, you have to deal with configuring Java in addition to your
driver. Anyone who's ever had to configure the `rJava` package will
share horror stories. Enough said.

In R I strongly recommend using an ODBC driver and using the `odbc` and
`DBI` packages to make this connection. There are other ODBC packages
and there are JDBC drivers available. But `odbc` and `DBI` are very
actively maintained by Posit.

In Python, `pyodbc` is the main package for using ODBC connections.

### Connecting to APIs

If you're using R, you'll use the `httr2` package to query an API. In
Python, you'll use `requests`. Both of these packages have great
documentation and you can look at the specifics of *how* to use them in
those packages.

But both of those packages are just wrappers to help you write idiomatic
R or Python that gets passed along to the system `curl` command to
actually get information from an API via an `http` call.

In this section, we're going to focus on what happens at that level.
What is an `http` call `curl` might make and how to understand what it
requests and what comes back.

An API call asks for an endpoint to do something.

Your computer communicates with an API via a request-response model.
Your computer requests a resource and the API sends it back. Your
computer actually does this constantly as your navigate across the web.

What you experience as visiting a website, your computer views as a
handful of `http` requests to a server to fetch whatever is at that URL.
The site owner's server responds with the various assets that make up
the web page, which might include the HTML skeleton for the site, the
CSS styling, interactive javascript elements, and more.

Once your computer receives them, your browser reassembles them into a
webpage for you to interact with. And when you click on a button in the
site, one or more `http` requests go off and the responses dictate what
happens next.

So when you manually query an API via `httr2` or `requests`, you're just
manually making the kind of requests your computer is already making
constantly.

You may have heard the term REST API or REST-ful.

REST is a set of architectural standards for how to build an API. An API
that conforms to those standards is called REST-ful or a REST API.

If you're using standard methods for constructing an API like R's
`{plumber}` package or `FastAPI` in Python, they're going to be REST-ful
-- or at least close enough for standard usage.

In this section, I'm using the term API and REST API interchangeably.

The best way to understand `http` traffic is to take a close look at
some. Luckily, you've got an easy tool -- your web browser!

Open a new tab in your browser and open your developer tools. How this
works will depend on your browser. In Chrome, you'll go to View \>
Developer \> Developer Tools and then make sure the Network tab is open.

Now, navigate to a URL in your browser (say google.com).

As you do this, you'll see the traffic pane fill up. These are the
individual requests and responses going back and forth between your
computer and the server.

There are a few parts of the requests and responses that are worth
understanding in some depth. The first is the *status code*. Status
codes appear only on responses and indicate what happened with your
request to the server. In your browser, you're probably seeing mostly
200 codes, which indicates a successful response. There's a cheatsheet
below of some other codes and what they mean.

If you click on an individual line in the traffic pane, you can see some
additional details. One key component is the *request method*. The
`http` protocol specifies a number of verbs or request methods you're
allowed to use.

When you're just loading a page on the web, it'll be almost entirely
`GET` requests, which fetch something. The other three basic `http`
methods are `POST` or `PUT` to change or update something or a `DELETE`
to (you guessed it) delete something. There are also a number of more
esoteric `http` methods, but I've never seen a need for one.

You can see the response codes right next to each request, in chrome,
codes other than 200s show up in other colors to make them easy to find.

The other most useful part of the display is the waterfall chart on the
far right. This shows how long each request took to come back. If you're
finding that something is slow to load, inspecting the waterfall chart
can help diagnose which requests are taking a long time.

If you click into an individual request, you can see a variety of
information, including the headers and the response. Very often,
inspecting the headers is a great way to debug malfunctioning `http`
requests.

Another important component of both requests and responses are the
*headers.*

Headers specify metadata information about the request or response.
These often include the type of machine that is sending the request,
authentication credentials or tokens, cookies, and more. Making sure
you've got the correct headers on requests and responses is an important
thing to check if you're running into trouble with API calls --
especially with authentication.

Lastly, requests and responses often include a *body*.

Bodies are *allowed* for `GET` and `DELETE` requests, but they generally
are not used. Instead, the request endpoint should fully specify the
resource for a `DELETE`, and details for `GET` requests are specified
via *query parameters*, the part of the URL that shows up after the `?`,
like `?first_name=alex&last_name=gold`.

In a request, the body is going to provide details on what you're trying
to do with your request. In a response, the body is the information that
is being sent back. JSON is the most common body response types for an
API you might build, but a website is likely to have other types of
bodies, like images.

#### Special HTTP Codes

As you work more with `http` traffic, you'll learn some of the common
codes. Here's a cheatshet for some of the most frequent you'll see.

| Code            | Meaning                                                                                                                                                    |
|-----------------|------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `200`           | Everyone's favorite, a successful response.                                                                                                                |
| `3xx`           | Your query was redirected somewhere else, usually ok.                                                                                                      |
| `4xx`           | Errors with the request                                                                                                                                    |
| `400`           | Bad request. This isn't a request the server can understand.                                                                                               |
| `401` and `403` | Unauthorized or forbidden. Required authentication hasn't been provided.                                                                                   |
| `404`           | Not found. There isn't any content to access here.                                                                                                         |
| `5xx`           | Errors with the server once your request got there.                                                                                                        |
| `500`           | Generic server-side error. Your request was received, but there was an error processing it.                                                                |
| `504`           | Gateway timeout. This means that a proxy or gateway between you and the server you're trying to access timed out before it got a response from the server. |

## Securing data connections

This is a question you probably don't think about much as you're
puttering around inside RStudio or in a Jupyter Notebook. But when you
take an app to production, this becomes a crucial question. In
particular, securely managing credentials becomes a big time concern, as
does how you actually provide your data source with the credentials it
needs to let you in.

The exact pattern for how you provide authentication to the service will
depend on whether it's a database or an API and also how exactly that
service works.

Most data sources are authenticated in one of the following ways:

1.  Providing a credential in your code
2.  Using a provisioned credential from the system
3.  Passing a token

The single most important thing you can do to secure your credentials
for your outside services is to avoid ever putting credentials in your
code. That means that the actual value of your username or password or
API key should never actually appear in your code.

If you are using ODBC, you or your admin may want to consider
configuring a Data Source Name (DSN). A DSN provides connection details
for an ODBC connection. They can be configured at a system or a user
level. This means that you could put the connection details like the
hostname, port, database name, etc into a simple text file right on your
system and users don't need to know anything beyond their username and
password to login.

For an API, authentication is generally provided as a header. Sometimes
you'll provide a token of some sort in the API header.

For simple API key or username and password auth, there's a format that
you provide those credentials in specified by the API provider. When you
provide those credentials in your code, you should never expose the
actual value of the secret. Instead, you should always use an
environemnt variable.

Both of the above options assume that you actually do have a credential
that you can use. Some organizations are beginning to adopt systems
where you never actually see a credential. Instead you are given a token
at some point -- or the server you're working on is -- and these tokens
are exchanged one-for-another. This is something you'll need help from
an IT/Admin to accomplish and there's more on how to discuss the issue
in [Chapter @sec-auth].

### Managing secrets with environment variables

The first, and simplest way to do this is to set them into an
environment variable. An environment variable is a value that is set
before your code starts to change how your code behaves.

::: callout-note
It is convention to make environment variable names in all caps with
words separated by underscores. The values are always simple character
values, though these can be cast to some other type inside R or Python.
:::

They can be used for a variety of different purposes, but one of the
most common is managing secrets like usernames, passwords, and API keys.

The power of using an environment variable is that you reference them
*by name*. Referencing secrets by name makes it safe to share your code
freely and make secrets management a separate concern.

In Python, you can read environment variables from the `os.environ`
dictionary or by using `os.getenv("<VAR_NAME>")`. In R, you can get and
set environment variables with `Sys.getenv("<VAR_NAME>")`.

Environment variables are set at the outset of your code running to
influence it throughout its runtime. There are a few different ways to
set environment variables.

The most common way is to load secrets from a plain text file. In
Python, environment variables are usually set by reading a `.env` file
into your Python session. The [`{python-dotenv}`
package](https://pypi.org/project/python-dotenv/) is a good choice for
doing this.

R automatically reads the `.Renviron` file as environment variables and
also sources the `.Rprofile` file, where you can set environment
variables with `Sys.setenv()`. I personally prefer putting everything in
`.Rprofile` for simplicity -- but that's not a universal opinion.

For some organizations, having the credentials available in plaintext at
all is to be avoided. An `.Rprofile` file is just a simple text file. If
an unauthorized user were to get access to my laptop, they could steal
the credentials from my `.Rprofile` and use them themselves.

Some organizations have prohibitions against ever storing credentials in
plaintext. In these cases, the credentials must be stored in a
cryptographically secure way and are only decrypted when they're
actually used.

::: callout-note
## Encryption at rest

Information that is encrypted at rest is cryptographically secured
against unauthorized access. There are at least two different things
people can mean. Whole-disk encryption means that your drive is
encrypted so that it can only be accessed by your machine. That means
that if someone were to take your hard drive to another computer, your
data wouldn't be usable. This has become more-or-less standard.
:::

If this is the case for you, you have a few options.

There are packages in both R and Python called `keyring` that allow you
to use the system keyring to securely store environment variables and
recall them at runtime. These can be good in a development environment,
but run into trouble in a production environment because they generally
rely on a user actually inputting a password for the system keyring.

Your environment may also provide environment management tools. For
example, GitHub Actions and Posit Connect both provide you the ability
to set secrets that aren't visible to the users, but are accessible to
the code at runtime in an environment variable.

## Comprehension Questions

1.  What are the different options for data storage formats for apps?
    What are the advantages and disadvantages of each?
2.  When should an app fetch all of the data up front? When is it better
    for the app to do live queries?
3.  What are the different flat file types and how can they be stored?
4.  What is a good way to create a database query that performs badly?
    (Hint: what's the slowest part of the query)

## Lab 3: Work with the data in a database

The process here is quite straight forward. We're going to load the data
into a database and then alter our existing code to use the data in the
database.

For now, we're going to use [DuckDB](https://duckdb.org). DuckDB is an
in-memory database that's great for analytics use cases. Because it's
in-memory, we don't have to set up an external database. In chapter
\[TODO\], you can set up an AWS database if you'd like practice doing
that.

To start, let's load the data.

Here's what that looks like in R:

``` {.r include="../../_labs/data-load/r-data-load.R"}
```

Or equivalently, in Python:

``` {.python include="../../_labs/data-load/py-data-load.py"}
```

Now that the data is loaded, let's adjust our scripts to use the
database.

In R, we are just going to replace our data loading with connecting to
the database. Leaving out all the parts that don't change, it looks like

``` {.r include="../../_labs/eda/eda-db.qmd" filename="eda.qmd" start-line="14" end-line="19"}
```

There's also a call to `DBI::dbDisconnect(con)` at the end of the
script. The code for the data processing part doesn't change at all, but
the operation changes a lot.

``` {.r include="../../_labs/eda/eda-db.qmd" filename="eda.qmd" start-line="23" end-line="32"}
```

I've added a call to `dplyr::collect` in line 31. That's not actually
necessary -- it will be implied if I don't put it there manually, but it
helps make obvious that everything up to that point happens in the
database, not by loading the data into R. Obviously for this small
dataset it doesn't matter, but if the dataset were much larger, this
would be a huge benefit.

In Python, we're just going to load the entire dataset into memory for
modeling, so the line loading the dataset changes to

``` {.python include="../../_labs/model/model-db.qmd" filename="model.qmd" start-line="18" end-line="21"}
```

There are no other changes on the Python side.

In order to get this to work in CI/CD, there don't need to be any
changes to the GitHub workflows, but you will need to make sure the
DuckDB packages and others get added. So before you push the code to
GitHub, make sure to run `renv::snapshot()` and
`pip freeze > requirements.txt` to make sure all the requirements are
present.

Because you're working with an on-disk database, you'll also need to
push the `my-db.duckdb` file. At some point, we probably want to pull
the data out of the bundle and locate it somewhere else, either on an
actual database or by storing the DuckDB file in a centralized location.

More on that in the lab for [Chapter @sec-cloud].

We're also going to write our model somewhere for use later in the lab.
We're going to make use of the `{vetiver}` package, which is an R and
Python language package that makes it easy to version, deploy, and
monitor a machine learning model.

We can take our existing model and turn it into a `{vetiver}` model with

``` {.python include="../../_labs/model/model-vetiver.qmd" filename="model.qmd" start-line="47" end-line="49"}
```

Once we've done that, we can specify where the model will be saved. For
now, we're going to just store it at a location on the filesystem. I'm
using `/data/model`. If that directory doesn't exist on your machine,
you can create it, or use a directory that does exist.

Whatever path you use, I'd recommend using an absolute file path, rather
than a relative one.

The code to make that happen looks like this:

``` {.python include="../../_labs/model/model-vetiver.qmd" filename="model.qmd" start-line="55" end-line="59"}
```

Depending on the tooling you're using, you may also have access to
integrated secrets management. The exact configuration will depend on
that environment, but you should use those tools when they're available.

In R, it's also very common for organizations to create internal
packages that provide connections to databases. The right way to do this
is to create a function that returns a database connection object that
you can use in conjunction with the `DBI` or `dplyr` packages.

Here's an example of what that might look like if you were using a
Postgres database:

```{r}
#| eval: false

#' Return a database connection
#'
#' @param user username, character, defaults to value of DB_USER
#' @param pw password, character, defaults to value of DB_PW
#' @param ... other arguments passed to 
#' @param driver driver, defaults to RPostgres::Postgres
#'
#' @return DBI connection
#' @export
#'
#' @examples
#' my_db_con()
my_db_con <- function(
    user = Sys.getenv("DB_USER"), 
    pw = Sys.getenv("DB_PW"), 
    ..., 
    driver = RPostgres::Postgres()
) {
  DBI::dbConnect(
    driver,
    dbname = 'my-db-name', 
    host = 'my-db.example.com', 
    port = 5432, # default for Postgres
    user = user,
    password = pw)
}
```
