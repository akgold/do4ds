# Accessing data sources {#sec-data-access}

For most data science projects, you're going to have preexisting data
sources you have to work with. This chapter is all about how to access
and work with those data sources.

There are really just two key questions you need to know about those
data sources that dictate your design pattern:

1.  What type are they?
2.  Is there auth needed? If so, how does it work?

In this chapter, we'll get into how to determine the answers to these
three questions for any data source you've got and how you'll need to
think about architecting your project as a result.

::: callout-note
One thing you'll note *isn't* here is data size. In general, the size of
your input data is irrelevant. What matters is how much data actually
makes its way into your project, which we'll get into in [Chapter
@sec-proj-arch].
:::

## Data source type dictates connection method

Data sources come in three basic types:

-   **Flat files** are files that live on disk that you directly load
    into your R or Python session.

-   **Databases** are external entities that are query-able from your R
    and Python code that directly return data.

-   **Data APIs** are external entities query-able from your R and
    Python code that return data in the body of an HTTP return.

The data source type mainly dictates how you connect to it from your R
or Python session.

### Connecting to flat files

For flat files, you'll just read them into memory in R or Python using
standard file read methods.

In some cases, you'll need to connect to files that are on some sort of
share drive. That means that the share drive will need to be accessible
from the place where you're working. This process is called *mounting* a
drive and you'll need to coordinate with the IT/Admins at your
organization to get it done.

If you have flat files that are too big to read into memory all at once,
you'll have to process them first - more on that in [Chapter
@sec-proj-arch].

### Connecting to and using Databases

In both R and Python you connect to a database by creating a database
connection object in your code and then using that object as part of the
operations you're doing on the database like reading and writing.

Whenever you connect to a database, you need a *driver* specific to that
database to establish connections and send data back and forth. In the
best case, there is a driver package that is specific to your database.
For example, when you're connecting to a Postgres database, there are
Postgres-specific connectors -- `{psychopg2}` in Python and
`{RPostgreSQL}` in R or `{pyspark}` and `{sparklyr}` for a Spark
cluster.

These connectors are most often thin R or Python wrappers around a bunch
of C++ code. If this exists for the database you're using, it's
generally the most optimized for your database and your should probably
use it.

If there isn't a driver specific to the database in your language of
choice, you can use a system-level database driver and then call the
driver from R or Python.

Many organizations like this pattern because IT/Admins can configure the
drivers and be agnostic about whether users are using them from R,
Python, or something else entirely.

There are two main standards for system drivers -- Java Database
Connectivity (JDBC) and Open Database Connectivity (ODBC).

::: callout-tip
If you're using R and have the choice, I strongly recommend using an
ODBC driver. If you're using a JDBC driver, you've got an additional hop
through Java and the `{rJava}` package, which is a huge pain to
configure.[^1-2-data-access-1]
:::

[^1-2-data-access-1]: I have heard that some write operations may be
    faster with a JDBC driver than an ODBC one. I would argue that if
    you're doing enough writing to a database that speed matters, you
    probably should be using database-specific data loading tools, not
    just writing from R or Python.

If you are using a system driver, you may want to consider configuring a
Data Source Name (DSN). A DSN provides connection details for the driver
connection. They can be configured at a system or a user level. This
means that you could put the connection details like the hostname, port,
and database name into a simple text file right on your system and users
don't need to know anything beyond their username and password to login.
If you're sharing database credentials among many users, you can also
include the credentials in the DSN and then users really only need to
know the name of the DSN.

Once the driver is configured on the system outside Python and/or R, you
have to connect. In Python, `{pyodbc}` is the main package for using
ODBC connections and `{JayDeBeApi}` for connecting using JDBC. In R,
`{odbc}` is the best package for using system ODBC connections and
`{RJDBC}` is the standard way to use JDBC.

Both Python and R have standardized database connection APIs that
facilitate operations like connecting and disconnecting, sending
queries, and getting back results from a query.

In Python, the connection API is in a [database API
specification](https://peps.python.org/pep-0249/) and is implemented in
each database's packages. In R, it is implemented in the `{DBI}`
package. Packages that interface with `{DBI}` are called
*DBI-compliant*. Not every database package uses the Python database API
specification or is DBI-compliant, but you should prefer packages that
are.

That means that creating a connection looks somewhat different in R and
Python.

In Python, the connector package returns a connection you use directly.
For example, the `{psychopg2}` package has a `connect` method you call
directly to get the database connection.

```{python}
con = psycopg2.connect()
```

In R, it is `{DBI}` that returns the connection and you supply the
connection type and details to it.

```{r}
con <- DBI::dbConnect(RPostgres::postgres())
```

From that point on, you can send literal SQL queries to the database
using the connection -- directly in Python or as an argument to
functions in the `{DBI}` package in R, or from packages that generate
SQL from R or Python code like `{sqlalchemy}` in Python and `{dplyr}` in
R.[^1-2-data-access-2]

[^1-2-data-access-2]: Again, the mental model here holds, but the code
    pattern may be somewhat different. `{dplyr}` explicitly consumes a
    database connection object whereas it's idiomatic to provide the
    driver as an argument to a `create_engine` statement in
    `{sqlalchemy}`.

### Connecting to APIs

An *API* (application programming interface) is the standard way for two
computer systems to communicate with each other. APIs are the backbone
of machine-to-machine communcation in the general purpose software.

In the data science world, APIs are most often used to provide data
feeds and on-demand predictions from machine learning models.

In both R and Python, it's common to have in-language packages that wrap
APIs to provide a nice interface without having to worry about the API
itself. However, if you're consuming a private API that package probably
doesn't exist, or you may have to write it.

If you have to access an API without a helper package, you can directly
use the `{requests}` package in Python or `{httr2}` in R. These packages
are wrappers to pass commands along to the system-level `curl` command,
which communicates with APIs via the `http` protocol.

You are probably most familiar with `http` as the thing at the beginning
of a URL in your browser. The `http` protocol defines a
requests-response model that is the basic way that information is
transmitted over the web.

When you're making an API call, there are a few things you'll need to
know. The first thing is the endpoint.

Next is the verb you need.

The `http` protocol defines several *verbs* that computers can use to
communicate with each other. When you're using an API to load data,
you'll probably almost exclusively use `GET` requests, which fetch
something.

The other basic `http` methods are `POST`, `PUT`, or `PATCH` to change
or update something or a `DELETE` to (you guessed it) delete something.
There are also a number of more esoteric `http` methods, but I've never
seen a need for one.

Depending on the type of request, you also may need to provide specifics
on what you're asking the endpoint on the other side to do.

For `GET` requests, the details are specified via *query parameters*
that end up embedded in the URL after a `?`. So if you ever see a URL in
your browser that looks like `?first_name=alex&last_name=gold`, those
are query parameters.

For `POST`, `PUT`, and `PATCH` requests, details are provided in a
*body*, which is usually formatted as *JSON*. Both Python and R have
packages for formatting and reading JSON -- `{json}` in Python and
`{jsonlite}` in R.

Lastly, you may need to provide credentials or other authentication as
*headers* to your API call. Headers specify metadata information about
the request. These appear on both requests and responses and include a
lot of information, like the type of machine that is sending the
request, authentication credentials or tokens, cookies, and more. Most
often you'll only need to manually alter them to provide credentials to
your API.

When you get a response back, there are a few key components.

The first is the *status code*. Status codes indicate what happened with
your request to the server. You always hope to see `200` codes, which
indicate a successful response. There's a [cheatsheet](#cheat-http)
below with some other codes and what they mean.

Responses also include a *body*, which are the actual contents of what
you got back. Most often bodies are in `json`, but other response types
are possible.

#### Special HTTP Codes  {#cheat-http}

As you work more with `http` traffic, you'll learn some of the common
codes. Here's a cheatshet for some of the most frequent you'll see.

| Code            | Meaning                                                                                                                                                    |
|------------------|------------------------------------------------------|
| `200`           | Everyone's favorite, a successful response.                                                                                                                |
| `3xx`           | Your query was redirected somewhere else, usually ok.                                                                                                      |
| `4xx`           | Errors with the request                                                                                                                                    |
| `400`           | Bad request. This isn't a request the server can understand.                                                                                               |
| `401` and `403` | Unauthorized or forbidden. Required authentication hasn't been provided.                                                                                   |
| `404`           | Not found. There isn't any content to access here.                                                                                                         |
| `5xx`           | Errors with the server once your request got there.                                                                                                        |
| `500`           | Generic server-side error. Your request was received, but there was an error processing it.                                                                |
| `504`           | Gateway timeout. This means that a proxy or gateway between you and the server you're trying to access timed out before it got a response from the server. |

## Securing data connections

This is a question you probably don't think about much as you're
puttering around inside RStudio or in a Jupyter Notebook. But when you
take an app to production, this becomes a crucial question. In
particular, securely managing credentials becomes a big time concern, as
does how you actually provide your data source with the credentials it
needs to let you in.

The exact pattern for how you provide authentication to the service will
depend on whether it's a database or an API and also how exactly that
service works.

Most data sources are authenticated in one of the following ways:

1.  Providing a credential in your code
2.  Using a provisioned credential from the system
3.  Passing a token

The single most important thing you can do to secure your credentials
for your outside services is to avoid ever putting credentials in your
code. That means that the actual value of your username or password or
API key should never actually appear in your code.

For an API, authentication is generally provided as a header. Sometimes
you'll provide a token of some sort in the API header.

For simple API key or username and password auth, there's a format that
you provide those credentials in specified by the API provider. When you
provide those credentials in your code, you should never expose the
actual value of the secret. Instead, you should always use an
environemnt variable.

Both of the above options assume that you actually do have a credential
that you can use. Some organizations are beginning to adopt systems
where you never actually see a credential. Instead you are given a token
at some point -- or the server you're working on is -- and these tokens
are exchanged one-for-another. This is something you'll need help from
an IT/Admin to accomplish and there's more on how to discuss the issue
in [Chapter @sec-auth].

### Managing secrets with environment variables

The first, and simplest way to do this is to set them into an
environment variable. An environment variable is a value that is set
before your code starts to change how your code behaves.

::: callout-note
It is convention to make environment variable names in all caps with
words separated by underscores. The values are always simple character
values, though these can be cast to some other type inside R or Python.
:::

They can be used for a variety of different purposes, but one of the
most common is managing secrets like usernames, passwords, and API keys.

The power of using an environment variable is that you reference them
*by name*. Referencing secrets by name makes it safe to share your code
freely and make secrets management a separate concern.

In Python, you can read environment variables from the `os.environ`
dictionary or by using `os.getenv("<VAR_NAME>")`. In R, you can get and
set environment variables with `Sys.getenv("<VAR_NAME>")`.

Environment variables are set at the outset of your code running to
influence it throughout its runtime. There are a few different ways to
set environment variables.

The most common way is to load secrets from a plain text file. In
Python, environment variables are usually set by reading a `.env` file
into your Python session. The [`{python-dotenv}`
package](https://pypi.org/project/python-dotenv/) is a good choice for
doing this.

R automatically reads the `.Renviron` file as environment variables and
also sources the `.Rprofile` file, where you can set environment
variables with `Sys.setenv()`. I personally prefer putting everything in
`.Rprofile` for simplicity -- but that's not a universal opinion.

For some organizations, having the credentials available in plaintext at
all is to be avoided. An `.Rprofile` file is just a simple text file. If
an unauthorized user were to get access to my laptop, they could steal
the credentials from my `.Rprofile` and use them themselves.

Some organizations have prohibitions against ever storing credentials in
plaintext. In these cases, the credentials must be stored in a
cryptographically secure way and are only decrypted when they're
actually used.

::: callout-note
## Encryption at rest

Information that is encrypted at rest is cryptographically secured
against unauthorized access. There are at least two different things
people can mean. Whole-disk encryption means that your drive is
encrypted so that it can only be accessed by your machine. That means
that if someone were to take your hard drive to another computer, your
data wouldn't be usable. This has become more-or-less standard.
:::

If this is the case for you, you have a few options.

There are packages in both R and Python called `keyring` that allow you
to use the system keyring to securely store environment variables and
recall them at runtime. These can be good in a development environment,
but run into trouble in a production environment because they generally
rely on a user actually inputting a password for the system keyring.

Your environment may also provide environment management tools. For
example, GitHub Actions and Posit Connect both provide you the ability
to set secrets that aren't visible to the users, but are accessible to
the code at runtime in an environment variable.

## Comprehension Questions

1.  What are the different options for data storage formats for apps?
    What are the advantages and disadvantages of each?
2.  When should an app fetch all of the data up front? When is it better
    for the app to do live queries?
3.  What are the different flat file types and how can they be stored?
4.  What is a good way to create a database query that performs badly?
    (Hint: what's the slowest part of the query)

## Lab 3: Work with the data in a database

The process here is quite straight forward. We're going to load the data
into a database and then alter our existing code to use the data in the
database.

For now, we're going to use [DuckDB](https://duckdb.org). DuckDB is an
in-memory database that's great for analytics use cases. Because it's
in-memory, we don't have to set up an external database. In chapter
\[TODO\], you can set up an AWS database if you'd like practice doing
that.

To start, let's load the data.

Here's what that looks like in R:

``` {.r include="../../_labs/data-load/r-data-load.R"}
```

Or equivalently, in Python:

``` {.python include="../../_labs/data-load/py-data-load.py"}
```

Now that the data is loaded, let's adjust our scripts to use the
database.

In R, we are just going to replace our data loading with connecting to
the database. Leaving out all the parts that don't change, it looks like

``` {.r include="../../_labs/eda/eda-db.qmd" filename="eda.qmd" start-line="14" end-line="19"}
```

There's also a call to `DBI::dbDisconnect(con)` at the end of the
script. The code for the data processing part doesn't change at all, but
the operation changes a lot.

``` {.r include="../../_labs/eda/eda-db.qmd" filename="eda.qmd" start-line="23" end-line="32"}
```

I've added a call to `dplyr::collect` in line 31. That's not actually
necessary -- it will be implied if I don't put it there manually, but it
helps make obvious that everything up to that point happens in the
database, not by loading the data into R. Obviously for this small
dataset it doesn't matter, but if the dataset were much larger, this
would be a huge benefit.

In Python, we're just going to load the entire dataset into memory for
modeling, so the line loading the dataset changes to

``` {.python include="../../_labs/model/model-db.qmd" filename="model.qmd" start-line="18" end-line="21"}
```

There are no other changes on the Python side.

In order to get this to work in CI/CD, there don't need to be any
changes to the GitHub workflows, but you will need to make sure the
DuckDB packages and others get added. So before you push the code to
GitHub, make sure to run `renv::snapshot()` and
`pip freeze > requirements.txt` to make sure all the requirements are
present.

Because you're working with an on-disk database, you'll also need to
push the `my-db.duckdb` file. At some point, we probably want to pull
the data out of the bundle and locate it somewhere else, either on an
actual database or by storing the DuckDB file in a centralized location.

More on that in the lab for [Chapter @sec-cloud].

We're also going to write our model somewhere for use later in the lab.
We're going to make use of the `{vetiver}` package, which is an R and
Python language package that makes it easy to version, deploy, and
monitor a machine learning model.

We can take our existing model and turn it into a `{vetiver}` model with

``` {.python include="../../_labs/model/model-vetiver.qmd" filename="model.qmd" start-line="47" end-line="49"}
```

Once we've done that, we can specify where the model will be saved. For
now, we're going to just store it at a location on the filesystem. I'm
using `/data/model`. If that directory doesn't exist on your machine,
you can create it, or use a directory that does exist.

Whatever path you use, I'd recommend using an absolute file path, rather
than a relative one.

The code to make that happen looks like this:

``` {.python include="../../_labs/model/model-vetiver.qmd" filename="model.qmd" start-line="55" end-line="59"}
```

Depending on the tooling you're using, you may also have access to
integrated secrets management. The exact configuration will depend on
that environment, but you should use those tools when they're available.

In R, it's also very common for organizations to create internal
packages that provide connections to databases. The right way to do this
is to create a function that returns a database connection object that
you can use in conjunction with the `DBI` or `dplyr` packages.

Here's an example of what that might look like if you were using a
Postgres database:

```{r}
#| eval: false

#' Return a database connection
#'
#' @param user username, character, defaults to value of DB_USER
#' @param pw password, character, defaults to value of DB_PW
#' @param ... other arguments passed to 
#' @param driver driver, defaults to RPostgres::Postgres
#'
#' @return DBI connection
#' @export
#'
#' @examples
#' my_db_con()
my_db_con <- function(
    user = Sys.getenv("DB_USER"), 
    pw = Sys.getenv("DB_PW"), 
    ..., 
    driver = RPostgres::Postgres()
) {
  DBI::dbConnect(
    driver,
    dbname = 'my-db-name', 
    host = 'my-db.example.com', 
    port = 5432, # default for Postgres
    user = user,
    password = pw)
}
```
