# Logging and Monitoring {#sec-log-monitor}

You get a call, a text, a slack. A bead of cold sweat runs down your
back. All you know -- it seems all you've every known -- is your failed
job or error-bound app, which the CEO, of course, needs *right now*.

It didn't have to be this way. You can't control whether code will fail
-- it will. And you can't ensure it'll happen at a convenient time. But
you can live in a world where you sleep soundly at night knowing that
you'll know when issues arise and that you'll be able to quickly figure
out why they're happening.

The key to living in that world is *observability*. If your code is
observable, you'll be alerted when something goes wrong because you have
enabled *monitoring* on the system and when you dig in, you'll have
*logging* that lets you reconstruct the pathway to failure.

Beyond sleeping soundly, making your work more observable can also help
you demonstrate its value. Being able to demonstrate who is using your
work and what they're accessing can really help show decision makers
that your team matters.

In this chapter we'll get into how to make your code observable. You'll
learn how to use tooling in R and Python to see what's going on inside
your data science project. I'll also give you some tips on particular
things I always monitor or log and how to consume and use the metrics
and logs your project is now emitting.

## Observing Correctness

Observability of general purpose software is primarily concerned with
the *operational* qualities of the software. A software engineer wants
to know whether their software is using too much RAM or CPU, whether its
fast enough, and whether its crashed.

For a general purpose software engineer, an uncaught exception that
makes the software crash is about as bad as it gets.

But for a data scientist there's something even scarier -- an issue that
doesn't result in code failure but yields incorrect answers. Data joins
usually complete even if the merge quality is really bad. Model APIs
will return a prediction even if the prediction is very, very bad.

It's hard to check the actual correctness of the numbers and figures
work work returns because you're (basically by definition) doing
something novel. So you're basically left putting process metrics in
place that can help reveal a problem before it surfaces.

One important tool you have in your toolbox is correctly architecting
your project. Jobs are generally much easier to check for correctness
than presentation layers. By moving as much processing as possible out
of the presentation layer and into the data and processing layers, you
can make it easier to observe.

Moreover, you're already very familiar with tools for *literate
programming* like Jupyter Notebooks, R Markdown Documents, and Quarto
Documents.

One of my spicier opinions is that *all* jobs should be in a literate
programming format. These tools, when used well, intersperse code,
commentary, and output, which is one of the best ways of observing the
correctness of a job.

On a job, there are three particular things I always monitor.

The first is the quality of data joins. Based on the number of rows (or
unique ids), you know how many rows should be in the data set after a
join. Checking that the joined data matches expectations can reveal many
data quality issues just waiting to ruin your day.

The second checking is cross-tabulations before and after recoding a
categorical variable. Making sure the recode logic does what you think
and that the values coming in aren't changing over time is always worth
the effort.

The last is goodness-of-fit metrics of an ML model in production. There
are many, many frameworks and products for monitoring model quality and
model drift once your model is in production. I don't have strong
opinions on these other than that you need to use one if you've got a
model that's producing results you hope to rely on.

## Observing Operations

Now let's turn to the issue of observing the operational qualities of
your code. The operational qualities of your project are things like the
system resources its consuming, the number of users, and user
interactions just before an error occurred.

The first step to making your app or API observable is to add logging.
You may be used to just adding `print` statements throughout your code.
And, honestly, this is far better than nothing. But purpose-built
tooling for logging includes ways to apply consistent formats, emit logs
in useful ways, and provide visibility into severity of issues.

There are great logging packages in both Python and R. Python's logging
package is standard. There is not a standard logging package in R, but I
recommend
[`{log4r}`](https://cran.r-project.org/web/packages/log4r/index.html).

These packages -- and basically every other logging package -- work very
similarly. At the outset of your code, you'll create and parameterize a
*log session* that persists as long as the Python or R session. When
your code does something you want to love, you'll you'll use the log
session to write *log statements*. When the log statement runs, it
creates a *log entry*.

For example, here's what logging for an app starting up might look like
in Python

```{python filename="app.py"}
import logging

# Configure the log object
logging.basicConfig(
    format='%(asctime)s - %(message)s',
    level=logging.INFO
)

# Log app start
logging.info("App Started")
```

And here's what that looks like using `{log4r}`

```{r filename="app.R"}
# Configure the log object
log <- log4r::logger()

# Log app start
log4r::info(log, "App Started")
```

When the R or Python interpreter hits either of these lines, it will
create a log entry that looks something like this:

```         
2022-11-18 21:57:50 INFO App Started
```

Like all log entries, this entry has three components:

-   The *log metadata* is data what the logging library automatically
    includes on every entry. It is configured when you initialize
    logging. In the example above, the only metadata is the timestamp.
    Log metadata can include additional information, like which server
    you're running on.

-   The second component is the *log level*. The log level indicates the
    severity of the event you're logging. In the example above the log
    level was `INFO`.

-   The last component is the *log data*, which provides details on the
    event you want to log -- `App Started` in this case.

### Understanding log levels

The log level indicates how serious the logged event is. Most logging
libraries have 5-7 log levels. As you're writing statements into your
code, you'll have to think carefully about the appropriate logging level
for a given event.

Both the Python `{logging}` library and `{log4r}` have five levels from
least to most scary:

1.  *Debug*: what the code was doing in detail that will only make sense
    to someone who really knows the code. For example, you might include
    which function ran and with what arguments in a debug log.
2.  *Info*: something normal happened in the app. Info statements record
    things like starting and stopping, successfully making database and
    other connections, and runtime configuration options.
3.  *Warn/Warning*: an unexpected application issue that isn't fatal.
    For example, you might include having to retry doing something or
    noticing that resource usage is high. If something were to go wrong
    later, these might be helpful breadcrumbs to look at.
4.  *Error*: an issue that will make an operation not work, but that
    won't bring down your app. An example might be a user submitting
    invalid input and the app recovering.
5.  *Critical*: an error so big that the app itself shuts down. This is
    the SOS your app sends as it shuts down. For example, if your app
    cannot run without a connection to an outside service, you might log
    an inability to connect as a Critical error.

When you initialize your logging session, you'll set your log level,
which is the **least critical** level you want to see in the session. In
development, you probably want to log everything down to the debug
level, while that probably isn't ideal in prod.

### Configuring log formats and log handling

When you initialize your logging session, you'll choose where logs will
be written and in what format. You'll configure the format with a
*formatter* or *layout* and where it goes with a *handler* or an
*appender*.

For most logging frameworks, the default is to emit logs to the
*console* in *plain text*.

For example, a plain text log of an app starting might put this on your
console

```         
2022-11-18 21:57:50 INFO App Started
```

You'll decide the format of your log based on how you're planning to
consume them.

Plain text logs is a great choice if humans are going to be directly
reading them. If you're shipping your logs off to an aggregation
service, you might prefer to have structured logs.

The most common structured logging format is JSON, though YAML and XML
are often options. If you used JSON logging, the same record might be
emitted as

```         
{
  "time": "2022-11-18 21:57:50",
  "level": "INFO", 
  "data": "App Started"
}
```

Where your logs go should be determined by where your code is running.

In development, printing logs for the console makes it easy to iterate
quickly.

In production, the most common way to handle logs is to append them to a
file. It makes them easy for humans to access and many tools for
aggregating and consuming logs are comfortable watching a file and
aggregating lines as they are written.

If you are emitting logs to file, you may also want to consider how long
those logs stay around.

*Log rotation* is the process of periodically creating new log files,
storing old logs for a set retention period, and deleting files outside
that period. A common log rotation pattern is to have a log file that
lasts for 24 hours, is retained for 30 days, and is then deleted.

The Python `{logging}` library does log rotation itself. `{log4r}` does
not, but there is a Linux library called `logrotate` that you can use in
concert with `{log4r}`.[^1-4-monitor-log-1]

[^1-4-monitor-log-1]: There are two common naming patterns with rotating
    log files.

    The first is to have dated log filenames that look like
    `my-log-20221118.log`.

    The other pattern is to keep one file that's current and have the
    older ones numbered. So today's log would be `my-log.log`,
    yesterday's would be `my-log.log.1` , the day before `my-log.log.2`,
    etc. This second pattern works particularly well if you're using
    `logrotate` with `log4r`, because then `log4r` doesn't need to know
    anything about the log rotation. It's just always writing to
    `my-log.log`.

If you're running in a Docker container you don't want to write to a
file on disk. As you'll learn more about in [Chapter @sec-docker],
anything that lives inside a Docker container is ephemeral. This is
obviously bad if you're writing a log that might contain clues for why a
Docker container was unexpectedly killed.

In that case, it's common practice for a service running in a container
to emit logs inside the container and then have some sort of more
permanent service collecting the logs outside. This is usually
accomplished by sending normal operating logs to go to *stdout* (usually
pronounced standard out) and failures to go to *stderr* (standard
error).

It's also possible you want to do something else completely custom with
your logs. This is most common for critical or error logs. For example,
you may want to send an email, slack, or text message immediately if
your system emits a high-level log message.

It's also very common to have different format and location settings in
development vs in production. The most common way to enable different
logging configurations in different environments is with config files
and environment variables. More on how to use these tools in [Chapter
@sec-env-as-code].

### Working with Metrics

The most common place to see metrics in a data science context is when
deploying and monitoring ML models in production. Additionally,
monitoring ETL data quality is ripe for more monitoring.

If you are going to configure metrics emission or consumption, most
modern metrics stacks are built around the open source tools
*Prometheus* and *Grafana*.

[Prometheus](https://prometheus.io/) is an open source monitoring tool
that makes it easy to store metrics data, query that data, and alert
based on it. [Grafana](https://grafana.com/) is an open source
dashboarding tool that sits on top of Prometheus to do visualization of
the metrics. They are usually used together to do monitoring and
visualization of metrics.

You can run Prometheus and Grafana yourself, but Grafana Labs provides a
generous free tier for their SaaS service. This is great because you can
just set up their service and point your app to it.

Because the Prometheus/Grafana stack started out in the DevOps world,
they are most optimized to do monitoring of a whole server or fleet of
servers -- but it's not hard to use them to monitor things you might
care about like data quality, API response times, or other things.

If you want to register metrics from your API or app with Prometheus,
there is an official [Prometheus
client](https://github.com/prometheus/client_python) in Python and the
`{openmetrics}` package in R makes it easy to register metrics from a
Plumber API or Shiny app.

There's a great [Get Started with Grafana and
Prometheus](https://grafana.com/docs/grafana/latest/getting-started/get-started-grafana-prometheus/)
doc on the Grafana Labs website if you want to actually try it out.

## Comprehension Questions

1.  What is the difference between monitoring and logging? What are the
    two halves of the monitoring and logging process?
2.  In general, logging is good, but what are some things you should be
    careful *not to log*?
3.  At what level would you log each of the following events:
    1.  Someone clicks on a particular tab in your Shiny app.

    2.  Someone puts an invalid entry into a text entry box.

    3.  An `http` call your app makes to an external API fails.

    4.  The numeric values that are going into your computational
        function.

## Lab 4: An App with Logging

Let's go back to the prediction generator app from the last lab and add
a little logging. This is quite easy in both R and Python. In both, we
just declare that we're using the logger and then we put logging
statements into our code.

I decided to log when the app starts, just before and after each
request, and an error logger if an HTTP error code comes back from the
API.

With the logging now added, here's what the app looks like in R:

``` {.r include="../../_labs/app/app-log.R" filename="app.R"}
```

And in Python:

``` {.python include="../../_labs/app/app-log.py" filename="app.py"}
```

Now, if you load up this app locally, you can see the logs of what's
happening stream in as you're pressing buttons in the app.

You can feel free to log whatever you think is helpful -- for example,
it'd probably be more useful to get the actual error contents if an HTTP
error comes back.
