# The Cloud {#sec-cloud}

Nearly every data science organization is already working in the cloud
or is considering a cloud transition. As someone who doesn't spend your
day working directly with cloud services, trying to understand what the
cloud is can feel like trying to catch a -- well, you know -- and pin it
down.[^2-1-cloud-1]

[^2-1-cloud-1]: Yes, that is a Sound of Music reference.

But the cloud is very real and very important to the way production data
science gets done. In this chapter, you'll learn about what the cloud is
and get an introduction to important cloud services for data science.

This chapter has two labs. In the first, you'll get started with a
server in *AWS (Amazon Web Services)* -- getting it stood up and
learning how to start and stop it. In the second lab, you'll put the
model from our penguin mass modeling lab into an S3 bucket (more on what
that is in a bit).

## The cloud is rental servers

At one time, the only way to get servers was to buy physical machines
and hire someone to install and maintain them. This is called running
the servers *on-prem* (short for premises). There are some
organizations, especially those with highly sensitive data, that still
run on-prem servers.

The problem is that on-prem servers require a large up-front investment
in server hardware and professional capacity. If your company has a use
case that only requires a single server or one with uncertain payoff, it
probably isn't worth it to hire someone and buy a bunch of hardware.

Around the year 2000, Amazon took all the server farms across the
company and centralized them so teams would use this central server
capacity instead of running their own. Over the next few years, Amazon
execs (correctly) realized that other companies and organizations would
value this ability to rent server capacity. They launched this "rent a
server" business as AWS in 2006.

These days, the cloud platform business is enormous -- collectively
nearly a quarter of a trillion dollars. It's also highly profitable. AWS
was only 13% of Amazon's revenue in 2021, but a whopping 74% of the
company's profits for that year.[^2-1-cloud-2]

[^2-1-cloud-2]: https://www.visualcapitalist.com/aws-powering-the-internet-and-amazons-profits/

AWS is still the biggest cloud platform by a considerable margin, but
it's far from alone. Approximately 2/3 of the market consists of *the
big three* -- AWS, Microsoft Azure, and Google Cloud Platform (GCP) with
the final third made up of numerous smaller companies.[^2-1-cloud-3]

[^2-1-cloud-3]: https://www.statista.com/chart/18819/worldwide-market-share-of-leading-cloud-infrastructure-service-providers/

## Real (and fake) cloud benefits

The cloud arrived with an avalanche of marketing fluff. Over a decade
after the cloud went mainstream, it's clear that some of the purported
benefits are real and some are less so. You can be a much more
intelligent cloud consumer if you understand which is which.

The most important cloud benefit is flexibility. Moving to the cloud
allows you to get a new server or re-scale an existing one in minutes,
and you only pay for what you use, often on an hourly
basis.[^2-1-cloud-4] The risk of incorrectly guessing how much capacity
you'll need is drastically reduced, making it way less risky to get
started on a server.

[^2-1-cloud-4]: Although these days a huge amount of cloud spending is
    done via annual pre-commitments. The cloud providers offer big
    discounts for making an up-front commitment, which the organization
    then spends down over the course of the year.

The other big benefit of the cloud is that it allows IT/Admin teams to
narrow their scope and focus. For most organizations, managing physical
servers isn't part of their core competency and outsourcing that work to
a cloud provider is a great choice.

::: callout-note
One other dynamic is the incentives of individual IT/Admins. As
technical professionals, IT/Admins want evidence on their resumes that
they have experience with the latest and greatest technologies, which
are generally cloud services these days rather than managing physical
hardware.
:::

Along with these very real benefits, the cloud was supposed to enable
big time savings relative to on-prem operations. For the most part, that
hasn't materialized.

The theory was that the cloud would enable organiations to scale their
capacity to match need at any given moment. So even if the hourly price
was higher, the organization would turn servers off at night or during
slow periods and save money.

It turns out that dynamic server scaling loads takes a fair amount of
engineering effort and only the most sophisticated IT/Admin
organizations have implemented effective autoscaling. And even for the
organizations that do autoscale, cloud providers are very good at
pricing their products to capture a lot of those savings.

Some large organizations with stable workloads have actually started
doing *cloud repatriations* -- bringing workloads back on-prem for
significant cost savings. An [a16z
study](https://a16z.com/2021/05/27/cost-of-cloud-paradox-market-cap-cloud-lifecycle-scale-growth-repatriation-optimization/)
found that for certain organizations, the total cost of repatriated
workloads, including staffing, could be 1/3 to 1/2 the cost of using a
cloud provider.[^2-1-cloud-5]

[^2-1-cloud-5]: https://a16z.com/2021/05/27/cost-of-cloud-paradox-market-cap-cloud-lifecycle-scale-growth-repatriation-optimization/

That said, even if the cash savings aren't meaningful, the cloud is a
key enabler for many businesses. The ability to start small, focus on
what matters, and scale up quickly is more than worth it.

You may be interested in buying a physical server or re-purposing an old
computer just for fun. You're in good company; I've run Ubuntu Server on
more than one old laptop. But if you're trying to spend more time
getting important things done and less time playing, getting a server
from the cloud is the way to go.

## Understanding cloud services

In the beginning, cloud providers did just one thing -- rent you a
server. But they didn't stop there. Instead, they started building
layers and layers of services that abstract away more IT/Admin tasks.

At the end of the day, all cloud services boil down to "rent me an
$\text{X}$". As a data scientist, you should start by decoding "What is
the $\text{X}$?"

Unfortunately, cloud marketing materials aren't usually oriented to the
data scientist trying to decide whether to use the services, instead
they're oriented at your boss and your boss's boss, who wants to hear
about benefits of using the service. That can make it difficult to
decode what $\text{X}$ is.

It's helpful to keep in mind that, at the end of the day, every service
that isn't directly renting a server is just renting server that already
has certain software pre-installed and configured.[^2-1-cloud-6]

[^2-1-cloud-6]: There are also some wild services that do specific
    things, like let you rent you satellite ground station
    infrastructure or do Internet of Things (IoT) workloads. Those
    services are really cool, but so far outside the scope of this book
    that I'm fine with talking like they don't exist.

::: callout-note
## Less of serverless computing

You might hear people talking about going *serverless*. There is no such
thing as serverless computing. Serverless is a marketing term meant to
convey that **you** don't have to manage the servers. The cloud provider
manages them for you, but they're still there.
:::

Cloud services are sometimes grouped into three layers to indicate
whether you're renting a basic computing service or something more
complete.

An analogy to a more familiar layered object may serve to make things
clear. Let's say you're throwing a birthday party for a friend. You're
planning to bring a chocolate layer cake with vanilla frosting topped
with lavender rosettes and "Happy Birthday!" in teal.[^2-1-cloud-7]

[^2-1-cloud-7]: If you're planning my birthday party, this is the
    correct cake configuration.

::: callout-note
## Big Three Service Naming

In this next section, I'll mention services for common tasks from the
big three. AWS tends use cutesy names that have a tangential
relationship to the task at hand. Azure and GCP name their offerings
more literally.

This makes AWS names a little harder to learn, but much easier to recall
once you've learned them. A table of all the services mentioned in this
chapter is in [Appendix @sec-append-cheat].
:::

### IaaS Offerings

*Infrastructure as a service* (*IaaS*, pronounced eye-ahzz) is the basic
rent a server premise from the earliest days of the cloud.

::: callout-note
When you rent a server from a cloud provider, you usually not renting a
whole server. Instead, you're renting a *virtualized* server or a
*virtual machine (vm)*. What you see as your server is probably just a
part of a much larger physical server that you're sharing with other
customers of the cloud provider.
:::

From a data science perspective, a IaaS offering might look like what
we're doing in the lab in this book -- acquiring a server, networking,
and storage from the cloud provider and assembling it into a data
science workbench. This is definitely the best way to learn how to
administer a data science environment and it's the cheapest option, but
it's also the most time-consuming.

This would be like choosing to go to the grocery store, buy all the
ingredients, and bake and decorate your friend's cake from scratch.

Along with a server, you'll rent storage and networking to make
everything work properly.

Some common IaaS services you're likely to use include:

-   Renting a server from AWS with *EC2 (Elastic Cloud Compute)*, from
    Azure with *Azure VMs*, and from GCP with *Google Compute Engine
    Instances*.

-   Unlike your laptop, rented servers don't include a hard drive, so
    you'll have to attach storage with AWS's *EBS (Elastic Block
    Store)*, *Azure Managed Disk*, or *Google Persistent Disk*.

-   Creating and managing the networking where your servers sit with
    AWS's *VPC (Virtual Private Cloud)*, Azure's*Virtual Network*, and
    GCP's *Virtual Private Cloud*.

-   Managing DNS records via AWS's *Route 53*, *Azure* *DNS*, and
    *Google Cloud DNS*. (More on what this means in [Chapter @sec-dns].

While IaaS means the IT/Admins don't have to be responsible for physical
management of servers, they're responsible for everything else,
including keeping the servers updated and secured. For that reason, many
organizations are moving away from IaaS towards something more managed
these days.

### PaaS Offerings

In a *PaaS (Platform as a Service)* solution, you hand off management of
the servers, but manage the applications you need via an API specific to
that service. From a data science perspective, a PaaS setup might look
like hosting a JupyterHub, RStudio, or Posit implementation in EKS, or
running an ML API in Lambda.

In the cake baking world, PaaS would be like buying a pre-made cake and
some tins of frosting and doing only the writing and rosettes yourself.

One PaaS service that already came up in the book is *blob (Binary Large
Object)* storage. Blob storage allows you to store individual objects
somewhere and recall them to any other machine that has access to the
blob store. The major blob stores are AWS's *S3 (Simple Storage
Service)*, *Azure Blob Storage*, and *Google Cloud Storage*.

You're also likely to make use of cloud-based database, data lake, and
data warehouse offerings. There are numerous different offerings, and
the ones that you use will depend a lot on your use case and your
organization. The ones I've seen used most frequently are *RDS* or
*Redshift* from AWS, *Azure Database* or *Azure Datalake*, and *Google
Cloud Database* and *Google BigQuery.* This category also includes a
number of offerings from outside the big three, most notably *Snowflake*
and *Databricks*.

Depending on your organization, you may also use services that run APIs
or applications from containers or machine images like AWS's *ECS
(Elastic Container Service)*, *Elastic Beanstalk*, or *Lambda*, Azure's
*Container Apps* or *Functions*, or GCP's *App Engine* or *Cloud
Functions*.

Increasingly, organizations are turning to *Kubernetes* as a way to host
services. (More on that in [Chapter @sec-ent-scale].) Most organizations
who do so use a cloud provider's Kubernetes cluster as a service: AWS's
*EKS (Elastic Kubernetes Service)* or *Fargate*, Azure's *AKS (Azure
Kubernetes Service)*, or GCP's *GKE (Google Kubernetes Engine)*.

Many organizations are moving to PaaS solutions for hosting applications
for internal use. It takes away the hassle of managing and updating
actual servers. On the flipside, these offerings are somewhat less
flexible than just renting a server, and some applications don't run
well in these environments.

### SaaS Offerings

SaaS (Software as a Service) is where you just rent the end-user
software for usage, often on the basis of seats or usage. You're already
used to consumer SaaS software like Gmail, Slack, and Office365.

The cake equivalent of SaaS would be just going to a bakery and buying a
cake for your friend.

Depending on your organization, you might use a SaaS data science
offering like AWS's *SageMaker*, Azure's *Azure ML*, or GCP's *Vertex
AI* or *Cloud Workstations*.

The great thing about SaaS offerings is that you get immediate access to
the end-user application and it's usually trivial (aside from cost) to
add more users. IT/Admin configuration is generally limited to hooking
up integrations, most often authentication and/or data sources.

The tradeoff for this ease is that they're generally more expensive and
you're at the mercy of the provider for configuration and upgrades.

### Notes on Data Storage

In most cases, you'll have data storage handed to you by an IT/Admin.
Here are my thoughts on different kinds of data stores.

Redshift, Azure Datalake, BigQuery, Databricks, and Snowflake are
full-fledged data warehouses that catalog and store data from all across
your organization. As a data scientist, you might use one of these, but
you probably won't (and shouldn't) set one up.

However, it's likely that you'll own one or more databases within the
data warehouse, and you may have to choose what kind of database to use.

In my experience, *Postgres* is good enough for most things involving
rectangular data of moderate size. And if you're storing non-rectangular
data, you can't go wrong with bucket storage. There are more advanced
options, but unless you've tried the combo of a Postgres database and
bucket storage and found it lacking, I probably wouldn't do anything
more complicated.

### Common Services

Irrespective of the particular services you want to use, there are a few
basic services you'll almost certainly have to interact with.

Regardless of what you're trying to do, if you're working in the cloud,
you have to make sure that the right people have the right permissions.
In order to manage these permissions, AWS has *IAM (Identity and Access
Management)*, GCP has *Identity Access Management*, and Azure has
*Microsoft Entra ID*, which was called *Azure Active Directory* until
the summer of 2023. Your organization might integrate these services
with a SaaS identity management solution like Okta or OneLogin.

Additionally, some cloud services are geographically specific. Each of
the cloud providers has split the world into a number of geographic
areas, which they all call *regions*.

Some services are region-specific and can only interact with other
services in that region by default. If you're doing things yourself, I
recommend just choosing the region where you live and putting everything
there. Costs and service availability does vary somewhat across region,
but it shouldn't be materially different for what you're trying to do.

Regions are subdivided into *availability zones* (AZs). AZs are
subdivisions of regions that are designed to be independent. Some
organizations want to run services that span multiple availability zones
to provide protection against outages in any particular geography. If
you're running something sophisticated enough to need multi-AZ
configuration, you should really be working with a professional
IT/Admin.

## Comprehension Questions

1.  What are two reasons you should consider going to the cloud? What's
    one reason you shouldn't?
2.  What is the difference between PaaS, IaaS, and SaaS? What's an
    example of each that you're familiar with?
3.  What are the names for AWS's services for: renting a server, file
    system storage, blob storage

## Introduction to Labs

Welcome to the lab!

The point of these exercises is to get you hands on with running servers
and get you practicing the things you're learning in the rest of the
book.

If you walk through the labs sequentially, you'll end up with a working
data science workbench. It won't suffice for any enterprise-level
requirements, but it'll be secure enough for a hobby project or even a
small team.

For this lab, we're going to use services from AWS, as they're the
biggest cloud provider and the one you're most likely to run into in the
real world. Because we'll be mostly using IaaS services, there are very
close analogs from Azure and GCP should you want to use one of them
instead.

## Lab: Getting started with AWS

In this first lab, we're going to get you up and running with an AWS
account and show you how to manage, start, and stop EC2 instances in
AWS.

The server we'll stand up will be from AWS's *free tier* -- so there
will be no cost involved as long as you haven't used up all your AWS
free tier credits before now.

::: callout-tip
Throughout the labs, I'll suggest you name things in certain ways. You
can do what you want, but I'll be consistent with those names, so you
can copy commands straight from the book if you use the same name.

If you want to follow along in that way, start by creating a standalone
directory for this lab, named `do4ds-lab`.
:::

### Step 1: Login to the AWS Console

We're going to start by logging into AWS at <https://aws.amazon.com>.

::: callout-note
An AWS account is separate from an Amazon account for ordering stuff
online and watching movies. You'll have to create one if you've never
used AWS before.
:::

Once you've logged in, you'll be confronted by the AWS console. There
are a ton of things here. Poke around if you want and then continue when
you're ready.

### Step 2: Stand up an EC2 instance

There are five attributes about your EC2 instance you'll want to
configure. If it's not mentioned here, just stick with the defaults for
now.

In particular, just stick with the default *Security Group*. We'll get
into what they are and how to configure them later.

#### Name + Tags

Instance *name and tags* are human-readable labels so you can remember
what this instance is. Neither name nor tag are required, but I'd
recommend you name the server something like `do4ds-lab` in case you
stand up others later.

If you're doing this at work, there may be tagging policies so that the
IT/Admin team can figure out who servers belong to later.

#### Image

An *image* is a snapshot of a system and serves as the starting point
for your server. AWS's are called *AMIs* (*Amazon Machine Images*). They
range from free images of bare operating system to paid images that come
bundled with software you might want.

Choose an AMI that's just the newest LTS Ubuntu operating system. As of
this writing, that's 22.04. It should say *free tier eligible*.

#### Instance Type

The *instance type* identifies the capability of the machine you're
renting. An instance type is made up of a *family* and a *size*. The
family is the category of server and is denoted by letters and numbers,
so there are T2s and T3s, C4s, R5s, and many more.

Within each family, there are different sizes. Possible sizes vary by
the family, but generally range from *nano* to multiples of *xlarge*
like *24.xlarge*.

For now, I'd recommend you get the largest server that is free tier
eligible. As of this writing, that's a *t2.micro* with 1 CPU and 1 Gb of
memory.

::: callout-note
## Server sizing for the lab

A *t2.micro* with 1 CPU and 1 Gb of memory is a very small server. For
example, your laptop probably has at least 8 CPUs and 16 Gb of memory.

A t2.micro should be sufficient for finishing the lab, but you'll need a
substantially larger server to do any real data science work.

Luckily, it's easy to upgrade cloud server sizes later. More on how, as
well as advice on sizing servers for real data science work in [Chapter
@sec-scale].
:::

#### Keypair

The *keypair* is the skeleton key to your server. We'll get more into
how to use and configure it in [Chapter @sec-cmd-line]. For now, create
a new keypair. I'd recommend naming it `do4ds-lab-key`. Download the
`.pem` version and put it in your `do4ds-lab` directory.

#### Storage

Bump up the storage to as much as you can get under the free tier,
because why not? As of this writing, that's 30 Gb.

### Step 3: Start the Server

If you followed these instructions, you should now be looking at a
summary that lists the operating system, server type, firewall, and
storage. Go ahead an launch your instance.

If you go back to the EC2 page and click on `Instances` you can see your
instance as it comes up. When it's up, it will transition to
`State: Running`.

### Optional: Stop the Server

Whenever you're stopping for the day, you may want to suspend your
server so you're not using up your free tier hours or paying for it. You
can suspend an instance in the state it's in so it can be restarted
later. Suspended instances aren't always free, but they're generally
very cheap.

Whenever you want to suspend your instance, go to the EC2 page for your
server. Under the `Instance State` drop down in the upper right, choose
`Stop Instance`.

After a couple minutes the instance will stop. Before you come back to
the next lab, you'll need to start the instance back up so it's ready to
go.

If you want to completely delete the instance at any point, you can
choose to `Terminate Instance` from that same `Instance State` dropdown.

## Lab: Put the penguins data and model in S3

Whether or not you're hosting your own server, most data scientists
working at an organization that uses AWS will run into S3, AWS's blob
store.

One really common thing to store in S3 is an ML model. So we're going to
store the mass prediction model we created in Chapters [@sec-proj-arch]
and [@sec-data-access] in an S3 bucket.

### Step 1: Create an S3 bucket

To start off with, you'll have to create a bucket, most commonly from
the AWS console. I'm naming mine `do4ds-lab`.

### Step 2: Push new models to S3

Let's change the code in our Quarto doc to push the model into S3 when
the model rebuilds, instead of just saving it locally.

There are a variety of different ways to access an S3 bucket. The
simplest way is by using the AWS CLI on the command line. There are also
R and Python packages for interacting with S3 (and other AWS services).
The most common are Python's `{boto3}` package or R's `{paws}` and
`{aws.s3}`.

Regardless of what tooling you're using, you'll generally configure your
credentials in three environment variables -- `AWS_ACCESS_KEY_ID`,
`AWS_SECRET_ACCESS_KEY`, and `AWS_REGION`. You can get the access key
and secret access key from the AWS console and you should know the
region.

As always, when you're developing in Python or R, I'd recommend putting
these into a `.env` or a `config.yml` file and loading them from there.

Since we built the model using `{vetiver}`, it's really easy to push the
model to S3 just by changing the board type to `board_s3` and making
sure our credentials are defined in an environment variable.

It'll look something like this.

``` {.python include="../../_labs/model/model-vetiver-s3.qmd" filename="model.qmd" start-line="64" end-line="68"}
```

Under the hood, `{vetiver}` is making use of standard R and Python
tooling to access an S3 bucket.

Instead of using credentials, you could configure an *instance profile*
using IAM, so the entire EC2 instance has access to the S3 bucket
without needing credentials. Configuring instance profiles is the kind
of thing you should work with a real IT/Admin to do.

### Step 3: Pull the API model from S3

You'll also have to configure the API to load the model from the S3
bucket. Luckily, this is very easy. Just update the script you used to
build your `Dockerfile` so it pulls from the pin in the S3 bucket rather
than the local folder.

Now, the script to build the `Dockerfile` looks like this:

``` {.python include="../../_labs/docker/docker-s3/build-docker-s3.qmd" start_line="11" end_line="19"}
```

### Step 4: Give GitHub Actions S3 credentials

We want our model building to correctly push to S3 even when it's
running in GitHub Actions, but GitHub doesn't have our S3 credentials by
default, so we'll need to provide them.

We're going to declare the variables we need in the `Render and Publish`
step of the Action.

Once you're done, that section of the `publish.yml` should look
something like this.

``` {.yaml include="../../_labs/gha/publish-s3.yml" filename=".github/workflows/publish.yml" start_line="45" end_line="49"}
```

Now, unlike the `GITHUB_TOKEN` secret, which GitHub Actions
automatically provides to itself, we'll have to provide these secrets to
the GitHub interface.

### Lab Extensions

You might also want to put the actual data you're using into S3. This
can be a great way to separate the data from the project, as recommended
in [Chapter @sec-proj-arch].

Putting the data in S3 is such a common pattern that DuckDB allows you
to directly interface with parquet files stored in S3.
