# The Cloud {#sec-cloud}

Nearly every data science organization is already doing work in the
cloud or is considering a cloud transition. But as a data scientist, it
can all feel a little impenetrable. Trying to pierce the veil of
marketing fluff and understand exactly what the cloud is feels like
trying to catch a -- well, you know -- and pin it down.[^2-1-cloud-1]

[^2-1-cloud-1]: Yes, that is a Sound of Music reference.

Despite that feeling, the cloud is very real and very important to the
way data science gets done. In this chapter, you'll learn about what the
cloud is, important cloud services to know about for data science.

This chapter has two labs. In the first, you'll get started with a
server in AWS -- getting it stood up and learning how to start and stop
it. In the second lab, you'll work with an *S3 bucket* to store the
model from our penguin model.

## What is the cloud anyway?

To explain what the cloud is today, let's start with where we're coming
from.

At one time, the only way to get servers was to buy physical machines
and hire someone to install and maintain them. This is called running
the servers *on-prem* (short for premises). There are some
organizations, especially those with highly sensitive data, that still
run on-prem servers.

The problem is that on-prem servers require large up-front investment in
server hardware and professional capacity. If your company has a use
case that only requires a single server or one with uncertain payoff, it
probably isn't worth it to hire someone and buy a bunch of hardware.

Around the year 2000, Amazon took all the server farms across the
company and centralized them. So instead of each team running its own
servers, one team ran all the servers and other teams could use that
server capacity.

Over the next few years, Amazon execs realized that this service could
be valuable beyond other teams at Amazon. They decided to rent server
capacity to other organizations. This "rent a server" business launched
as Amazon Web Services (AWS) in 2006.

These days, the cloud platform business is huge -- collectively nearly a
quarter of a trillion dollars. AWS is still the biggest cloud platform
by a considerable margin, but it's far from alone in the business of
renting out server capacity. Approximately 2/3 of the market consists of
*the big three* -- AWS, Microsoft Azure, and Google Cloud Platform
(GCP)[^2-1-cloud-2] with the final third made up of numerous smaller
companies.

[^2-1-cloud-2]: https://www.statista.com/chart/18819/worldwide-market-share-of-leading-cloud-infrastructure-service-providers/

## Real (and fake) cloud benefits

The cloud arrived with an avalanche of hype and many purported benefits.
Over a decade after the cloud went mainstream, it's clear that some of
those benefits are real and some are less so.

The most important cloud benefit is flexibility. In an on-prem-only
world, your organization has to buy and run all the server capacity it
needs. Adding capacity requires requisitioning a new server, ordering it
from a hardware manufacturer, and installing it in a server rack. That
takes time and isn't going to happen until the anticipated benefits of
getting servers is worth the high up-front cost.

In the cloud, you can have a new server up or re-scale an existing one
in minutes. This has a few benefits. Most obviously, it is incredible
for organizations whose server capacity needs change rapidly. Perhaps
more importantly, it drastically lowers the barrier to entry to doing
things on a server. Instead of a large up-front investment, you could
just pay for server capacity on an hourly basis.

The other big benefit of the cloud is that it allows IT/Admin teams to
narrow their scope and focus. For most organizations, managing physical
servers isn't part of their core competency and outsourcing that work to
a cloud provider is a great choice.

::: callout-note
One other dynamic is the incentives of individual IT/Admins. As
technical professionals, IT/Admins want evidence on their resumes that
they have experience with the latest and greatest technologies, which
are generally cloud services these days.
:::

Among all these very real benefits, the supposed cost savings from
moving to the cloud are unlikely to materialize for most organizations.

The theory was that the cloud would enable dynamic scaling
(*autoscaling*) to match server resources to need at any given moment.
So even if the hourly price was higher, the organization would turn
servers off at night or during slow periods and save money.

It turns out that dynamically scaling server loads takes a fair amount
of engineering effort and only the most sophisticated IT/Admin
organizations have implemented effective autoscaling.

Even for the organizations that do autoscale, cloud providers are very
good at pricing their products to capture a lot of those savings.
There's a reason there's a small consulting industry solely focused on
helping organizations save money on AWS bills and why AWS was only 13%
of Amazon's revenue in 2021, but a whopping 74% of the company's profits
for that year.[^2-1-cloud-3]

[^2-1-cloud-3]: https://www.visualcapitalist.com/aws-powering-the-internet-and-amazons-profits/

Some large organizations with stable workloads have actually started
doing *cloud repatriations* -- bringing workloads back on-prem for
significant cost savings. An [a16z
study](https://a16z.com/2021/05/27/cost-of-cloud-paradox-market-cap-cloud-lifecycle-scale-growth-repatriation-optimization/)
found that for certain organizations, the total cost of repatriated
workloads, including staffing, could be 1/3 to 1/2 the cost of using a
cloud provider.[^2-1-cloud-4]

[^2-1-cloud-4]: https://a16z.com/2021/05/27/cost-of-cloud-paradox-market-cap-cloud-lifecycle-scale-growth-repatriation-optimization/

Still, moving to the cloud is a good idea for many organizations. Even
if it doesn't save much cash, the ability to start small, focus on what
matters, and scale up quickly is more than worth it.

If you're just getting started with server-based work, you should do it
in the cloud. You, nerd that you are, may be interested in buying a
physical server or re-purposing an old computer just for fun. You're in
good company. I've run Ubuntu Server on more than one old laptop. But if
you're trying to spend more time getting important things done and less
time playing, getting a server from the cloud is the way to go.

## Understanding cloud services 

In the beginning, cloud providers did just one thing -- rent you a
server. But they didn't stop there. Instead, they started building
layers and layers of services that abstract away more IT/Admin tasks.

At the end of the day, all cloud services are just
$\text{X as a service}$. When you're trying to evaluate whether you want
to use a cloud service, the first question is, "What is the $\text{X}$
for this service?"

Unfortunately, cloud marketing materials are usually oriented around the
benefits of the service, making it difficult to decode what $\text{X}$
actually is. Moreover, cloud providers often have several different
services for similar purposes and it can be hard to concretely tell the
difference.

It's helpful to keep in mind that you're fundamentally renting server
capacity. These days, you might hear people talking about going
*serverless*. The important thing to understand about serverless
computing is that there is no such thing as serverless computing.

Serverless is a marketing term meant to convey that **you** don't have
to manage the servers. The cloud provider manages them for you, but
they're still there. Every service that isn't directly renting server
capacity is renting server capacity with software pre-installed to get
you started faster.[^2-1-cloud-5]

[^2-1-cloud-5]: There are also some wild services that do specific
    things, like let you rent you satellite ground station
    infrastructure or do Internet of Things (IoT) workloads. Those
    services are really cool, but so far outside the scope of this book
    that I'm fine with talking like they don't exist.

These services are sometimes grouped into three categories to indicate
whether you're renting a basic computing service or something more
complete.

An analogy to a familiar layered object may serve to make things clear.
Let's say you're throwing a birthday party for a friend. You're planning
to bring a chocolate layer cake with white frosting topped with lavender
rosettes and "Happy Birthday!" in teal.[^2-1-cloud-6]

[^2-1-cloud-6]: If you're planning my birthday party, this is the
    correct cake configuration.

::: callout-note
## Big Three Service Naming

In this next section, I'll mention services for common tasks from the
big three. AWS tends use cutesy names that have a tangential
relationship to the task at hand. Azure and GCP name their offerings
more literally.

This makes AWS names a little harder to learn, but much easier to recall
once you've learned them. In this section -- contrary to standard
practice -- I'm going to use the common abbreviated names for AWS
services and put the full name in parentheses as these are just trivia.

A table of all the services mentioned in this chapter is in [Appendix
@sec-append-cheat].
:::

### IaaS Offerings

If you've got really particular cake preferences or more time than
money, you might choose to go to the grocery store, buy all the
ingredients, and bake and decorate your friend's cake from scratch.

*I*This is the equivalent of using *Infrastructure as a service*
(*IaaS*, pronounced eye-ahzz) offerings. IaaS is the basic rent a server
premise from the earliest days of the cloud. Along with a server, you'll
get storage and networking to make everything work properly.

Some common IaaS services you're likely to use include:

-   Renting a server with *EC2 (Elastic Cloud Compute)* from AWS, *Azure
    VMs* from Azure, and *Google Compute Engine Instances* from GCP.

-   Unlike your laptop, rented servers don't include a hard drive, so
    you'll have to attach storage with AWS's *EBS (Elastic Block
    Store)*, *Azure Managed Disk*, or *Google Persistent Disk*.

-   Creating and managing the networking where your servers sit with
    AWS's *VPC (Virtual Private Cloud)*, Azure's *Virtual Network*, and
    GCP's *Virtual Private Cloud*.

-   Managing DNS records via AWS's *Route 53*, *Azure* *DNS*, and
    *Google Cloud DNS*.

From a data science perspective, a IaaS offering might look like what
we're doing in the lab in this book -- acquiring a server, networking,
and storage from the cloud provider and assembling it into a data
science workbench. This is definitely the best way to learn how to
administer a data science environment and the cheapest option, but it's
also the most time-consuming.

These days, many organizations are moving away from IaaS offerings.
While IaaS means the IT/Admins don't have to be responsible for physical
management of servers, they're responsible for everything else,
including keeping the servers updated and secured.

### PaaS Offerings

Now, maybe you're not such a DIY-er or or you just don't have the time
to do it all from scratch. In that case, you might buy a blank white
cake along with some tins of frosting and only do the decoration
yourself.

This is like a PaaS solution. In a PaaS solution, you're not managing
the servers, but instead are interacting with a cloud service via an API
specific to that service. There are a handful of PaaS services that come
up a lot in data science contexts.

One PaaS service that already came up in the book is *blob (Binary Large
Object)* storage. Blob storage allows you to store individual objects
somewhere and recall them to any other machine that has access to the
blob store. The major blob stores are *AWS Simple Storage Service (S3)*,
*Azure Blob Storage*, and *Google Cloud Storage*.

You're also likely to make use of cloud-based database, data lake, and
data warehouse offerings. There are numerous different offerings, and
the ones that you use will depend a lot on your use case and your
organization. The ones I've seen used most frequently are *RDS* or
*Redshift* from AWS, *Azure Database*, and *Google BigQuery.* This
category also includes a number of offerings from outside the big three,
most notably *Snowflake* and *Databricks*.

Depending on your organization, you may also use services that run APIs
or applications from containers or machine images like AWS's *ECS
(Elastic Container Service)*, *Elastic Beanstalk*, or *Lambda*, Azure's
*Container Apps* or *Functions*, or GCP's *App Engine* or *Cloud
Functions*.

Increasingly, organizations are turning to *Kubernetes* as a way to host
services. (More on that in [Chapter @sec-ent-servers].) Most
organizations who do so use a cloud provider's kubernetes cluster as a
service: AWS's *EKS (Elastic Kubernetes Service)* or *Fargate*, Azure's
*AKS (Azure Kubernetes Service)*, or GCP's *GKE (Google Kubernetes
Engine)*.

From a data science perspective, a PaaS setup might look like hosting a
JupyterHub, RStudio, or Posit implementation in EKS, or running an ML
API in Lambda.

In general PaaS solutions abstract away the management of actual
servers. You're usually managing some entity one level up from that. For
example, if you want to scale to more servers, there's usually just an
"add more servers" button for a PaaS offering, where you'd have to
configure it all yourself in an IaaS context.

### SaaS Offerings

Now, if you're really not a DIY-er, you might just buy a cake for your
friend. This is like a SaaS solution, which gives you immediate access
to the end-user application. IT/Admin configuration is generally limited
to hooking up integrations, most often authentication and/or data
sources.

You're already used to consumer SaaS software like Gmail, Slack, and
Office365. Depending on your organization, you might also use a SaaS
data science offering like AWS's *SageMaker*, Azure's *Azure ML*, or
GCP's *Vertex AI* or *Cloud Workstations*.

In my opinion, the difference PaaS and SaaS slippery. For example,
depending on your perspective, Databricks and Snowflake could be a SaaS
offering and as a data scientist, SageMaker would be a platform for you
to do your work.

Relative to other offerings, SaaS are obviously much simpler to get
started with. The tradeoff is that they are generally more expensive and
you're at the mercy of the provider for configuration and upgrades.

### Common Considerations

Irrespective of the particular services you are renting, there are a few
common things you'll have to manage.

You'll definitely have to interact with a service that dictates who can
do what. In order to do this, AWS has *IAM (Identity and Access
Management)*, GCP has *Identity Access Management*, and Azure has
*Microsoft Entra ID*, which was called *Azure Active Directory* until
the summer of 2023. You may additionally use a SaaS identity management
solution like Okta or OneLogin.

Additionally, some cloud services are geographically specific. Each of
the cloud providers has split the world into a number of geographic
areas, which they all call *regions*. Regions are subdivided into
*availability zones* (AZs).

Some services are region-specific and can only interact with other
services in that region by default. If you're doing things yourself, I
recommend just choosing the region where you live and putting everything
there. It's worth noting that costs and service availability does vary
somewhat across region.

AZs are subdivisions of regions that are designed to be independent.
Some organizations want to run services that span multiple availability
zones to provide protection against outages in any particular geography.
If you're running something sophisticated enough to need multi-AZ usage,
you should really be working with a professional IT/Admin.

## Comprehension Questions

1.  What are two reasons you should consider going to the cloud? What's
    one reason you shouldn't?
2.  What is the difference between PaaS, IaaS, and SaaS? What's an
    example of each that you're familiar with?
3.  What are the names for AWS's services for: renting a server, file
    system storage, blob storage

## Introduction to Labs

Welcome to the lab!

The point of these exercises is to get you hands on with running servers
and get you practicing the things you're learning in the rest of the
book.

If you walk through the labs sequentially, you'll end up with a working
data science workbench. It won't suffice for any enterprise-level
requirements, but it'll be secure enough for a hobby project or even a
small team.

For this lab, we're going to use services from AWS, as they're the
biggest cloud provider and the one you're most likely to run into in the
real world. Because we'll be mostly using IaaS services, there are very
close analogs from Azure and GCP should you want to use one of those
services.

## Lab: Getting started with AWS

In this first lab, we're going to get you up and running with an AWS
account and show you how to manage, start, and stop EC2 instances in
AWS.

The server we'll stand up will be from AWS's *free tier* -- so there
will be no cost involved as long as you haven't used up all your AWS
free tier credits before now.

::: callout-tip
Throughout the labs, I'll suggest you name things in certain ways. You
can do what you want, but I'll be consistent with those names, so you
can copy commands straight from the book if you use the same name.

If you want to follow along in that way, start by creating a standalone
directory for this lab, named `do4ds-lab`.
:::

### Step 1: Login to the AWS Console

We're going to start by logging into AWS at <https://aws.amazon.com>.

::: callout-note
An AWS account is separate from an Amazon account for ordering stuff
online and watching movies. You'll have to create one if you've never
used AWS before.
:::

Once you've logged in, you'll be confronted by the AWS console. There
are a ton of things here. Poke around if you want and then continue when
you're ready.

### Step 2: Stand up an EC2 instance

There are five attributes about your EC2 instance you'll want to
configure. If it's not mentioned here, just stick with the defaults for
now.

In particular, just stick with the default *Security Group*. We'll get
into what they are and how to configure them later.

#### Name + Tags

Instance *name and tags* are just human-readable labels so you can
remember what they're for. They aren't required, but I'd recommend you
name the server something like `do4ds-lab` in case you stand up others
later.

If you're doing this at work, there may be tagging policies so that the
IT/Admin team can figure out who servers belong to and what they're for
later.

#### Image

An *image* is a snapshot of a system and serves as the starting point
for your server. AWS's are called *AMIs* (*Amazon Machine Images*). They
range from free images of bare operating system to paid images that come
bundled with software you might want.

Choose an AMI that's just the newest LTS Ubuntu operating system. As of
this writing, that's 22.04. It should say *free tier eligible*.

#### Instance Type

The *instance type* identifies the capability of the machine you're
renting. An instance type is made up of a *family* and a *size*. The
family is the category of server and are denoted by letters and numbers,
so there are T2s and T3s, C4s, R5s, and many more.

Within each family, there are different sizes. Possible sizes vary by
the family, but generally range from *nano* to multiples of *xlarge*
like *24.xlarge*.

For now, I'd recommend you get the largest server that is free tier
eligible. As of this writing, that's a *t2.micro* with 1 CPU and 1 Gb of
memory.

::: callout-note
## Server sizing for the lab

A *t2.micro* with 1 CPU and 1 Gb of memory is a very small server. For
example, your laptop probably has at least 8 CPUs and 16 Gb of memory.

If all you're doing is walking through the lab, it should be sufficient,
but if you actually want to do any data science work, you'll need a
substantially larger server.

Luckily, it's easy to upgrade cloud server sizes later. More on how, as
well as advice on sizing servers for real data science work in [Chapter
@sec-scale].
:::

#### Keypair

The *keypair* is the skeleton key to your server. We'll get more into
how to use and configure it in [Chapter @sec-cmd-line]. For now, create
a new keypair. I'd recommend naming it `do4ds-lab-key`. Download the
`pem` version and put it in your `do4ds-lab` directory.

#### Storage

Bump up the storage to as much as you can get under the free tier,
because why not? As of this writing, that's 30 Gb.

### Step 3: Start the Server

If you followed these instructions, you should now be looking at a
summary that lists the operating system, server type, firewall, and
storage. Go ahead an launch your instance.

If you go back to the EC2 page and click on `Instances` you can see your
instance as it comes up. When it's up, it will transition to
`State: Running`.

### Optional: Stop the Server

Whenever you're stopping for the day, you may want to suspend your
server so you're not paying for it overnight or using up your free tier
hours. You can suspend an instance in the state it's in so it can be
restarted later.

If you're storing a lot of data with your instance, it may not be free,
but it is quite cheap. In the free tier, a suspended instance should be
free for some time.

Whenever you want to suspend your instance, go to the EC2 page for your
server. Under the `Instance State` drop down in the upper right, choose
`Stop Instance`.

After a couple minutes the instance will stop. Before you come back to
the next lab, you'll need to start the instance back up so it's ready to
go.

::: callout-note
If you stop your server, the server address will change and you'll have
to use the new one. In [Chapter @sec-dns], we'll get into getting a
stable IP address for the server.
:::

If you want to completely delete the instance at any point, you can
choose to `Terminate Instance` from that same `Instance State` dropdown.

## Lab: Put the penguins data and model in S3

Whether or not you're hosting your own server, most data scientists
working at an organization that uses AWS will run into S3, AWS's blob
store.

One really common thing to store in S3 is an ML model. So we're going to
store the mass prediction model we created in Chapters [@sec-proj-arch]
and [@sec-data-access] in an S3 bucket.

### Step 1: Create an S3 bucket

To start off with, you'll have to create a bucket, most commonly from
the AWS console. I'm naming mine `do4ds-lab`.

### Step 2: Push the model to S3 on build

The first thing you'll have to do is configure the model building code
to push the model into S3, rather that just saving it locally. Using
`{vetiver}` makes it easy to push a model to `s3` just by changing the
board type to `board_s3`.

It'll look something like this.

``` {.python include="../../_labs/model/model-vetiver-s3.qmd" filename="model.qmd" start-line="64" end-line="68"}
```

Under the hood, `{vetier}` is making use of standard R and Python
tooling to access an S3 bucket. If you want to interact with AWS
services or S3 from within Python or R, the most common ways are via
Python's `{boto3}` package or R's `{paws}` and `{aws.s3}`. You can also
interact directly from the command line using the AWS CLI.

Regardless of what tooling you're using, you'll generally configure your
credentials in three environment variables -- `AWS_ACCESS_KEY_ID`,
`AWS_SECRET_ACCESS_KEY`, and `AWS_REGION`. You can get the access key
and secret access key from the AWS console and you should know the
region.

As always, when you're developing in Python or R, I'd recommend putting
these into a `.env` or a `config.yml` file and loading them from there.

When you access and S3 bucket from an EC2 instance, you can also
configure an *instance profile* using IAM, so the entire EC2 instance
has access to the S3 bucket without needing credentials.

### Step 3: Pull the model from S3 for the API

You'll also have to configure the API to load the model from the S3
bucket. Luckily, this is very easy. Just update the script you used to
build your `Dockerfile` so it pulls from the pin in the S3 bucket rather
than the local folder.

Now, the script to build the `Dockerfile` looks like this:

``` {.python include="../../_labs/docker/docker-s3/build-docker-s3.qmd" start_line="11" end_line="19"}
```

### Step 4: Configure GitHub Actions to push on build

Now, we want GitHub Actions to push both the data and the model into the
bucket every time they're updated. And we want our code to pull from the
bucket when it needs the model or data.

In order to push into the bucket, you'll need to configure the GitHub
Action to do that. So in the `Render and Publish` step, we're going to
declare those variables as well as environment variables.

Once you're done, that section of the `publish.yml` should look
something like this.

``` {.yaml include="../../_labs/gha/publish-s3.yml" filename=".github/workflows/publish.yml" start_line="45" end_line="49"}
```

Now, unlike the `GITHUB_TOKEN` secret, which GitHub Actions
automatically provides to itself, we'll have to provide these secrets to
the GitHub interface.

### Lab Extensions

You might also want to put the actual data you're using into S3. This
can be a great way to separate the data from the project, as recommended
in [Chapter @sec-proj-arch].

Putting the data in S3 is such a common pattern that DuckDB allows you
to directly interface with parquet files stored in S3.
