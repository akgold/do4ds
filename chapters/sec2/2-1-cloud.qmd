# The Cloud {#sec-cloud}

These days, almost every organization I talk to is running their servers
in *the cloud*, or is in the midst of a transition to the cloud.

Much like trying to pin down a, you know, trying to put a finger on
exactly what the cloud is can be a little difficult. Especially in the
beginning of cloud-migration, there was so much marketing fluff around
the cloud that it felt about as touchable as a fluffy white
conglomeration of water vapor in the stratosphere.

But in the last decade or so, the cloud has become the primary place
people do things with servers. It's very real, and really important to
understand if you want to do work in a centralized way.

It used to be the case that the only way to have a server for your
organization was to buy a server and hire someone to physically install
and maintain the server racks you needed to run your company's
workloads. This is called running the servers *on-prem* (short for
premises). There are some organizations, especially those with highly
sensitive data that still run on-prem servers.

Around the year 2000, the Amazon.com team centralized their IT platform
across the company so instead of each team running and hosting its own
servers, they would get them from a centralized platform.

Over the next few years, Amazon execs realized that just like this
service was valuable to other teams at Amazon, they could rent server
capacity to other organizations. Amazon launched Amazon Web Services
(AWS) in 2006, which was the first cloud service to get serious
commercial traction.

Since then, AWS has been joined by Microsoft Azure and Google Cloud
Platform (GCP) as the *big three* or the *hyperscalers*, which
collectively run nearly 2/3 of the global cloud
marketplace.[^2-1-cloud-1] AWS is by far the biggest of the three,
though Azure has been catching up more recently.

[^2-1-cloud-1]: https://www.statista.com/chart/18819/worldwide-market-share-of-leading-cloud-infrastructure-service-providers/

But the cloud providers didn't stop at just renting server capacity to
other organizations. Instead, they started building layers and layers of
services on top of the immense server farms they were now running.

## Real (and fake) cloud benefits

The hype around the cloud came with several purported benefits, some of
which are true.

There are two main benefits of using the cloud.

The first -- and most important -- reason is flexibility. In an
on-prem-only world, your organization has to buy and host all the server
capacity it needs. Adding capacity requires requisitioning a new server,
ordering it from a hardware manufacturer, and installing it in a server
rack.

In a world with cloud resources, you can have a new server up or rescale
an existing one in minutes or even seconds. This is an incredible
ability for a small organization with fewer dedicated IT/Admin
resources, a quickly-growing organization that can't accurately predict
how much capacity it'll need in a few months, or an organization that
wants to run temporary workloads without buying permanent capacity for
it.

The second reason is about IT/Admin scope and focus. For most
organizations, managing physical servers isn't part of their core
competency and so many would prefer to outsource that work. In today's
world, all you need to do is feel comfortable working on the command
line -- and you can get a server of your own. And as cloud services get
more and more robust, you can outsource these things to a greater
extent.

It's also worth considering the incentives of IT/Admin teams and
professionals. IT/Admin teams tend to be stretched really thin, so the
chance to offload a major component of their work is really appealing.
Moreover, as IT/Admin professionals have observed the world moving in a
cloud-first direction, the resume-building incentives of individual
professionals have moved towards the clouds.

For an IT/Admin, it's way more valuable to say that I'm an expert at
working with cloud-based Kubernetes clusters than saying that I'm really
good at managing physical servers.

But then there's the fake benefit of the cloud -- cost savings.

It's hard to prove that any particular organization lost money in a
cloud migration, but there's a reason why AWS was only 13% of Amazon's
revenue in 2021, but a whopping 74% of the company's profits for that
year.[^2-1-cloud-2]

[^2-1-cloud-2]: https://www.visualcapitalist.com/aws-powering-the-internet-and-amazons-profits/

Many organizations claimed that moving to the cloud was going to reduce
server maintenance costs. Often, the explanation was that cloud
flexibility would enable dynamic autoscaling of server capacity. So even
if they paid a little more per hour, they were going to save so much by
turning capacity way down at night that it'd work out in their favor.

It turns out that making this work smoothly is extremely hard and only
the most sophisticated IT/Admin organizations have achieved anything
close to this goal. In fact, there's a small consulting industry solely
focused on helping organizations save money on AWS bills.

Some large organizations with stable workloads have actually started
doing *cloud repatriations* -- bringing workloads back on-prem from the
cloud for significant cost savings. An [a16z
study](https://a16z.com/2021/05/27/cost-of-cloud-paradox-market-cap-cloud-lifecycle-scale-growth-repatriation-optimization/)
found that total cost of ownership (including staffing) after
repatriation could be 1/3 to 1/2 the cost of using a cloud
provider.[^2-1-cloud-3]

[^2-1-cloud-3]: https://a16z.com/2021/05/27/cost-of-cloud-paradox-market-cap-cloud-lifecycle-scale-growth-repatriation-optimization/

Still, moving to the cloud is a good idea for many organizations. The
higher cash costs of cloud services is more than offset by letting the
organization iterate faster on the things it actually cares about.

As a data scientist you may want to run your own server just for
fun.[^2-1-cloud-4] That said, if you're trying to spend more time
getting important things done and less just playing, getting a server
from the cloud is the way to go.

[^2-1-cloud-4]: You're in good company, I once set up an old laptop as a
    Ubuntu server.

## The flavors of cloud services

The first service the cloud offered was to rent a server. Over time,
they have added layers and layers of services on top of that basic
premise. Having some basic understanding of the different layers of
services makes it easier to talk about cloud offerings.

In general, there are three layers people talk about -- *software as a
service (SaaS)*, *platform as a service (PaaS)*, and *infrastructure as
a service* (*IaaS*, pronounced eye-azz).

To make these different layers clear, let's talk about a layered object
you're familiar with -- a birthday cake.

Let's say you're planning to bake and decorate a beautiful layer cake
for your friend's birthday. It's going to have white frosting and it's
going to say "Happy Birthday!" in teal with giant lavender frosting
rosettes.

The first option would be to buy ingredients and bake the cake from
scratch. This is like IaaS. In an IaaS solution, you just rent
infrastructure from the cloud provider and put it together yourself. You
decide exactly what individual services you need -- a server here, a
database there, and put all the pieces together yourself. Most of the
services you'll see in the rest of this chapter are IaaS.

The second option would be to buy a cake mix of some sort -- then it's
less likely you'll get the wrong ingredients or end up with unnecessary
leftovers. This is like a PaaS solution. In a PaaS solution, you bring
some sort of entity, an application or container or machine image, make
some declaration of how you want it to work and the cloud service takes
care of the underlying servers.

The last option would be to buy a pre-made blank cake and just do the
decoration. This is a SaaS solution. In a SaaS solution, you get
immediate access to the end-user application with minimal IT/Admin
configuration. Gmail, Slack, and Office365 are examples of SaaS
services.

You may hear about PaaS or SaaS offerings described as "going
serverless". The important thing to understand about serverless
computing is that there is no such thing as serverless computing. It's a
marketing term meant to convey that **you** don't have to manage the
servers. The cloud provider manages them for you, but they're still
there.

In my opinion, the difference between IaaS, PaaS, and SaaS is slippery,
and debating over whether this service is one or the other is a road to
nowhere. The real question is what does the IT/Admin have to manage.

For an IaaS implementation, they're directly managing servers and other
entities like databases and network devices. For PaaS, they're managing
an application -- often packaged into a container these days and the
cloud provider takes care of everything underneath. And for SaaS,
they're just managing an integration, which usually means just hooking
up data connectors and authentication to the SaaS offering.

## Services to know

At the end of the day, cloud services, especially IaaS ones, are just
$\text{X as a service}$. When you're trying to evaluate whether you want
to use a cloud service, the first question is, "What is the $\text{X}$
for this service?"

Unfortunately, cloud marketing materials usually orient around the
purported benefits of the service, making it difficult to decode what
$\text{X}$ actually is. Moreover, cloud providers often have several
different services for similar purposes and it can be hard to concretely
tell the difference.

It's helpful to keep in mind that you're fundamentally renting servers,
along with the networking and storage you need. Every other service is
just a combination of those things that comes with software
pre-installed or configured to make your life easier.[^2-1-cloud-5]

[^2-1-cloud-5]: There are also some wild services that do specific
    things, like let you rent you satellite ground station
    infrastructure or do Internet of Things (IoT) workloads. Those
    services are really cool, but so far outside the scope of this book
    that I'm fine with talking like they don't exist.

It's most likely that you're working in one of the big three clouds. In
this book, the labs are going to be exclusively in AWS, but I'll address
Azure and GCP services in this chapter in case you're running there.

::: callout-note
## Big Three Service Naming

AWS tends to name their services with cutesy names that have, at best, a
tangential relationship to the task at hand. Azure and GCP on the other
hand tend to name their offerings pretty literally.

This makes AWS names a little harder to learn, but much easier to recall
once you've learned them. In this section -- contrary to standard
practice -- I'm going to use the common abbreviated names for AWS
services and put the full name in parentheses as these are just trivia.
:::

There are a handful of basic cloud services whose names you should just
know because they'll probably come up constantly.

The most basic cloud service is to rent a server. You can rent a server
from any of the big three with *EC2 (Elastic Cloud Compute)* from AWS,
*Azure VMs* from Azure, and *Google Compute Engine Instances* from GCP.

When you buy a personal computer or phone, it comes with a certain CPU,
RAM, and hard disk (storage). In the cloud, you rent a compute instance
with a certain amount of CPU and RAM and rent storage separately.

File storage is the type of storage that's attached to your server, like
the hard drive on your laptop. In AWS, you've got *EBS (Elastic Block
Store)*. Azure has *Azure Managed Disk*, and GCP has *Google Persistent
Disk*.

The major cloud providers all also have *blob (Binary Large Object)*
storage. Blob storage allows you to store individual objects somewhere
and recall them to any other machine that has access to the blob store.
The major blob stores are *AWS Simple Storage Service (S3)*, *Azure Blob
Storage*, and *Google Cloud Storage*.

There are also two important networking services for each of the clouds
-- a way to make a private network and a way to do DNS routing. If you
don't know what these mean, there will be a lot more detail in [Chapter
@sec-basic-networks].

For now, it's enough to know that private networking is done in AWS's
*VPC (Virtual Private Cloud)*, Azure's *Virtual Network*, and GCP's
*Virtual Private Cloud*. DNS is done in AWS's *Route 53*, *Azure* *DNS*,
and *Google Cloud DNS*.

Once you've got all this stuff up and running, you need to make sure
that permissions are set in the right way. AWS has *IAM (Identity and
Access Management)*, GCP has *Identity Access Management*, and Azure has
*Microsoft Entra ID*, which was called *Azure Active Directory* until
the summer of 2023.

In addition to these basic cloud services, there are also a number of
cloud services that seem to come up particularly often in the context of
data science work, which I've listed in the table below. It's worth
noting that I'm just trying to provide general categories of services.
There are substantial differences in the details.

| Service                     | AWS                                                  | Azure                          | GCP                            |
|------------------|-------------------|------------------|------------------|
| Kubernetes cluster          | EKS (Elastic Kubernetes Service) or Fargate          | AKS (Azure Kubernetes Service) | GKE (Google Kubernetes Engine) |
| Run a container/application | ECS (Elastic Container Service) or Elastic Beanstalk | Azure Container Apps           | Google App Engine              |
| Run an API                  | Lambda                                               | Azure Functions                | Google Cloud Functions         |
| Database                    | RDS or Redshift[^2-1-cloud-6]                        | Azure Database                 | Google Cloud SQL               |
| ML Platform                 | SageMaker                                            | Azure ML                       | Vertex AI                      |

[^2-1-cloud-6]: There are many others. These are just the most popular
    I've seen for data science use cases.

One other important detail about cloud services is that some are
geographically specific. For example, AWS has split the world up into a
number of regions. Some resources you create are region-specific and
some are not. When you are creating region-specific resources, I
recommend just choosing the region where you live and putting everything
there. It's worth noting that costs and service availability does vary
somewhat across region.

## Comprehension Questions

1.  What are two reasons you should consider going to the cloud? What's
    one reason you shouldn't?
2.  What is the difference between PaaS, IaaS, and SaaS? What's an
    example of each that you're familiar with?
3.  What are the names for AWS's services for: renting a server, file
    system storage, blob storage

## Lab: Getting started with AWS

Welcome to the lab!

The point of these exercises is to get you hands on with running servers
and get you practicing the things you're learning in the rest of the
book.

If you walk through the labs sequentially, you'll end up with a working
data science workbench. It won't suffice for any enterprise-level
requirements, but it'll be secure enough for a hobby project or even a
small team.

These days most people are using servers from a cloud provider. We're
going to use AWS, as they're by far the biggest cloud provider and the
one you're most likely to run into in the real world.

In the lab for this chapter, we're going to get you up and running with
an AWS account and show you how to manage, start, and stop EC2 instances
in AWS.

The server we'll stand up will be from AWS's *free tier* -- so there
will be no cost involved as long as you haven't used up all your AWS
free tier credits before now.

For AWS things, I'm going to tell you what to do, but not the exact
buttons, as AWS frequently changes the interface and this will almost
certainly be out-of-date by the time you read it.

::: callout-tip
Throughout the labs, I'll suggest you name things in certain ways. You
can do what you want, but I'll be consistent with those names, so you
can copy commands straight from the book if you use the same name.

I'd start by creating a standalone directory for this lab, named
`do4ds-lab`.
:::

### Step 1: Login to the AWS Console

We're going to start by logging into AWS.

::: callout-note
An AWS account is separate from an Amazon account for ordering stuff
online and watching movies. You'll have to create one if you've never
used AWS before.
:::

Start by signing in at <https://aws.amazon.com>.

Once you've logged in, you'll be confronted by the AWS console. There
are a ton of things here. Poke around if you want and then continue when
you're ready.

### Step 2: Stand up an EC2 instance

There are five attributes about your EC2 instance you'll want to
configure. If I don't mention it here, you should just stick with the
defaults for now.

In particular, just stick with the default *Security Group*. We'll get
into what they are and how to configure them later.

#### Name + Tags

Instance *name and tags* are just human-readable labels so you can
remember what they're for. They aren't required, but I'd recommend you
name the server something like `do4ds-lab` in case you stand up others
later.

If you're doing this at work, there may be tagging policies so that the
IT/Admin team can figure out who servers belong to and what they're for
later.

#### Image

An *image* is a snapshot of a system and serves as the starting point
for your server. AWS's are *Amazon Machine Images* or *AMIs*. And they
range from free images of bare operating system to paid images that come
bundled with software you might want.

Choose an AMI that's just the newest LTS Ubuntu operating system. As of
this writing, that's 22.04. It should say *free tier eligible*.

#### Instance Type

The *instance type* identifies the capability of the machine you're
renting. An instance type is made up of a *family* and a *size*. The
family is the category of server. Families are denoted by letters and
numbers, so there are T2s and T3s, C4s, R5s, and many more.

Within each family, there are different sizes. Possible sizes vary by
the family, but generally range from *nano* to multiples of *xlarge*
like *24.xlarge*.

For now, I'd recommend you get the largest server that is free tier
eligible. As of this writing, that's a *t2.micro* with 1 CPU and 1 Gb of
memory.

::: callout-note
## Server sizing for the lab

A *t2.micro* with 1 CPU and 1 Gb of memory is a very small server. For
example, your laptop probably has at least 8 CPUs and 16 Gb of memory.

If all you're doing is walking through the lab, it should be sufficient,
but if you actually want to do any data science work, you'll need a
substantially larger server.

Luckily, it's easy to upgrade cloud server sizes later. More on how, as
well as advice on sizing servers for real data science work in [Chapter
@sec-scale].
:::

#### Keypair

The *keypair* is the skeleton key to your server. We'll get more into
how to use and configure it in [Chapter @sec-cmd-line]. For now, create
a new keypair. I'd recommend naming it `do4ds-lab-key`. Download the
`pem` version and put it in your `do4ds-lab` directory.

#### Storage

Bump up the storage to as much as you can get under the free tier
because why not? As of this writing, that's 30 Gb.

### Step 3: Start the Server

If you followed these instructions, you should now be looking at a
summary that lists the operating system, server type, firewall, and
storage. Go ahead an launch your instance.

If you go back to the EC2 page and click on `Instances` you can see your
instance as it comes up. When it's up, it will transition to
`State: Running`.

### Optional: Stop the Server

Whenever you're stopping for the day, you may want to suspend your
server so you're not paying for it overnight or using up your free tier
hours. You can suspend an instance in the state it's in so it can be
restarted later.

If you're storing a lot of data with your instance, it may not be free
-- but it is quite cheap. In the free tier, a suspended instance should
be free for some time.

Whenever you want to suspend your instance, go to the EC2 page for your
server. Under the `Instance State` drop down in the upper right, choose
`Stop Instance`.

After a couple minutes the instance will stop. Before you come back to
the next lab, you'll need to start the instance back up so it's ready to
go.

::: callout-note
If you stop your server, the server address will change and you'll have
to use the new one. In [Chapter @sec-dns], we'll get into getting a
stable IP address and URL for the server.
:::

If you want to completely delete the instance at any point, you can
choose to `Terminate Instance` from that same `Instance State` dropdown.

### Step 4: Put the penguins data and model in S3

We're going to come back to how to use the server in future labs. Aside
from EC2, S3 is the most common service data scientists use all the time
from AWS. We're going to store the penguin database and mass prediction
model we created in Chapters [@sec-proj-arch] and [@sec-data-access] in
an S3 bucket.

First, you'll need to create an S3 bucket in the AWS console. Give it a
name that's descriptive and memorable -- it will also have to be unique.
I named mine `do4ds-lab`.

Now, let's go back to the code to write our model to a board. Luckily,
this is quite simple if you're using `{vetiver}`. All you have to do is
change the board type to `board_s3`.

So your code to write the model to the board will look something like

``` {.python include="../../_labs/model/model-vetiver-s3.qmd" filename="model.qmd" start-line="64" end-line="68"}
```

Now, if you try this line in your code, it won't work. Why? Because of
authentication. The bucket we're using isn't public and AWS needs to
know that we're actually allowed to access it. We'll provide our AWS
secrets via environment variables.

The 3 environment variables are `AWS_ACCESS_KEY_ID`,
`AWS_SECRET_ACCESS_KEY`, and `AWS_REGION`. You can get the access key
and secret access key from the AWS console, and you should know the
region.

Then, create a `.env` or `config.yml` file and load it into your script.

Your secrets file should look look something like this.[^2-1-cloud-7]

[^2-1-cloud-7]: Not my real access key or secret access key. Never share
    them.

``` {.yaml filename=".env"}
AWS_ACCESS_KEY_ID=AJAASJKD88ALLKAIS8A
AWS_SECRET_ACCESS_KEY=0JAJSduasdjkASDLISjkasd8AD78
AWS_REGION=us-east-1
```

If you can't remember how to make use of environment secrets, go back to
[Chapter @sec-data-access] for details. Once the variables are loaded
into the environment, the call to `board_s3` should just work.

We have one more step to make this work so that the model can also be
written from GitHub actions. GitHub Actions needs to know how to access
those environment variables too.

So in the `Render and Publish` step, we're going to declare those
variables as well as environment variables.

Once you're done, that section of the `publish.yml` should look
something like this.

``` {.yaml include="../../_labs/gha/publish-s3.yml" filename=".github/workflows/publish.yml" start_line="45" end_line="49"}
```

Now, unlike the `GITHUB_TOKEN` secret, which GitHub Actions
automatically provides to itself, we'll have to provide these secrets to
the GitHub interface.

The other thing you'll need to do is to update the API to use the S3
pin. Luckily, this is very easy. Just update the script you used to
build your `Dockerfile` so it pulls from the pin in the S3 bucket rather
than the local folder.

Now, the script to build the `Dockerfile` looks like this:

``` {.python include="../../_labs/docker/docker-s3/build-docker-s3.qmd" start_line="11" end_line="19"}
```

There are a variety of ways to put the data into S3 as well. I would
recommend doing this if you are going to be working with data that will
update. In fact, DuckDB allows you to directly interface with parquet
files on S3.
