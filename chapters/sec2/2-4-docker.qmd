# Docker for Data Science {#sec-docker}

In [Chapter @sec-env-as-code], we considered the "runs on my machine"
problem. In that chapter, you learned about `{renv}` and `{venv}` for
reproducing and isolating the package environment where your R or Python
code runs.

And that's true -- but those same reproducibility and isolation needs
are present all the way down the data science reproducibility stack.

For example, what if you have content that needs a particular version of
R or Python or two pieces of content that rely on different versions of
the same system library, or run on different operating systems?

Let me introduce you to my friend Docker.

TODO: rework this image to show the different layers

![](images-docker/docker-usage.png)

**Data Science Reproducibility Stack**

+-----------------------+----------------------------------------------+
| Layer                 | Contents                                     |
+=======================+==============================================+
| Package               | R + Python Packages                          |
+-----------------------+----------------------------------------------+
| System                | R + Python Language Versions                 |
|                       |                                              |
|                       | Other System Libraries                       |
|                       |                                              |
|                       | Operating System                             |
+-----------------------+----------------------------------------------+
| Hardware              | Virtual Hardware                             |
|                       |                                              |
|                       | Physical Hardware                            |
+-----------------------+----------------------------------------------+

Docker is the most popular tool to reproduce and isolate the *System*
level of the reproducibility stack. If you put your project in a Docker
container and it runs today, you can be reasonably confident that the
container itself can run almost anywhere else.[^2-4-docker-1]

[^2-4-docker-1]: Usually. Users of Apple's M1 and M2 chips have run into
    many issues running containers due to differences in hardware
    architecture.

It's worth noting that in some environments -- especially
highly-regulated ones -- a Docker container may not be a sufficient
level of reproducibility. Differences between machines at the physical
hardware level could potentially mean that numeric solutions could
differ across machines, even with the same container. You probably know
if you're in this kind of environment and you have to maintain physical
machines.

That's a great thing for reproducibility. It's also really nice if
you're a team that manages services another team builds. If the build
team can run their containers, you've meaningfully reduced how much can
go wrong when you go to prod.

There are two basic things you might want to with Docker containers --
use them to share a development environment with collaborators or use
them as a tool for reproducibility across time and system.

I am not a big fan of sharing Docker containers as a way to collaborate
with others on data science projects. It requires everyone to have a
pretty high level of understanding of Docker. I generally am a bigger
fan of moving data science work [to a server](@sec-servers). If you're
on the same server as your collaborator, you know the *System Level*
matches and you just need to worry about the package environment for any
given project.

For that reason, I'm not going to explicitly address how to use
containers for collaboration, but this chapter will give you all the
tools you need if you really want to go down that road.

Instead, we're going to stick with talking about how actual data
scientists would want to use Docker: to archive and share completed data
science assets. This chapter starts with a general intro to how to use
Docker for Data Science, dives deep into how to actually use Docker, and
closes out by taking the model prediction API from [Chapter @sec-apis]
and Docker-izes it.

## But what are containers?

Docker is the most popular open-source containerization platform. As a
rough mental model, you can think of a Docker container as a little
sub-computer running inside your computer or server.

Because they are so ubiquitous these days, some people mistakenly think
using Docker and DevOps best practices are synonymous. And it's true,
many DevOps best practices can be facilitated by Docker containers. But
using Docker containers are neither necessary or sufficient for
achieving a high level of reproducibility and isolation.

Docker is far from the only way to do this. Virtual machines and machine
images have been around since the 1960s, and other sorts of environment
management tools are still quite popular in certain
contexts.[^2-4-docker-2]

[^2-4-docker-2]: For example, Linux Environment Modules are quite
    popular in the Pharmaceutical industry, and Singularity containers
    are popular for High-Performance Computing (HPC).

But relative to those other systems, containers are more
system-agnostic, more lightweight, and are easier to run. And Docker
containers in particular are by far the most popular type of container.
So much so that for most purposes container is a synonym for Docker
container.

Docker containers have very high degrees of ephemerality and isolation
that make them extremely powerful, but also tricky.

*What happens in a Docker container stays in a Docker container.*

Docker containers are almost completely ephemeral. Whatever operations
happen inside a Docker container are restricted to that container.
That's awesome from the perspective of security and resource management.
It's also just nice because everything gets cleaned up after itself.

But it can also be a challenge -- when a container needs to access data
outside the container or someone else needs to get inside, it has to be
explicitly given to the container. That means anyone running a Docker
container has to develop a good mental model of the relationship of the
Docker container to the rest of your machine in order to be able to
develop effectively.

## Trying out Docker

Let's get started with an example that demonstrates the power of Docker
right off the bat.

::: callout-note
Before you start, make sure to install [Docker
Desktop](https://www.docker.com/products/docker-desktop) on your laptop.
:::

Open a terminal and put the following in:

```         
docker run --rm -d \
  -p 8080:8080 \
  --name penguin-model \
  alexkgold/penguin-model
```

Once you type in this command, it'll take a minute to pull, extract, and
start the container.

Once the container starts up, you'll see a long sequence of letters and
numbers. Now, navigate to `http://localhost:8080` in your browser (this
URL has to be exact!), and you should see the model API we built back in
the lab in [Chapter @sec-apis].

That was probably pretty uninspiring. It took a long time to download
and get started. In order to show the real power of Docker, let's now
kill the container with

```         
docker kill penguin-model
```

You can check that the container isn't running by trying to visit that
URL again. You'll get an error.

Now run the `docker run` command again.

This time is should be quick -- probably less than a second -- now that
you've got the container downloaded. THIS is the power of Docker.

------------------------------------------------------------------------

As you click around, seeing penguin stats and seeing plots, you might
notice that nothing is showing up on the command line...but what if I
want logs of what people are doing? Or I need to look at the app code?

You can get into the container to poke around using the command

```         
docker exec -it palmer-plumber /bin/bash
```

Once you're in, try `cat api/plumber.R` to look at the code of the
running API.

When you need to get out, you can leave by typing `exit`.

`docker exec` is a general purpose command for executing a command
inside a running container. The overwhelming majority of the time I use
it, it's to get a terminal inside a running container so I can poke
around.

You can spend a lot of time getting deep into why the command works, but
just memorizing (or, more likely, repeatedly googling)
`docker exec -it <container> /bin/bash` will get you pretty far.

::: callout-note
If you're used to running things on servers, you might be in the habit
of `SSH`-ing in, poking around, and fixing things that are broken. This
isn't great for a lot of reasons, but it's a huge anti-pattern in Docker
land.

Containers are *stateless* and *immutable*. This means that anything
that happens in the container stays in the container -- even when the
container goes away. If something goes wrong in your running container,
you may need to exec in to poke around, but you should fix it by
rebuilding and redeploying your image, not by changing the running
container.
:::

One nicety of Docker is that it gives you quick access to the most
common reason you'd probably exec into the container -- looking at logs.

After you've clicked around a little in the API, try running:

```         
docker logs palmer-plumber
```

We're done with this container now. Feel free to kill it before you move
on.

------------------------------------------------------------------------

Great! We've played around with this container pretty thoroughly.

Before we get into how this all works, let's try one more example.

Go back into your terminal and navigate to a directory you can play
around in (the `cd` command is your friend here, see [Chapter
@sec-cmd-line] if you're not familiar). Run the following in your
terminal:

```         
docker run \
-v ${PWD}:/project-out \
alexkgold/batch:0.1
```

It'll take a minute to download -- this container is about 600Mb. You
may need to grant the container access to a directory on your machine
when it runs. This container will take a few moments to run. If you go
to the directory in file browser, you should be able to open
`hello.html` in your web browser -- it should be a rendered version of a
Jupyter Notebook.

This notebook is just a very basic visualization, but you can see how
it's nice to be able to render a Jupyter Notebook locally without having
to worry about making sure you had any of the dependencies installed.
This is good both for running on demand, and also for archival purposes.

------------------------------------------------------------------------

Now that we've got Docker working for you, let's take a step back,
explain what we just did, and dive deeper into how this can be helpful.

Hopefully these two examples are exciting -- in the first, we got an
interactive web API running like a server on our laptop in just a few
seconds -- and without installing any of the packages or even a version
of R locally. In the second, we rendered a Jupyter Notebook using the
[quarto](https://quarto.org/) library -- again, without worrying about
downloading it locally.

## Container Lifecycle

Before we dig into the nitty-gritty of how that all worked -- and how
you might change it for your own purposes, let's spend just a minute
clarifying the lifecycle of a Docker container.

This image explains the different states a Docker container can be in,
and the commands you'll need to move them around.

![](images-docker/docker-lifecycle.png){width="561"}

A container starts its life as a *Dockerfile*. A Dockerfile is a set of
instructions for how to build a container. Dockerfiles are usually
stored in a git repository, just like any other code, and it's common to
build them on push via a CI/CD pipeline.[^2-4-docker-3]

[^2-4-docker-3]: Remember, we talked about them back in [Chapter
    @sec-code-promotion]!

A working Dockerfile gets built into a *Docker image* with the *build*
command. Images are immutable snapshots of the state of the container at
a given time.

It is possible to interactively build a container as you go and snapshot
to create an image, but for the purposes of reproducibility, it's
generally preferable to build the image from a Dockerfile, and adjust
the Dockerfile if you need to adjust the image.

Usually, the image is going to be the thing that you share with other
people, as it's the version of the container that's compiled and ready
to go.

Docker images can be shared directly like any other file, or via sharing
on an *image registry* via the *push* and *pull* commands.

If you're familiar with git, the mental model for Docker is quite
similar. There is a public Docker Hub you can use, and it's also
possible to run private image registries. Many organizations make use of
the *image registries as a service* offerings from cloud providers. The
big 3's are Amazon's Elastic Container Registry (ECR), Azure Container
Registry, and Google Container Registry.

Once you've got an image downloaded locally, you can run it with the
*run* command. Note that you generally don't have to pull before running
a container, as it will auto-pull if it's not available.

Now that you're all excited, let's dig in on how the `docker run`
command works, and the command line flags we used here, which are the
ones you'll use most often.

## Understanding `docker run`

At it's most basic, all you need to know is that you can run a Docker
image locally using the `docker run <name>` command. However, Docker
commands usually use a lot of command line flags -- so many that it's
easy to miss what the command actually is.

::: callout-note
A command line flag is an argument passed to a command line program.

There's a lot more about using command line tools in [Chapter
@sec-cmd-line].
:::

Let's pull apart the two commands we just used, which use the command
line flags you're most likely to need.

### Parsing container names

To start with, let's parse the name of the container. In this example,
you used two different container names -- `alexkgold/plumber` and
`alexkgold/batch:0.1`. All containers have an id, and they may also have
a tag. If you're using the public [DockerHub](https://hub.docker.com)
registry, like I am, container ids are of the form `<user>/<name>`. This
should look very familiar if you already use a git repository.

In addition to an id, containers can also have a tag. For example, for
the `alexkgold/batch` image, we specified a version: `0.1`. If you don't
specify a tag when pulling or pushing an image, you'll automatically
create or get `latest` -- the newest version of a container that was
pushed to the registry.

Users often create tags that are relevant to the container -- often
versions of the software contained within. For example, the
`rocker/r-ver` container, which is a container pre-built with a version
of R in it uses tags for the version of R.

All these examples use the public DockerHub. Many organizations use a
private image registry, in which case you can prefix the container name
with the URL of the registry.

### `docker run` flags

In this section we're going to go through the docker run flags we used
in quite a bit of detail.

::: callout-note
If you just want a quick reference later, there's a cheatsheet in
[Appendix @docker-cheat].
:::

Let's first look at how we ran the container with the plumber API in it.

For this container, we used the `--rm` flag, the `-d` flag, the `-p`
flag with the argument `8080:8080`, and the `--name` flag with the
argument `plumber-palmer`.

The `--rm` flag *removes* the container after it finishes running. This
is nice when you're just playing around with a container locally because
then you can use the same container name repeatedly, but it's a flag
you'll almost never use in production because it removes everything from
the container, including logs.

You can check this by running `docker kill palmer-plumber` to make sure
the container is down and then try to get to the logs with
`docker logs palmer-plumber`. But they don't exist because they got
cleaned up!

Feel free to try running to container without the `--rm` flag, playing
around, killing the container, and then looking at the logs. Before
you're able to bring back another container with the same name, you'll
have to remove the container with `docker rm palmer-plumber`.

The `-d` flag instructs the container to run in *detached* mode so the
container won't block the terminal session. You can feel free to run the
container attached -- but you'll have to quit the container by aborting
the command from inside the terminal (`Ctrl + c`), or opening another
terminal to `docker kill` the container.

The `-p` flag *publishes* a port from inside the container to the host
machine. So by specifying `-p 8080:8080`, we're taking whatever's
available on the port `8080` inside the container and making it
available at the same port on the `localhost` of the machine that's
hosting the container.

TODO: picture of ports

Port forwarding is always specified as `<host port>:<container port>`.
Try playing around with changing the values to make the API available on
a different port, perhaps `9876`. For a more in-depth treatment of
ports, see [Chapter @sec-basic-networks].

The `--name` flag gives our container a name. This is really just a
convenience so that you could do commands like `docker kill` in terms of
the container name, rather than the container ID, which will be
different for each person who runs the command.

In a lot of cases, you won't bother with a name for the container.

You can find container ID using the `docker ps` command to get the
*process status*. In the case below, I could control the container with
the name `palmer-plumber`, or with the container ID. You can abbreviate
container IDs as long as they're unique -- I tend to use the first three
characters.

```         
❯ docker ps                                                         [12:23:13]

CONTAINER ID   IMAGE               COMMAND                  CREATED          STATUS          PORTS                    NAMES

35bd54e44015   alexkgold/plumber   "R -e 'pr <- plumber…"   29 seconds ago   Up 28 seconds   0.0.0.0:8080->8080/tcp   palmer-plumber
```

------------------------------------------------------------------------

Now let's head over to the batch document rendering, where we only used
one command line flag `-v ${PWD}:/project-out`, short for *volume*. To
demonstrate what this argument does, navigate to a new directory on your
command line and re-run the container without the argument.

Wait...where'd my document go?

Remember -- containers are completely ephemeral. *What happens in the
container stays in the container*. This means that when my document is
rendered inside the container, it gets deleted when the container ends
its job.

But that's not what I wanted -- I wanted to get the output back out of
the container.

The solution -- making data outside the container available to the
container and vice-versa -- is accomplished by *mounting a volume* into
the container using the `-v` flag. Like with mounting a port, the syntax
is `-v <directory outside container>:<directory inside container>`.

![](images-docker/docker-on-host.png){width="2048"}

This is an essential concept to understand when working with containers.
Because containers are so ephemeral, volumes are the way to get anything
from your host machine in, and to persist anything that you want to
outlast the lifecycle of the container.

In this case, we actually used a variable `${PWD}`, which will be
evaluated to the current working directory to be the directory
`project-out` inside the container, so the rendered document can be
persisted after the container goes away.

## Building Data Science Containers with Dockerfiles

My recommendation for using containers for data science is to complete a
data science project and get everything working as if no containers will
be involved and then put all the immutable data inside the container.

This last bit is particularly important -- only *immutable* data should
go inside the container. As we discussed in [Chapter @sec-data-arch],
you don't want to have to rebuild your environment every time the data
changes. Instead, you want to provide a way for the container to pick up
the data or model from outside.

Likewise because containers are so ephemeral, you shouldn't count on
anything in there sticking around. If it's important, the container
should explicitly write it somewhere outside the container. That
includes the logs of the container itself -- if the container errors
out, it's gone and you'll want those logs to try to figure out what went
wrong.

In this pattern, you'll put your whole reproducibility stack inside the
container itself -- perhaps minus your data.

There are also several antipatterns that using a container could
facilitate.

The biggest reproducibility headache for most data scientists is
managing R and Python package environments. While you can just install a
bunch of packages into a container, save the container state, and move
on, this really isn't a good solution.

If you do this, you've got the last state of your environment saved, but
it's not really reproducible. If you come back next year and need to add
a new package, you'll have no way to do it without potentially breaking
the whole environment.

The obvious solution is to write down the steps for creating your Docker
container -- in a file called a Dockerfile. Here, it's tempting to
create a Dockerfile that looks like:

``` dockerfile
RUN /opt/R/4.1.0/bin/R install.packages(c("shiny", "dplyr"))
```

But this is also completely non-reproducible. Whenever you rebuild your
container, you'll install the newest versions of Shiny and Dplyr afresh,
potentially ruining the reproducibility of your code. For that reason,
the best move is to still use R- and Python-specific libraries for
capturing package state -- like `renv` and `rig` in R and `virtualenv` ,
`conda` , and `pyenv` in Python -- rather than relying on Docker for
that job. There's more on those topics in the [Chapter @sec-env-as-code]
on environments.

So far, we've just been running containers based on images I've already
prepared for you. Let's look at how those images were created so you can
try building your own.

A Dockerfile is just a set of instructions that you use to build a
Docker image. If you have a pretty good idea how to accomplish something
on a running machine, you shouldn't have too much trouble building a
Dockerfile to do the same, as long as you remember two things:

TODO: Image of build vs run time

1.  The difference between *build* time and *run* time. There are things
    that should happen at build time -- like setting up the versions of
    R and Python, copying in the code you'll run, and installing the
    system requirements. That's very different from the thing I want to
    have happen at run time -- rendering the notebook or running the
    API.
2.  Docker containers only have access to exactly the resources you
    provide to them at both build and runtime. That means that they
    won't have access to libraries or programs unless you give them
    access, and you also won't have access to files from your computer
    unless you make them available.

There are many different commands you can use inside a Dockerfile, but
with just a handful, you'll be able to build most images you might need.

Here are the important commands you'll need for getting everything you
need into your images.

-   `FROM` -- every container starts from a base image. In some cases,
    like in my Jupyter example, you might start with a bare bones
    container that's just the operating system (`ubuntu:20.04`). In
    other cases, like in my `shiny` example, you might start with a
    container that's almost there, and all you need to do is to copy in
    a file or two.

-   `RUN` -- run any command as if you were sitting at the command line
    inside the container. Just remember, if you're starting from a very
    basic container, you may need to make a command available before you
    can run it (like `wget` in my container below).

-   `COPY` -- copy a file from the host filesystem into the container.
    Note that the working directory for your Dockerfile will be whatever
    your working directory is when you run your build command.

One really nice thing about Docker containers is that they're built in
*layers*. Each command in the Dockerfile defines a new layer. If you
make changes below a given layer in your Dockerfile, rebuilding will be
easy, because Docker will only start rebuilding at the layer with
changes.

If you're mainly building containers for finished data science assets to
be re-run on demand, there's only one command you need:

-   `CMD` - Specifies what command to run inside the container's shell
    at runtime. This would be the same command you'd use to run your
    project from the command line.

If you do much digging, you'll probably run into the `ENTRYPOINT`
command, which can take a while to tell apart from `CMD`. If you're
building containers to run finished data science assets, you shouldn't
need `ENTRYPOINT`. If you're building containers to -- for example --
accept a different asset to run or allow for particular arguments,
you'll need to use `ENTRYPOINT` to specify the command that will always
run and `CMD` to specify the default arguments to `ENTRYPOINT`, which
can be overridden on the command line.[^2-4-docker-4]

[^2-4-docker-4]: This might sound backward -- the key is to realize that
    this is always how it works. The default `ENTRYPOINT` is
    `/bin/sh -c`, so `CMD` is always just providing arguments to the
    `ENTRYPOINT`.

Here's the Dockerfile I used to build the container for the Jupyter
Notebook rendering. Look through it. Can you understand what it's doing?

``` dockerfile
# syntax=docker/dockerfile:1
FROM ubuntu:20.04

# Copy external files
RUN mkdir -p /project/out/

COPY ./requirements.txt /project/
COPY ./hello.ipynb /project/

# Install system packages
RUN apt-get update && apt-get install -y \
  wget python3 python3-pip

# Install quarto CLI + clean up
RUN wget https://github.com/quarto-dev/quarto-cli/releases/download/v0.9.83/quarto-0.9.83-linux-amd64.deb
RUN dpkg -i ./quarto-0.9.83-linux-amd64.deb
RUN rm -f ./quarto-0.9.83-linux-amd64.deb

# Install Python requirements
RUN pip3 install -r /project/requirements.txt

# Render notebook
CMD cd /project && \
  quarto render ./hello.ipynb && \
  # Move output to correct directory
  # Needed because quarto requires relative paths in --output-dir: 
  # https://github.com/quarto-dev/quarto-cli/issues/362
  rm -rf /project-out/hello_files/ && \
  mkdir -p /project-out/hello_files && \
  mv ./hello_files/* /project-out/hello_files/ && \
  mv ./hello.html /project-out/
```

Once you've created your Dockerfile, you build it into an image using
`docker build -t <image name>`. You can then push that to DockerHub or
another registry using `docker push`.

## Comprehension Questions

1.  What does using a Docker container for a data science project make
    easier? What does it make harder?
2.  Draw a mental map of the relationship between the following:
    Dockerfile, Docker Image, Docker Registry, Docker Container
3.  When would you want to use each of the following flags for
    `docker run`? When wouldn't you?
    -   `-p`, `--name`, `-d`, `--rm`, `-v`
4.  What are the most important Dockerfile commands?

## Lab: Putting an API in a Container

Let's put our Penguin model prediction API from [Chapter @sec-apis] into
a container.

Again, Vetiver has some nice tooling to make this very easy to do. You
should follow the instructions on the `{vetiver}` website for how to
generate your Dockerfile.

One thing to note about this container -- it follows best practices for
how to put data (in this case the machine-learning model) into the
container. That means the model *isn't* built into the container.
Instead, the container knows how to fetch the model.

That means that you'll need to have the `model_board` object available.
Look back at [Lab 4](#lab4) if you've restarted your Python session
since you last ran the `{vetiver}` code.

I'm using this code to generate my Dockerfile

``` {.python include="../../_labs/docker/docker-local/build-docker-local.qmd" start-line="9" end-line="13"}
```

Once you've generated your Dockerfile, take a look at it. Here's the one
for my model:

``` {.Dockerfile include="../../_labs/docker/docker-local/docker/Dockerfile" filename="Dockerfile"}
```

This auto-generated Dockerfile is very nicely commented, so its easy to
follow.

Now build the container using `docker build -t penguin-model-local .`.

::: callout-tip
The `docker build` command will only work if you're in the directory
with the container.
:::

Ok, now let's run the container using

``` {.bash eval="false"}
docker run --rm -d \
  -p 8080:8080 \
  --name penguin-model \
  penguin-model-local
```

If you go to `http://localhost:8080` you'll find that...it doesn't work?
Why? If you run the container attached (remove the `-d` from the run
command) you'll get some feedback that might be helpful.

In line 15 of the Dockerfile, we copied the `app.py` in to the
container. Let's take a look at that file to see if we can find any
hints.

``` {.python include="../../_labs/docker/docker-local/docker/app.py" filename="app.py"}
```

Look at that (very long) line 6. The API is connecting to a local
directory to pull the model. Is your spidey sense tingling? Something
about container filesystem vs host filesystem?

That's right -- the temp file indicated is inside the `/var` directory
on *my machine*, but that directory doesn't exist at `/var` inside the
container when it's running. In order to get this to work, I've got to
mount the model in at run time.

Add a `-v /var:/var` to your run command like so

``` bash
docker run --rm -d \
  -p 8080:8080 \
  --name penguin-model \
  -v /var:/var \
  penguin-model-local
```

::: callout-tip
Depending on the permissions of the `/var` directory on your machine,
you *may* need to run the `docker run` command prefixed by `sudo`.
:::

And NOW you should be able to get your model up in no time.

In [Chapter @sec-cloud], we'll worry about putting the model somewhere
more permanent so you don't have to worry about this issue.

### Lab Extension

Right now, logs from the API just stay inside the app. But that means
that the logs go away when the container does. That's obviously bad if
the container dies because something goes wrong. How might you make sure
that the container's logs get written somewhere more permanent?
