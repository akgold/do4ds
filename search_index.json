[["index.html", "DevOps for Data Science (working title) Preface 0.1 Moving Data Science to A Server Software information and conventions Acknowledgments", " DevOps for Data Science (working title) Alex Gold 2021-11-23 Preface At some point, most data scientists reach the point where they want to show their work to others. But the skills and tools to deploy data science are completely different from the skills and tools needed to do data science. If you’re a data scientist who wants to get your work in front of the right people, this book aims to equip you with all the technical things you need to know that aren’t data science. Hopefully, once you’ve read this book, you’ll understand how to deploy your data science, whether you’re building a DIY deployment system or trying to work with your organization’s IT/DevOps/SysAdmin/SRE group to make that happen. 0.1 Moving Data Science to A Server In recent years, as data science has become more central to organizations, many have been moving their operations off of individual contributors’ laptops and onto centralized servers. Depending on your organization, the centralization of data science operations can make your life way easier – or it can be kinda a bummer. Server migrations can work well regardless of whether they’re instigated by the data science or the IT organization. The biggest determinant is how well the data science and IT/DevOps teams can collaborate. Data scientists are good at manipulating and using data, but most have little expertise in SysAdmin work, and aren’t really that interested. On the flip side, IT/DevOps organizations usually don’t really understand data science workflows, the data science development process, or how data scientists use R and Python. Often, migrations to a server are instigated by the data scientists themselves – usually because they’ve run out of horsepower on their laptops. If you, or one of your teammates, enjoys and is good as SysAdmin work, this can be a great situation! You get the hardware you need for your project quickly and with minimal interference. On the other hand, most data scientists don’t really want to be SysAdmins, and these systems are often fragile, isolated from other corporate systems, and potentially susceptible to security vulnerabilities. Other organizations are moving to servers as well, but led by the IT group. For many IT groups, it’s way easier to maintain a centralized server environment, as opposed to helping each data scientist maintain their own environment on their laptop. Having just one platform makes it much easier to give shared access to more powerful computing platforms, to data sources that require some configuration, and to R and Python packages that wrap around system libraries and can be a pain to configure (looking at you, rJava). This can be a great situation for data scientists! If the platform is well-configured and scoped, you can get instant access through their web browser to more compute resources, and don’t have to worry about maintaining local installations of data science tools like R, Python, RStudio, and Jupyter, and you don’t need to worry about how to connect to important data sources – those things are just available for use. But this can also be a bad experience. Long wait times for hardware or software updates, overly restrictive policies – especially around package management – and misunderstandings of what data scientists are trying to do on the platforms can lead to servers going largely unused. So much of whether the server-based experience is good or not depends on the relationship between the data science and IT/Admin group. In organizations where these groups work together smoothly, this can be a huge win for everyone involved. However, there are some organizations where IT/Admins are so concerned with stability and security that they make it impossible to do data science, and the data scientists spend all their time playing cat-and-mouse games to try to get work done behind IT/Admin’s backs. If you work at such a place, it’s frankly hard to get much done on the server. It’s probably worth investing some time into improving your relationship with your favorite person on the IT/Admin team. Hopefully, this book will help you understand a little of what’s on the minds of people in the IT group, and a sense of how to talk to them better. Software information and conventions I used the knitr package (Xie 2015) and the bookdown package (Xie 2021) to compile my book. My R session information is shown below: xfun::session_info() ## R version 4.1.2 (2021-11-01) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Catalina 10.15.7 ## ## Locale: en_US.UTF-8 / en_US.UTF-8 / en_US.UTF-8 / C / en_US.UTF-8 / en_US.UTF-8 ## ## Package version: ## base64enc_0.1.3 bookdown_0.24 compiler_4.1.2 ## digest_0.6.28 evaluate_0.14 fastmap_1.1.0 ## glue_1.5.0 graphics_4.1.2 grDevices_4.1.2 ## highr_0.9 htmltools_0.5.2 jquerylib_0.1.4 ## jsonlite_1.7.2 knitr_1.36 magrittr_2.0.1 ## methods_4.1.2 rlang_0.4.12 rmarkdown_2.11 ## stats_4.1.2 stringi_1.7.5 stringr_1.4.0 ## tinytex_0.35 tools_4.1.2 utils_4.1.2 ## xfun_0.28 yaml_2.2.1 Package names are in bold text (e.g., rmarkdown), and inline code and filenames are formatted in a typewriter font (e.g., knitr::knit('foo.Rmd')). Function names are followed by parentheses (e.g., bookdown::render_book()). Acknowledgments A lot of people are helping me write this book. This book is published to the web using GitHub Actions from rOpenSci. References "],["about-the-author.html", "About the Author", " About the Author Alex Gold is a manager on the Solutions Engineering team at RStudio. He works with customers of RStudio’s professional software to help them deploy, configure, and use RStudio’s professional software and open source tooling in R and Python. In his free time, he enjoys landscaping, handstands, and Tai Chi. He occasionally blogs about data, management, and leadership at alexkgold.space. Color palette: Tea Green: CAFFDO Steel Blue: 3E7CB1 Kombu Green: 273c2c Bright Maroon: B33951 Sandy Brown: FCAA67 "],["introduction.html", "Chapter 1 Introduction 1.1 Components", " Chapter 1 Introduction One of the most frequent questions from data scientists is, “how do I ask my IT team for that?” Hopefully, reading this book will help you learn how to ask, or even to DIY. Getting work off your laptop often starts with deployment, but increasingly organizations are also doing data science development in centralized data science platforms. Encompasses parts of DevOps, MLOps, DataOps. If you’re in an environment where you just do data science on your laptop, you probably don’t really need this book. You might find the sections on the command line and on data access interesting, but most of this book will be geared towards server-based data science – whether that’s working in a browser-based data science IDE or trying to deploy data science assets to a server. 1.1 Components Workbench Package Management Change Control (git) Deployment Environment Front End vs Back End Data Ingest/Access Determining Requirements for your platform Platform requirements combination of Work being done on the platform Data Science needs (i.e. horsepower) Existing state of org’s data Type of work data scientists might do Batch Jobs ETL Reports Data Generation Real-Time Interactive Models APIs Apps Data Science + IT Needs Languages (R, Python, SQL) Sharing – internal, external Online or offline? Data Security – row based access? Centralized workbench vs individual workstations Existing Assets BI/Analytics Platforms Data Connections "],["command-line.html", "Chapter 2 The Command Line", " Chapter 2 The Command Line Most SysAdmin work is done on the command line Unlike your laptop, servers don’t often have a desktop interface The interactive sections of this book will require you to use the command line -&gt; How to get to command line on different systems -&gt; Set up docker desktop #TODO: Address using vi "],["computers-and-servers.html", "Chapter 3 Computers and Servers 3.1 Computers are addition factories 3.2 Choosing the right data science machine 3.3 Is a server different? 3.4 Getting a server of your own 3.5 Exercises", " Chapter 3 Computers and Servers Data Science is a delightful mashup of statistics and computer science. While you can be a great data scientists without a deep understanding of computational theory, a mental model of how your computer works is helpful, especially when you head to production. In this chapter, we’ll develop a mental model for how computers work, and explore how well that mental model applies to both the familiar computers in your life, but also more remote servers. If you’re into pedantic nitpicking, you’re going to love this chapter apart, as I’ve grossly oversimplified how computers work. On the other hand, this basic mental model has served me well across hundreds of interactions with data scientists and IT/DevOps professionals. And by the end of the chapter, we’ll get super practical – giving you a how-to on getting a server of your very own to play with. 3.1 Computers are addition factories As a data scientist, the amount of computational theory it’s really helpful to understand in your day-to-day can be summarized in three sentences: Computers can only add. Modern ones do so very well and very fast. Everything a computer “does” is just adding two (usually very large) numbers, reinterpreted.1 I like to think of computers as factories for doing addition problems. We see meaning in typing the word umbrella or jumping Mario over a Chomp Chain and we interpret something from the output of some R code or listening to Carly Rae Jepsen’s newest bop, but to your computer it’s all just addition. Every bit of input you provide your computer is homogenized into addition problems. Once those problems are done, the results are reverted back into something we interpret as meaningful. Obviously the details of that conversion are complicated and important – but for the purposes of understanding what your computer’s doing when you clean some data or run a machine learning model, you don’t have to understand much more than that. 3.1.1 Compute The addition assembly line itself – where the work actually gets done – is referred to as compute. It’s where 2+2 gets turned into 4, and where 345619912 + 182347910 gets turned into 527967822. The heart of the factory in your computer is the central processing unit (CPU). There are two elements to the total speed of your compute – the total number of cores, which you can think of as an individual conveyor belt doing a single problem at a time, and the speed at which each belt is running. These days, most consumer-grade laptops have between 4 and 16 cores, and may have additional capabilities that effectively doubles that number. So most laptop CPUs can do between 4 and 32 simultaneous addition problems. In your computer, the basic measure of conveyor belt speed is single-core “clock speed” in hertz (hz) – operations per second. The cores in your laptop probably run between 2-5 gigahertz (GHz): 2-5 billion operations per second. A few decades ago, there was a steady march in increases in clock speed – increasing by approximately 10x during the 90s, by 2-3x in the 2000s, and somewhere between not at all and 1.5x in the 2010s. Computers have gotten much faster through this entire time, but mostly from sources other than boosts in single-core clock speeds. For example, in 2020, Apple released a series of Macs with its new M1 chip. Real-world performance of the M1 chip was dramatically better than earlier models, despite modest improvements in clock speeds. 3.1.1.1 GPU Computing While compute usually just refers to the CPU, it’s not completely synonymous. Computers can offload some problems to a graphical processing unit (GPU). GPUs are specialized chips used for tasks like editing photo or videos, rendering video game graphics, some kinds of machine learning, and (yes) Bitcoin mining. Where the CPU has a few fast cores, the GPU takes the opposite approach, with many slower cores. Where a consumer-grade CPU has 4-16 cores, mid-range GPUs have 700-4,000, but each one runs between 1% and 10% the speed of a CPU core. For GPU-centric tasks, the overwhelming parallelism of a GPU is more important than the speed of any individual core, and GPU computation can be dramatically faster. For the purposes of data science, many popular machine learning techniques – including neural networks, XGBoost, and other tree-based models – potentially run much much faster on GPUs relative to CPUs. 3.1.2 Memory (RAM) Your computer’s random access memory (RAM) is its short term storage. Your computer uses RAM to store addition problems it’s going to tackle soon, and results it thinks it might need again in the near future. The benefit of RAM is that it’s very fast to access. The downside is that it’s temporary. When your computer turns off, the RAM gets wiped.2 You probably know this, but memory and storage is measured in bytes. These days, most consumer hard drives are between 1/2 to a few terrabytes (trillion bytes). Some enterprise data stores run on the scales of thousands of terrabytes (pettabytes) or even thousands of pettabytes (yottabytes). Modern consumer-grade laptops come with somewhere between 4 and 16 Gb of memory. 3.1.3 Storage (Hard Drive/Disk) Your computer’s storage is like a freezer. It keeps things permanently, whether the factory is operating or not. The tradeoff is that things there are cold, and take some time to warm up before they can be used. A few years ago, all hard drives were (HDDs), and were basically the same. HDDs have a bunch of magnetic disks that spin very fast (5,400 and 7,200 RPM are common speeds). Magnetized read/write heads move among the disks and save and read your data. In the last few years, solid-state drives (SSDs) have been taking over. SSDs, which are collections of flash memory chips, are up to 15x faster than HDDs. They also can take a wider variety of shapes and sizes, and are more reliable and durable because they have no moving parts. The main drawback is that they’re usually more expensive per byte, but prices are still quite reasonable. Many consumer laptops have only an SSD at this point. Some desktops and high-end laptops combine a smaller SSD with a larger HDD. 3.2 Choosing the right data science machine In my experience as a data scientist and talking to IT/DevOps organizations trying to equip data scientists, the same questions about choosing a computer come up over and over again. Here are the guidelines I often share. 3.2.1 Get as much RAM as feasible In most cases, R and Python have to load all of your data into memory. Thus, the size of the data you can use is limited to the size of your machine’s RAM. Most other limits of your machine will just result in things being slower than you’d really want, but trying to load too much data into memory will result in a session crash, and you won’t be able to do your analysis. You can get around the in-memory limitation by using a database or libraries that facilitate on-disk operations like Apache Arrow or dask. It’s easy to say that you’ll always want more RAM, but a rough rule of thumb for whether you’ve got enough is the following: Amount of RAM = max amount of data * 3 Because you’ll often be doing some sort of transformation that results in invisible data copies and your computer can’t devote all of its memory, you’ll want to leave plenty of room over your actual data size. I’ve never run into problems with a scaling factor of 3, but this is definitely only a rough rule-of-thumb. 3.2.2 Go for fewer, faster cores in the CPU R and Python are single-threaded. Unless you’re using special libraries for parallel processing, you’ll end up red-lining a single CPU core while the other just look on in silence. Therefore, single core clock speed matters more than the number of cores, and fewer, faster cores are usually preferable to many slower If you’re buying a laptop or desktop, there usually aren’t explicit choices between a few fast cores and more slow cores. Most modern CPUs are pretty darn good, and you should just get one that fits your budget. If you’re standing up a server, you often have an explicit choice between more slower cores and fewer faster ones. 3.2.3 Get a GPU…maybe… If you’re doing machine learning that can be improved by GPU-backed operations, you might want a GPU. In general, only highly parallel machine learning problems like training a neural network or tree-based models will benefit from GPU computation. On the other hand, GPUs are expensive, non-machine learning tasks like data processing don’t benefit from GPU computation, and many machine learning tasks are amenable to linear models that run well CPU-only. 3.2.4 Get a lot of storage, it’s cheap As for storage – get a lot – but don’t think about it too hard, because it’s cheap. Both a 1TB SSD and a 4TB HDD are around $100. Storage is cheap enough these days that it is almost always more cost efficient to buy more storage rather than making a highly-paid professional spend their time trying to figure out how to move things around. One litmus test of an IT organization that is well-equipped to support data science is whether they understand this. Smart organizations know that just getting more storage is easily worth the cost in terms of the time of admins and data scientists. 3.3 Is a server different? No. But also yes. At its core, a server is exactly the same sort of addition factory as your laptop, and the same mental model of what is happening under the hood will serve you well. The big difference is in how the input and output is done. While you interact directly with a computer through keyboard and mouse/touchpad, servers generally don’t have built in graphical interfaces – by default all interaction occurs via command line tools. One of the reasons is that the overwhelming majority of the world’s servers run the Linux operating system, as opposed to the Windows or Mac OS your laptop probably runs.3 There are many different distributions (usually called “distros”) of Linux. For day-to-day enterprise server use, the most common of these are Ubuntu, CentOS, Red Hat Enterprise Linux (RHEL), SUSE Enterprise Linux. Along with most of the world’s servers, almost all of the world’s embedded computers – in ATMs, cars and planes, TVs, and most other gadgets and gizmos – run on Linux. If you have an Android phone or a Chromebook – that’s Linux. Basically all of the world’s supercomputers use Linux. 3.4 Getting a server of your own For the rest of this chapter, we’re going to walk through how to set up a server of your own. If you’ve never set up a server before, I’d strongly recommend you play along. I’m going to refer back to this server frequently in the rest of the book, and it’ll be really helpful if you have a good feeling for how this works. In contrast to the computer sitting on your desk, you’ll have to access it over the internet and you’ll use a specialized access protocol called SSH. Those topics are important enough that there are standalone chapters on them. For now, we’re going to gloss entirely over the how and why and just get you to running a server. If you follow along from this point, it’ll probably take you 10-15 minutes to be running a server of your own. We’re going to be standing up a server on Amazon Web Services (AWS). In particular, we’ll be standing up a server in their free tier – so there will be no cost involved as long as you haven’t used up all your AWS free tier credits before now. If you follow these instructions, you’ll get a server of your own up and running in less than 10 minutes, and you may even have gotten to a point where you could do data science on that server. While that’s certainly exciting, calling that a data science-ready server is kinda like throwing a tarp over a tree branch in the forest and calling it a house. It might be a nice place to stay for a little, but you’re going to need something more robust longer-term. 3.4.1 Login to the AWS Console We’re going to start by logging into AWS. If you’ve done this before, just go ahead and log in. If not, go to aws.amazon.com and click Sign In to the Console . If you’ve never set up an AWS account before, click Create a New AWS account and follow the instructions to create an account. Note that even if you’ve got an Amazon account for ordering stuff online and watching movies, an AWS account is separate. #TODO: Add link to cloud chapter Once you’ve logged in, you’ll be confronted by the AWS console. There are a ton of things here, and it’s rather overwhelming. There’s a chapter on the business model behind this, so skip ahead if you want, or spend a minute poking around before continuing. 3.4.2 Stand up an instance In the next few paragraphs, I’m going to give you instructions to quickly get a server up – with basically no explanation. If you read this whole book, you’ll understand all of this, the alternatives you could take, and the reasons I’m going to strongly recommend you take this server back offline in just a few minutes. For now, click on the EC2 service (it’s under Launch a virtual machine or Compute depending on where you landed). Scroll down the launch instance button. Here are all the different Quick Start Amazon Machine Images (AMIs). Find and click Ubuntu Server 20.04 LTS – it’ll be one of the first handful. Now you’ll be seeing the instance size chooser. It should have auto-selected a server with the label Free tier eligible. Just stick with this for now. Scroll down and click Review and Launch, and Launch on the next page. When you click Launch, you’ll be asked to use a key pair. Assuming you don’t have an existing keypair, select Create a new key pair, name it my_test_key, and click Download. Keep track of the my_test_key.pem file your computer downloads. Click Launch Instances. AWS is now creating a virtual server just for you. If you click View Instances in the lower right, you’ll see your instance. When the instance state switches to Running, it’s up and running! 3.4.3 SSH into the server The .pem key you downloaded is the skeleton key to your server. If you were setting up a real server, you’d need to be extremely careful with this key, as it allows anyone who has it unrestricted access to the server. For the same reason, it’s also great for playing around quickly with a server. Before we can use it to open the server, we’ll need to make a quick change to the permissions on the key. More details on what that means in the [] chapter. #TODO: which chapter? To take the next steps, you’ll need to at least be able to open your computer’s terminal and copy/paste some commands below. If that’s new to you, feel free to check out the chapter on using the command line. If you’re on a Mac or Linux system, you’ll do the following: $ cd ~/Downloads #or whatever directory the key is in $ chmod 600 my_test_key.pem #TODO: Windows? To access your server, click on the Instance ID link for your server, and copy the Public IPv4 DNS, which will start with ec2- and end with amazonaws.com. In your terminal type the following $ ssh -i my_test_key.pem ubuntu@&lt;Public IPv4 DNS you copied&gt; Type yes when prompted, and you’re now logged in to your server! 3.4.4 Doing A Thing Before we log off and kill this server, let’s do one little thing. We’re going to stand up Nginx, which is a common webserver, and serve a little webpage to ourselves. Let’s start by installing Nginx, copy and paste the command below. $ sudo apt update $ sudo apt-get install nginx -y By default, our EC2 instance only allows SSH traffic, which is on port 22 by default. We need to open up HTTP traffic. Go back to your instance in the AWS console and scroll down to the Security tab. Click the blue link under Security Groups, which will start with sg- and include launch-wizard- in parentheses. Click Edit inbound rules , then Add rule. Under Type, scroll down and select HTTP, and under Source Type, select Anywhere-IPv4. Scroll down and click Save rules. Go back to your instance page and copy the Public IPv4 DNS again. Paste this into your browser’s navigation bar, and add http:// right before. When you navigate to the page, you’ll see the default Nginx home page. Let’s make a quick change to the page, just for fun. Back in your terminal that’s still SSH-ed into the instance, navigate to where that page is located: $ cd /var/www/html You can edit the page by typing $ sudo vi index.nginx-debian.html If you don’t recall how to use vi, check out the command line tutorial, but for now, you can enter edit mode by pressing i, navigating around with the arrow keys, and typing. Edit something – say change this line &lt;h1&gt;Welcome to nginx!&lt;/h1 &gt; to something a little more personalized. When you’re done, hit esc followed by :wq. Now, when you re-load the page in your browser, you should be able to see your changes reflected there. #TODO: consider using a container to get shiny server up quickly. 3.4.5 Burn it all down One of the best things about cloud infrastructure is that it can go away as easily as it came up. We made a number of choices here that are fine for ephemeral infrastructure for playing around. But this server should not be used for anything real. When you’ve had your fill of playing, let’s take the server down. Go back to the EC2 page for your server. Under the Instance State drop down in the upper right, choose Terminate Instance. If you go to the Instances page, it’ll take just a minute for the instance to go away. 3.5 Exercises Think about the scenarios below – which part of your computer would you want to upgrade to solve the problem? You try to load a big csv file into pandas in Python. It churns for a while and then crashes. You go to build a new ML model on your data. You’d like to re-train the model once a day, but it turns out training this model takes 26 hours on your laptop. You design an visualization Matplotlib , and create a whole bunch in a loop/ Try standing up a new server and installing R and RStudio Server Open Source, or Python and JupyterHub. Hint 1: Remember that your instance only allows traffic to SSH in on port 22 by default. You access RStudio on port 8787 by default and JupyterHub on port 8000. You control what ports are open via the Security Group. Hint 2: You’ll need to create a user on the server. The adduser command is your friend. #TODO: test out JupyterHub The reason why this is the case and how it works is fascinating. If you’re interested, it comes back to Alan Turing’s famous paper on computability. I recommend The Annotated Turing: A Guided Tour Through Alan Turing’s Historic Paper on Computability and the Turing Machine by Charles Petzold for a surprisingly readable walkthrough of the paper.↩︎ You probably don’t experience this personally. Modern computers are pretty smart about dumping RAM onto the hard disk before shutting down, and bringing it back on startup, so you usually won’t notice this happening.↩︎ There are Windows Server versions that are reasonably popular in enterprises. There are no Mac servers. There is a product called Mac Server, but it’s used to manage Mac desktops and iOS devices, not a real server.↩︎ "],["networking.html", "Chapter 4 Getting Traffic Across the Internet – URLs and https 4.1 Anatomy of a URL 4.2 Application Layer Protocols: http(s) 4.3 Getting a real URL 4.4 Securing your traffic with SSL/TLS 4.5 Exercises", " Chapter 4 Getting Traffic Across the Internet – URLs and https Collectively, the topic of the next two chapters – getting traffic across the public internet and to a listening service is called networking. I’ve broken it down into two chapters. This first chapter will discuss how traffic gets across the public internet and to the server you’ve configured, and in the next chapter, we’ll get into how to configure a private network to make sure your traffic can get where it needs to go. When you go to a website, that website address starts with https:// . We’ll get into the s more later in this chapter. For now, we’ll just focus on the http part. Most modern browsers automatically add the http for you, so you can, for example, type google.com, and your browser will automatically know that you intend to go to https://www.google.com. In this chapter, we’ll pull apart the components of this URL and how you can get one of your very own. 4.1 Anatomy of a URL You probably use URLs every day, but let’s take just a minute and break down the components of a URL. In this chapter and the next, we’ll get into how each of these components of the URL is created and assigned. 4.2 Application Layer Protocols: http(s) The very first part of a URL is the application layer protocol. Application layer protocols define how the server should understand the request that was sent and what are legitimate things to send back. http is the protocol that is used when you go to a webpage on the internet. It defines how to fetch documents across networks – the most common types of documents on the internet being HTML web pages. There are many other application layer protocols that are used for a variety of purposes. Some you’ll see in this book include SSH for direct server access, (S)FTP for file transfers, LDAP(S) for authentication, and websockets for persistent bi-directional communication – used for interactive webapps created by the Shiny package in R and the Streamlit package in Python. 4.2.1 What really is http traffic? The best way to understand http traffic is to inspect it directly. Luckily, you’ve got an easy tool to do that – your browser! Open a new tab in your browser and open your developer tools. How this works will depend on your browser. In Chrome, you’ll go to View &gt; Developer &gt; Developer Tools and then make sure the Network tab is open. Now, navigate to a URL in your browser (say google.com). As you do this, you’ll see the traffic pane fill up. These are the requests and responses going back and forth between your computer and the server. If you click on any of them, there are a few useful things you can learn. At the top, you can see the timing. This can be helpful in debugging things that take a long time to load. Sometimes it’s helpful to see what stage in the process bogs down. In this pane, you can inspect the actual content that is going back and forth between your computer and the server you’re accessing. What’s often more useful is being able to inspect the metadata on the requests and responses. Here are a few of the most important fields: Request Method – getting deep into HTTP request methods is beyond the scope of this book, but there are a variety of different methods you might use to interact with things on the internet. The most common are GET to get a webpage, POST or PUT to change something, and DELETE to delete something. Status Code - each HTTP response includes a status code indicating the response category. Some special codes you’ll quickly learn to recognize are below. The one you’ll (hopefully) see the most is 200, which is a successful response. Response and Request Headers – headers are metadata included with the request and response. These include things like the type of the request, the type of machine you’re coming from, cookie-setting requests and more. In some cases, these headers include authentication credentials and tokens, and other things you might want to inspect. 4.2.1.1 Special HTTP Codes 200 - everyone’s favorite, a successful response. 3xx - Your query was redirected somewhere else. 4xx - are errors with the requests. There are a few you’ll probably see particularly often: 400 - bad request. This isn’t a request the server can understand. 401 and 403 - unauthorized or forbidden. Often means required authentication hasn’t been provided. 404 - Not found. There isn’t any content at the address you’re trying to access. 5xx - server-side errors. Some kind of error occurred as the server was trying to process your request. 504 - gateway timeout. This means that a proxy or gateway between you and the server you’re trying to access timed out before it got a response from the server. 4.3 Getting a real URL We accessed our basic server on the public IP address provided to us by AWS. But that was very likely the first time you’d accessed something on the internet directly using an IP address – for good reason. IP addresses are how entities are actually identified in computer network, but the IP address 143.122.8.32 is pretty hard to remember. Even worse, IP addresses generally aren’t permanent – they can change when individual servers are replaced, or if you were to change the server architecture (say by adding and load-balancing a second instance – see chapter XX). In order to have something human-friendly and permanent, we generally access network resources at a URL, like google.com, rather than an IP address. 4.3.1 An aside about IP addresses… As you start working more intimately with servers, you’ll spend more time staring at IP addresses, so it’s worth taking a moment to talk about how to recognize them. The IP addresses so far in this book are IPv4 addresses. They’re four blocks of 8-bit fields (integers up to 256), so they look like 0.0.0.0 to 255.255.255.255. If you do the math, you’ll realize there are “only” a little more than 4 billion of these, which it turns out is not enough and we’re running out. In the last few years, adoption of the new standard, IPv6, has started. IPv6 addresses are eight blocks of hexadecimal (0-9 + a-f) digits separated by colons, with certain rules that allow them to be shortened, so 4b01:0db8:85a3:0000:0000:8a2e:0370:7334 or 3da4:66a::1 are both examples of valid IPv6 addresses. IPv6 will coexist with IPv4 for a few decades, and we’ll eventually switch entirely to IPv6. There’s no worry about running out of IPv6 addresses any time soon, because the total number of IPv6 addresses is a number 39 digits long. There is one IP address you may frequently see, which is 127.0.0.1 or localhost – sometimes called the loopback address. When you see this, it means that the traffic is being directed back to the same machine where it’s originating. This is sometimes useful for testing, or to reach another service on the same machine. 4.3.2 Getting a Domain The first step to getting a real URL is buying a domain from a domain name registrar. There are many different registrars that will sell you domain names on the internet. In general, the process of buying one is super easy. Usually, you’ll just need to search for the domain you want, say you want it, and pull out a credit card. The cost of a domain name varies widely. Buying a meaningless domain in a less popular top-level domain, say ladskfmlsdf.me can cost as little as $3 per year. On the other hand, buying a .com domain that’s a real word or phrase can be a few thousand dollars. Once you’ve got an IP address and a domain, you’ve got to connect the two. This is where the domain name system (DNS) comes in. DNS is the decentralized internet phonebook that translates back and forth between URLs and IP addresses. There are many layers of DNS servers, so pointing your browser at example.com starts with a query to a DNS server asking, “Where is the DNS server for .com websites?” Eventually, a query for your website will lead to the DNS server of your domain name registrar. You’ll have to configure your domain name registrar to send traffic on to the right place. Configuration of DNS is done by way of records, of which there are a menagerie of types you can configure. Luckily, most simple configurations only need CNAME and A records. Here’s an imaginary DNS record table for the domain example.com: Path/Host Type Target @ A 143.122.8.32 www CNAME example.com * A 143.122.8.33 Let’s go through how to read this table. Since we’re configuring example.com, the paths/hosts in this table are relative to example.com. In the first row we’re configuring an A record to go to the target IP address. A records (or their IPv6 cousin AAAA records) map a domain to an actual IP address. The path @ is a special symbol meaning exact match. So by this configuration, any traffic to example.com will be passed straight through to the specified IP address. The second row deals with traffic to the www subdomain. CNAME records alias sub-domains. They’re most frequently used to map sub-domains to the main domain. Since this is a CNAME record for example.com, this record indicates that traffic to www.example.com should be treated exactly like traffic to example.com. Some domain providers do automatic redirection of www traffic, and so this row may not be necessary in some configurations. The last record uses the wildcard symbol * to send all subdomain traffic that’s not already spoken for – say blog.example.com or info.example.com directly to the IP address specified. So what happens is that your query goes through several layers of public DNS servers to get to the DNS entry for your domain name registrar. In many cases, you’ll directly configure your domain name registrar to point to your website or server – but you also can configure the domain name registrar to point at another set of DNS servers you actually control with an NS record. If you’re setting up your own server, this probably isn’t the case, but some large enterprises do run their own private DNS servers. You should always configure your domain provider as narrowly as possible – and you should configure your website or server first. #TODO: why? 4.3.3 Learning to Hate DNS As you get deeper into using servers, you will learn to hate DNS with a fiery passion. While it’s necessary so we’re not running around trying to remember incomprehensible IP addresses, it’s also very hard to debug as a server admin. Let’s say I’ve got the public domain example.com, and I’m taking down the server and putting up a new one. I’ve got to alter the public DNS record so that everyone going to example.com gets routed to the new IP address, and not the old one. The thing that makes it particularly challenging is that the DNS system is decentralized. There are thousands of public DNS servers that a request could get routed to, and many of them may need updating. Obviously, this is a difficult problem to solve, and it can take up to 24 hours for DNS changes to propagate across the network. So making changes to DNS records and checking if they’ve worked is kinda a guessing game of whether enough time has passed that you can conclude that your change didn’t work right, or if you should just wait longer. To add an additional layer of complexity, DNS lookups are slow, so your browser caches the results of DNS lookups it has done before. That means that it’s possible you’ll still get an old website even once the public DNS record has been updated. If a website has ever not worked for you and then worked when you tried a private browser, DNS caching is likely the culprit. Using a private browsing window sidesteps your main DNS cache and forces lookups to happen afresh. 4.3.4 Trying it out Go through hosting this book somewhere. 4.4 Securing your traffic with SSL/TLS When you go to a website on the internet, you’ll see the URL prefixed by the https (though it’s sometimes hidden by your browser because it’s assumed). https is actually a mashup that is short for http with secure sockets layer (SSL). These days, almost everyone actually uses the successor to SSL, transport layer security (TLS). However, because the experience of configuring TLS is identical to SSL, admins usually just talk about configuring SSL even when they mean TLS. These days, almost every bit of internet traffic is actually https traffic. You will occasionally see http traffic inside private networks where encryption might not be as important – but more and more organizations are requiring end-to-end use of SSL. Securing your website or server using SSL/TLS is one of the most basic things you can do to make sure your website traffic is safe. You should always configure https – full stop. SSL/TLS security is accomplished by configuring your site or server to use a SSL certificate (often abbreviated to cert). We’ll go through the details of how to get and configure an SSL certificate in this chapter – but first a little background on how SSL/TLS works. 4.4.1 How SSL/TLS Enhances Security SSL accomplishes two things for you – identity validation and traffic encryption. When you go to a website, SSL/TLS is the technology that verifies that you’re actually reaching the website you think you’re reaching. This prevents something called a man-in-the-middle attack where a malicious actor manages to get in between the server and the client of network traffic. So, for example, you might think you’re putting your bank login information into your normal bank website, but there’s a hacker sitting in the middle, reading all of the traffic back and forth. [TODO: Image of man-in-the-middle] You can see this in action in your web browser. When you go to a website protected by https, you’ll see a little lock icon to the left of the URL. That means that this website’s SSL certificate matches the website and therefore your computer can verify you’re actually at the website you mean to be at. But how does your computer know what a valid SSL certificate is? Your computer has a list of trusted Certificate Authorities (CAs) who create, sell, and validate SSL/TLS certificates. So when you navigate to a website, the website sends back a digital signature. Your computer checks the signature against the indicated CA to verify that it was issued to the site in question. [TODO: image of SSL validation] The second type of scary scenario SSL prevents is a snooping/sniffing attack. Even if you’re getting to the right place, your traffic travels through many different channels along the way – routers, network switches, and more. This means that someone could theoretically look at all your traffic along the way to its meaningful destination. When your computer gets back the digital signature to verify the site’s identity, it also prompts an exchange of encryption keys. These keys are used to encrypt traffic back and forth between you and the server so anyone snooping on your message will just see garbled nonsense and not your actual content. You can think of the SSL/TLS encryption as the equivalent of writing a message on a note inside an envelope, rather than on a postcard anyone could read along the way. 4.4.2 Getting a cert of your own In order to configure your site or server with SSL, there are three steps you’ll want to take: getting an SSL certificate, putting the certificate on the server, and making sure the server only accepts https traffic. You can either buy an SSL certificate or make one yourself, using what’s called a self-signed cert. There are a variety of places you can buy an SSL/TLS certificate, in many cases, your domain name registrar can issue you one when you buy your domain. When you create or buy your cert, you’ll have to choose the scope. A basic SSL certificate covers just the domain itself, formally known as a fully qualified domain name (FQDN). So if you get a basic SSL certificate for www.example.com, www.blog.example.com will not be covered. You can get a wildcard certificate that would cover every subdomain of *.example.com. Note that basic SSL/TLS certification only validates that when you type example.com in your browser, that you’ve gotten the real example.com. It doesn’t in any way validate who owns example.com, whether they’re reputable, or whether you should trust them. There are higher levels of SSL certification that do validate that, for example, the company that owns google.com is actually the company Google. But sometimes it’s not feasible to buy certificates. While a basic SSL certificate for a single domain can cost $10 per year or less, wildcard certificates will all the bells and whistles can cost thousands per year. This can get particularly expensive if you’ve got a lot of domains for some reason. Moreover, there are times when you can’t buy a certificate. If you’re encrypting traffic inside a private network, you will need certificates for hosts or IP addresses that are only valid inside the private network, so there’s no public CA to validate them. There are two potential avenues to follow. In some cases, like inside a private network, you want SSL/TLS for the encryption, but don’t really care about the identity validation part. In this case, it’s usually possible to skip that identity validation part and automatically trust the certificate for encryption purposes. It’s also possible to create your own private CA, which would verify all your SSL certificates. This is pretty common in large organizations. At some point, every server and laptop needs to have the private CA added to its set of trusted certificate validators. A warning: it is deceptively easy to generate and configure a self-signed SSL certificate. It’s usually just a few lines of shell commands to create a certificate, and adding the certificate to your server or website is usually just a copy/paste affair. However, it’s pretty common to run into problems with self-signed certs or private CAs. Making sure the certificate chain is correct, or running into a piece of software that doesn’t ignore the identity validation piece right is pretty common. This shouldn’t dissuade you from using SSL/TLS. It’s an essential, and basic, component of any security plan – but using a self-signed cert probably isn’t as easy as it seems. When you configure your site or server, there will likely be an option to redirect all http traffic to https traffic. If your server or site is open to the internet, you should set this option. 4.5 Exercises Find a cheap domain you like and buy it. Put an EC2 server back up with the Nginx hello-world example. Configure your server to be available at your new domain. Hint: In AWS, Route 53 is the service that handles incoming networking. They can serve as a domain name registrar, or you can buy a domain elsewhere and just configure the DNS using Route 53. Create or buy an SSL certificate. Hint: AWS Certificate Manager is AWS’s SSL/TLS certificate issuer, but you can also bring-your-own certificate. Add your certificate to Nginx and redirect all http traffic to https. Hint: http traffic comes in on port 80 by default. Hint: Return code 301 is for a permanent redirect. What happens when you try to go to that URL. "],["private-networking-configuration.html", "Chapter 5 Private Networking Configuration 5.1 Finding the right server with hostnames 5.2 The role of forward and reverse proxies 5.3 Using ports to get to the right service 5.4 Configuring the private network 5.5 Special IP Addresses and Ports 5.6 Exercises", " Chapter 5 Private Networking Configuration In the previous chapter, we talked about the first level of upgrades for your data science server – getting a real URL and configuring https for security. So now, the internet is knocking on the front gate of your server. This chapter will be all about how to configure networking on the private siIn this chapter, we’ll talk about how to get traffic from the front door to the right apartment. It wasn’t obvious, but when you set up the server in the first chapter, you also actually created a private network for the server to live in called a virtual private cloud (VPC). As your server configuration gets more complicated, you’re going to put more different servers inside the virtual private cloud. So instead of thinking of your traffic as coming to the front door of your little server/cottage, I think it’s a more helpful analogy to think of coming to the guard box at the front gate of a gated community of apartment buildings. You’re at the front gate, but still need to find your way to the right building and then to the right apartment inside that building. Up until now, we’ve been pretending like the server you put up in the first chapter was actually just on the public internet – so it makes sense to ask why we wouldn’t actually just do that. The two reasons are security and reducing administrative headaches. The other reason to make extensive use of private networks has to do with ease-of-configuration. Inside a private network, it’s generally ok to say “accept connections from any server inside the network”, and take care of security at the front gate of the network. Otherwise, you’ve got to be much more careful about how you configure each individual server, which is both more work and a potential security vulnerability if you don’t do it right. In the case of a data science environment you may have a number of different servers you’re setting up – a server for doing data science work, a database, a package repository mirror, and a server for deploying your apps to others. Similarly, because there are single points of ingress and egress (sometimes), security is heightened by having only one way in or out, as opposed to opening the entire network or each server individually to the internet. Using public and private subnets is also really nice for server resiliency. Say your network was attacked with a DDOS attack, where someone tries to overwhelm your servers with junk attacks to take down your servers. It’s way better if they manage to succeed at taking down your server, but it’s just a proxy as opposed to your analytics workhorse server or a mission-critical database. When configuring servers this way, it’s very common to configure an additional server called a bastion host or jumpbox that is used solely to pass SSH traffic from server administrators through to the hosts in the private network. 5.1 Finding the right server with hostnames Like we configured a URL to have an abstraction layer between our IP address and our server, we also will usually configure a hostname rather than using IP addresses. 5.2 The role of forward and reverse proxies 5.3 Using ports to get to the right service Let’s say you want to run software on a server. One of the big differences between server software, and software on your laptop is that server software needs to be able to interact with the outside world to be useful. For example, when you want to use Microsoft Word on your computer, you just click on the button and then it’s ready to go. But say I want to use RStudio Server. I don’t have a desktop where I click to open RStudio Server. Instead, I go to a particular URL and I expect that RStudio will be there, ready to listen. 5.4 Configuring the private network A private network is defined by a range of IP addresses, called CIDR blocks. A CIDR block defines the sub-addresses available inside an IP address using a /n notation. So the CIDR block 10.0.0.0/16 is the largest possible CIDR block and includes all IP addresses from 10.0.0.0 to 10.255.255.255. The numbers after the / are related to powers of 2 (binary something blah), so you can fit 2 /x+1 CIDR blocks in a single /x block. So, for example, there are 128 IP addresses in a /25 CIDR block, and 256 in a /24 CIDR block, so 10.0.0.0/24 could have two subnets, 10.0.0.0/25 and 10.0.0.128/25. 5.4.1 Restricting Outbound Connections In AWS, you’ll configure egress from private subnets using a NAT Gateway. Egress from the public subnets goes through the Internet Gateway. Egress is actually needed less than you might think - in our current setup, it’s needed for package updates to RSPM (the RSPM service, not RSP to RSPM), to do google OAuth to the google servers (rstudioservices only), get stuff from github, update products, and get content from apt/yum repos. In Colorado, it’s not needed for RSC because the SAML token is in the browser. It is needed for RSP because Onelogin is outside the VPC. This is generally not the case for customers, who are mostly using on-prem AD servers. 5.5 Special IP Addresses and Ports All ports below 1024 reserved. 80 - HTTP default 443 - HTTPS default 22 - SSH default 5.6 Exercises Consider going to the website google.com. Draw a diagram of how the following are incorporated: TCP/IP, DNS, HTTP, HTTPS. Set up a free-tier EC2 instance and put an NGINX server up. Figure out how to allow your computer to access the server, but not your phone. Try accessing it on a non-default port. Try to HTTP into a fresh EC2 with the default security group. Take a look at the inbound security group rules. Hint: is there an inbound rule on a default HTTP port? SSH into your EC2 instance and try to reach out to something on the internet (curl…). See if you can change security group rules to shut down access. Can you do it by changing the IP address range it’s accepting connections from? Can you do it by changing the listening ports? "],["ssh.html", "Chapter 6 Accessing Servers via SSH 6.1 What is SSH? 6.2 How does SSH work? 6.3 Basic SSH Use 6.4 Getting Comfortable in your own setup 6.5 Advanced SSH Tips + Tricks 6.6 Exercises", " Chapter 6 Accessing Servers via SSH Learning Objectives: Understanding of what SSH is Understanding of how to use SSH to access a server 6.1 What is SSH? Stands for secure (socket) shell Way to remotely access a different (virtual) machine Workhorse of doing work on another server 6.2 How does SSH work? Via Public Key Encryption Public/Private Key Known Hosts [Diagram: SSH Keys] &gt; Sidebar on public key encryption By default, available on port 22 6.3 Basic SSH Use The terminal 3 step process generate public/private keys ssh-keygen place keys in appropriate place use ssh to do work Permissions on key 6.4 Getting Comfortable in your own setup Using ssh-config 6.5 Advanced SSH Tips + Tricks SSH Tunneling/Port Forwarding -vvvv, -i, -p [Diagram: Port Forwarding] 6.6 Exercises Draw your own diagram of the sequence of things that occur when you execute ssh user@server.company.com in the terminal. Stand up a new server on one of the major cloud providers. Try logging in using the provided key file. Create a new user on the server. Generate a new SSH key locally and copy the correct key onto the server (think for a moment about which key is the correct one – consult your diagram from step 1 if necessary). Set up an SSH alias so you can SSH into your server using just ssh testserver (hint: look at your SSH config). Create a local text file and SFTP it to the server. SSH in, edit the file, and SFTP it back. [Advanced] Stand up a nginx server on your remote instance. Don’t open the port to the world, but SSH port forward the server page to your local browser. "],["auth.html", "Chapter 7 How login works 7.1 The many flavors of auth (or what does SSO mean?) 7.2 Auth Techniques 7.3 Auth Technologies", " Chapter 7 How login works You do auth all the time Private auth – login to your bank with a unique username and pw MFA (2FA) – get a text message or use an authenticator token Or yubikey, etc Public auth – you might login to spotify using your facebook, apple, or google credentials Spotify is a Service Provider and the other side an Identity Provider For the most part, this chapter is designed to help you talk to the folks who manage auth at your organization. Unless you’ve got a small organization with only a few data scientists, you probably won’t have to manage this yourself. But it can be extremely helpful to understand how auth works when you’re trying to ask for something from the organization’s admins. In order to talk concretely about logging into systems, it’s helpful to clarify some terms. For the most part, these terms are industry standard, but I’m also going to generalize some terms because they’re used for certain types of auth, and they’re really useful. For the purposes of this document, we’re going to be talking about trying to login to a service. A service is something you want to login to – it could be your phone or your email, or a server, a database, or software like RStudio Server or JupyterHub. When you go to login to a service, there are two things that have to happen. First, you have to assert and verify who you are. This process is called authentication, and the assertion and proof of identity are your credentials or creds. The most common credentials are a username and password, but there are other options including SSH keys, multi-factor authentication, or biometrics. Once you’ve proven who you are, then the system needs to determine what you’re supposed to have access to. This process is called authorization. Often, the authentication + authorization process is referred to collectively as auth. [#TODO: Image of Auth Make the user provide creds (username/password) System checks credentials against store (For SSO) Check if user is allowed to access resource Send back something User is authorized] So, to summarize when you go to login to a service, the service reaches out to some sort of system to verify your identity. Depending on the method, it may also check your authorization. The name of that system varies by the auth method, but for the purposes of this chapter, we’ll refer to it generally as the identity store. Depending on the auth method, the identity store may store just authentication records, or both authentication and authorization. 7.1 The many flavors of auth (or what does SSO mean?) Single Sign On (SSO) is a slippery term, so it is almost always necessary to clarify what is meant by the term when you hear it. At some organizations, identity management isn’t centralized at all. This means that usernames and passwords are unique to each service, onboarding and offboarding of users has to be handled independently for each service, and users have to login frequently. In short, it’s often not a great system. This is never referred to as SSO. Most organizations of a meaningful size have centralized identity management. This means that identities, credentials, authorization, onboarding, and offboarding are handled centrally. However, you may still need to independently login to each system. For example, in this system, every service might take the same username and password as your credentials, but if you go to RStudio Server followed by JupyterHub, you’ll need to provide that username and password independently to each service. This system is often facilitated by PAM, and LDAP/AD. Some organizations call this SSO, because there’s only one set of credentials. In true SSO, users login once and are given a token or ticket.4 Then, when they go to the next service, they don’t have to login again because that service can just look at the token or ticket to do auth for that user. For example, in this system, I could go to RStudio Server and login, and then go to JupyterHub and get in without being prompted again for my password. This type of auth is facilitated by Kerberos, SAML, or OAuth. 7.2 Auth Techniques If you have five data scientists in your group, and the only shared resource you have is an RStudio Server instance, you probably don’t need to think terribly hard about auth. It’s pretty straightforward to just make users on a server and give them access to everything. But as organizations get larger with hundreds or thousands of users, there’s constant churn of people joining and leaving. The number of services can creep into the dozens or hundreds and people may have very different authorization levels to different services. Trying to manage auth on the individual services is a nightmare – as is trying to keep that many usernames and passwords straight for users. That is why almost all organizations with more than a few users have centrally managed auth. 7.2.1 You get a permission and you get a permission! For the most part, we think of people being authenticated and authorized into services. However, it’s sometimes useful to consider the broader class of entities that could do auth. There are two common non-human entities that are included in auth systems that are worth considering. Service Accounts are accounts given to non-human entities when you want it to be able to do something on its own behalf. For example, maybe you’ve got a Shiny app that users use to visualize data that’s in a database. Very often, you don’t want the app to have the same permisions as the app’s author, or to inherit the permissions of the people viewing the app. Instead, you want the app to be able to have permissions to do certain database operations. In that case, you would create a service account to give to the Shiny app that has exactly those permissions. There are also times where it’s useful to go one level up and give permissions to an entire instance or service. In that case, you might assign permissions to an instance. For example, you could make it the case that anyone who is logged into the JupyterHub server is allowed to read from the database. Instance permissions are rather broad, and so they are usually only applied when you’ve got multiple resources inside a private network. In that case, authentication and authorization are only done at a single point and authorization is pretty broad. 7.2.2 Authorization is kinda hard From a management perspective, authentication is pretty simple. A person is given a set of credentials, and they have to supply those credentials when prompted to prove they are who they say they are. Authorization is a whole other can of worms. There is a meaningful literature on varieties of authorization and how they work. We’re not going to get too deep into the weeds, other than to define some common terms and how they’re used. The atomic basis for authorization is a permission. Permissions are a binary switch that answers the question is this person allowed to do the thing they are trying to do?5 The simplest way of assigning permissions is called an access control list (ACL). In systems that use ACLs, each piece of content has a list of users who are allowed access. Sometimes, ACLs are also assigned to groups, which are simply sets of users – think data-scientists. One ACL implementation with which you may be familiar is file permissions on a Linux server. For example, if you have a Mac or are on a Linux server, you can open your terminal, navigate to a directory and do the following: $ ls -l -rwxr-xr-x 1 alexkgold staff 2274 May 10 12:09 README.md That first set of characters describes the ACL for the README.md file. The first character - indicates this is a file, as opposed to a directory of files (which would be d). Then there are three sets of 3 characters, rwx, which are short for read, write, and execute, with the first group for the owner, alexkgold, the second group for anyone else in the group staff, and the third set for anyone else. So you can read -rwxr-xr-x as, this is a file that alexkgold can read, write or execute, and anyone else can read or execute, but not edit. ACLs are pretty intuitive, but it turns out that when you are managing a lot of users across a lot of files, directories, and services, they can get pretty difficult to manage, so many organizations use Role Based Access Control (RBAC). RBAC adds a layer of abstraction between users and permissions, which makes it a little harder to understand, but ultimately results in a much more flexible system. In RBAC, permissions are not assigned to individual pieces of content or to users or groups. Instead, permissions are assigned to roles, and roles are given to users or groups.6 There are also further iterations on the RBAC model, like Attribute Based Access Control (ABAC) or Policy Based Access Control (PBAC) in which there’s a long list of attributes that could be considered for a user to compute their permissions for a given service. 7.3 Auth Technologies 7.3.1 Username + Password Many pieces of software come with integrated authentication. When you use those system, the product stores encrypted username and password pairs in a database. These setups are often really easy from an admin perspective – you just set up individual users on the server. However, the flip side is that users have one more username and password to remember, which is annoying for them. Moreover, if you have more than a few users, or the system is one of more than a few, it’s hard to manage users on a lot of different systems. It can be a real pain to create accounts on a ton of different systems when a new person joins the organization, or to remove their permissions one-by-one when they leave. For this reason, most IT/Admin organizations strongly prefer using some sort of centralized identity store. 7.3.2 PAM Pluggable Authentication Modules (PAM) is a Linux system for doing authentication. As of this writing, PAM is the default authentication method for both RStudio Server and JupyterHub. Conceptually PAM is pretty straightforward. You install a service on a Linux machine and configure it to use PAM authentication from the underlying host. By default, PAM just authenticates against the users configured on the Linux server, but it can also be configured to use other sorts of “modules” to authenticate against other systems – most commonly LDAP/AD or Kerberos. PAM can also be used to do things when users login – the most common being initializing tokens or tickets to other systems, like a database. PAM is often paired with System Security Services Daemon (SSSD), which is most commonly used to automatically create Linux users on a server based on the identities stored in an LDAP/AD instance. Though conceptually simple, reading, writing, and managing PAM modules is kinda painful. #TODO: Add PAM example 7.3.3 LDAP/AD Lightweight Directory Access Protocol (LDAP) is a relatively old, open, protocol used for maintaining a set of entities and their attributes. To be precise, LDAP is actually a protocol for maintaining and accessing entities and their attributes in a tree. It happens that this is a really good structure for maintaining permissions and roles of users at an organization, and it’s the main thing LDAP is used for. Active Directory (AD) is Microsoft’s implementation of LDAP, and is by-far the most common LDAP “flavor” out there. AD so thoroughly owns the LDAP enterprise market, that LDAP is often referred to as LDAP/AD. There are other implementations you may run across, the most common being OpenLDAP. Azure, Microsoft’s cloud provider, offers an authentication service called Azure Active Directory (AAD). Confusingly, AAD is usually used in combination with SAML, not LDAP. It’s worth distinguishing the use of LDAP as an identity store from its use as an authentication technology. As a tree-based database, LDAP is uniquely well-suited to storing the identities, and other attributes of people at the organization. However, as discussed below, using LDAP to authenticate into actual services has security and convenience drawbacks, and many organizations consider it outdated and insecure. A lot of organizations are moving away from LDAP for authentication in favor of token-based technologies like SAML or OAuth, but many are keeping LDAP as their identity “source of truth” that is referenced by the SAML or OAuth Identity Provider. LDAP has three main disadvantages relative to other technologies. First, LDAP requires that your credentials (username and password, usually) actually be provided to the service you’re trying to use. This is fundamentally insecure relative to a system where your credentials are provided only to the identity provider, and the service just gets a token verifying who you are. In token-based systems, adding additional requirements like MFA or biometrics are easy, as they’re simply added at the IdP layer. In contrast, doing those things in LDAP would require the service to implement them, which usually is not the case, so you’re usually limited to username and password. The second disadvantage of LDAP is that it does not allow for central administration of permissions. LDAP directly records only objects and their attributes. Say, for example, you want only users of a particular group to have access to a certain resource. In LDAP, you would have to specify in that resource that it should only allow in users of that group. This is in contrast to SAML/OAuth, where the authorization is centrally managed. Lastly, LDAP authentication is based on each service authenticating. Once you authenticate, the service might give you a cookie so that your login persists, but there is no general-purpose token that will allow you to login to multiple services. 7.3.3.1 How LDAP Works While the technical downsides of LDAP are real, the tradeoff is that the technical operations of LDAP are pretty straightforward. In short, you try to login to a service, the service collects your username and password, sends it off to the LDAP server, and checks that your username and password are valid. Note that LDAP is purely for authentication. When you’re using LDAP, authorization has to be handled separately, which is one of the disadvantages. 7.3.3.2 Deeper Than You Need on LDAP LDAP is a tree-based entity and value store. This means that LDAP stores things and their attributes, which include a name and one or more values. For example, my entry in a corporate LDAP directory might look like this: cn: Alex Gold mail: alex.gold@example.com mail: alex.gold@example.org department: solutions mobile: 555-555-5555 objectClass = Person Most of these attributes should be pretty straightforward. cn is short for common name, and is part of the way you look up an entity in LDAP (more on that below). Each entity in LDAP must have an objectClass, which determines the type of entity it is. In this case, I am a Person , as opposed to a device, domain, organizationalRole, or room – all of which are standard objectClasses. Let’s say that your corporate LDAP looks like the tree below: #TODO: make solutions an OU in final The most common way to look up LDAP entities is with their distinguished name (DN), which is the path of names from the point you’re starting all the way back to the root of the tree. In the tree above, my DN would be cn=alex,ou=solutions,dc=example,dc=com. Note that you read the DN from right to left to work your way down the tree. Aside from cn for common name, other common fields include ou for organizational unit, and dc for domain component. 7.3.3.3 Trying out LDAP Now that we understand in theory how LDAP works, let’s try out an actual example. To start, let’s stand up LDAP in a docker container: #TODO: update ldif docker network create ldap-net docker run -p 6389:389 \\ --name ldap-service \\ --network ldap-net \\ --detach alexkgold/auth ldapsearch is a utility that lets us run queries against the LDAP tree. Let’s try it out against the LDAP container we just stood up. Let’s say I want to return everything in the subtree under example.org. In that case, I would run ldapsearch -b dc=example,dc=org, where b indicates my search base, which is a dn. But in order to make this actually work, we’ll need to include a few more arguments, including the host where the LDAP server is, indicated by -H the bind DN we’ll be using, flagged with -D the bind password we’ll be using, indicated by -w Since we’re testing, we’re also going to provide the flag -x to use whatever certificate is present on the server. Putting it altogether, along with the commands to reach the docker container, let’s try: ldapsearch -x -H ldap://localhost:6389 -b dc=example,dc=org -D &quot;cn=admin,dc=example,dc=org&quot; -w admin # extended LDIF # # LDAPv3 # base &lt;dc=example,dc=org&gt; with scope subtree # filter: (objectclass=*) # requesting: ALL # # example.org dn: dc=example,dc=org objectClass: top objectClass: dcObject objectClass: organization o: Example Inc. dc: example # admin, example.org dn: cn=admin,dc=example,dc=org objectClass: simpleSecurityObject objectClass: organizationalRole cn: admin description: LDAP administrator userPassword:: e1NTSEF9d3IyVFp6SlAyKy9xT2RsQ0owTDYzR0RzNFo0NUFrQ00= # search result search: 2 result: 0 Success # numResponses: 3 # numEntries: 2 You should be able to read what got returned pretty seamlessly. One thing to notice is that the user password is returned, so it can be compared to a password provided. It is encrypted, so it doesn’t appear in plain text. Note that ldap is a protocol – so it takes the place of the http you’re used to in normal web operations. Like there’s https, there is also a protocol called LDAPS, which is ldap + tls for the same reason you’ve got https. LDAP is (almost) always running in the same private network as the service, so many organizations don’t require using LDAPS, but others do require it. Running the ldapadmin docker run -p 6443:443 \\ --name ldap-admin \\ --env PHPLDAPADMIN_LDAP_HOSTS=ldap-service \\ --network ldap-net \\ --detach osixia/phpldapadmin dn for admin cn=admin,dc=example,dc=org pw: admin https://localhost:6443 # Replace with valid license export RSC_LICENSE=XXXX-XXXX-XXXX-XXXX-XXXX-XXXX-XXXX # Run without persistent data and using default configuration docker run -it --privileged \\ --name rsc \\ --volume $PWD/rstudio-connect.gcfg:/etc/rstudio-connect/rstudio-connect.gcfg \\ -p 3939:3939 \\ -e RSC_LICENSE=$RSC_LICENSE \\ --network ldap-net \\ rstudio/rstudio-connect:latest 7.3.3.4 Single vs Double Bind There are two different ways to establish a connection between your server and the LDAP server. The first method is called Single Bind. In a single bind authentication, the user credentials are used both to authenticate to the LDAP server, and to query the server. In a Double Bind configuration, there is a separate administrative service account, used to authenticate to the LDAP server. Once authentication is complete, then the user is queried in the system. Single bind configurations are often more limited than double bind ones. For example, in most cases you’ll only be able to see the single user as well as the groups they’re a part of. This can limit application functionality in some cases. On the other hand, there need be no master key maintained on your server, and some admins may prefer it for security reasons. We can see this really concretely. In the example above, you used a double bind by supplying admin credentials to LDAP. Let’s say instead, you just provide a single user’s credentials. In that case, I don’t get anything back if I just do a general search. ldapsearch -x -H ldap://localhost:6389 -b dc=example,dc=org -D &quot;cn=joe,dc=engineering,dc=example,dc=org&quot; -w joe # extended LDIF # # LDAPv3 # base &lt;dc=example,dc=org&gt; with scope subtree # filter: (objectclass=*) # requesting: ALL # # search result search: 2 result: 32 No such object # numResponses: 1 But just searching for information about Joe does return his own information. ldapsearch -x -H ldap://localhost:6389 -b cn=joe,dc=engineering,dc=example,dc=org -D &quot;cn=joe,dc=engineering,dc=example,dc=org&quot; -w joe  32 ✘ # extended LDIF # # LDAPv3 # base &lt;cn=joe,dc=engineering,dc=example,dc=org&gt; with scope subtree # filter: (objectclass=*) # requesting: ALL # # joe, engineering.example.org dn: cn=joe,dc=engineering,dc=example,dc=org cn: joe gidNumber: 500 givenName: Joe homeDirectory: /home/joe loginShell: /bin/sh mail: joe@example.org objectClass: inetOrgPerson objectClass: posixAccount objectClass: top sn: Golly uid: test\\joe uidNumber: 1000 userPassword:: e01ENX1qL01raWZrdk0wRm1sTDZQM0MxTUlnPT0= # search result search: 2 result: 0 Success # numResponses: 2 # numEntries: 1 7.3.4 Kerberos Tickets Kerberos is a relatively old ticket-based auth technology. In Kerberos, encrypted tickets are passed around between servers. Because these tickets live entirely on servers under the control of the organization, they are generally quite secure. Though Kerberos is freely available, it was widely adopted along with Active Directory, and it’s used almost exclusively in places that are running a lot of Microsoft products. A frequent use of Kerberos tickets is to establish database connections. Because the tickets are passed around from server to server, Kerberos can be used to create a true SSO experience for users. 7.3.4.1 How Kerberos Works All of Kerberos works by sending information to and from the central Kerberos Domain Controller (KDC). In Kerberos, authentication and authorization are handled independently. When a Kerberos session is initialized, the service sends the users credentials off to the KDC and requests something called the Ticket Granting Ticket (TGT) from the KDC. TGTs have a set expiration period. When they expire, the client has to request an updated TGT. This is one reason why Kerberos is considered quite secure - even if someone managed to steal a TGT, they’d only be able to use it for a little while before it went stale and could be revoked. When the user wants to actually do something, they send the TGT back to the KDC again and get a session key (sometimes referred to as a service ticket) that allows access to the service, usually with a specified expiration period. 7.3.4.2 Try out Kerberos #TODO 7.3.5 SAML These days Security Assertion Markup Language (SAML) is probably the most common system that provides true SSO – including single login and centrally-managed permissions. SAML does this by passing around XML tokens.7 The way this generally works is that a user attempts to login to a Service Provider (SP). The SP redirects the user to an Identity Provider (IdP), which checks either for a preexisting token in the users browser, or verifies the users credentials. The IdP checks for the user’s authorization to access the SP in question, and sends an authorization token back to the SP. Relative to LDAP/AD, which is from the early 1990s, SAML is a new kid on the block. SAML 1.0 was introduced in 2002, and SAML 2.0, which is the current standard, came out in 2005. Many large enterprises are switching their systems over to use SAML or have already done so. One superpower of SAML IdPs is that many of them can federate identity management to other systems. So, it’s pretty common for large enterprises to maintain their user base in one or more LDAP/AD system, but actually use a SAML IdP to do authentication and authorization. In fact, this is what Azure Active Directory (AAD), which is Microsoft Azure’s hosted authentication offering does. It is possible to use LDAP/AD with AAD, but most organizations use it with SAML. One of the nice things about SAML is that credentials are never shared directly with the SP. This is one of the ways in which SAML is fundamentally more secure than LDAP/AD – the users credentials are only ever shared with the IdP. There are two different ways logins can occur – starting from the SP, or starting from the IdP. In SAML, the XML tokens that are passed back and forth are called assertions. 7.3.5.1 Try SAML We’re going to use a simple SAML IdP to try out SAML a bit. This container only supports a single SP. Any IdP that might be used in an enterprise environment is going to support many SPs simultaneously. Let’s go through the environment variables we’re providing to this docker run command. We’re providing three different arguments: The SP_ENTITY_ID is the URL of the SP_ASSERTION_CONSUMER_SERVICE is the URL of the SP that is prepared to receive the authorized responses coming back from the SAML IdP. SP_SINGLE_LOGOUT_SERVICE is the URL where the SP will receive a logout command once someone has been logged out at the IdP level. Many SPs do not implement single logout. docker run --name=saml_idp \\ -p 8080:8080 \\ -p 8443:8443 \\ -e SIMPLESAMLPHP_SP_ENTITY_ID=http://app.example.com \\ -e SIMPLESAMLPHP_SP_ASSERTION_CONSUMER_SERVICE=http://localhost/simplesaml/module.php/saml/sp/saml2-acs.php/test-sp \\ -e SIMPLESAMLPHP_SP_SINGLE_LOGOUT_SERVICE=http://localhost/simplesaml/module.php/saml/sp/saml2-logout.php/test-sp \\ -d kristophjunge/test-saml-idp:1.15 http://localhost:8080/simplesaml admin/secret 7.3.6 OIDC/OAuth2.0 OIDC/OAuth is slightly newer than SAML, created in 2007 by engineers at Google and Twitter. OAuth 2.0 – the current standard was released in 2012. If you’re being pedantic, OAuth is a authorization protocol, and OpenID Connect (OIDC) is an authorization protocol that uses OAuth. In most cases, people will just call it OAuth. #TODO: this picture is bad In an enterprise context, OAuth/OIDC is conceptually very similar to SAML – but instead of passing around XML tokens, it’s based on JSON Web Tokens (JWT, usually pronounced “jot”). The fact that JSON is much more human-readable than XML is one of the big advantages of OIDC/OAuth compared to SAML. #TODO: try it out 7.3.6.1 OAuth/OIDC vs SAML From a practical perspective, the biggest difference between OAuth/OIDC and SAML is that SAML is quite strict about what SPs are allowed. Each SP needs to be registered at a specific web address that the IdP knows it’s allowed to receive requests from. In contrast, OAuth/OIDC was designed to be used to delegate authentication and authorization to different kinds of services that might be widely available on the internet. If you’ve ever allowed a website to Login with Apple/Google/Facebook/Github, that has been an application of OAuth/OIDC. Because the set of allowable SPs is fixed under SAML, it’s more common in enterprise settings. Some admins consider SAML more secure for that reason as well. In some situations, SAML is used for authentication and OAuth is used for access to other services. Most commonly in the data science world, this can come up when a user logs into a service like RStudio Server and is then authorized to a database using an OAuth JWT. Resources: https://www.okta.com/identity-101/saml-vs-oauth/ https://www.okta.com/identity-101/whats-the-difference-between-oauth-openid-connect-and-saml/ https://phoenixnap.com/blog/kerberos-authentication https://www.dnsstuff.com/rbac-vs-abac-access-control Depending on the system, this token or ticket can live in various places.↩︎ As we’ll see in a second, permissions can actually belong to entities other than people…but that’s a little complicated for the moment.↩︎ If you think about this for a minute, it may occur to you “isn’t RBAC basically the same as doing ACLs at the group level?” Yes, yes it is, but it can be helpful to have groups and roles be separated, even if there’s a 1-1 mapping.↩︎ XML is a markup language, much like HTML. The good thing about XML is that it’s very flexible. The bad thing is that it’s relatively hard for a human to easily read.↩︎ "],["choosing-and-using-a-data-architecture.html", "Chapter 8 Choosing and Using A Data Architecture 8.1 Options for Data Storage 8.2 How to pick 8.3 Working with File Systems 8.4 Working with Databases 8.5 Exercises", " Chapter 8 Choosing and Using A Data Architecture Learning Objectives Understand options for data architectures Understand interactions between data and auth DataOps is a huge field – a lot of it touches data science Some of the biggest consideration when you’re thinking about data science 8.1 Options for Data Storage 3 basic types of data stores: flat file (csv, pickle/rds), flat file w/ interactive read (arrow), database Implications how to configure access Database is standalone option, usually requires configuring separate server Other two can be configured two main ways: File system Bucket Storage Data Lake – usually for raw data, basically bucket storage writ large, sometimes include interacticve read capabilities Data Warehouse – usually a database 8.2 How to pick Flat files are simplest Flat files w/ interactive read are great for med-large data Databases often already exist If not, a few tips: SQL-based databases are the backbone of analytics work Database as a service is a basic cloud provider There are SO many options TODO: How much more in depth am I going here? 8.3 Working with File Systems Working on your laptop file system isn’t great Working in a cloud provider is different than on your laptop Volumes are separate from compute Often have automated snapshots/backups Volumes can also be shared across multiple servers Process of putting a volume on a server is called mounting 8.4 Working with Databases Two main ways to work with databases from R/Python - direct connection or w/ connector/driver (JDBC/ODBC) https://docs.google.com/presentation/d/1wJ3YnB4ob3AkNRDLpvYy3B-JMUaVsPjl3AA5QkN7fII/edit 8.4.1 Security Row-level security Kerberos JWT IAM roles 8.5 Exercises Connect to and use an S3 bucket from the command line and from R/Python. Stand up a container with Postgres in it, connect using isql, then from R/Python using ODBC/JDBC. Stand up an EC2 instance w/ EBS volume, change EBS to different instances. Stand up an NFS instance and mount onto a server. "],["environments-and-reproducibility.html", "Chapter 9 Environments and Reproducibility", " Chapter 9 Environments and Reproducibility Learning Objectives: Understand the layers that go into reproducing an output Understand which part is “the environment” Understand some general strategies/tooling for reproducing Managing environments is hard. Necessary in data science primarily for repro. At least 7 levels: Hardware Operating System System Libraries Programming Language Packages App Code App Data -&gt; Difference between environment and data is sketchy All of it contributes to “state” [graphic: Image of concentric environments] Different requirements depending on if you’re talking about development (IDE) environment or production environment. Repository vs library mgmt Tools to Reproduce Parts of Stack: IaC tooling Provisioning Configuration mgmt AMIs virtualenv/conda/renv Docker/Containers "],["the-cloud.html", "Chapter 10 The Cloud 10.1 The Rise of Services 10.2 Common Services That will be helpful to know offhand 10.3 Cloud Tooling 10.4 Exercises", " Chapter 10 The Cloud Learning Objectives: Understand what the cloud is and why it’s so hype Demystify general cloud offerings General description for servers that you rent instead of buy 3 major cloud providers AWS = Amazon Azure = Microsoft GCP = Google 10.1 The Rise of Services Server/Cloud usage has gone the way of the rest of the economy – services The “rent me a server + connectors” model is generally called IaaS Layers of abstraction on top of that – PaaS, Saas Metaphor: Cake shop [Diagram: Cake Shop] These metaphors are helpful for general understanding…actually applying them gets really fuzzy in data science land, because something that is SaaS from IT perspective is PaaS from data scientist perspective. Often find it more helpful to just categorize as IaaS/“something more abstracted”. These services are generally surrounded my marketing jargon – rarely do they just explain this is ___ as a service. https://stackoverflow.com/questions/16820336/what-is-saas-paas-and-iaas-with-examples 10.2 Common Services That will be helpful to know offhand Amazon’s names aren’t completely obvious. They’re usually referred to by their 3-letter acronyms. Azure and Google’s offerings tend to be named more literally. 10.3 Cloud Tooling Identity MGMT - IAM, Azure AD Billing Mgmt 10.3.1 IaaS Compute - AWS EC2, Azure VMs, Google Compute Engine Storage file - EBS, Azure managed disk, Google Persistent Disk Network drives - EFS, Azure Files, Google Filestore block/blob - S3, Azure Blob Storage, Google Cloud Storage Networking: Private Clouds: VPC, Virtual Network, Google Virtual Private Cloud DNS - Route53, Azure DNS + Traffic Manager, Google Cloud DNS 10.3.2 Not IaaS Container Hosting - ECS, Azure Container Instances + Container Registry K8S cluster as a service - EKS, AKS, GKE Run a function as a service - Lambda, Azure Functions, Google Cloud Functions Database - RDS/Redshift, Azure Database, Google Cloud SQL SageMaker - ML platform as a service, Azure ML, Google Notebooks https://docs.microsoft.com/en-us/azure/architecture/aws-professional/services#networking https://cloud.google.com/free/docs/aws-azure-gcp-service-comparison 10.4 Exercises Example of something you’d want to build – for each of the 3 providers, which services would you use if you wanted an IaaS solution, a PaaS solution? "],["it-and-data-science-workflows.html", "Chapter 11 IT and Data Science Workflows 11.1 Dev/Test/Prod 11.2 Cattle, Not Pets 11.3 How Does Stuff Actually Get Deployed 11.4 Exercises", " Chapter 11 IT and Data Science Workflows Learning Objectives: Understand what dev/test/prod means Understand how dev/test/prod might apply to data science Well developed patterns for putting things into prod in IT Data science starting to adopt, but not exactly the same 11.1 Dev/Test/Prod Classic workflow - three precise mirrors Do dev work in dev env Test for functionality and with users (User Acceptance Testing - UAT) Deploy to production environment Doesn’t quite work for data science Often, dev is a distinct (RStudio IDE/Jupyter) Because more exploratory than pure software development Test/Prod can be on one server or multiple, depending on org If you’re managing your own Data Science server, you’ll want an additional staging environment – unrelated to app changes, purely for IT changes [Graphic: deploying app changes and server changes] 11.2 Cattle, Not Pets Manual promotion from place to place = bad State maintenance Drift Cattle not Pets Infrastructure as code tooling Terraform AWS CFN, Azure Resource Manager, Google Cloud Deployment Manager Chef Puppet Saltstack Ansible Docker Vagrant Helm/Helmfile Important to ensure idempotence 11.3 How Does Stuff Actually Get Deployed [Graphic: CI/CD deployment into test/prod] CI/CD - Jenkins, Travis, GHA Powered by Git, mostly Intro to using GHA 11.4 Exercises Create something and add GHA integration [TODO: What to use as example?] Stand up some virtual environments using ___ [TODO: Which ones to try?] "],["scaling.html", "Chapter 12 Scaling 12.1 Types of Scaling 12.2 Load-Balancing configurations 12.3 Adding servers in real time", " Chapter 12 Scaling 12.1 Types of Scaling Vertical Scaling – just make the server bigger Horizontal – add more parallel servers Sometimes called load-balancing Reliability - or high-availability (HA) Make sure the service doesn’t go down (or less often) Eliminate single-points-of-failure Really hard to do all the way – spectrum of completeness + difficulty Health Checks/Heartbeats – periodic checkins to ensure server/service healthy 12.2 Load-Balancing configurations [Graphic: network diagram of lb-config] Active/Active - all servers accept traffic Single vs multiple masters Active/Passive (fallover/failover) - secondary server, remains inert until 2nd one fails Disaster Recovery - backup data to another disk – somewhat slower resumption of service 12.3 Adding servers in real time If you find yourself under heavy load, you’ll want to add more servers If you’re using infrastructure as code tooling, this is easy It’s also easy if using some sort of docker orchestration, as the underlying hosts are all the same from the perspective of docker/K8S Spannning Multiplee AZs "],["docker.html", "Chapter 13 Docker 13.1 Container Orchestration 13.2 Exercises", " Chapter 13 Docker Docker is so hype right now. Get ready for layers of abstractions till your head spins. Docker containers are a way to isolate an app and its dependencies from the underlying host [Graphic: Docker on underlying host] A docker container is easily portable from one place to another for that reason The “secret sauce” of docker is that containers launch REALLY quickly. Lifecycle: Dockerfile -&gt; Image -&gt; Container [Graphic: Docker lifecycle] 13.1 Container Orchestration Kubernetes (K8S) – software for deploying and managing containers Helm is the standard tool for defining kubernetes deployments. Helmfile is a templating system for helm. There are other competitors, most notably docker swarm, but K8S is by far the biggest The line is fuzzy though – there are container orchestration services that aren’t K8S or even abstract a level up from K8S. [TODO: Graphic of K8S/Docker] 13.2 Exercises Put a shiny app in a container, run it on your desktop. Put that container into a container registry. Use a local K8S distribution to run several instances of that container. "],["offline.html", "Chapter 14 Offline", " Chapter 14 Offline Some organizations require that servers not be connected to the internet (airgapped/offline) This makes things hard. Understanding whether it’s inbound connections that are disallowed or both outbound + inbound is important. Difficulties that tend to arise: Downloading software updates Getting R/Python packages Software licensing (often reach to license servers) "],["things-to-add-additional-topics.html", "Chapter 15 Things To Add (Additional Topics)", " Chapter 15 Things To Add (Additional Topics) Security groups "],["determining-what-you-need.html", "Chapter 16 Determining What You Need", " Chapter 16 Determining What You Need Determining Requirements for your platform Platform requirements combination of Work being done on the platform Data Science needs (i.e. horsepower) Existing state of org’s data Type of work data scientists might do Batch Jobs ETL Reports Data Generation Real-Time Interactive Models APIs Apps Data Science + IT Needs Languages (R, Python, SQL) Sharing – internal, external Online or offline? Data Security – row based access? Centralized workbench vs individual workstations Existing Assets BI/Analytics Platforms Data Connections Checklist of above "],["getting-started-with-terminal.html", "Chapter 17 Getting Started with Terminal", " Chapter 17 Getting Started with Terminal Terminal in Mac Iterm2 bash, zsh, fish windows WSL Powershell PuTTY – no longer needed, but still an option Do need to enable SSH client tmux vim/nano "],["useful-shell-commands.html", "Chapter 18 Useful Shell Commands 18.1 Miscellaneous Symbols 18.2 Moving yourself and your files 18.3 Checking out Files 18.4 Checking out Server Activity 18.5 Checking out Networking 18.6 User Management", " Chapter 18 Useful Shell Commands Unlike on your Windows or Mac desktop, most work on a server is done via the shell – an interactive text-based prompt. If you master the set of commands below, you’ll always feel like a real hacker. In most cases, you’ll be using the bash shell. If you’re using Linux or Mac, that’ll be the default. Below, I’m intentionally mixing up bash commands and Linux system commands because they’re useful. If you know the difference and are pedantic enough to care, this list isn’t for you anyway. 18.1 Miscellaneous Symbols Symbol What it is Helpful options Example / system root ~ your home directory echo ~ / home/alex.gold . current working directory man manual | the pipe echo $ sudo su 18.2 Moving yourself and your files C ommand What it does Helpful options Example pwd print working directory $ pwd /U sers/alex.gold/ cd change directory $ cd ~/Documents ls list -l - format as list -a - all include hidden files $ ls . $ ls -la rm remove delete permanently! -r - recursively a directory and included files -f - force - don’t ask for each file $ rm old_doc r m -rf old_docs/ BE VERY CAREFUL WITH -rf cp copy mv move chmod 18.3 Checking out Files Often useful in server contexts for reading log files. C ommand What it does Helpful options Example cat less tail -f grep tar 18.4 Checking out Server Activity C ommand What it does Helpful options Example df -h top ps lsof 18.5 Checking out Networking C ommand What it does Helpful options Example ping ne tstat curl 18.6 User Management C ommand What it does Helpful options Example w hoami p asswd us eradd "],["more-to-say.html", "A More to Say A.1 An aside on open source A.2 Headers", " A More to Say Yeah! I have finished my book, but I have more to say about some topics. Let me explain them in this appendix. To know more about bookdown, see https://bookdown.org. A.1 An aside on open source I personally find open source software fascinating. As a data scientist working in R or Python, you might care about the water you’re swimming in. If you don’t, that’s fine too and you don’t need to worry about this. Feel free to skip ahead. Unlike Mac OS or Windows, Linux is open source. You’ve probably already heard of open source licensing, as the R and Python languages themselves, as well as the packages and libraries you use with them, are open source licensed. What it means to be open sourced is that if you want to read the code for Windows or Mac OS, you have to go get a job at Microsoft or Apple and join a team working on those code bases. In contrast, a quick google search will allow you to read the source code for Linux, R, Python, pandas, or the tidyverse. Reading source code is one thing, but the fascinating (and controversial) part of open source licensing is when it comes to reuse. There is a menagerie of different types of open source licenses. The differences between open source licenses mainly concern how permissible they are in terms of reuse of the software. In contrast to the permissibility of standard copyright, which is about whether and how you’re allowed to create derivative works, the variations in open source licenses are about how you’re allowed to license permissive works. The most permissive open source licenses basically amount to “do whatever you want with my software”, while more restrictive ones, often called copyleft, require that any derivative works also be open source. There have even been discussions recently of creating open source licenses that restrict the usage of the software along certain ethical dimensions. In a “traditional” software company, software is written by the company, and the software is then sold to make money. Increasingly, companies are writing open source software and monetizing this business in a number of different ways. Many versions of Linux are free, while others have an associated cost. Even more confusingly, there are several distributions that share a common codebase, where some are free and some paid. For example, Red Hat, the company behind the paid RHEL are also the primary maintainers of CentOS, and CentOS is the basis for Amazon Linux, which you can get on an Amazon Web Services Server (more on that in a moment). A.2 Headers When you make a request to a server, metadata about the request is embedded. This metadata doesn’t appear explicitly in the URL at all, but is included in every piece of internet traffic, and is extremely important for the way the internet and other computer networks work. One example of a header you use a lot in your daily life is the User-Agent header. This header identifies the type of computer a request is coming from – and is used to determine whether you are accessing from a phone and should get a mobile-friendly version of a website. You can see this metadata when you visit a website by opening the developer tools panel in your browser. In Chrome, you can go to View &gt; Developer &gt; Developer Tools &gt; Network. Then, when you navigate to a website, you’ll see every bit of traffic back and forth. If you click on any of the lines, you can see all the headers that are being used to identify the traffic back and forth. "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
