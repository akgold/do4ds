[["index.html", "DevOps for Data Science (working title) Preface Software information and conventions Acknowledgments", " DevOps for Data Science (working title) Alex Gold 2021-08-31 Preface At some point, most data scientists reach the point where they want to show their work to others. But the skills and tools to deploy data science are completely different from the skills and tools needed to do data science. If you’re a data scientist who wants to get your work in front of the right people, this book aims to equip you with all the technical things you need to know that aren’t data science. Hopefully, once you’ve read this book, you’ll understand how to deploy your data science, whether you’re building a DIY deployment system or trying to work with your organization’s IT/DevOps/SysAdmin/SRE group to make that happen. Software information and conventions I used the knitr package (Xie 2015) and the bookdown package (Xie 2021) to compile my book. My R session information is shown below: xfun::session_info() ## R version 4.1.1 (2021-08-10) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Catalina 10.15.7 ## ## Locale: en_US.UTF-8 / en_US.UTF-8 / en_US.UTF-8 / C / en_US.UTF-8 / en_US.UTF-8 ## ## Package version: ## base64enc_0.1.3 bookdown_0.23 compiler_4.1.1 ## digest_0.6.27 evaluate_0.14 fastmap_1.1.0 ## glue_1.4.2 graphics_4.1.1 grDevices_4.1.1 ## highr_0.9 htmltools_0.5.2 jquerylib_0.1.4 ## jsonlite_1.7.2 knitr_1.33 magrittr_2.0.1 ## markdown_1.1 methods_4.1.1 mime_0.11 ## rlang_0.4.11 rmarkdown_2.10 stats_4.1.1 ## stringi_1.7.4 stringr_1.4.0 tinytex_0.33 ## tools_4.1.1 utils_4.1.1 xfun_0.25 ## yaml_2.2.1 Package names are in bold text (e.g., rmarkdown), and inline code and filenames are formatted in a typewriter font (e.g., knitr::knit('foo.Rmd')). Function names are followed by parentheses (e.g., bookdown::render_book()). Acknowledgments A lot of people are helping me write this book. This book is published to the web using GitHub Actions from rOpenSci. References "],["about-the-author.html", "About the Author", " About the Author Alex Gold is a manager on the Solutions Engineering team at RStudio. He works with customers of RStudio’s professional software to help them deploy, configure, and use RStudio’s professional software and open source tooling in R and Python. In his free time, he enjoys landscaping, handstands, and Tai Chi. He occasionally blogs about data, management, and leadership at alexkgold.space. Color palette: Tea Green: CAFFDO Steel Blue: 3E7CB1 Kombu Green: 273c2c Bright Maroon: B33951 Sandy Brown: FCAA67 "],["introduction.html", "Chapter 1 Introduction 1.1 Components", " Chapter 1 Introduction One of the most frequent questions from data scientists is, “how do I ask my IT team for that?” Hopefully, reading this book will help you learn how to ask, or even to DIY. Getting work off your laptop often starts with deployment, but increasingly organizations are also doing data science development in centralized data science platforms. Encompasses parts of DevOps, MLOps, DataOps. If you’re in an environment where you just do data science on your laptop, you probably don’t really need this book. You might find the sections on the command line and on data access interesting, but most of this book will be geared towards server-based data science – whether that’s working in a browser-based data science IDE or trying to deploy data science assets to a server. 1.1 Components Workbench Package Management Change Control (git) Deployment Environment Front End vs Back End Data Ingest/Access Determining Requirements for your platform Platform requirements combination of Work being done on the platform Data Science needs (i.e. horsepower) Existing state of org’s data Type of work data scientists might do Batch Jobs ETL Reports Data Generation Real-Time Interactive Models APIs Apps Data Science + IT Needs Languages (R, Python, SQL) Sharing – internal, external Online or offline? Data Security – row based access? Centralized workbench vs individual workstations Existing Assets BI/Analytics Platforms Data Connections "],["command-line.html", "Chapter 2 The Command Line", " Chapter 2 The Command Line Most SysAdmin work is done on the command line Unlike your laptop, servers don’t often have a desktop interface The interactive sections of this book will require you to use the command line -&gt; How to get to command line on different systems -&gt; Set up docker desktop #TODO: Address using vi "],["computers-and-servers.html", "Chapter 3 Computers and Servers 3.1 A computer is really just three things… 3.2 Choosing the right data science machine 3.3 How is a server different? 3.4 Moving Data Science to A Server 3.5 Getting a server of your own 3.6 Exercises", " Chapter 3 Computers and Servers Data Science is fundamentally a computational practice. At some level, the day-to-day work of data scientists is inextricable from the computational waters in which that work occurs. Luckily, being a productive data science does not rely on deep understanding of computational theory. But for a working data scientist, understanding the computational substrate below your R or Python code can give you a much better feel for how to get your work done quickly and efficiently. Moreover, as more and more data science moves into centralized server-based environments, having a mental model for how that environment is like and unlike your more-familiar laptop or phone can help you be more productive. In this chapter, you’ll learn about the important components inside your computer for data science work and the similarities and differences it has with a server-based environment. If you’re near your computer and ready to try it out, you’ll have stood up a server of your very own by the end of the chapter. 3.1 A computer is really just three things… For the purposes of doing data science, a computer is just a collection of three interacting components. This is obviously a gross oversimplification of how a computer works, Everything I’m saying below is an oversimplification, and if you’re into pedantic nitpicking, I hope you have a good time picking apart why what I’m saying is not quite right. However, in the spirit of all models are wrong, but some are useful, I’ve found this to be an extremely useful mental model of what’s going on in a computer that hasn’t served me wrong across hundreds of interactions with data scientists and IT/DevOps professionals. Let’s go through each of these pieces. 3.1.1 Input + Output Input and Output are whatever is going into and coming out from the computer. For human purposes, there are myriad ways to provide input and receive output. You might provide input with your keyboard and mouse, a microphone or camera, or by sending a call to a server. And you might get output through your computer or phone screen, a set of speakers, or a returned API call. For the purposes of understanding how your computer does things, it’s helpful to think of all of this input and output as having been homogenized. Your computer homogenizes everything you do on your computer – from walking left in a video game to typing the word umbrella for your next novel, creating a plot in R to sending the results of a machine learning model. Fundamentally, you can think of every bit of input as a request to add some numbers, and every bit of output as the result of an addition.1 3.1.2 Compute (CPU + GPU) If everything your computer does is adding two numbers, the compute is where that addition happens. Your computer is very, very good at adding very, very fast. Every computer has a central processing unit (CPU), which is the brain of your computer. For the purposes of this mental model, you can think of it as the main adding factory. Those additions happen inside a core, which you can think of as a place that can do a single addition at a time. These days, most consumer-grade laptops have between 4 and 16 cores pretty fast cores, and may have additional capabilities that effectively doubles that number. Some computers also have a graphical processing unit (GPU). If you have a GPU, you can think of it as part of the same compute complex as your CPU, but differently specialized. Where a CPU has a relatively low number of fast cores, a GPU has a lot of weak cores. Where a consumer-grade CPU has 4-16 cores, mid-range GPUs have 700-4,000. For some uses, like editing photo or videos, playing video games, some kinds of machine learning, and Bitcoin mining, it turns out the “many slow cores” architecture of a GPU drastically speeds up computation relative to having to do those same computations in serial (one at a time). From a configuration and usage perspective CPUs and GPUs are quite different. In particular, using a CPU is automatic, while using a GPU can be a hassle – though it’s getting easier all the time. However, for your mental model of a computer, you can fundamentally think of the two as being combined into a big adding factory you think of as the compute available. Two of the three important attributes of your computer are about the available compute - the number of cores and single-core speed. As you’ve probably heard, R and Python are single-threaded. This means that, unless you’re using special libraries for parallel processing, R and Python will just use a single core. For data science work in R and Python, single-core speed matters much more than number of cores. Fewer, faster cores is generally more important than having many cores. If you want to scale across multiple R or Python sessions at the same time – for example to allow multiple users on a server – then you’ll care more about the number of cores. The number of cores in your computer is straightforward – it’s the number of cores your CPU has, sometimes times 2. Regardless of your operating system, it’s pretty trivial to determine how many cores your machine has. Comparing CPU speed is harder to boil down to a single number. The basic measure of single-core speed is its “clock speed” in hertz (hz) – operations per second. Modern consumer-grade cores run on the order of 2-5 gigahertz (GHz): 2-5 billion operations per second. A few decades ago, there was a steady march in increases in clock speed – increasing by approximately 10x during the 90s, by 2-3x in the 2000s, and somewhere between not at all and 1.5x in the 2010s. Computers have gotten much faster through this entire time, but mostly from sources other than boosts in single-core clock speeds. But real-world performance doesn’t always track precisely with the GHz speed of the cores. For example, in 2020, Apple released a series of Macs with its new M1 chip. It boasted improved real-world performance over earlier Macs – even those with similar clock speeds – through some clever architectural changes. The good news is that if you’re shopping for a new laptop, pretty much all modern CPUs are pretty darn fast and they scale roughly with cost. If you’re looking to set up a server to do data science, the big cloud providers use standard chipsets, so clock speed in GHz actually is a decent guide to the relative speed of the different CPU options available. 3.1.3 Storage (Hard Drive/Disk) Your computer’s hard drive is its permanent storage. Even when no electricity is supplied to your hard drive, it will keep the things you’ve stored there. You probably know this, but storage is measured in bytes. These days, most consumer hard drives are between 1/2 to a few terrabytes (trillion bytes). Enterprise-grade applications sometimes run on the scales of thousands of terrabytes (pettabytes) or even thousands of pettabytes (yottabytes). A few years ago, all hard drives were basically the same. For standard hard drives (HDDs), there are a bunch of disks that spin very very fast (5,400 and 7,200 RPMs are common speeds). A magnetized read/write head moved among the disks and did the data operations you needed. These days, solid-state drives (SSDs), which are collections of flash memory chips, are taking over the world. The main advantage is that they’re much faster than HDDs – up to 15x or so. They also can take a wider variety of shapes and sizes, and are more reliable and durable because they have no moving parts. The main drawback is that you get much less storage for the price. Many computers and systems are SSD only at this point. Some high-end ones combine a smaller SSD with a larger HDD to make sure you never run out of room, but can keep things you need a lot in an easier-to-access location. The main thing that matters in your mental model of storage is that it’s cold storage. Everything has to move out of permanent storage before it can be used, which is a relatively slow operation, even on an SSD. If you’re thinking about what hard drive to get for your laptop or server, the short answer is just to get a lot. Storage is very cheap these days, and also easily extensible via the cloud or external hard drives. This attribute of your computer sorta matters, but is of second-tier importance. 3.1.4 Memory (RAM) RAM (Random Access Memory) is temporary storage where your computer puts things it’s actively using, or thinks it might need again soon. Modern consumer-grade laptops come with somewhere between 4 and 16 Gb of memory. In theory, your computer doesn’t need any RAM at all – it could just save everything to the hard drive, but even with an SSD, your CPU to RAM read/write times are somewhere between 10-100 times faster than going to the hard drive. The tradeoff for this speed is that RAM requires electricity to function. When you turn off your computer, the RAM is emptied. These days, computers mostly suspend what you had going on to the hard disk, so everything comes back more-or-less how it was when you turn your computer back on. To a first approximation, putting your computer to sleep means turning off everything but keeping the RAM intact. If you’re coding in R or Python, amount of RAM is likely to be the first IT hurdle you run into. R and Python only work with data that’s been loaded into memory, so you when you look at a command like df &lt;- read.csv(&quot;my_csv.csv&quot;) or import pandas as pd data = pd.read_csv(&quot;my_csv.csv&quot;) you can read this as “take a csv that’s on my hard drive and move that data into memory”. It’s pretty easy in R or Python to try to load too much data into memory and end up crashing your session. You can get around the in-memory limitation by using a database or libraries that facilitate on-disk operations like Apache Arrow. Depending on the size of your data and how computationally intensive your work is, this can be a real limitation on where you can do your work. Getting more RAM is one reason some data scientists decide to move to a server – as you can rent a server with 1000 times as much RAM – though ones that big are quite costly. 3.2 Choosing the right data science machine Now that you understand how compute, storage, and memory work together to execute your R or Python code, let’s dig into the attributes you might care about when choosing a data science machine. It’s pretty hard to make any definitive rules for how big of a machine you need, because it depends so heavily on what you’re trying to do, but there are some guides and rules of thumb that can be useful to keep in mind. In general, there are only three attributes (plus two) to really care about for your data science machine. The three attributes you should choose a machine on are (in order): the amount of RAM, the single-core speed, and the number of cores. Depending on your use case, you may also care about making sure you have access to a GPU, and you should make sure you’ve got enough storage for what you need to do. The number one consideration for a data science machine is having enough RAM. Most other issues are pretty quickly fixable or will just result in operations being somewhat slower than you’d ideally want. If you don’t have enough RAM, you simply won’t be able to do an analysis because your computer can’t hold all the data in memory. As a very rough rule of thumb, I’ve never run into trouble with the following formula: Amount of RAM = average data size * number of simultaneous data sets loaded * 3 Because you’ll often be doing some sort of transformation to your data, you’ll want to leave plenty of head room for invisible copies of your data, or for the possibility you underestimated your average data size. I’ve never run into problems with a scaling factor of 3, but this is definitely only a rough rule-of-thumb. Once you’re sure you’ve got enough RAM – and you can basically never have enough – the two other most important considerations are the number of cores and their speed. Since R and Python are single-threaded, single-core performance is generally more important than the number of cores. The number of cores generally only becomes important if you’re trying to support many people or jobs working on the same server at once. When you’re thinking about how to choose a CPU, fewer, faster cores are preferable to lots of cores. If you’re buying a laptop or desktop, you probably don’t need to think much about this, but if you’re configuring a server, this is often a choice you’re able to make quite directly. If you’re doing massively parallel operations, you should use a GPU anyway. 3.3 How is a server different? Before I was intimately acquainted with servers, they felt like the Shaolin Temple of computers – incredibly powerful, probably housed on a mountainside shrouded by clouds, maybe even a little mystical. As I started knowingly interacting with servers on a daily basis, I learned the reality was far more prosaic and stripmall-esque. You already understand how a server works. It’s as simple as taking the input streams you’ve conceptualized as coming from a keyboard, mouse, and microphone and thinking of them instead as requests from other computers, and taking the output going directly to your desktop and thinking of them as responses to those requests. While those input and output streams may change, what goes on in between is exactly the same. It’s all just a bunch of addition in compute, with short term memory, and long-term storage. That’s not to say the input and output format don’t matter. We’ll spend a lot of this book reviewing how to make sure servers can get the input they need, can filter out bad input, and return output to the right place. It’s just that your mental model of what the server does needn’t incorporate this element. In terms of where these machines live, the most common answer is really boring-looking office parks. In today’s world, most servers are rented from one of the big cloud providers or are in rented space in a compute center. In the remainder of this book, you’ll get really comfortable with servers. By the time you’re done, you should have a pretty thorough understanding of what servers are, how to get and interact with one, and how to make sure the way you use them is secure. But, in case you didn’t realize it, you already interact with servers constantly in your daily life. A few examples: When you request directions from a mapping app on your phone, your phone makes a request to a server programming interface (API). The servers compute the directions you need and send them back to your phone in a specialized format. Your phone then renders the directions you need to your screen. When you navigate to a website, that company’s servers send your computer back the components that make up that webpage. Your computer knows how to take those components and render them for you to view. When you use your credit card at the grocery store, the credit card machine reaches out to the company’s servers to verify that your card is valid and check that the item you’re trying to purchase doesn’t put you over your credit limit before verifying that the purchase is ok. As you can see in the examples above, the biggest shift in mental model between your desktop computer and a server is where the connection comes from. For your desktop computer, a human is just one hop away – typing on the keyboard and looking at the screen. For a server, humans are always at least two hops away, using another computer to send requests off to the server and get responses back. The big difference between those examples and what we’ll be doing for much of the rest of this book is how you’re interacting with the server. In the examples above, you’re interacting with a publicly-exposed application layer of a server. Throughout the rest of this book, we’ll get comfortable interacting with servers you rent or own as an administrator. 3.3.1 Most servers run Linux When you want to do something administrative on your laptop – say install a new program, or change screen resolutions – you probably just open a window on your machine. Those are features of your computer’s operating system – the most common being Mac OS or Windows. A computer’s operating system forms the basic interface between the computer’s hardware and the software you want to run to actually do things. It includes the software for how the computer boots up, the background services (audio, printing, etc), and the desktop you interact with when switching from one application to another. Most people think exclusively of Mac OS or Windows when thinking of operating systems, as they rule the desktop computing world. But those computers are vastly outnumbered by the number and variety of servers and other computing platform that are running Linux.2 Along with most of the world’s servers, almost all of the world’s embedded computers – in ATMs, cars and planes, TVs, and most other gadgets and gizmos – run on Linux. If you have an Android phone or a Chromebook – that’s Linux. Basically all of the world’s supercomputers use Linux. There are many different distributions (usually called “distros”) of Linux. For day-to-day enterprise server use, the most common of these are Ubuntu, CentOS, Red Hat Enterprise Linux (RHEL), SUSE Enterprise Linux. Because of its ubiquity in the server world, we’ll be using Linux servers and distributions for the rest of this book. 3.3.2 An aside on open source I personally find open source software fascinating.3 As a data scientist working in R or Python, you might care about the water you’re swimming in. If you don’t, that’s fine too and you don’t need to worry about this. Feel free to skip ahead. Unlike Mac OS or Windows, Linux is open source. You’ve probably already heard of open source licensing, as the R and Python languages themselves, as well as the packages and libraries you use with them, are open source licensed. What it means to be open sourced is that if you want to read the code for Windows or Mac OS, you have to go get a job at Microsoft or Apple and join a team working on those code bases. In contrast, a quick google search will allow you to read the source code for Linux, R, Python, pandas, or the tidyverse. Reading source code is one thing, but the fascinating (and controversial) part of open source licensing is when it comes to reuse. There is a menagerie of different types of open source licenses. The differences between open source licenses mainly concern how permissible they are in terms of reuse of the software. In contrast to the permissibility of standard copyright, which is about whether and how you’re allowed to create derivative works, the variations in open source licenses are about how you’re allowed to license permissive works. The most permissive open source licenses basically amount to “do whatever you want with my software”, while more restrictive ones, often called copyleft, require that any derivative works also be open source. There have even been discussions recently of creating open source licenses that restrict the usage of the software along certain ethical dimensions. In a “traditional” software company, software is written by the company, and the software is then sold to make money. Increasingly, companies are writing open source software and monetizing this business in a number of different ways. Many versions of Linux are free, while others have an associated cost. Even more confusingly, there are several distributions that share a common codebase, where some are free and some paid. For example, Red Hat, the company behind the paid RHEL are also the primary maintainers of CentOS, and CentOS is the basis for Amazon Linux, which you can get on an Amazon Web Services Server (more on that in a moment). 3.4 Moving Data Science to A Server Many organizations run their data science workloads on a server, as opposed to on each employee’s laptop. In recent years, as data science has become more important, this has been happening more and more commonly. Depending on your organization, the centralization of data science operations can make your life way easier – or it can be kinda a bummer. 3.4.1 Data Scientist-led server migration Often, migrations to a server are instigated by the data scientists themselves – usually because they’ve run out of horsepower on their laptops. These migrations tend to be partial, temporary, and often kinda slapdash. They’re configured for the purposes of a single project, and are managed by the data scientists themselves. If you, or one of your teammates, enjoys and is good as SysAdmin work, this can be a great situation! You get the hardware you need for your project quickly and with minimal interference. On the other hand, most data scientists don’t really want to be SysAdmins, and these systems are often fragile, isolated from other corporate systems, and potentially susceptible to security vulnerabilities. 3.4.2 SysAdmin-led server migration Many other organizations are moving to servers as well, but led by the IT group. For many IT groups, it’s way easier to maintain a centralized server environment, as opposed to helping each data scientist maintain their own environment on their laptop. Having just one platform makes it much easier to give shared access to more powerful computing platforms, to data sources that require some configuration, and to R and Python packages that wrap around system libraries and can be a pain to configure (looking at you, rJava). This can be a great situation for data scientists! If the platform is well-configured and scoped, you can get instant access through their web browser to more compute resources, and don’t have to worry about maintaining local installations of data science tools like R, Python, RStudio, and Jupyter, and you don’t need to worry about how to connect to important data sources – those things are just available for use. But this can also be a bad experience. Long wait times for hardware or software updates, overly restrictive policies – especially around package management – and misunderstandings of what data scientists are trying to do on the platforms can lead to servers going largely unused. So much of whether the server-based experience is good or not depends on the relationship between the data science and IT/Admin group. In organizations where these groups work together smoothly, this can be a huge win for everyone involved. However, there are some organizations where IT/Admins are so concerned with stability and security that they make it impossible to do data science, and the data scientists spend all their time playing cat-and-mouse games to try to get work done behind IT/Admin’s backs. If you work at such a place, it’s frankly hard to get much done on the server. It’s probably worth investing some time into improving your relationship with your favorite person on the IT/Admin team. Hopefully, this book will help you understand a little of what’s on the minds of people in the IT group, and a sense of how to talk to them better. 3.5 Getting a server of your own Let’s completely switch gears and get practical. Until now, this chapter has been entirely background. If you read it, hopefully you learned a little about a computer works, how your computer is mostly like, but a little unlike a server, and why you might be doing data science on a server. If you’re reading this book, you probably want a deeper understanding of how using and maintaining a server actually works. In the last section of this chapter, we’ll walk through how you can get a server of your very own. In contrast to the computer sitting on your desk, you’ll have to access it over the internet and you’ll use a specialized access protocol called SSH. Those topics are important enough that there are standalone chapters on them. For now, we’re going to gloss entirely over the how and why and just get you to running a server. If you follow along from this point, it’ll probably take you 10-15 minutes to be running a server of your own. We’re going to be standing up a server on Amazon Web Services (AWS). In particular, we’ll be standing up a server in their free tier – so there will be no cost involved as long as you haven’t used up all your AWS free tier credits before now. 3.5.1 Login to the AWS Console We’re going to start by logging into AWS. If you’ve done this before, just go ahead and log in. If not, go to aws.amazon.com and click Sign In to the Console . If you’ve never set up an AWS account before, click Create a New AWS account and follow the instructions to create an account. Note that even if you’ve got an Amazon account for ordering stuff online and watching movies, an AWS account is separate. #TODO: Add link to cloud chapter Once you’ve logged in, you’ll be confronted by the AWS console. There are a ton of things here, and it’s rather overwhelming. There’s a chapter on the business model behind this, so skip ahead if you want, or spend a minute poking around before continuing. 3.5.2 Stand up an instance In the next few paragraphs, I’m going to give you instructions to quickly get a server up – with basically no explanation. If you read this whole book, you’ll understand all of this, the alternatives you could take, and the reasons I’m going to strongly recommend you take this server back offline in just a few minutes. For now, click on the EC2 service (it’s under Launch a virtual machine or Compute depending on where you landed). Scroll down the launch instance button. Here are all the different Quick Start Amazon Machine Images (AMIs). Find and click Ubuntu Server 20.04 LTS – it’ll be one of the first handful. Now you’ll be seeing the instance size chooser. It should have auto-selected a server with the label Free tier eligible. Just stick with this for now. Scroll down and click Review and Launch, and Launch on the next page. When you click Launch, you’ll be asked to use a key pair. Assuming you don’t have an existing keypair, select Create a new key pair, name it my_test_key, and click Download. Keep track of the my_test_key.pem file your computer downloads. Click Launch Instances. AWS is now creating a virtual server just for you. If you click View Instances in the lower right, you’ll see your instance. When the instance state switches to Running, it’s up and running! 3.5.3 SSH into the server The .pem key you downloaded is the skeleton key to your server. If you were setting up a real server, you’d need to be extremely careful with this key, as it allows anyone who has it unrestricted access to the server. For the same reason, it’s also great for playing around quickly with a server. Before we can use it to open the server, we’ll need to make a quick change to the permissions on the key. More details on what that means in the [] chapter. #TODO: which chapter? To take the next steps, you’ll need to at least be able to open your computer’s terminal and copy/paste some commands below. If that’s new to you, feel free to check out the chapter on using the command line. If you’re on a Mac or Linux system, you’ll do the following: $ cd ~/Downloads #or whatever directory the key is in $ chmod 600 my_test_key.pem #TODO: Windows? To access your server, click on the Instance ID link for your server, and copy the Public IPv4 DNS, which will start with ec2- and end with amazonaws.com. In your terminal type the following $ ssh -i my_test_key.pem ubuntu@&lt;Public IPv4 DNS you copied&gt; Type yes when prompted, and you’re now logged in to your server! 3.5.4 Doing A Thing Before we log off and kill this server, let’s do one little thing. We’re going to stand up Nginx, which is a common webserver, and serve a little webpage to ourselves. Let’s start by installing Nginx, copy and paste the command below. $ sudo apt update $ sudo apt-get install nginx -y By default, our EC2 instance only allows SSH traffic. We need to open up HTTP traffic. Go back to your instance in the AWS console and scroll down to the Security tab. Click the blue link under Security Groups, which will start with sg- and include launch-wizard- in parentheses. Click Edit inbound rules , then Add rule. Under Type, scroll down and select HTTP, and under Source Type, select Anywhere-IPv4. Scroll down and click Save rules. Go back to your instance page and copy the Public IPv4 DNS again. Paste this into your browser’s navigation bar, and add http:// right before. When you navigate to the page, you’ll see the default Nginx home page. Let’s make a quick change to the page, just for fun. Back in your terminal that’s still SSH-ed into the instance, navigate to where that page is located: $ cd /var/www/html You can edit the page by typing $ sudo vi index.nginx-debian.html If you don’t recall how to use vi, check out the command line tutorial, but for now, you can enter edit mode by pressing i, navigating around with the arrow keys, and typing. Edit something – say change this line &lt;h1&gt;Welcome to nginx!&lt;/h1 &gt; to something a little more personalized. When you’re done, hit esc followed by :wq. Now, when you re-load the page in your browser, you should be able to see your changes reflected there. 3.5.5 Burn it all down One of the best things about cloud infrastructure is that it can go away as easily as it came up. We made a number of choices here that are fine for ephemeral infrastructure for playing around. But this server should not be used for anything real. When you’ve had your fill of playing, let’s take the server down. Go back to the EC2 page for your server. Under the Instance State drop down in the upper right, choose Terminate Instance. If you go to the Instances page, it’ll take just a minute for the instance to go away. 3.6 Exercises Think about the scenarios below – which part of your computer would you want to upgrade to solve the problem? You try to load a big csv file into pandas in Python. It churns for a while and then crashes. You go to build a new ML model on your data. You’d like to re-train the model once a day, but it turns out training this model takes 26 hours on your laptop. You design an visualization Matplotlib , and create a whole bunch in a loop/ Try standing up a new server and installing R and RStudio Server Open Source, or Python and JupyterHub. Hint 1: Remember that your instance only allows traffic to SSH in on port 22 by default. You access RStudio on port 8787 by default and JupyterHub on port 8000. You control what ports are open via the Security Group. Hint 2: You’ll need to create a user on the server. The adduser command is your friend. #TODO: test out JupyterHub The reason why this is the case and how it works is fascinating. If you’re interested, it comes back to Alan Turing’s famous paper on computability. I recommend The Annotated Turing: A Guided Tour Through Alan Turing’s Historic Paper on Computability and the Turing Machine for a surprisingly readable walkthrough of the paper.↩︎ There are Windows Server versions and they are reasonably popular. You might hear about something called Mac Server, but it’s not actually a server in the traditional sense – it’s a product used to manage Mac desktops and iOS devices.↩︎ I even wrote my undergraduate economics thesis on the open source software!↩︎ "],["networking.html", "Chapter 4 Networking and the Internet 4.1 TCP/IP: How packets find their way 4.2 Ports: The last mile 4.3 Application Protocols: Reading the mail 4.4 Networking for your data science servers 4.5 Special IP Addresses and Ports 4.6 Special HTTP(S) Codes 4.7 Exercises", " Chapter 4 Networking and the Internet Computers communicate with each other via networking. Generally, networking refers to the rules and protocols computers use to find and communicate with one another. Before we get into how you can do networking, a little background on what goes over the network. Computers communicate with each other on a call-and-response model. If I want to go to a website, check my bank account, send a chat message, or watch a movie on Netflix, my computer sends a request off to another computer (a server in most cases) and awaits a response. The way computer networks work is reasonably similar to the way physical mail works. When you ask your computer to do something over a network, it turns that request into one or more packets. A packet has a header that includes the address it’s going to along with details about the packet itself. Due to some cool information theory, it turns out that it works better to take large payloads, split them up into smaller payloads, and send each packet off independently. The header of each packet includes the destination address, as well as information about how to reconstruct the payloads on the other end. There are a number of different sets of rules and protocols in play here that range from the physical interconnections of the wires along the way. There are two really layers that are particularly important to understand from the perspective of a data scientist working on a server. The important layers to understand correspond to how to read what’s on the outside of the packet, and what’s on the inside. The first part of this chapter is some general information on how computer networks function. The latter part is about how networks tend to be configured in enterprise settings and how you can go about setting up your own network. 4.1 TCP/IP: How packets find their way TCP/IP is a set of protocols that collectively define how networked computers create and address packets, and how those packets are routed and reconstructed when they get where they’re going. You’ve probably heard of IP in the context of an IP address. When a computer joins a network, it is given an IP address. An IP address is a number, unique within any given network, that determines where a given computer is. As the internet has gotten more and more computers on it, it turns out that we’re running out of IP addresses. Currently, you will see both the old type of IP addresses, called IPv4 and the new type, IPv6, in use. There are only about 4.3 billion IPv4 addresses, which it turns out isn’t that much in the modern world, especially as large blocks of those addresses are reserved and are not available for public use. In constrast, the number of IPv6 addresses is 39 digits long. This is mostly just trivia, expect that in your every day life working with servers, you’ll see a lot of IP addresses, and it’s important to know what they look like. Each block has a particular meaning, which isn’t super important for our purposes. IPv4 address - four blocks of 8-bit fields (integers up to 256): 0.0.0.0 to 255.255.255.255 IPV6 address example: eight blocks of hexadecimal (0-9 + a-f) digits separated by colons, with certain rules about dropping multiple zeroes and using colons to denote successive zeroes - 2001:0db8:85a3:0000:0000:8a2e:0370:7334 or 2001:db8:: But for the most part, we don’t interact with IP addresses. We’re much more used to seeing the locations of web sites defined in terms of a URL like google.com. When you type a URL into your web browser, your computer sends a request off to a specialized phonebook server calls a DNS (Domain Name System) server. The DNS server translates the URL you typed into an IP address to actually locate the server. The way DNS actually works is complicated, but one of the most important things to know is that it’s kinda a pain. If you’re trying to set up a server at an actual URL, you’ll need to register a domain name and get it associated with your server. This takes time – they say as much as 24-48 hours for DNS changes to propagate through the public servers that every computer connected to the internet might reach. In order to avoid having to constantly reach out to DNS servers, your computer keeps a DNS cache locally. As you’re working on servers, you may find that something suddenly becomes unreachable. A great first troubleshooting step is always to try using an incognito window as it will ignore your DNS cache and reach out to the DNS server afresh. 4.2 Ports: The last mile Once a packet gets to the right server, it needs to actually walk through the door. The doors on an individual server are called ports. Each IP address has over 65,000 ports available, but many of those are reserved for special purposes. There’s a cheatsheet of a few you’ll learn pretty quickly at the end of the chapter. Each port can be bound to exactly one running process and can be open or closed. That means that providing a service to other computers on a network is a three step process – get the service up and running, bind it to a particular port, and make sure the port is accepting incoming traffic. The last step – checking that the port is open – is a common time-waster for people new to DevOps. Checking that your port is listening is the unplug it and plug it back in of networking configuration. Note that outbound communication also requires using a port. This isn’t something you have to think about much because your computer will automatically choose a random available port and keep it open for the duration of the network traffic. 4.3 Application Protocols: Reading the mail So, your packet request has made it to the server on the other end, found the right port, and made it to the listening service. Time to open the envelope. What’s inside? This is where the application layer protocol kicks in. The application layer protocol defines how the server should understand the request that was sent and what are legitimate things to send back. There are many application layer protocols for everything from file transfers to email traffic, checking user permissions to database access. There are a few particular application protocols that come up quite often for the working data scientist: HTTP is how you look at websites in a web browser. To be slightly more technical, defines a client/server relationship where the client makes requests that the server fulfills. FTP/SFTP is a protocol for transferring files directly from one computer to another. SSH is a protocol for securely accessing a remote computer over a network. It’s important enough for data scientists that there’s a whole chapter on it. Websockets are a protocol for doing interactivity with a website. Many popular R and Python web frameworks use websockets, including the Shiny R package and the Streamlit Python package. 4.3.1 HTTP and HTTPs HTTP is a particularly important protocol to understand because it forms the basis for most of the way we interact with the internet. As a data scientist, you also may need to interact directly with HTTP traffic making API calls or writing your own API to serve data science results to others. Secured HTTP End-to-end encryption using Transport Layer Security (TLS) or Secure Sockets Layer (SSL) HTTP over TLS/SSL Getting and configuring an SSL certificate 4.4 Networking for your data science servers The biggest and best known of computer networks is the internet. But at its core, the internet is just another computer network, so the same principles apply to both public and private networks. For the most part, [Diagram: Common Network Topology] Most work occurs inside a VPC IP addresses inside a VPC can be assigned as you wish Subject to CIDR block rules Need translation between inside and outside -&gt; proxy If being used for SSH access, usually called bastion Proxies commonly used to serve as firewalls, network address translators, load-balancers Forward (outbound) proxy vs reverse (inbound) proxy Private vs public subnets Particular ports being “open” 4.5 Special IP Addresses and Ports 127.0.0.1 - localhost 0.0.0.0 - unspecified All ports below 1024 reserved. 80 - HTTP default 443 - HTTPS default 22 - SSH default 4.6 Special HTTP(S) Codes 200 404 403 4.7 Exercises Consider going to the website google.com. Draw a diagram of how the following are incorporated: TCP/IP, DNS, HTTP, HTTPS. Set up a free-tier EC2 instance and put an NGINX server up. Figure out how to allow your computer to access the server, but not your phone. Try accessing it on a non-default port. Try to HTTP into a fresh EC2 with the default security group. Take a look at the inbound security group rules. Hint: is there an inbound rule on a default HTTP port? SSH into your EC2 instance and try to reach out to something on the internet (curl…). See if you can change security group rules to shut down access. Can you do it by changing the IP address range it’s accepting connections from? Can you do it by changing the listening ports? "],["the-cloud.html", "Chapter 5 The Cloud 5.1 The Rise of Services 5.2 Common Services That will be helpful to know offhand 5.3 Cloud Tooling 5.4 Exercises", " Chapter 5 The Cloud Learning Objectives: Understand what the cloud is and why it’s so hype Demystify general cloud offerings General description for servers that you rent instead of buy 3 major cloud providers AWS = Amazon Azure = Microsoft GCP = Google 5.1 The Rise of Services Server/Cloud usage has gone the way of the rest of the economy – services The “rent me a server + connectors” model is generally called IaaS Layers of abstraction on top of that – PaaS, Saas Metaphor: Cake shop [Diagram: Cake Shop] These metaphors are helpful for general understanding…actually applying them gets really fuzzy in data science land, because something that is SaaS from IT perspective is PaaS from data scientist perspective. Often find it more helpful to just categorize as IaaS/“something more abstracted”. These services are generally surrounded my marketing jargon – rarely do they just explain this is ___ as a service. https://stackoverflow.com/questions/16820336/what-is-saas-paas-and-iaas-with-examples 5.2 Common Services That will be helpful to know offhand Amazon’s names aren’t completely obvious. They’re usually referred to by their 3-letter acronyms. Azure and Google’s offerings tend to be named more literally. 5.3 Cloud Tooling Identity MGMT - IAM, Azure AD Billing Mgmt 5.3.1 IaaS Compute - AWS EC2, Azure VMs, Google Compute Engine Storage file - EBS, Azure managed disk, Google Persistent Disk Network drives - EFS, Azure Files, Google Filestore block/blob - S3, Azure Blob Storage, Google Cloud Storage Networking: Private Clouds: VPC, Virtual Network, Google Virtual Private Cloud DNS - Route53, Azure DNS + Traffic Manager, Google Cloud DNS 5.3.2 Not IaaS Container Hosting - ECS, Azure Container Instances + Container Registry K8S cluster as a service - EKS, AKS, GKE Run a function as a service - Lambda, Azure Functions, Google Cloud Functions Database - RDS/Redshift, Azure Database, Google Cloud SQL SageMaker - ML platform as a service, Azure ML, Google Notebooks https://docs.microsoft.com/en-us/azure/architecture/aws-professional/services#networking https://cloud.google.com/free/docs/aws-azure-gcp-service-comparison 5.4 Exercises Example of something you’d want to build – for each of the 3 providers, which services would you use if you wanted an IaaS solution, a PaaS solution? "],["ssh.html", "Chapter 6 Accessing Servers via SSH 6.1 What is SSH? 6.2 How does SSH work? 6.3 Basic SSH Use 6.4 Getting Comfortable in your own setup 6.5 Advanced SSH Tips + Tricks 6.6 Exercises", " Chapter 6 Accessing Servers via SSH Learning Objectives: Understanding of what SSH is Understanding of how to use SSH to access a server 6.1 What is SSH? Stands for secure (socket) shell Way to remotely access a different (virtual) machine Workhorse of doing work on another server 6.2 How does SSH work? Via Public Key Encryption Public/Private Key Known Hosts [Diagram: SSH Keys] &gt; Sidebar on public key encryption By default, available on port 22 6.3 Basic SSH Use The terminal 3 step process generate public/private keys ssh-keygen place keys in appropriate place use ssh to do work Permissions on key 6.4 Getting Comfortable in your own setup Using ssh-config 6.5 Advanced SSH Tips + Tricks SSH Tunneling/Port Forwarding -vvvv, -i, -p [Diagram: Port Forwarding] 6.6 Exercises Draw your own diagram of the sequence of things that occur when you execute ssh user@server.company.com in the terminal. Stand up a new server on one of the major cloud providers. Try logging in using the provided key file. Create a new user on the server. Generate a new SSH key locally and copy the correct key onto the server (think for a moment about which key is the correct one – consult your diagram from step 1 if necessary). Set up an SSH alias so you can SSH into your server using just ssh testserver (hint: look at your SSH config). Create a local text file and SFTP it to the server. SSH in, edit the file, and SFTP it back. [Advanced] Stand up a nginx server on your remote instance. Don’t open the port to the world, but SSH port forward the server page to your local browser. "],["auth.html", "Chapter 7 Authentication – Making Login Possible 7.1 Your Everyday Experience of Auth and How it Differs from Enterprise 7.2 What is Authentication? 7.3 Experiences of Different Auth Systems 7.4 Types of Credentials 7.5 Auth Techniques 7.6 Auth Technologies", " Chapter 7 Authentication – Making Login Possible 7.1 Your Everyday Experience of Auth and How it Differs from Enterprise You do auth all the time Private auth – login to your bank with a unique username and pw MFA (2FA) – get a text message or use an authenticator token Or yubikey, etc Public auth – you might login to spotify using your facebook, apple, or google credentials Spotify is a Service Provider and the other side an Identity Provider 7.2 What is Authentication? Check who you are - Authentication Check whether you should have access - Authorization User goes to login: Make the user provide creds (username/password) System checks credentials against store (For SSO) Check if user is allowed to access resource Send back something User is authorized 7.3 Experiences of Different Auth Systems For users Do I have to authenticate to do X? Where do I authenticate? With which credentials? For Admins Am I transmitting user details, or just an authentication token/ticket? Security Can I easily manage access centrally? Can I keep credential stores centralized? How are users created (provisioned) and deprovisioned Do we have to maintain our own auth server? Differences between auth types 1) How is the communication done? Transmit text vs token (type/encryption) 2) When is the identity checked? Once, every time 3) When is authorization checked? 4) How encrypted are things? What does SSO mean? * Most commonly used to mean login once and then authentication happens automatically for duration of session * Sometimes just used to mean that people use the same username and password across different systems 7.4 Types of Credentials 7.4.1 Usernames and Passwords 7.4.2 Keys 7.4.3 Tokens 7.4.4 More Factors 7.5 Auth Techniques There are two main cont How to manage large groups of users with different permissions 7.5.1 Authorization: Groups, Roles, and Permissions Groups: sets of users (e.g. data-science) Roles: Role a user should have in the system (e.g. admin) Permissions: things someone can do – can be granted on the basis of membership in a group, role grants, or other attributes Groups and roles are maintained in the central user store 7.5.2 Service Accounts Often used in contexts where a service is accessing something on behalf of itself e.g. an app accessing a database on behalf of any user who can access the app 7.5.3 Instance Permissions Often done inside private networks – instead of providing auth on a per-user/session basis, just allow an entire server access to another e.g. any connection to the database by server A is allowed 7.6 Auth Technologies 7.6.1 Username + Password Many pieces of software come with integrated authentication. When you use those system, the product stores encrypted username and password pairs in a database. These setups are often really easy from an admin perspective – you just set up individual users on the server. However, the flip side is that users have one more username and password to remember, which is annoying for them. Moreover, if you have more than a few users, or the system is one of more than a few, it’s hard to manage users on a lot of different systems. It can be a real pain to create accounts on a ton of different systems when a new person joins the organization, or to remove their permissions one-by-one when they leave. For this reason, most IT/Admin organizations strongly prefer using some sort of centralized identity store. 7.6.2 PAM 7.6.3 LDAP/AD Lightweight Directory Access Protocol (LDAP) is a relatively old, open, protocol used for maintaining a set of entities and their attributes. To be precise, LDAP is actually a protocol for maintaining and accessing entities and their attributes in a tree. It happens that this is a really good structure for maintaining permissions and roles of users at an organization, and it’s the main thing LDAP is used for. Active Directory (AD) is Microsoft’s implementation of LDAP, and is by-far the most common LDAP “flavor” out there. AD so thoroughly owns the LDAP enterprise market, that LDAP is often referred to as LDAP/AD. There are other implementations you may run across, the most common being OpenLDAP. Azure, Microsoft’s cloud provider, offers an authentication service called Azure Active Directory (AAD). Confusingly, AAD is usually used in combination with SAML, not LDAP. It’s worth distinguishing the use of LDAP as an identity store from its use as an authentication technology. As a tree-based database, LDAP is uniquely well-suited to storing the identities, and other attributes of people at the organization. However, as discussed below, using LDAP to authenticate into actual services has security and convenience drawbacks, and many organizations consider it outdated and insecure. A lot of organizations are moving away from LDAP for authentication in favor of token-based technologies like SAML or OAuth, but many are keeping LDAP as their identity “source of truth” that is referenced by the SAML or OAuth Identity Provider. LDAP has three main disadvantages relative to other technologies. First, LDAP requires that your credentials (username and password, usually) actually be provided to the service you’re trying to use. This is fundamentally insecure relative to a system where your credentials are provided only to the identity provider, and the service just gets a token verifying who you are. In token-based systems, adding additional requirements like MFA or biometrics are easy, as they’re simply added at the IdP layer. In contrast, doing those things in LDAP would require the service to implement them, which usually is not the case, so you’re usually limited to username and password. The second disadvantage of LDAP is that it does not allow for central administration of permissions. LDAP directly records only objects and their attributes. Say, for example, you want only users of a particular group to have access to a certain resource. In LDAP, you would have to specify in that resource that it should only allow in users of that group. This is in contrast to SAML/OAuth, where the authorization is centrally managed. Lastly, LDAP authentication is based on each service authenticating. Once you authenticate, the service might give you a cookie so that your login persists, but there is no general-purpose token that will allow you to login to multiple services. 7.6.3.1 How LDAP Works While the technical downsides of LDAP are real, the tradeoff is that the technical operations of LDAP are pretty straightforward. In short, you try to login to a service, the service collects your username and password, sends it off to the LDAP server, and checks that your username and password are valid. Note that LDAP is purely for authentication. When you’re using LDAP, authorization has to be handled separately, which is one of the disadvantages. 7.6.3.2 Deeper Than You Need on LDAP LDAP is a tree-based entity and value store. This means that LDAP stores things and their attributes, which include a name and one or more values. For example, my entry in a corporate LDAP directory might look like this: cn: Alex Gold mail: alex.gold@example.com mail: alex.gold@example.org department: solutions mobile: 555-555-5555 objectClass = Person Most of these attributes should be pretty straightforward. cn is short for common name, and is part of the way you look up an entity in LDAP (more on that below). Each entity in LDAP must have an objectClass, which determines the type of entity it is. In this case, I am a Person , as opposed to a device, domain, organizationalRole, or room – all of which are standard objectClasses. Let’s say that your corporate LDAP looks like the tree below: #TODO: make solutions an OU in final The most common way to look up LDAP entities is with their distinguished name (DN), which is the path of names from the point you’re starting all the way back to the root of the tree. In the tree above, my DN would be cn=alex,ou=solutions,dc=example,dc=com. Note that you read the DN from right to left to work your way down the tree. Aside from cn for common name, other common fields include ou for organizational unit, and dc for domain component. 7.6.3.3 Trying out LDAP Now that we understand in theory how LDAP works, let’s try out an actual example. To start, let’s stand up LDAP in a docker container: #TODO: update ldif docker network create ldap-net docker run -p 6389:389 \\ --name ldap-service \\ --network ldap-net \\ --detach alexkgold/auth ldapsearch is a utility that lets us run queries against the LDAP tree. Let’s try it out against the LDAP container we just stood up. Let’s say I want to return everything in the subtree under example.org. In that case, I would run ldapsearch -b dc=example,dc=org, where b indicates my search base, which is a dn. But in order to make this actually work, we’ll need to include a few more arguments, including the host where the LDAP server is, indicated by -H the bind DN we’ll be using, flagged with -D the bind password we’ll be using, indicated by -w Since we’re testing, we’re also going to provide the flag -x to use whatever certificate is present on the server. Putting it altogether, along with the commands to reach the docker container, let’s try: ldapsearch -x -H ldap://localhost:6389 -b dc=example,dc=org -D &quot;cn=admin,dc=example,dc=org&quot; -w admin # extended LDIF # # LDAPv3 # base &lt;dc=example,dc=org&gt; with scope subtree # filter: (objectclass=*) # requesting: ALL # # example.org dn: dc=example,dc=org objectClass: top objectClass: dcObject objectClass: organization o: Example Inc. dc: example # admin, example.org dn: cn=admin,dc=example,dc=org objectClass: simpleSecurityObject objectClass: organizationalRole cn: admin description: LDAP administrator userPassword:: e1NTSEF9d3IyVFp6SlAyKy9xT2RsQ0owTDYzR0RzNFo0NUFrQ00= # search result search: 2 result: 0 Success # numResponses: 3 # numEntries: 2 You should be able to read what got returned pretty seamlessly. One thing to notice is that the user password is returned, so it can be compared to a password provided. It is encrypted, so it doesn’t appear in plain text. Note that ldap is a protocol – so it takes the place of the http you’re used to in normal web operations. Like there’s https, there is also a protocol called LDAPS, which is ldap + tls for the same reason you’ve got https. LDAP is (almost) always running in the same private network as the service, so many organizations don’t require using LDAPS, but others do require it. Running the ldapadmin docker run -p 6443:443 \\ --name ldap-admin \\ --env PHPLDAPADMIN_LDAP_HOSTS=ldap-service \\ --network ldap-net \\ --detach osixia/phpldapadmin dn for admin cn=admin,dc=example,dc=org pw: admin https://localhost:6443 # Replace with valid license export RSC_LICENSE=XXXX-XXXX-XXXX-XXXX-XXXX-XXXX-XXXX # Run without persistent data and using default configuration docker run -it --privileged \\ --name rsc \\ --volume $PWD/rstudio-connect.gcfg:/etc/rstudio-connect/rstudio-connect.gcfg \\ -p 3939:3939 \\ -e RSC_LICENSE=$RSC_LICENSE \\ --network ldap-net \\ rstudio/rstudio-connect:latest 7.6.3.4 Single vs Double Bind There are two different ways to establish a connection between your server and the LDAP server. The first method is called Single Bind. In a single bind authentication, the user credentials are used both to authenticate to the LDAP server, and to query the server. In a Double Bind configuration, there is a separate administrative service account, used to authenticate to the LDAP server. Once authentication is complete, then the user is queried in the system. Single bind configurations are often more limited than double bind ones. For example, in most cases you’ll only be able to see the single user as well as the groups they’re a part of. This can limit application functionality in some cases. On the other hand, there need be no master key maintained on your server, and some admins may prefer it for security reasons. We can see this really concretely. In the example above, you used a double bind by supplying admin credentials to LDAP. Let’s say instead, you just provide a single user’s credentials. In that case, I don’t get anything back if I just do a general search. ldapsearch -x -H ldap://localhost:6389 -b dc=example,dc=org -D &quot;cn=joe,dc=engineering,dc=example,dc=org&quot; -w joe # extended LDIF # # LDAPv3 # base &lt;dc=example,dc=org&gt; with scope subtree # filter: (objectclass=*) # requesting: ALL # # search result search: 2 result: 32 No such object # numResponses: 1 But just searching for information about Joe does return his own information. ldapsearch -x -H ldap://localhost:6389 -b cn=joe,dc=engineering,dc=example,dc=org -D &quot;cn=joe,dc=engineering,dc=example,dc=org&quot; -w joe  32 ✘ # extended LDIF # # LDAPv3 # base &lt;cn=joe,dc=engineering,dc=example,dc=org&gt; with scope subtree # filter: (objectclass=*) # requesting: ALL # # joe, engineering.example.org dn: cn=joe,dc=engineering,dc=example,dc=org cn: joe gidNumber: 500 givenName: Joe homeDirectory: /home/joe loginShell: /bin/sh mail: joe@example.org objectClass: inetOrgPerson objectClass: posixAccount objectClass: top sn: Golly uid: test\\joe uidNumber: 1000 userPassword:: e01ENX1qL01raWZrdk0wRm1sTDZQM0MxTUlnPT0= # search result search: 2 result: 0 Success # numResponses: 2 # numEntries: 1 7.6.4 Kerberos Tickets An old technology, but still considered very secure - server to server transit of token Microsoft-based and integrates tightly with AD Used frequently to make connections to databases Can also be used to establish an SSO experience 7.6.5 SAML The most common type of true SSO for enterprises Login once to a central IdP and an identity token gets stored in web browser Go to a new service, and the identity token requests a service token to use the service XML-based Relatively new – many enterprises switching to this from LDAP/AD (Azure AD) 7.6.6 OIDC/OAuth2.0 Based on JSON Web Tokens (JWT, often pronounced “Jot” b/c reasons…) Technically OIDC is an authentication standard and OAuth an authorization You’ll sometimes see e.g. SAML auth + OAuth being used for db access Very similar to SAML from the user perspective More common in external services, SAML often preferred inside enterprises Resources: https://www.okta.com/identity-101/saml-vs-oauth/ https://www.okta.com/identity-101/whats-the-difference-between-oauth-openid-connect-and-saml/ https://phoenixnap.com/blog/kerberos-authentication https://www.dnsstuff.com/rbac-vs-abac-access-control "],["choosing-and-using-a-data-architecture.html", "Chapter 8 Choosing and Using A Data Architecture 8.1 Options for Data Storage 8.2 How to pick 8.3 Working with File Systems 8.4 Working with Databases 8.5 Exercises", " Chapter 8 Choosing and Using A Data Architecture Learning Objectives Understand options for data architectures Understand interactions between data and auth DataOps is a huge field – a lot of it touches data science Some of the biggest consideration when you’re thinking about data science 8.1 Options for Data Storage 3 basic types of data stores: flat file (csv, pickle/rds), flat file w/ interactive read (arrow), database Implications how to configure access Database is standalone option, usually requires configuring separate server Other two can be configured two main ways: File system Bucket Storage Data Lake – usually for raw data, basically bucket storage writ large, sometimes include interacticve read capabilities Data Warehouse – usually a database 8.2 How to pick Flat files are simplest Flat files w/ interactive read are great for med-large data Databases often already exist If not, a few tips: SQL-based databases are the backbone of analytics work Database as a service is a basic cloud provider There are SO many options TODO: How much more in depth am I going here? 8.3 Working with File Systems Working on your laptop file system isn’t great Working in a cloud provider is different than on your laptop Volumes are separate from compute Often have automated snapshots/backups Volumes can also be shared across multiple servers Process of putting a volume on a server is called mounting 8.4 Working with Databases Two main ways to work with databases from R/Python - direct connection or w/ connector/driver (JDBC/ODBC) https://docs.google.com/presentation/d/1wJ3YnB4ob3AkNRDLpvYy3B-JMUaVsPjl3AA5QkN7fII/edit 8.4.1 Security Row-level security Kerberos JWT IAM roles 8.5 Exercises Connect to and use an S3 bucket from the command line and from R/Python. Stand up a container with Postgres in it, connect using isql, then from R/Python using ODBC/JDBC. Stand up an EC2 instance w/ EBS volume, change EBS to different instances. Stand up an NFS instance and mount onto a server. "],["environments-and-reproducibility.html", "Chapter 9 Environments and Reproducibility", " Chapter 9 Environments and Reproducibility Learning Objectives: Understand the layers that go into reproducing an output Understand which part is “the environment” Understand some general strategies/tooling for reproducing Managing environments is hard. Necessary in data science primarily for repro. At least 7 levels: Hardware Operating System System Libraries Programming Language Packages App Code App Data -&gt; Difference between environment and data is sketchy All of it contributes to “state” [graphic: Image of concentric environments] Different requirements depending on if you’re talking about development (IDE) environment or production environment. Repository vs library mgmt Tools to Reproduce Parts of Stack: IaC tooling Provisioning Configuration mgmt AMIs virtualenv/conda/renv Docker/Containers "],["it-and-data-science-workflows.html", "Chapter 10 IT and Data Science Workflows 10.1 Dev/Test/Prod 10.2 Cattle, Not Pets 10.3 How Does Stuff Actually Get Deployed 10.4 Exercises", " Chapter 10 IT and Data Science Workflows Learning Objectives: Understand what dev/test/prod means Understand how dev/test/prod might apply to data science Well developed patterns for putting things into prod in IT Data science starting to adopt, but not exactly the same 10.1 Dev/Test/Prod Classic workflow - three precise mirrors Do dev work in dev env Test for functionality and with users (User Acceptance Testing - UAT) Deploy to production environment Doesn’t quite work for data science Often, dev is a distinct (RStudio IDE/Jupyter) Because more exploratory than pure software development Test/Prod can be on one server or multiple, depending on org If you’re managing your own Data Science server, you’ll want an additional staging environment – unrelated to app changes, purely for IT changes [Graphic: deploying app changes and server changes] 10.2 Cattle, Not Pets Manual promotion from place to place = bad State maintenance Drift Cattle not Pets Infrastructure as code tooling Terraform AWS CFN, Azure Resource Manager, Google Cloud Deployment Manager Chef Puppet Saltstack Ansible Docker Vagrant Helm/Helmfile Important to ensure idempotence 10.3 How Does Stuff Actually Get Deployed [Graphic: CI/CD deployment into test/prod] CI/CD - Jenkins, Travis, GHA Powered by Git, mostly Intro to using GHA 10.4 Exercises Create something and add GHA integration [TODO: What to use as example?] Stand up some virtual environments using ___ [TODO: Which ones to try?] "],["scaling.html", "Chapter 11 Scaling 11.1 Types of Scaling 11.2 Load-Balancing configurations 11.3 Adding servers in real time", " Chapter 11 Scaling 11.1 Types of Scaling Vertical Scaling – just make the server bigger Horizontal – add more parallel servers Sometimes called load-balancing Reliability - or high-availability (HA) Make sure the service doesn’t go down (or less often) Eliminate single-points-of-failure Really hard to do all the way – spectrum of completeness + difficulty Health Checks/Heartbeats – periodic checkins to ensure server/service healthy 11.2 Load-Balancing configurations [Graphic: network diagram of lb-config] Active/Active - all servers accept traffic Single vs multiple masters Active/Passive (fallover/failover) - secondary server, remains inert until 2nd one fails Disaster Recovery - backup data to another disk – somewhat slower resumption of service 11.3 Adding servers in real time If you find yourself under heavy load, you’ll want to add more servers If you’re using infrastructure as code tooling, this is easy It’s also easy if using some sort of docker orchestration, as the underlying hosts are all the same from the perspective of docker/K8S "],["docker.html", "Chapter 12 Docker 12.1 Container Orchestration 12.2 Exercises", " Chapter 12 Docker Docker is so hype right now. Get ready for layers of abstractions till your head spins. Docker containers are a way to isolate an app and its dependencies from the underlying host [Graphic: Docker on underlying host] A docker container is easily portable from one place to another for that reason The “secret sauce” of docker is that containers launch REALLY quickly. Lifecycle: Dockerfile -&gt; Image -&gt; Container [Graphic: Docker lifecycle] 12.1 Container Orchestration Kubernetes (K8S) – software for deploying and managing containers Helm is the standard tool for defining kubernetes deployments. Helmfile is a templating system for helm. There are other competitors, most notably docker swarm, but K8S is by far the biggest The line is fuzzy though – there are container orchestration services that aren’t K8S or even abstract a level up from K8S. [TODO: Graphic of K8S/Docker] 12.2 Exercises Put a shiny app in a container, run it on your desktop. Put that container into a container registry. Use a local K8S distribution to run several instances of that container. "],["offline.html", "Chapter 13 Offline", " Chapter 13 Offline Some organizations require that servers not be connected to the internet (airgapped/offline) This makes things hard. Understanding whether it’s inbound connections that are disallowed or both outbound + inbound is important. Difficulties that tend to arise: Downloading software updates Getting R/Python packages Software licensing (often reach to license servers) "],["things-to-add-additional-topics.html", "Chapter 14 Things To Add (Additional Topics)", " Chapter 14 Things To Add (Additional Topics) Security groups "],["determining-what-you-need.html", "Chapter 15 Determining What You Need", " Chapter 15 Determining What You Need Determining Requirements for your platform Platform requirements combination of Work being done on the platform Data Science needs (i.e. horsepower) Existing state of org’s data Type of work data scientists might do Batch Jobs ETL Reports Data Generation Real-Time Interactive Models APIs Apps Data Science + IT Needs Languages (R, Python, SQL) Sharing – internal, external Online or offline? Data Security – row based access? Centralized workbench vs individual workstations Existing Assets BI/Analytics Platforms Data Connections Checklist of above "],["getting-started-with-terminal.html", "Chapter 16 Getting Started with Terminal", " Chapter 16 Getting Started with Terminal Terminal in Mac Iterm2 bash, zsh, fish windows WSL Powershell PuTTY – no longer needed, but still an option Do need to enable SSH client tmux vim/nano "],["useful-shell-commands.html", "Chapter 17 Useful Shell Commands 17.1 Miscellaneous Symbols 17.2 Moving yourself and your files 17.3 Checking out Files 17.4 Checking out Server Activity 17.5 Checking out Networking 17.6 User Management", " Chapter 17 Useful Shell Commands Unlike on your Windows or Mac desktop, most work on a server is done via the shell – an interactive text-based prompt. If you master the set of commands below, you’ll always feel like a real hacker. In most cases, you’ll be using the bash shell. If you’re using Linux or Mac, that’ll be the default. Below, I’m intentionally mixing up bash commands and Linux system commands because they’re useful. If you know the difference and are pedantic enough to care, this list isn’t for you anyway. 17.1 Miscellaneous Symbols Symbol What it is Helpful options Example / system root ~ your home directory echo ~ / home/alex.gold . current working directory man manual | the pipe echo $ sudo su 17.2 Moving yourself and your files C ommand What it does Helpful options Example pwd print working directory $ pwd /U sers/alex.gold/ cd change directory $ cd ~/Documents ls list -l - format as list -a - all include hidden files $ ls . $ ls -la rm remove delete permanently! -r - recursively a directory and included files -f - force - don’t ask for each file $ rm old_doc r m -rf old_docs/ BE VERY CAREFUL WITH -rf cp copy mv move chmod 17.3 Checking out Files Often useful in server contexts for reading log files. C ommand What it does Helpful options Example cat less tail -f grep tar 17.4 Checking out Server Activity C ommand What it does Helpful options Example df -h top ps lsof 17.5 Checking out Networking C ommand What it does Helpful options Example ping ne tstat curl 17.6 User Management C ommand What it does Helpful options Example w hoami p asswd us eradd "],["more-to-say.html", "A More to Say", " A More to Say Yeah! I have finished my book, but I have more to say about some topics. Let me explain them in this appendix. To know more about bookdown, see https://bookdown.org. "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
