[["index.html", "DevOps for Data Science (working title) Preface Software information and conventions Acknowledgments", " DevOps for Data Science (working title) Alex Gold 2021-07-12 Preface At some point, most data scientists reach the point where they want to show their work to others. But the skills and tools to deploy data science are completely different from the skills and tools needed to do data science. If you’re a data scientist who wants to get your work in front of the right people, this book aims to equip you with all the technical things you need to know that aren’t data science. Hopefully, once you’ve read this book, you’ll understand how to deploy your data science, whether you’re building a DIY deployment system or trying to work with your organization’s IT/DevOps/SysAdmin/SRE group to make that happen. Software information and conventions I used the knitr package (Xie 2015) and the bookdown package (Xie 2021) to compile my book. My R session information is shown below: xfun::session_info() ## R version 4.1.0 (2021-05-18) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Catalina 10.15.7 ## ## Locale: en_US.UTF-8 / en_US.UTF-8 / en_US.UTF-8 / C / en_US.UTF-8 / en_US.UTF-8 ## ## Package version: ## base64enc_0.1.3 bookdown_0.22 ## compiler_4.1.0 digest_0.6.27 ## evaluate_0.14 glue_1.4.2 ## graphics_4.1.0 grDevices_4.1.0 ## highr_0.9 htmltools_0.5.1.1 ## jsonlite_1.7.2 knitr_1.33 ## magrittr_2.0.1 markdown_1.1 ## methods_4.1.0 mime_0.11 ## rlang_0.4.11 rmarkdown_2.9 ## stats_4.1.0 stringi_1.6.2 ## stringr_1.4.0 tinytex_0.32 ## tools_4.1.0 utils_4.1.0 ## xfun_0.24 yaml_2.2.1 Package names are in bold text (e.g., rmarkdown), and inline code and filenames are formatted in a typewriter font (e.g., knitr::knit('foo.Rmd')). Function names are followed by parentheses (e.g., bookdown::render_book()). Acknowledgments A lot of people are helping me write this book. This book is published to the web using GitHub Actions from rOpenSci. References "],["about-the-author.html", "About the Author", " About the Author Alex Gold is a manager on the Solutions Engineering team at RStudio. He works with customers of RStudio’s professional software to help them deploy, configure, and use RStudio’s professional software and open source tooling in R and Python. In his free time, he enjoys landscaping, handstands, and Tai Chi. He occasionally blogs about data, management, and leadership at alexkgold.space. "],["introduction.html", "Chapter 1 Introduction 1.1 Components", " Chapter 1 Introduction One of the most frequent questions from data scientists is, “how do I ask my IT team for that?” Hopefully, reading this book will help you learn how to ask, or even to DIY. Getting work off your laptop often starts with deployment, but increasingly organizations are also doing data science development in centralized data science platforms. Encompasses parts of DevOps, MLOps, DataOps. Reader Persona: Debby Data Scientist - reasonably senior data scientist, wants to start doing more than emailing reports around. Knows that the org has an IT group, but doesn’t know how to talk to them. Knows R/Python, ML, Stats, git 1.1 Components Workbench Package Management Change Control (git) Deployment Environment Front End vs Back End Data Ingest/Access Determining Requirements for your platform Platform requirements combination of Work being done on the platform Data Science needs (i.e. horsepower) Existing state of org’s data Type of work data scientists might do Batch Jobs ETL Reports Data Generation Real-Time Interactive Models APIs Apps Data Science + IT Needs Languages (R, Python, SQL) Sharing – internal, external Online or offline? Data Security – row based access? Centralized workbench vs individual workstations Existing Assets BI/Analytics Platforms Data Connections "],["the-command-line.html", "Chapter 2 The Command Line", " Chapter 2 The Command Line Most SysAdmin work is done on the command line Unlike your laptop, servers don’t often have a desktop interface The interactive sections of this book will require you to use the command line -&gt; How to get to command line on different systems -&gt; Set up docker desktop "],["computers-where-data-science-gets-done.html", "Chapter 3 Computers: where data science gets done 3.1 Choosing your data science machine 3.2 Desktop vs Server 3.3 Getting to a Server 3.4 How is a server different from my laptop? [maybe make a table?] 3.5 How can I get my very own server? 3.6 Exercises:", " Chapter 3 Computers: where data science gets done Photo by Massimo Botturi on Unsplash. A computer is just 1s and 0s. This is both true and entirely unhelpful. The mathematical basis for computing, which explains how optical, electrical, and magnetic impulses get turned into Facebook, Mario, a pandas dataframe, and everything else you’ve ever interacted with in a computer is both astounding and head-spinningly meta. So fear not – you don’t need to be some Alan Turing-level mathematician, but understanding the important components in your computer, and how they work together will really help you comprehend what’s going on when you use your computer in a professional context, and by the end of this chapter, we’ll get to how that translates onto a server – moving work off of your laptop and on to a remote server. 3.1 Choosing your data science machine Computers are kinda complicated. There are many interlocking components, each with specific attributes and a performance profile that make your computer work. Luckily, for a data scientist who just wants to be able to do their analyses, there are basically only five meaningful parts to a computer, and three (plus one) important attributes. This diagram shows the five important computer components for a data scientist: input, compute, storage, memory, and output. There’s a lot more detail below on each of these components and how they work together on your system. In terms of performance, there are really only three important attributes of your machine (plus one) and number of CPU cores, core speed, amount of memory. If you’re a data scientist working in R or Python, in most cases you’ll want to choose a machine with fewer, faster cores and as much memory as you can reasonably get. More details on why this is the case below. Amount of storage is also relevant as a data scientist – but only in the binary – if you’ve got enough you’ll never think about it and if you don’t you’ll never stop thinking about it. Get a lot. Storage is cheap these days – way cheaper than you spending your time trying to manage things from eating up your storage. 3.1.1 Input Input is the incoming signal to the computer. This could be keyboard and mouse input on a desktop computer, a request coming in from a webapp, or a request for something from another computer. No matter what the originating form, by the time this gets to the computer, you can think of it just as a request to do something – it doesn’t matter whether it’s to jump Mario over the chomp chain, type the word umbrella in your next novella, or run a regression in R. By the time it gets into your computer it’s all just a bunch of input. When you’re working on your home computer, you’ll never really think about this – except if you’re a person who loves those clacky mechanical keyboards. But it’s all the same to the computer. Similarly, if you’re on a server, whatever the default input speed is probably good enough. 3.1.2 Compute Anything that actually happens in your computer is done by the compute part. Back to the head-spinning genius of Alan Turing – it turns out literally everything you do on your computer can be reduced to adding integers that are interpreted in different ways – as colors, letters, shapes, or sometimes even as integers. Your CPU is very, very good at adding very, very fast. CPUs have two important attributes to keep track of – the number of cores and the speed of each core. You can think of a core as “a place where adding happens”. Each core can do one addition at a time, so you can make your computer faster by either increasing the speed of each core, or adding more cores and parallelizing the operations. These days, midrange consumer laptops have somewhere between 4 and 16 cores. You may also have access to a GPU. A GPU is exactly like a CPU conceptually. The main difference is that CPUs have a relatively small number of relatively fast cores, while a GPU has a very high number of relatively slow cores. As of this writing, CPUs for consumer-grade computers have around 4-16 cores, while GPUs have 700-4,000. Setting up a GPU to do deep learning can be somewhat of a hassle, though it’s getting easier all the time. This book isn’t gong to go into the specifics of how to set up a GPU for deep learning applications. It turns out that for a number of applications including playing video games, photo and video editing, some kinds of machine learning applications, and (yes) BitCoin mining, the “many slower cores” architecture results in huge speedups. 3.1.3 Hard Drive/Storage/Disk Storage is where your computer keeps things for later. Often called the hard drive. There are two important attributes of your storage – the amount measured in thousands (kilo-), millions (mega-), billions (giga-), trillions (terra-) of bytes. Depending on your application, you could also see thousands of terrabyes (petta-) or even thousands of pettabytes (yottabytes). How fast your storage is also relevant - though rarely in real life. If your storage is fast enough, and most is, you’ll never think about it. If your storage is not, your life will be full of pain. For anything to happen to something in storage, it needs to be moved out of storage. In your mental model of your computer, the main thing to know is that this is a slow operation (though much faster if your machine has an SSD rather than a HDD). 3.1.4 RAM/Memory RAM (Random Access Memory) is the temporary storage your computer keeps for things you’re actually using. It’s quite a bit faster to access than your long-term hard drive storage. RAM can vary widely in terms of speed, but unless you’re building your own computer, the main attribute you’ll be considering is how much of it you need. You can generally think of RAM as the working memory on your computer. Things you’re actively working on will be stored in RAM ready for you to pick back up at a moment’s notice. These days, computers are quite savvy about taking things you’ve walked away from, storing them to the hard drive temporarily, and then bringing them back again into RAM when you need them. Your computer does this all the time, and you probably haven’t realized. If you’re coding in R or Python, the amount of RAM on your system will likely be the first time you’ll pay attention to the computer itself. Any CPU or GPU can do anything any other one can (that whole Turing brain-spin again). But if you don’t have enough RAM to do what you want to do, you’ll quickly end up crashing your R or Python session. Most operations R or Python operations require that you load all of your data into memory before you use it. You can get around this limitation by intelligently using a database or libraries that facilitate on-disk operations like Apache Arrow. Depending on the size of your data and how computationally intensive your work is, this can be a real limitation on where you can do your work. These days, most consumer grade laptops have around 8-32 Gb of RAM. Getting more RAM is one reason some data scientists decide to move to a server – as you can rent a server with 1000 times as much RAM – though ones that big are quite costly. 3.1.5 Output There’s actually not a lot that’s important to understand about output as a data scientist, except that it happens. The output of a calculation will go somewhere. Sometimes this is just a write back into memory or onto the hard drive, but if you’re working on a desktop, some kind of notification is usually output to your monitor or speakers. If you’re on a server, there is no monitor or speakers, so the output goes out via some sort of networking device. 3.2 Desktop vs Server You’ve probably have heard of a server. Usually in regards to some far-off powerful computer, perhaps shrouded by clouds, and definitely capable of running circles around your puny laptop. The reality of servers is far more quotidian. In fact, whether you’re a data professional or not – even if you’ve never written a single line of code, our lives are positively surrounded by servers. By the end of this chapter, we’ll demystify servers, help you understand how you use them in your daily life, and even get to the point where you can have one up and running all by yourself! A server is just a computer that lives somewhere else. When you think of a computer, you probably think of your laptop – a beautiful screen attached to a keyboard, multitouch trackpad, and somewhere in there, some guts that make the whole thing run. Let’s take it a step back. Instead of everything in one box, imagine a desktop, which you might have – or probably are at least familiar with. You’ve got the computer itself – the whirring box with the fans – and then you separately plug in (or wirelessly attach) a keyboard, mouse, monitor, and maybe some speakers. Servers are simply the next evolution in this chain. A server is just the computer part of a computer – no keyboard, mouse, or monitor. A server is valuable for its pure computing ability. Servers are the way the entire internet is stored, processed, and delivered to your eyes. Here are a few ways you interact with servers in your daily life: When you request directions from Google Maps on your phone, your phone makes a request to Google’s servers to get the directions via a computer-to-computer application programming interface (API). Google’s servers compute the directions you need and sends them back to your phone in a specialized response language (usually JSON). Your phone then renders the directions you need to your screen. When you navigate to Facebook (or any other website), that company’s servers send your computer back the components that make up that webpage. Your computer knows how to take those components and render them for you to view. When you use your credit card at the grocery store, the credit card machine reaches out to the company’s servers to verify that your card is valid and check that the item you’re trying to purchase doesn’t put you over your credit limit before verifying that the purchase is ok. The list goes on and on and on. In 2021, servers are virtually all around us. As you can see from the list above, there are a few different ways servers get accessed in your daily life. Sometimes you access a server when you go to a website and it sends you back content. Other times, you indirectly access a server via a machine-to-machine connection (usually an application programming interface – API). One thing you’ll notice from the examples above is that servers (shockingly) are used to serve things to end-users. Unlike the desktop on your computer, you’re generally not accessing the computer itself. In terms of using a server, that’s the biggest difference between your laptop and a server. In most cases, a server doesn’t have a desktop full of icons you can click to navigate. In all the cases above, you were interacting with someone else’s servers. So you’re just interacting with the part of the server they’ve chosen to expose to the world. When you have your own server – and we’ll get there by the end of this chapter, you’ll connect using a protocol called SSH – there’s a whole chapter on that – and navigate and interact using command line interfaces (CLIs). 3.2.1 Data Science on the server Many data scientists and organizations are moving their data science work onto servers, as opposed to individual data scientists’ desktops. When this happens, it generally falls into one of two patterns: 3.2.2 Data Scientist-led server migration This generally happens when one (or a few) data scientists need more horsepower for their analyses than they have on their laptops. These migrations tend to be quite ad-hoc, often involving servers that are transient for the duration of a project. If you’re in this situation, you probably have the advantage of getting to request whatever hardware specs you need. The downside is that the environment is likely to be temporary. It’s often linked to a particular project with beefy server requirements, and the environment will go away when the project is over. In many organizations, these instances aren’t maintained as part of the IT organization’s regular order, so they often fall to the individual data scientists who use them to provision, use, and maintain. 3.2.3 SysAdmin-led server migration More and more frequently, organizations are moving all (or many) of their data scientists into a server-based development environment. This is common in organizations where the IT/Admin group wants to maintain one centralized data science environment for everyone, as opposed to being responsible for helping each data scientist maintain an environment on their laptop. This can be a great situation for data scientists - they get instant access through their web browser to more compute resources, and don’t have to worry about maintaining local installations of data science tools like R, Python, RStudio, Jupyter, and connections to important data sources – those things are just theirs for the taking. So much of whether the server-based experience is good or not depends on the relationship between the data science and IT/Admin group. In organizations where these groups work together smootly, this can be a huge win for everyone involved. However, there are some organizations where IT/Admins are so concerned with stability and security that they make it impossible to do data science, and the data scientists spend all their time playing cat-and-mouse games to try to get work done behind IT/Admin’s backs. If you work at such a place, it’s frankly hard to get much done. It’s probably worth investing some time into improving your relationship with your favorite person on the IT/Admin team. 3.3 Getting to a Server When you access the computer sitting on your desk, you just wiggle the mouse to wake it up, put in a password, and you’re off to the races. With a server, the situation is a little more complicated. How you know where to access a server Lots of layers, but controls access to the servers doors Different from authentication, which determines who gets let into doors 3.4 How is a server different from my laptop? [maybe make a table?] OS How you access Server software (NGINX) 3.5 How can I get my very own server? Cloud free tiers Walk through creating account, logging in, getting EC2 set up Access using PEM key (point to SSH chapter) Set up NGINX 3.6 Exercises: Consider a simple data science app that allows an end user to view data. Can you draw a diagram of how that app could be deployed on remote servers? Where could the data live? How would users access the app? How would you access the app to do maintenance? Try standing up your own server in one of the major cloud providers and accessing it using the provided .pem key. "],["networking-and-the-internet.html", "Chapter 4 Networking and the Internet 4.1 General Background on Networking 4.2 Networking for your data science servers 4.3 I’ve heard HTTPS is a thing 4.4 Exercises", " Chapter 4 Networking and the Internet Learning Objectives: What does the term networking mean? Understand public vs private networks and potential interfaces Understand broadly what packet switching is 4.1 General Background on Networking Computers communicate by sending packets from one to another Those packets get routed over a network Often frustratingly difficult Built on layers of protocols Physical Infrastructure -&gt; TCP/IP -&gt; HTTP/SSH -&gt; Start with single server on the web, gradually build up more complicated &lt;- 4.1.1 How do computers know where to find each other? [Diagram: Packet Routing] The internet uses a set of protocols collectively known as TCP/IP – they define how networked computers create and address packets, and then how those packets are sent, routed, and received. When a computer joins a network, it gets an Internet Protocol (IP) address V4 vs V6 IP addresses are split up into “blocks” – some reserved for particular uses Routing generally occurs via DNS – centralized servers that transform the usual human-friendly URLs we know into IP addresses. Public IP addresses are paid – you’ll get one with your EC2 instance 4.1.2 What happens once they find each other? [Diagram: HTTP Request] An “application layer” protocol kicks in. This is a protocol for sending actual content from one computer to another. For example, HTTP defines a client/server relationship where the client (your web browser) makes requests that the server fulfills. Other protocols: e.g. websockets Whether you are allowed to access a particular resource is about Authentication and Authorization 4.2 Networking for your data science servers [Diagram: Common Network Topology] Most work occurs inside a VPC IP addresses inside a VPC can be assigned as you wish Subject to CIDR block rules Need translation between inside and outside -&gt; proxy If being used for SSH access, usually called bastion Proxies commonly used to serve as firewalls, network address translators, load-balancers Forward (outbound) proxy vs reverse (inbound) proxy Private vs public subnets Particular ports being “open” 4.3 I’ve heard HTTPS is a thing Secured HTTP End-to-end encryption using Transport Layer Security (TLS) or Secure Sockets Layer (SSL) HTTP over TLS/SSL Getting and configuring an SSL certificate 4.4 Exercises Consider going to the website google.com. Draw a diagram of how the following are incorporated: TCP/IP, DNS, HTTP, HTTPS. TODO: More exercise "],["the-cloud.html", "Chapter 5 The Cloud 5.1 The Rise of Services 5.2 Common Services That will be helpful to know offhand 5.3 Cloud Tooling 5.4 Exercises", " Chapter 5 The Cloud Learning Objectives: Understand what the cloud is and why it’s so hype Demystify general cloud offerings General description for servers that you rent instead of buy 3 major cloud providers AWS = Amazon Azure = Microsoft GCP = Google 5.1 The Rise of Services Server/Cloud usage has gone the way of the rest of the economy – services The “rent me a server + connectors” model is generally called IaaS Layers of abstraction on top of that – PaaS, Saas Metaphor: Cake shop [Diagram: Cake Shop] These metaphors are helpful for general understanding…actually applying them gets really fuzzy in data science land, because something that is SaaS from IT perspective is PaaS from data scientist perspective. Often find it more helpful to just categorize as IaaS/“something more abstracted”. These services are generally surrounded my marketing jargon – rarely do they just explain this is ___ as a service. https://stackoverflow.com/questions/16820336/what-is-saas-paas-and-iaas-with-examples 5.2 Common Services That will be helpful to know offhand Amazon’s names aren’t completely obvious. They’re usually referred to by their 3-letter acronyms. Azure and Google’s offerings tend to be named more literally. 5.3 Cloud Tooling Identity MGMT - IAM, Azure AD Billing Mgmt 5.3.1 IaaS Compute - AWS EC2, Azure VMs, Google Compute Engine Storage file - EBS, Azure managed disk, Google Persistent Disk Network drives - EFS, Azure Files, Google Filestore block/blob - S3, Azure Blob Storage, Google Cloud Storage Networking: Private Clouds: VPC, Virtual Network, Google Virtual Private Cloud DNS - Route53, Azure DNS + Traffic Manager, Google Cloud DNS 5.3.2 Not IaaS Container Hosting - ECS, Azure Container Instances + Container Registry K8S cluster as a service - EKS, AKS, GKE Run a function as a service - Lambda, Azure Functions, Google Cloud Functions Database - RDS/Redshift, Azure Database, Google Cloud SQL SageMaker - ML platform as a service, Azure ML, Google Notebooks https://docs.microsoft.com/en-us/azure/architecture/aws-professional/services#networking https://cloud.google.com/free/docs/aws-azure-gcp-service-comparison 5.4 Exercises Example of something you’d want to build – for each of the 3 providers, which services would you use if you wanted an IaaS solution, a PaaS solution? "],["ssh.html", "Chapter 6 Accessing Servers via SSH 6.1 What is SSH? 6.2 How does SSH work? 6.3 Basic SSH Use 6.4 Getting Comfortable in your own setup 6.5 Advanced SSH Tips + Tricks 6.6 Exercises", " Chapter 6 Accessing Servers via SSH Learning Objectives: Understanding of what SSH is Understanding of how to use SSH to access a server 6.1 What is SSH? Stands for secure (socket) shell Way to remotely access a different (virtual) machine Workhorse of doing work on another server 6.2 How does SSH work? Via Public Key Encryption Public/Private Key Known Hosts [Diagram: SSH Keys] &gt; Sidebar on public key encryption By default, available on port 22 6.3 Basic SSH Use The terminal 3 step process generate public/private keys ssh-keygen place keys in appropriate place use ssh to do work Permissions on key 6.4 Getting Comfortable in your own setup Using ssh-config 6.5 Advanced SSH Tips + Tricks SSH Tunneling/Port Forwarding -vvvv, -i, -p [Diagram: Port Forwarding] 6.6 Exercises Draw your own diagram of the sequence of things that occur when you execute ssh user@server.company.com in the terminal. Stand up a new server on one of the major cloud providers. Try logging in using the provided key file. Create a new user on the server. Generate a new SSH key locally and copy the correct key onto the server (think for a moment about which key is the correct one – consult your diagram from step 1 if necessary). Set up an SSH alias so you can SSH into your server using just ssh testserver (hint: look at your SSH config). Create a local text file and SFTP it to the server. SSH in, edit the file, and SFTP it back. [Advanced] Stand up a nginx server on your remote instance. Don’t open the port to the world, but SSH port forward the server page to your local browser. "],["auth.html", "Chapter 7 Accessing Servers via Browser/HTTP 7.1 Your Everyday Experience of Auth and How it Differs from Enterprise 7.2 How auth works 7.3 Experiences of Different Auth Systems 7.4 Auth Techniques 7.5 Auth Technologies: Username + Password Systems 7.6 Auth Tech: Token/Ticket Based Systems", " Chapter 7 Accessing Servers via Browser/HTTP Learning Objectives: Understand what authentication + authorization are and where they occur General understanding of different kinds of auth they might encounter and implications of types 7.1 Your Everyday Experience of Auth and How it Differs from Enterprise You do auth all the time Private auth – login to your bank with a unique username and pw MFA (2FA) – get a text message or use an authenticator token Or yubikey, etc Public auth – you might login to spotify using your facebook, apple, or google credentials Spotify is a Service Provider and the other side an Identity Provider 7.2 How auth works Check who you are - Authentication Check whether you should have access - Authorization User goes to login: Make the user provide creds (username/password) System checks credentials against store (For SSO) Check if user is allowed to access resource Send back something User is authorized 7.3 Experiences of Different Auth Systems For users Do I have to authenticate to do X? Where do I authenticate? With which credentials? For Admins Am I transmitting user details, or just an authentication token/ticket? Security Can I easily manage access centrally? Can I keep credential stores centralized? How are users created (provisioned) and deprovisioned Do we have to maintain our own auth server? Differences between auth types 1) How is the communication done? Transmit text vs token (type/encryption) 2) When is the identity checked? Once, every time 3) When is authorization checked? 4) How encrypted are things? What does SSO mean? * Most commonly used to mean login once and then authentication happens automatically for duration of session * Sometimes just used to mean that people use the same username and password across different systems 7.4 Auth Techniques How to manage large groups of users with different permissions 7.4.1 Authorization: Groups, Roles, and Permissions Groups: sets of users (e.g. data-science) Roles: Role a user should have in the system (e.g. admin) Permissions: things someone can do – can be granted on the basis of membership in a group, role grants, or other attributes Groups and roles are maintained in the central user store 7.4.2 Service Accounts Often used in contexts where a service is accessing something on behalf of itself e.g. an app accessing a database on behalf of any user who can access the app 7.4.3 Whole-Server Auth (TODO: What is this really called?) Often done inside private networks – instead of providing auth on a per-user/session basis, just allow an entire server access to another e.g. any connection to the database by server A is allowed 7.5 Auth Technologies: Username + Password Systems 7.5.1 System Create user accounts within a given system (e.g. within a produdct, on a server) Have people use those accounts for just that system People need way more usernames/passwords Hard to provision/deprovision users system-wide 7.5.2 LDAP/AD Lightweight Directory Access Protocol (LDAP) is a relatively old, open, protocol used for doing auth Active Directory (AD) is Microsoft’s implementation of LDAP – huge enterprise install base Confusingly, Azure Active Directory is Microsoft Azure’s Cloud Based Identity Management as a Service – it is most often configured to use a different authentication technology (SAML), not LDAP Considered insecure/outdated by many IT groups Requires actually submitting credentials through the requester system No ability to centralize authentication No seamless SSO experience 7.6 Auth Tech: Token/Ticket Based Systems World moving this direction 7.6.1 Kerberos Tickets An old technology, but still considered very secure - server to server transit of token Microsoft-based and integrates tightly with AD Used frequently to make connections to databases Can also be used to establish an SSO experience 7.6.2 SAML The most common type of true SSO for enterprises Login once to a central IdP and an identity token gets stored in web browser Go to a new service, and the identity token requests a service token to use the service XML-based Relatively new – many enterprises switching to this from LDAP/AD (Azure AD) 7.6.3 OIDC/OAuth2.0 Based on JSON Web Tokens (JWT, often pronounced “Jot” b/c reasons…) Technically OIDC is an authentication standard and OAuth an authorization You’ll sometimes see e.g. SAML auth + OAuth being used for db access Very similar to SAML from the user perspective More common in external services, SAML often preferred inside enterprises Resources: https://www.okta.com/identity-101/saml-vs-oauth/ https://www.okta.com/identity-101/whats-the-difference-between-oauth-openid-connect-and-saml/ https://phoenixnap.com/blog/kerberos-authentication https://www.dnsstuff.com/rbac-vs-abac-access-control "],["choosing-and-using-a-data-architecture.html", "Chapter 8 Choosing and Using A Data Architecture 8.1 Options for Data Storage 8.2 How to pick 8.3 Working with File Systems 8.4 Working with Databases 8.5 Exercises", " Chapter 8 Choosing and Using A Data Architecture Learning Objectives Understand options for data architectures Understand interactions between data and auth DataOps is a huge field – a lot of it touches data science Some of the biggest consideration when you’re thinking about data science 8.1 Options for Data Storage 3 basic types of data stores: flat file (csv, pickle/rds), flat file w/ interactive read (arrow), database Implications how to configure access Database is standalone option, usually requires configuring separate server Other two can be configured two main ways: File system Bucket Storage Data Lake – usually for raw data, basically bucket storage writ large, sometimes include interacticve read capabilities Data Warehouse – usually a database 8.2 How to pick Flat files are simplest Flat files w/ interactive read are great for med-large data Databases often already exist If not, a few tips: SQL-based databases are the backbone of analytics work Database as a service is a basic cloud provider There are SO many options TODO: How much more in depth am I going here? 8.3 Working with File Systems Working on your laptop file system isn’t great Working in a cloud provider is different than on your laptop Volumes are separate from compute Often have automated snapshots/backups Volumes can also be shared across multiple servers Process of putting a volume on a server is called mounting 8.4 Working with Databases Two main ways to work with databases from R/Python - direct connection or w/ connector/driver (JDBC/ODBC) https://docs.google.com/presentation/d/1wJ3YnB4ob3AkNRDLpvYy3B-JMUaVsPjl3AA5QkN7fII/edit 8.4.1 Security Row-level security Kerberos JWT IAM roles 8.5 Exercises Connect to and use an S3 bucket from the command line and from R/Python. Stand up a container with Postgres in it, connect using isql, then from R/Python using ODBC/JDBC. Stand up an EC2 instance w/ EBS volume, change EBS to different instances. Stand up an NFS instance and mount onto a server. "],["environments-and-reproducibility.html", "Chapter 9 Environments and Reproducibility", " Chapter 9 Environments and Reproducibility Learning Objectives: Understand the layers that go into reproducing an output Understand which part is “the environment” Understand some general strategies/tooling for reproducing Managing environments is hard. Necessary in data science primarily for repro. At least 7 levels: Hardware Operating System System Libraries Programming Language Packages App Code App Data -&gt; Difference between environment and data is sketchy All of it contributes to “state” [graphic: Image of concentric environments] Different requirements depending on if you’re talking about development (IDE) environment or production environment. Repository vs library mgmt Tools to Reproduce Parts of Stack: IaC tooling Provisioning Configuration mgmt AMIs virtualenv/conda/renv Docker/Containers "],["it-and-data-science-workflows.html", "Chapter 10 IT and Data Science Workflows 10.1 Dev/Test/Prod 10.2 Cattle, Not Pets 10.3 How Does Stuff Actually Get Deployed 10.4 Exercises", " Chapter 10 IT and Data Science Workflows Learning Objectives: Understand what dev/test/prod means Understand how dev/test/prod might apply to data science Well developed patterns for putting things into prod in IT Data science starting to adopt, but not exactly the same 10.1 Dev/Test/Prod Classic workflow - three precise mirrors Do dev work in dev env Test for functionality and with users (User Acceptance Testing - UAT) Deploy to production environment Doesn’t quite work for data science Often, dev is a distinct (RStudio IDE/Jupyter) Because more exploratory than pure software development Test/Prod can be on one server or multiple, depending on org If you’re managing your own Data Science server, you’ll want an additional staging environment – unrelated to app changes, purely for IT changes [Graphic: deploying app changes and server changes] 10.2 Cattle, Not Pets Manual promotion from place to place = bad State maintenance Drift Cattle not Pets Infrastructure as code tooling Terraform AWS CFN, Azure Resource Manager, Google Cloud Deployment Manager Chef Puppet Saltstack Ansible Docker Vagrant Helm/Helmfile Important to ensure idempotence 10.3 How Does Stuff Actually Get Deployed [Graphic: CI/CD deployment into test/prod] CI/CD - Jenkins, Travis, GHA Powered by Git, mostly Intro to using GHA 10.4 Exercises Create something and add GHA integration [TODO: What to use as example?] Stand up some virtual environments using ___ [TODO: Which ones to try?] "],["scaling.html", "Chapter 11 Scaling 11.1 Types of Scaling 11.2 Load-Balancing configurations 11.3 Adding servers in real time", " Chapter 11 Scaling 11.1 Types of Scaling Vertical Scaling – just make the server bigger Horizontal – add more parallel servers Sometimes called load-balancing Reliability - or high-availability (HA) Make sure the service doesn’t go down (or less often) Eliminate single-points-of-failure Really hard to do all the way – spectrum of completeness + difficulty Health Checks/Heartbeats – periodic checkins to ensure server/service healthy 11.2 Load-Balancing configurations [Graphic: network diagram of lb-config] Active/Active - all servers accept traffic Single vs multiple masters Active/Passive (fallover/failover) - secondary server, remains inert until 2nd one fails Disaster Recovery - backup data to another disk – somewhat slower resumption of service 11.3 Adding servers in real time If you find yourself under heavy load, you’ll want to add more servers If you’re using infrastructure as code tooling, this is easy It’s also easy if using some sort of docker orchestration, as the underlying hosts are all the same from the perspective of docker/K8S "],["docker.html", "Chapter 12 Docker 12.1 Container Orchestration 12.2 Exercises", " Chapter 12 Docker Docker is so hype right now. Get ready for layers of abstractions till your head spins. Docker containers are a way to isolate an app and its dependencies from the underlying host [Graphic: Docker on underlying host] A docker container is easily portable from one place to another for that reason The “secret sauce” of docker is that containers launch REALLY quickly. Lifecycle: Dockerfile -&gt; Image -&gt; Container [Graphic: Docker lifecycle] 12.1 Container Orchestration Kubernetes (K8S) – software for deploying and managing containers Helm is the standard tool for defining kubernetes deployments. Helmfile is a templating system for helm. There are other competitors, most notably docker swarm, but K8S is by far the biggest The line is fuzzy though – there are container orchestration services that aren’t K8S or even abstract a level up from K8S. [TODO: Graphic of K8S/Docker] 12.2 Exercises Put a shiny app in a container, run it on your desktop. Put that container into a container registry. Use a local K8S distribution to run several instances of that container. "],["offline.html", "Chapter 13 Offline", " Chapter 13 Offline Some organizations require that servers not be connected to the internet (airgapped/offline) This makes things hard. Understanding whether it’s inbound connections that are disallowed or both outbound + inbound is important. Difficulties that tend to arise: Downloading software updates Getting R/Python packages Software licensing (often reach to license servers) "],["things-to-add-additional-topics.html", "Chapter 14 Things To Add (Additional Topics)", " Chapter 14 Things To Add (Additional Topics) Security groups "],["determining-what-you-need.html", "Chapter 15 Determining What You Need", " Chapter 15 Determining What You Need Determining Requirements for your platform Platform requirements combination of Work being done on the platform Data Science needs (i.e. horsepower) Existing state of org’s data Type of work data scientists might do Batch Jobs ETL Reports Data Generation Real-Time Interactive Models APIs Apps Data Science + IT Needs Languages (R, Python, SQL) Sharing – internal, external Online or offline? Data Security – row based access? Centralized workbench vs individual workstations Existing Assets BI/Analytics Platforms Data Connections Checklist of above "],["getting-started-with-terminal.html", "Chapter 16 Getting Started with Terminal", " Chapter 16 Getting Started with Terminal Terminal in Mac Iterm2 bash, zsh, fish windows WSL Powershell PuTTY – no longer needed, but still an option Do need to enable SSH client tmux vim/nano "],["useful-shell-commands.html", "Chapter 17 Useful Shell Commands 17.1 Miscellaneous Symbols 17.2 Moving yourself and your files 17.3 Checking out Files 17.4 Checking out Server Activity 17.5 Checking out Networking 17.6 User Management", " Chapter 17 Useful Shell Commands Unlike on your Windows or Mac desktop, most work on a server is done via the shell – an interactive text-based prompt. If you master the set of commands below, you’ll always feel like a real hacker. In most cases, you’ll be using the bash shell. If you’re using Linux or Mac, that’ll be the default. Below, I’m intentionally mixing up bash commands and Linux system commands because they’re useful. If you know the difference and are pedantic enough to care, this list isn’t for you anyway. 17.1 Miscellaneous Symbols Symbol What it is Helpful options Example / system root ~ your home directory echo ~ / home/alex.gold . current working directory man manual | the pipe echo $ sudo su 17.2 Moving yourself and your files C ommand What it does Helpful options Example pwd print working directory $ pwd /Users/alex.gold/ cd change directory $ cd ~/Documents ls list -l - format as list -a - all (include hidden files) $ ls . $ ls -la rm remove (delete permanently!) -r - recursively (a directory and included files) -f - force - don’t ask for each file $ rm old_doc rm -rf old_docs/ BE VERY CAREFUL WITH -rf cp copy mv move chmod 17.3 Checking out Files Often useful in server contexts for reading log files. C ommand What it does Helpful options Example cat less tail -f grep tar 17.4 Checking out Server Activity C ommand What it does Helpful options Example df -h top ps lsof 17.5 Checking out Networking C ommand What it does Helpful options Example ping ne tstat curl 17.6 User Management C ommand What it does Helpful options Example w hoami p asswd us eradd "],["more-to-say.html", "A More to Say", " A More to Say Yeah! I have finished my book, but I have more to say about some topics. Let me explain them in this appendix. To know more about bookdown, see https://bookdown.org. "],["references.html", "References", " References "]]
