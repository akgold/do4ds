[["index.html", "DevOps for Data Science (working title) Preface 0.1 Moving Data Science to A Server Software information and conventions Acknowledgments", " DevOps for Data Science (working title) Alex Gold 2021-09-23 Preface At some point, most data scientists reach the point where they want to show their work to others. But the skills and tools to deploy data science are completely different from the skills and tools needed to do data science. If you’re a data scientist who wants to get your work in front of the right people, this book aims to equip you with all the technical things you need to know that aren’t data science. Hopefully, once you’ve read this book, you’ll understand how to deploy your data science, whether you’re building a DIY deployment system or trying to work with your organization’s IT/DevOps/SysAdmin/SRE group to make that happen. 0.1 Moving Data Science to A Server In recent years, as data science has become more central to organizations, many have been moving their operations off of individual contributors’ laptops and onto centralized servers. Depending on your organization, the centralization of data science operations can make your life way easier – or it can be kinda a bummer. Server migrations can work well regardless of whether they’re instigated by the data science or the IT organization. The biggest determinant is how well the data science and IT/DevOps teams can collaborate. Data scientists are good at manipulating and using data, but most have little expertise in SysAdmin work, and aren’t really that interested. On the flip side, IT/DevOps organizations usually don’t really understand data science workflows, the data science development process, or how data scientists use R and Python. Often, migrations to a server are instigated by the data scientists themselves – usually because they’ve run out of horsepower on their laptops. If you, or one of your teammates, enjoys and is good as SysAdmin work, this can be a great situation! You get the hardware you need for your project quickly and with minimal interference. On the other hand, most data scientists don’t really want to be SysAdmins, and these systems are often fragile, isolated from other corporate systems, and potentially susceptible to security vulnerabilities. Other organizations are moving to servers as well, but led by the IT group. For many IT groups, it’s way easier to maintain a centralized server environment, as opposed to helping each data scientist maintain their own environment on their laptop. Having just one platform makes it much easier to give shared access to more powerful computing platforms, to data sources that require some configuration, and to R and Python packages that wrap around system libraries and can be a pain to configure (looking at you, rJava). This can be a great situation for data scientists! If the platform is well-configured and scoped, you can get instant access through their web browser to more compute resources, and don’t have to worry about maintaining local installations of data science tools like R, Python, RStudio, and Jupyter, and you don’t need to worry about how to connect to important data sources – those things are just available for use. But this can also be a bad experience. Long wait times for hardware or software updates, overly restrictive policies – especially around package management – and misunderstandings of what data scientists are trying to do on the platforms can lead to servers going largely unused. So much of whether the server-based experience is good or not depends on the relationship between the data science and IT/Admin group. In organizations where these groups work together smoothly, this can be a huge win for everyone involved. However, there are some organizations where IT/Admins are so concerned with stability and security that they make it impossible to do data science, and the data scientists spend all their time playing cat-and-mouse games to try to get work done behind IT/Admin’s backs. If you work at such a place, it’s frankly hard to get much done on the server. It’s probably worth investing some time into improving your relationship with your favorite person on the IT/Admin team. Hopefully, this book will help you understand a little of what’s on the minds of people in the IT group, and a sense of how to talk to them better. Software information and conventions I used the knitr package (Xie 2015) and the bookdown package (Xie 2021) to compile my book. My R session information is shown below: xfun::session_info() ## R version 4.1.1 (2021-08-10) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Catalina 10.15.7 ## ## Locale: en_US.UTF-8 / en_US.UTF-8 / en_US.UTF-8 / C / en_US.UTF-8 / en_US.UTF-8 ## ## Package version: ## base64enc_0.1.3 bookdown_0.24 compiler_4.1.1 ## digest_0.6.27 evaluate_0.14 fastmap_1.1.0 ## glue_1.4.2 graphics_4.1.1 grDevices_4.1.1 ## highr_0.9 htmltools_0.5.2 jquerylib_0.1.4 ## jsonlite_1.7.2 knitr_1.34 magrittr_2.0.1 ## methods_4.1.1 rlang_0.4.11 rmarkdown_2.11 ## stats_4.1.1 stringi_1.7.4 stringr_1.4.0 ## tinytex_0.33 tools_4.1.1 utils_4.1.1 ## xfun_0.26 yaml_2.2.1 Package names are in bold text (e.g., rmarkdown), and inline code and filenames are formatted in a typewriter font (e.g., knitr::knit('foo.Rmd')). Function names are followed by parentheses (e.g., bookdown::render_book()). Acknowledgments A lot of people are helping me write this book. This book is published to the web using GitHub Actions from rOpenSci. References "],["about-the-author.html", "About the Author", " About the Author Alex Gold is a manager on the Solutions Engineering team at RStudio. He works with customers of RStudio’s professional software to help them deploy, configure, and use RStudio’s professional software and open source tooling in R and Python. In his free time, he enjoys landscaping, handstands, and Tai Chi. He occasionally blogs about data, management, and leadership at alexkgold.space. Color palette: Tea Green: CAFFDO Steel Blue: 3E7CB1 Kombu Green: 273c2c Bright Maroon: B33951 Sandy Brown: FCAA67 "],["introduction.html", "Chapter 1 Introduction 1.1 Components", " Chapter 1 Introduction One of the most frequent questions from data scientists is, “how do I ask my IT team for that?” Hopefully, reading this book will help you learn how to ask, or even to DIY. Getting work off your laptop often starts with deployment, but increasingly organizations are also doing data science development in centralized data science platforms. Encompasses parts of DevOps, MLOps, DataOps. If you’re in an environment where you just do data science on your laptop, you probably don’t really need this book. You might find the sections on the command line and on data access interesting, but most of this book will be geared towards server-based data science – whether that’s working in a browser-based data science IDE or trying to deploy data science assets to a server. 1.1 Components Workbench Package Management Change Control (git) Deployment Environment Front End vs Back End Data Ingest/Access Determining Requirements for your platform Platform requirements combination of Work being done on the platform Data Science needs (i.e. horsepower) Existing state of org’s data Type of work data scientists might do Batch Jobs ETL Reports Data Generation Real-Time Interactive Models APIs Apps Data Science + IT Needs Languages (R, Python, SQL) Sharing – internal, external Online or offline? Data Security – row based access? Centralized workbench vs individual workstations Existing Assets BI/Analytics Platforms Data Connections "],["command-line.html", "Chapter 2 The Command Line", " Chapter 2 The Command Line Most SysAdmin work is done on the command line Unlike your laptop, servers don’t often have a desktop interface The interactive sections of this book will require you to use the command line -&gt; How to get to command line on different systems -&gt; Set up docker desktop #TODO: Address using vi "],["computers-and-servers.html", "Chapter 3 Computers and Servers 3.1 Computers are addition factories 3.2 Choosing the right data science machine 3.3 Is a server different? 3.4 Getting a server of your own 3.5 Exercises", " Chapter 3 Computers and Servers Data Science is a delightful mashup of statistics and computer science. While you can be a great data scientists without a deep understanding of computational theory, a mental model of how your computer works is helpful, especially when you head to production. In this chapter, we’ll develop a mental model for how computers work, and explore how well that mental model applies to both the familiar computers in your life, but also more remote servers. If you’re into pedantic nitpicking, you’re going to love this chapter apart, as I’ve grossly oversimplified how computers work. On the other hand, this basic mental model has served me well across hundreds of interactions with data scientists and IT/DevOps professionals. And by the end of the chapter, we’ll get super practical – giving you a how-to on getting a server of your very own to play with. 3.1 Computers are addition factories As a data scientist, the amount of computational theory it’s really helpful to understand in your day-to-day can be summarized in three sentences: Computers can only add. Modern ones do so very well and very fast. Everything a computer “does” is just adding two (usually very large) numbers, reinterpreted.1 I like to think of computers as factories for doing addition problems. We see meaning in typing the word umbrella or jumping Mario over a Chomp Chain and we interpret something from the output of some R code or listening to Carly Rae Jepsen’s newest bop, but to your computer it’s all just addition. Every bit of input you provide your computer is homogenized into addition problems. Once those problems are done, the results are reverted back into something we interpret as meaningful. Obviously the details of that conversion are complicated and important – but for the purposes of understanding what your computer’s doing when you clean some data or run a machine learning model, you don’t have to understand much more than that. 3.1.1 Compute The addition assembly line itself – where the work actually gets done – is referred to as compute. It’s where 2+2 gets turned into 4, and where 345619912 + 182347910 gets turned into 527967822. The heart of the factory in your computer is the central processing unit (CPU). There are two elements to the total speed of your compute – the total number of cores, which you can think of as an individual conveyor belt doing a single problem at a time, and the speed at which each belt is running. These days, most consumer-grade laptops have between 4 and 16 cores, and may have additional capabilities that effectively doubles that number. So most laptop CPUs can do between 4 and 32 simultaneous addition problems. In your computer, the basic measure of conveyor belt speed is single-core “clock speed” in hertz (hz) – operations per second. The cores in your laptop probably run between 2-5 gigahertz (GHz): 2-5 billion operations per second. A few decades ago, there was a steady march in increases in clock speed – increasing by approximately 10x during the 90s, by 2-3x in the 2000s, and somewhere between not at all and 1.5x in the 2010s. Computers have gotten much faster through this entire time, but mostly from sources other than boosts in single-core clock speeds. For example, in 2020, Apple released a series of Macs with its new M1 chip. Real-world performance of the M1 chip was dramatically better than earlier models, despite modest improvements in clock speeds. 3.1.1.1 GPU Computing While compute usually just refers to the CPU, it’s not completely synonymous. Computers can offload some problems to a graphical processing unit (GPU). GPUs are specialized chips used for tasks like editing photo or videos, rendering video game graphics, some kinds of machine learning, and (yes) Bitcoin mining. Where the CPU has a few fast cores, the GPU takes the opposite approach, with many slower cores. Where a consumer-grade CPU has 4-16 cores, mid-range GPUs have 700-4,000, but each one runs between 1% and 10% the speed of a CPU core. For GPU-centric tasks, the overwhelming parallelism of a GPU is more important than the speed of any individual core, and GPU computation can be dramatically faster. For the purposes of data science, many popular machine learning techniques – including neural networks, XGBoost, and other tree-based models – potentially run much much faster on GPUs relative to CPUs. 3.1.2 Memory (RAM) Your computer’s random access memory (RAM) is its short term storage. Your computer uses RAM to store addition problems it’s going to tackle soon, and results it thinks it might need again in the near future. The benefit of RAM is that it’s very fast to access. The downside is that it’s temporary. When your computer turns off, the RAM gets wiped.2 You probably know this, but memory and storage is measured in bytes. These days, most consumer hard drives are between 1/2 to a few terrabytes (trillion bytes). Some enterprise data stores run on the scales of thousands of terrabytes (pettabytes) or even thousands of pettabytes (yottabytes). Modern consumer-grade laptops come with somewhere between 4 and 16 Gb of memory. 3.1.3 Storage (Hard Drive/Disk) Your computer’s storage is like a freezer. It keeps things permanently, whether the factory is operating or not. The tradeoff is that things there are cold, and take some time to warm up before they can be used. A few years ago, all hard drives were (HDDs), and were basically the same. HDDs have a bunch of magnetic disks that spin very fast (5,400 and 7,200 RPM are common speeds). Magnetized read/write heads move among the disks and save and read your data. In the last few years, solid-state drives (SSDs) have been taking over. SSDs, which are collections of flash memory chips, are up to 15x faster than HDDs. They also can take a wider variety of shapes and sizes, and are more reliable and durable because they have no moving parts. The main drawback is that they’re usually more expensive per byte, but prices are still quite reasonable. Many consumer laptops have only an SSD at this point. Some desktops and high-end laptops combine a smaller SSD with a larger HDD. 3.2 Choosing the right data science machine In my experience as a data scientist and talking to IT/DevOps organizations trying to equip data scientists, the same questions about choosing a computer come up over and over again. Here are the guidelines I often share. 3.2.1 Get as much RAM as feasible In most cases, R and Python have to load all of your data into memory. Thus, the size of the data you can use is limited to the size of your machine’s RAM. Most other limits of your machine will just result in things being slower than you’d really want, but trying to load too much data into memory will result in a session crash, and you won’t be able to do your analysis. You can get around the in-memory limitation by using a database or libraries that facilitate on-disk operations like Apache Arrow or dask. It’s easy to say that you’ll always want more RAM, but a rough rule of thumb for whether you’ve got enough is the following: Amount of RAM = max amount of data * 3 Because you’ll often be doing some sort of transformation that results in invisible data copies and your computer can’t devote all of its memory, you’ll want to leave plenty of room over your actual data size. I’ve never run into problems with a scaling factor of 3, but this is definitely only a rough rule-of-thumb. 3.2.2 Go for fewer, faster cores in the CPU R and Python are single-threaded. Unless you’re using special libraries for parallel processing, you’ll end up red-lining a single CPU core while the other just look on in silence. Therefore, single core clock speed matters more than the number of cores, and fewer, faster cores are usually preferable to many slower If you’re buying a laptop or desktop, there usually aren’t explicit choices between a few fast cores and more slow cores. Most modern CPUs are pretty darn good, and you should just get one that fits your budget. If you’re standing up a server, you often have an explicit choice between more slower cores and fewer faster ones. 3.2.3 Get a GPU…maybe… If you’re doing machine learning that can be improved by GPU-backed operations, you might want a GPU. In general, only highly parallel machine learning problems like training a neural network or tree-based models will benefit from GPU computation. On the other hand, GPUs are expensive, non-machine learning tasks like data processing don’t benefit from GPU computation, and many machine learning tasks are amenable to linear models that run well CPU-only. 3.2.4 Get a lot of storage, it’s cheap As for storage – get a lot – but don’t think about it too hard, because it’s cheap. Both a 1TB SSD and a 4TB HDD are around $100. Storage is cheap enough these days that it is almost always more cost efficient to buy more storage rather than making a highly-paid professional spend their time trying to figure out how to move things around. One litmus test of an IT organization that is well-equipped to support data science is whether they understand this. Smart organizations know that just getting more storage is easily worth the cost in terms of the time of admins and data scientists. 3.3 Is a server different? No. But also yes. At its core, a server is exactly the same sort of addition factory as your laptop, and the same mental model of what is happening under the hood will serve you well. The big difference is in how the input and output is done. While you interact directly with a computer through keyboard and mouse/touchpad, servers generally don’t have built in graphical interfaces – by default all interaction occurs via command line tools. One of the reasons is that the overwhelming majority of the world’s servers run the Linux operating system, as opposed to the Windows or Mac OS your laptop probably runs.3 There are many different distributions (usually called “distros”) of Linux. For day-to-day enterprise server use, the most common of these are Ubuntu, CentOS, Red Hat Enterprise Linux (RHEL), SUSE Enterprise Linux. Along with most of the world’s servers, almost all of the world’s embedded computers – in ATMs, cars and planes, TVs, and most other gadgets and gizmos – run on Linux. If you have an Android phone or a Chromebook – that’s Linux. Basically all of the world’s supercomputers use Linux. 3.4 Getting a server of your own Let’s completely switch gears and get practical. Until now, this chapter has been entirely background. If you read it, hopefully you learned a little about a computer works, how your computer is mostly like, but a little unlike a server, and why you might be doing data science on a server. If you’re reading this book, you probably want a deeper understanding of how using and maintaining a server actually works. In the last section of this chapter, we’ll walk through how you can get a server of your very own. In contrast to the computer sitting on your desk, you’ll have to access it over the internet and you’ll use a specialized access protocol called SSH. Those topics are important enough that there are standalone chapters on them. For now, we’re going to gloss entirely over the how and why and just get you to running a server. If you follow along from this point, it’ll probably take you 10-15 minutes to be running a server of your own. We’re going to be standing up a server on Amazon Web Services (AWS). In particular, we’ll be standing up a server in their free tier – so there will be no cost involved as long as you haven’t used up all your AWS free tier credits before now. 3.4.1 Login to the AWS Console We’re going to start by logging into AWS. If you’ve done this before, just go ahead and log in. If not, go to aws.amazon.com and click Sign In to the Console . If you’ve never set up an AWS account before, click Create a New AWS account and follow the instructions to create an account. Note that even if you’ve got an Amazon account for ordering stuff online and watching movies, an AWS account is separate. #TODO: Add link to cloud chapter Once you’ve logged in, you’ll be confronted by the AWS console. There are a ton of things here, and it’s rather overwhelming. There’s a chapter on the business model behind this, so skip ahead if you want, or spend a minute poking around before continuing. 3.4.2 Stand up an instance In the next few paragraphs, I’m going to give you instructions to quickly get a server up – with basically no explanation. If you read this whole book, you’ll understand all of this, the alternatives you could take, and the reasons I’m going to strongly recommend you take this server back offline in just a few minutes. For now, click on the EC2 service (it’s under Launch a virtual machine or Compute depending on where you landed). Scroll down the launch instance button. Here are all the different Quick Start Amazon Machine Images (AMIs). Find and click Ubuntu Server 20.04 LTS – it’ll be one of the first handful. Now you’ll be seeing the instance size chooser. It should have auto-selected a server with the label Free tier eligible. Just stick with this for now. Scroll down and click Review and Launch, and Launch on the next page. When you click Launch, you’ll be asked to use a key pair. Assuming you don’t have an existing keypair, select Create a new key pair, name it my_test_key, and click Download. Keep track of the my_test_key.pem file your computer downloads. Click Launch Instances. AWS is now creating a virtual server just for you. If you click View Instances in the lower right, you’ll see your instance. When the instance state switches to Running, it’s up and running! 3.4.3 SSH into the server The .pem key you downloaded is the skeleton key to your server. If you were setting up a real server, you’d need to be extremely careful with this key, as it allows anyone who has it unrestricted access to the server. For the same reason, it’s also great for playing around quickly with a server. Before we can use it to open the server, we’ll need to make a quick change to the permissions on the key. More details on what that means in the [] chapter. #TODO: which chapter? To take the next steps, you’ll need to at least be able to open your computer’s terminal and copy/paste some commands below. If that’s new to you, feel free to check out the chapter on using the command line. If you’re on a Mac or Linux system, you’ll do the following: $ cd ~/Downloads #or whatever directory the key is in $ chmod 600 my_test_key.pem #TODO: Windows? To access your server, click on the Instance ID link for your server, and copy the Public IPv4 DNS, which will start with ec2- and end with amazonaws.com. In your terminal type the following $ ssh -i my_test_key.pem ubuntu@&lt;Public IPv4 DNS you copied&gt; Type yes when prompted, and you’re now logged in to your server! 3.4.4 Doing A Thing Before we log off and kill this server, let’s do one little thing. We’re going to stand up Nginx, which is a common webserver, and serve a little webpage to ourselves. Let’s start by installing Nginx, copy and paste the command below. $ sudo apt update $ sudo apt-get install nginx -y By default, our EC2 instance only allows SSH traffic, which is on port 22 by default. We need to open up HTTP traffic. Go back to your instance in the AWS console and scroll down to the Security tab. Click the blue link under Security Groups, which will start with sg- and include launch-wizard- in parentheses. Click Edit inbound rules , then Add rule. Under Type, scroll down and select HTTP, and under Source Type, select Anywhere-IPv4. Scroll down and click Save rules. Go back to your instance page and copy the Public IPv4 DNS again. Paste this into your browser’s navigation bar, and add http:// right before. When you navigate to the page, you’ll see the default Nginx home page. Let’s make a quick change to the page, just for fun. Back in your terminal that’s still SSH-ed into the instance, navigate to where that page is located: $ cd /var/www/html You can edit the page by typing $ sudo vi index.nginx-debian.html If you don’t recall how to use vi, check out the command line tutorial, but for now, you can enter edit mode by pressing i, navigating around with the arrow keys, and typing. Edit something – say change this line &lt;h1&gt;Welcome to nginx!&lt;/h1 &gt; to something a little more personalized. When you’re done, hit esc followed by :wq. Now, when you re-load the page in your browser, you should be able to see your changes reflected there. 3.4.5 Burn it all down One of the best things about cloud infrastructure is that it can go away as easily as it came up. We made a number of choices here that are fine for ephemeral infrastructure for playing around. But this server should not be used for anything real. When you’ve had your fill of playing, let’s take the server down. Go back to the EC2 page for your server. Under the Instance State drop down in the upper right, choose Terminate Instance. If you go to the Instances page, it’ll take just a minute for the instance to go away. 3.5 Exercises Think about the scenarios below – which part of your computer would you want to upgrade to solve the problem? You try to load a big csv file into pandas in Python. It churns for a while and then crashes. You go to build a new ML model on your data. You’d like to re-train the model once a day, but it turns out training this model takes 26 hours on your laptop. You design an visualization Matplotlib , and create a whole bunch in a loop/ Try standing up a new server and installing R and RStudio Server Open Source, or Python and JupyterHub. Hint 1: Remember that your instance only allows traffic to SSH in on port 22 by default. You access RStudio on port 8787 by default and JupyterHub on port 8000. You control what ports are open via the Security Group. Hint 2: You’ll need to create a user on the server. The adduser command is your friend. #TODO: test out JupyterHub The reason why this is the case and how it works is fascinating. If you’re interested, it comes back to Alan Turing’s famous paper on computability. I recommend The Annotated Turing: A Guided Tour Through Alan Turing’s Historic Paper on Computability and the Turing Machine for a surprisingly readable walkthrough of the paper.↩︎ You probably don’t experience this personally. Modern computers are pretty smart about dumping RAM onto the hard disk before shutting down, and bringing it back on startup, so you usually won’t notice this happening.↩︎ There are Windows Server versions that are reasonably popular in enterprises. There are no Mac servers. There is a product called Mac Server, but it’s used to manage Mac desktops and iOS devices, not a real server.↩︎ "],["networking.html", "Chapter 4 Networking and the Internet 4.1 TCP/IP: How packets find their way 4.2 Ports: The last mile 4.3 Application Protocols: Reading the mail 4.4 Networking for your data science servers 4.5 Special IP Addresses and Ports 4.6 Special HTTP(S) Codes 4.7 Exercises", " Chapter 4 Networking and the Internet Computers communicate with each other via networking. Generally, networking refers to the rules and protocols computers use to find and communicate with one another. Before we get into how you can do networking, a little background on what goes over the network. Computers communicate with each other on a call-and-response model. If I want to go to a website, check my bank account, send a chat message, or watch a movie on Netflix, my computer sends a request off to another computer (a server in most cases) and awaits a response. The way computer networks work is reasonably similar to the way physical mail works. When you ask your computer to do something over a network, it turns that request into one or more packets. A packet has a header that includes the address it’s going to along with details about the packet itself. Due to some cool information theory, it turns out that it works better to take large payloads, split them up into smaller payloads, and send each packet off independently. The header of each packet includes the destination address, as well as information about how to reconstruct the payloads on the other end. There are a number of different sets of rules and protocols in play here that range from the physical interconnections of the wires along the way. There are two really layers that are particularly important to understand from the perspective of a data scientist working on a server. The important layers to understand correspond to how to read what’s on the outside of the packet, and what’s on the inside. The first part of this chapter is some general information on how computer networks function. The latter part is about how networks tend to be configured in enterprise settings and how you can go about setting up your own network. 4.1 TCP/IP: How packets find their way TCP/IP is a set of protocols that collectively define how networked computers create and address packets, and how those packets are routed and reconstructed when they get where they’re going. You’ve probably heard of IP in the context of an IP address. When a computer joins a network, it is given an IP address. An IP address is a number, unique within any given network, that determines where a given computer is. As the internet has gotten more and more computers on it, it turns out that we’re running out of IP addresses. Currently, you will see both the old type of IP addresses, called IPv4 and the new type, IPv6, in use. There are only about 4.3 billion IPv4 addresses, which it turns out isn’t that much in the modern world, especially as large blocks of those addresses are reserved and are not available for public use. In constrast, the number of IPv6 addresses is 39 digits long. This is mostly just trivia, expect that in your every day life working with servers, you’ll see a lot of IP addresses, and it’s important to know what they look like. Each block has a particular meaning, which isn’t super important for our purposes. IPv4 address - four blocks of 8-bit fields (integers up to 256): 0.0.0.0 to 255.255.255.255 IPV6 address example: eight blocks of hexadecimal (0-9 + a-f) digits separated by colons, with certain rules about dropping multiple zeroes and using colons to denote successive zeroes - 2001:0db8:85a3:0000:0000:8a2e:0370:7334 or 2001:db8:: But for the most part, we don’t interact with IP addresses. We’re much more used to seeing the locations of web sites defined in terms of a URL like google.com. When you type a URL into your web browser, your computer sends a request off to a specialized phonebook server calls a DNS (Domain Name System) server. The DNS server translates the URL you typed into an IP address to actually locate the server. The way DNS actually works is complicated, but one of the most important things to know is that it’s kinda a pain. If you’re trying to set up a server at an actual URL, you’ll need to register a domain name and get it associated with your server. This takes time – they say as much as 24-48 hours for DNS changes to propagate through the public servers that every computer connected to the internet might reach. In order to avoid having to constantly reach out to DNS servers, your computer keeps a DNS cache locally. As you’re working on servers, you may find that something suddenly becomes unreachable. A great first troubleshooting step is always to try using an incognito window as it will ignore your DNS cache and reach out to the DNS server afresh. 4.2 Ports: The last mile Once a packet gets to the right server, it needs to actually walk through the door. The doors on an individual server are called ports. Each IP address has over 65,000 ports available, but many of those are reserved for special purposes. There’s a cheatsheet of a few you’ll learn pretty quickly at the end of the chapter. Each port can be bound to exactly one running process and can be open or closed. That means that providing a service to other computers on a network is a three step process – get the service up and running, bind it to a particular port, and make sure the port is accepting incoming traffic. The last step – checking that the port is open – is a common time-waster for people new to DevOps. Checking that your port is listening is the unplug it and plug it back in of networking configuration. Note that outbound communication also requires using a port. This isn’t something you have to think about much because your computer will automatically choose a random available port and keep it open for the duration of the network traffic. 4.3 Application Protocols: Reading the mail So, your packet request has made it to the server on the other end, found the right port, and made it to the listening service. Time to open the envelope. What’s inside? This is where the application layer protocol kicks in. The application layer protocol defines how the server should understand the request that was sent and what are legitimate things to send back. There are many application layer protocols for everything from file transfers to email traffic, checking user permissions to database access. There are a few particular application protocols that come up quite often for the working data scientist: HTTP is how you look at websites in a web browser. To be slightly more technical, defines a client/server relationship where the client makes requests that the server fulfills. FTP/SFTP is a protocol for transferring files directly from one computer to another. SSH is a protocol for securely accessing a remote computer over a network. It’s important enough for data scientists that there’s a whole chapter on it. Websockets are a protocol for doing interactivity with a website. Many popular R and Python web frameworks use websockets, including the Shiny R package and the Streamlit Python package. 4.3.1 HTTP and HTTPs HTTP is a particularly important protocol to understand because it forms the basis for most of the way we interact with the internet. As a data scientist, you also may need to interact directly with HTTP traffic making API calls or writing your own API to serve data science results to others. Secured HTTP End-to-end encryption using Transport Layer Security (TLS) or Secure Sockets Layer (SSL) HTTP over TLS/SSL Getting and configuring an SSL certificate 4.4 Networking for your data science servers The biggest and best known of computer networks is the internet. But at its core, the internet is just another computer network, so the same principles apply to both public and private networks. For the most part, [Diagram: Common Network Topology] Most work occurs inside a VPC IP addresses inside a VPC can be assigned as you wish Subject to CIDR block rules Need translation between inside and outside -&gt; proxy If being used for SSH access, usually called bastion Proxies commonly used to serve as firewalls, network address translators, load-balancers Forward (outbound) proxy vs reverse (inbound) proxy Private vs public subnets Particular ports being “open” 4.5 Special IP Addresses and Ports 127.0.0.1 - localhost 0.0.0.0 - unspecified All ports below 1024 reserved. 80 - HTTP default 443 - HTTPS default 22 - SSH default 4.6 Special HTTP(S) Codes 200 404 403 4.7 Exercises Consider going to the website google.com. Draw a diagram of how the following are incorporated: TCP/IP, DNS, HTTP, HTTPS. Set up a free-tier EC2 instance and put an NGINX server up. Figure out how to allow your computer to access the server, but not your phone. Try accessing it on a non-default port. Try to HTTP into a fresh EC2 with the default security group. Take a look at the inbound security group rules. Hint: is there an inbound rule on a default HTTP port? SSH into your EC2 instance and try to reach out to something on the internet (curl…). See if you can change security group rules to shut down access. Can you do it by changing the IP address range it’s accepting connections from? Can you do it by changing the listening ports? "],["the-cloud.html", "Chapter 5 The Cloud 5.1 The Rise of Services 5.2 Common Services That will be helpful to know offhand 5.3 Cloud Tooling 5.4 Exercises", " Chapter 5 The Cloud Learning Objectives: Understand what the cloud is and why it’s so hype Demystify general cloud offerings General description for servers that you rent instead of buy 3 major cloud providers AWS = Amazon Azure = Microsoft GCP = Google 5.1 The Rise of Services Server/Cloud usage has gone the way of the rest of the economy – services The “rent me a server + connectors” model is generally called IaaS Layers of abstraction on top of that – PaaS, Saas Metaphor: Cake shop [Diagram: Cake Shop] These metaphors are helpful for general understanding…actually applying them gets really fuzzy in data science land, because something that is SaaS from IT perspective is PaaS from data scientist perspective. Often find it more helpful to just categorize as IaaS/“something more abstracted”. These services are generally surrounded my marketing jargon – rarely do they just explain this is ___ as a service. https://stackoverflow.com/questions/16820336/what-is-saas-paas-and-iaas-with-examples 5.2 Common Services That will be helpful to know offhand Amazon’s names aren’t completely obvious. They’re usually referred to by their 3-letter acronyms. Azure and Google’s offerings tend to be named more literally. 5.3 Cloud Tooling Identity MGMT - IAM, Azure AD Billing Mgmt 5.3.1 IaaS Compute - AWS EC2, Azure VMs, Google Compute Engine Storage file - EBS, Azure managed disk, Google Persistent Disk Network drives - EFS, Azure Files, Google Filestore block/blob - S3, Azure Blob Storage, Google Cloud Storage Networking: Private Clouds: VPC, Virtual Network, Google Virtual Private Cloud DNS - Route53, Azure DNS + Traffic Manager, Google Cloud DNS 5.3.2 Not IaaS Container Hosting - ECS, Azure Container Instances + Container Registry K8S cluster as a service - EKS, AKS, GKE Run a function as a service - Lambda, Azure Functions, Google Cloud Functions Database - RDS/Redshift, Azure Database, Google Cloud SQL SageMaker - ML platform as a service, Azure ML, Google Notebooks https://docs.microsoft.com/en-us/azure/architecture/aws-professional/services#networking https://cloud.google.com/free/docs/aws-azure-gcp-service-comparison 5.4 Exercises Example of something you’d want to build – for each of the 3 providers, which services would you use if you wanted an IaaS solution, a PaaS solution? "],["ssh.html", "Chapter 6 Accessing Servers via SSH 6.1 What is SSH? 6.2 How does SSH work? 6.3 Basic SSH Use 6.4 Getting Comfortable in your own setup 6.5 Advanced SSH Tips + Tricks 6.6 Exercises", " Chapter 6 Accessing Servers via SSH Learning Objectives: Understanding of what SSH is Understanding of how to use SSH to access a server 6.1 What is SSH? Stands for secure (socket) shell Way to remotely access a different (virtual) machine Workhorse of doing work on another server 6.2 How does SSH work? Via Public Key Encryption Public/Private Key Known Hosts [Diagram: SSH Keys] &gt; Sidebar on public key encryption By default, available on port 22 6.3 Basic SSH Use The terminal 3 step process generate public/private keys ssh-keygen place keys in appropriate place use ssh to do work Permissions on key 6.4 Getting Comfortable in your own setup Using ssh-config 6.5 Advanced SSH Tips + Tricks SSH Tunneling/Port Forwarding -vvvv, -i, -p [Diagram: Port Forwarding] 6.6 Exercises Draw your own diagram of the sequence of things that occur when you execute ssh user@server.company.com in the terminal. Stand up a new server on one of the major cloud providers. Try logging in using the provided key file. Create a new user on the server. Generate a new SSH key locally and copy the correct key onto the server (think for a moment about which key is the correct one – consult your diagram from step 1 if necessary). Set up an SSH alias so you can SSH into your server using just ssh testserver (hint: look at your SSH config). Create a local text file and SFTP it to the server. SSH in, edit the file, and SFTP it back. [Advanced] Stand up a nginx server on your remote instance. Don’t open the port to the world, but SSH port forward the server page to your local browser. "],["auth.html", "Chapter 7 How login works 7.1 The many flavors of auth (or what does SSO mean?) 7.2 Auth Techniques 7.3 Auth Technologies", " Chapter 7 How login works You do auth all the time Private auth – login to your bank with a unique username and pw MFA (2FA) – get a text message or use an authenticator token Or yubikey, etc Public auth – you might login to spotify using your facebook, apple, or google credentials Spotify is a Service Provider and the other side an Identity Provider For the most part, this chapter is designed to help you talk to the folks who manage auth at your organization. Unless you’ve got a small organization with only a few data scientists, you probably won’t have to manage this yourself. But it can be extremely helpful to understand how auth works when you’re trying to ask for something from the organization’s admins. In order to talk concretely about logging into systems, it’s helpful to clarify some terms. For the most part, these terms are industry standard, but I’m also going to generalize some terms because they’re used for certain types of auth, and they’re really useful. For the purposes of this document, we’re going to be talking about trying to login to a service. A service is something you want to login to – it could be your phone or your email, or a server, a database, or software like RStudio Server or JupyterHub. When you go to login to a service, there are two things that have to happen. First, you have to assert and verify who you are. This process is called authentication, and the assertion and proof of identity are your credentials or creds. The most common credentials are a username and password, but there are other options including SSH keys, multi-factor authentication, or biometrics. Once you’ve proven who you are, then the system needs to determine what you’re supposed to have access to. This process is called authorization. Often, the authentication + authorization process is referred to collectively as auth. [#TODO: Image of Auth Make the user provide creds (username/password) System checks credentials against store (For SSO) Check if user is allowed to access resource Send back something User is authorized] So, to summarize when you go to login to a service, the service reaches out to some sort of system to verify your identity. Depending on the method, it may also check your authorization. The name of that system varies by the auth method, but for the purposes of this chapter, we’ll refer to it generally as the identity store. Depending on the auth method, the identity store may store just authentication records, or both authentication and authorization. 7.1 The many flavors of auth (or what does SSO mean?) Single Sign On (SSO) is a slippery term, so it is almost always necessary to clarify what is meant by the term when you hear it. At some organizations, identity management isn’t centralized at all. This means that usernames and passwords are unique to each service, onboarding and offboarding of users has to be handled independently for each service, and users have to login frequently. In short, it’s often not a great system. This is never referred to as SSO. Most organizations of a meaningful size have centralized identity management. This means that identities, credentials, authorization, onboarding, and offboarding are handled centrally. However, you may still need to independently login to each system. For example, in this system, every service might take the same username and password as your credentials, but if you go to RStudio Server followed by JupyterHub, you’ll need to provide that username and password independently to each service. This system is often facilitated by PAM, and LDAP/AD. Some organizations call this SSO, because there’s only one set of credentials. In true SSO, users login once and are given a token or ticket.4 Then, when they go to the next service, they don’t have to login again because that service can just look at the token or ticket to do auth for that user. For example, in this system, I could go to RStudio Server and login, and then go to JupyterHub and get in without being prompted again for my password. This type of auth is facilitated by Kerberos, SAML, or OAuth. 7.2 Auth Techniques If you have five data scientists in your group, and the only shared resource you have is an RStudio Server instance, you probably don’t need to think terribly hard about auth. It’s pretty straightforward to just make users on a server and give them access to everything. But as organizations get larger with hundreds or thousands of users, there’s constant churn of people joining and leaving. The number of services can creep into the dozens or hundreds and people may have very different authorization levels to different services. Trying to manage auth on the individual services is a nightmare – as is trying to keep that many usernames and passwords straight for users. That is why almost all organizations with more than a few users have centrally managed auth. 7.2.1 You get a permission and you get a permission! For the most part, we think of people being authenticated and authorized into services. However, it’s sometimes useful to consider the broader class of entities that could do auth. There are two common non-human entities that are included in auth systems that are worth considering. Service Accounts are accounts given to non-human entities when you want it to be able to do something on its own behalf. For example, maybe you’ve got a Shiny app that users use to visualize data that’s in a database. Very often, you don’t want the app to have the same permisions as the app’s author, or to inherit the permissions of the people viewing the app. Instead, you want the app to be able to have permissions to do certain database operations. In that case, you would create a service account to give to the Shiny app that has exactly those permissions. There are also times where it’s useful to go one level up and give permissions to an entire instance or service. In that case, you might assign permissions to an instance. For example, you could make it the case that anyone who is logged into the JupyterHub server is allowed to read from the database. Instance permissions are rather broad, and so they are usually only applied when you’ve got multiple resources inside a private network. In that case, authentication and authorization are only done at a single point and authorization is pretty broad. 7.2.2 Authorization is kinda hard From a management perspective, authentication is pretty simple. A person is given a set of credentials, and they have to supply those credentials when prompted to prove they are who they say they are. Authorization is a whole other can of worms. There is a meaningful literature on varieties of authorization and how they work. We’re not going to get too deep into the weeds, other than to define some common terms and how they’re used. The atomic basis for authorization is a permission. Permissions are a binary switch that answers the question is this person allowed to do the thing they are trying to do?5 The simplest way of assigning permissions is called an access control list (ACL). In systems that use ACLs, each piece of content has a list of users who are allowed access. Sometimes, ACLs are also assigned to groups, which are simply sets of users – think data-scientists. One ACL implementation with which you may be familiar is file permissions on a Linux server. For example, if you have a Mac or are on a Linux server, you can open your terminal, navigate to a directory and do the following: $ ls -l -rwxr-xr-x 1 alexkgold staff 2274 May 10 12:09 README.md That first set of characters describes the ACL for the README.md file. The first character - indicates this is a file, as opposed to a directory of files (which would be d). Then there are three sets of 3 characters, rwx, which are short for read, write, and execute, with the first group for the owner, alexkgold, the second group for anyone else in the group staff, and the third set for anyone else. So you can read -rwxr-xr-x as, this is a file that alexkgold can read, write or execute, and anyone else can read or execute, but not edit. ACLs are pretty intuitive, but it turns out that when you are managing a lot of users across a lot of files, directories, and services, they can get pretty difficult to manage, so many organizations use Role Based Access Control (RBAC). RBAC adds a layer of abstraction between users and permissions, which makes it a little harder to understand, but ultimately results in a much more flexible system. In RBAC, permissions are not assigned to individual pieces of content or to users or groups. Instead, permissions are assigned to roles, and roles are given to users or groups.6 There are also further iterations on the RBAC model, like Attribute Based Access Control (ABAC) or Policy Based Access Control (PBAC) in which there’s a long list of attributes that could be considered for a user to compute their permissions for a given service. 7.3 Auth Technologies 7.3.1 Username + Password Many pieces of software come with integrated authentication. When you use those system, the product stores encrypted username and password pairs in a database. These setups are often really easy from an admin perspective – you just set up individual users on the server. However, the flip side is that users have one more username and password to remember, which is annoying for them. Moreover, if you have more than a few users, or the system is one of more than a few, it’s hard to manage users on a lot of different systems. It can be a real pain to create accounts on a ton of different systems when a new person joins the organization, or to remove their permissions one-by-one when they leave. For this reason, most IT/Admin organizations strongly prefer using some sort of centralized identity store. 7.3.2 PAM Pluggable Authentication Modules (PAM) is a Linux system for doing authentication. As of this writing, PAM is the default authentication method for both RStudio Server and JupyterHub. Conceptually PAM is pretty straightforward. You install a service on a Linux machine and configure it to use PAM authentication from the underlying host. By default, PAM just authenticates against the users configured on the Linux server, but it can also be configured to use other sorts of “modules” to authenticate against other systems – most commonly LDAP/AD or Kerberos. PAM can also be used to do things when users login – the most common being initializing tokens or tickets to other systems, like a database. PAM is often paired with System Security Services Daemon (SSSD), which is most commonly used to automatically create Linux users on a server based on the identities stored in an LDAP/AD instance. Though conceptually simple, reading, writing, and managing PAM modules is kinda painful. #TODO: Add PAM example 7.3.3 LDAP/AD Lightweight Directory Access Protocol (LDAP) is a relatively old, open, protocol used for maintaining a set of entities and their attributes. To be precise, LDAP is actually a protocol for maintaining and accessing entities and their attributes in a tree. It happens that this is a really good structure for maintaining permissions and roles of users at an organization, and it’s the main thing LDAP is used for. Active Directory (AD) is Microsoft’s implementation of LDAP, and is by-far the most common LDAP “flavor” out there. AD so thoroughly owns the LDAP enterprise market, that LDAP is often referred to as LDAP/AD. There are other implementations you may run across, the most common being OpenLDAP. Azure, Microsoft’s cloud provider, offers an authentication service called Azure Active Directory (AAD). Confusingly, AAD is usually used in combination with SAML, not LDAP. It’s worth distinguishing the use of LDAP as an identity store from its use as an authentication technology. As a tree-based database, LDAP is uniquely well-suited to storing the identities, and other attributes of people at the organization. However, as discussed below, using LDAP to authenticate into actual services has security and convenience drawbacks, and many organizations consider it outdated and insecure. A lot of organizations are moving away from LDAP for authentication in favor of token-based technologies like SAML or OAuth, but many are keeping LDAP as their identity “source of truth” that is referenced by the SAML or OAuth Identity Provider. LDAP has three main disadvantages relative to other technologies. First, LDAP requires that your credentials (username and password, usually) actually be provided to the service you’re trying to use. This is fundamentally insecure relative to a system where your credentials are provided only to the identity provider, and the service just gets a token verifying who you are. In token-based systems, adding additional requirements like MFA or biometrics are easy, as they’re simply added at the IdP layer. In contrast, doing those things in LDAP would require the service to implement them, which usually is not the case, so you’re usually limited to username and password. The second disadvantage of LDAP is that it does not allow for central administration of permissions. LDAP directly records only objects and their attributes. Say, for example, you want only users of a particular group to have access to a certain resource. In LDAP, you would have to specify in that resource that it should only allow in users of that group. This is in contrast to SAML/OAuth, where the authorization is centrally managed. Lastly, LDAP authentication is based on each service authenticating. Once you authenticate, the service might give you a cookie so that your login persists, but there is no general-purpose token that will allow you to login to multiple services. 7.3.3.1 How LDAP Works While the technical downsides of LDAP are real, the tradeoff is that the technical operations of LDAP are pretty straightforward. In short, you try to login to a service, the service collects your username and password, sends it off to the LDAP server, and checks that your username and password are valid. Note that LDAP is purely for authentication. When you’re using LDAP, authorization has to be handled separately, which is one of the disadvantages. 7.3.3.2 Deeper Than You Need on LDAP LDAP is a tree-based entity and value store. This means that LDAP stores things and their attributes, which include a name and one or more values. For example, my entry in a corporate LDAP directory might look like this: cn: Alex Gold mail: alex.gold@example.com mail: alex.gold@example.org department: solutions mobile: 555-555-5555 objectClass = Person Most of these attributes should be pretty straightforward. cn is short for common name, and is part of the way you look up an entity in LDAP (more on that below). Each entity in LDAP must have an objectClass, which determines the type of entity it is. In this case, I am a Person , as opposed to a device, domain, organizationalRole, or room – all of which are standard objectClasses. Let’s say that your corporate LDAP looks like the tree below: #TODO: make solutions an OU in final The most common way to look up LDAP entities is with their distinguished name (DN), which is the path of names from the point you’re starting all the way back to the root of the tree. In the tree above, my DN would be cn=alex,ou=solutions,dc=example,dc=com. Note that you read the DN from right to left to work your way down the tree. Aside from cn for common name, other common fields include ou for organizational unit, and dc for domain component. 7.3.3.3 Trying out LDAP Now that we understand in theory how LDAP works, let’s try out an actual example. To start, let’s stand up LDAP in a docker container: #TODO: update ldif docker network create ldap-net docker run -p 6389:389 \\ --name ldap-service \\ --network ldap-net \\ --detach alexkgold/auth ldapsearch is a utility that lets us run queries against the LDAP tree. Let’s try it out against the LDAP container we just stood up. Let’s say I want to return everything in the subtree under example.org. In that case, I would run ldapsearch -b dc=example,dc=org, where b indicates my search base, which is a dn. But in order to make this actually work, we’ll need to include a few more arguments, including the host where the LDAP server is, indicated by -H the bind DN we’ll be using, flagged with -D the bind password we’ll be using, indicated by -w Since we’re testing, we’re also going to provide the flag -x to use whatever certificate is present on the server. Putting it altogether, along with the commands to reach the docker container, let’s try: ldapsearch -x -H ldap://localhost:6389 -b dc=example,dc=org -D &quot;cn=admin,dc=example,dc=org&quot; -w admin # extended LDIF # # LDAPv3 # base &lt;dc=example,dc=org&gt; with scope subtree # filter: (objectclass=*) # requesting: ALL # # example.org dn: dc=example,dc=org objectClass: top objectClass: dcObject objectClass: organization o: Example Inc. dc: example # admin, example.org dn: cn=admin,dc=example,dc=org objectClass: simpleSecurityObject objectClass: organizationalRole cn: admin description: LDAP administrator userPassword:: e1NTSEF9d3IyVFp6SlAyKy9xT2RsQ0owTDYzR0RzNFo0NUFrQ00= # search result search: 2 result: 0 Success # numResponses: 3 # numEntries: 2 You should be able to read what got returned pretty seamlessly. One thing to notice is that the user password is returned, so it can be compared to a password provided. It is encrypted, so it doesn’t appear in plain text. Note that ldap is a protocol – so it takes the place of the http you’re used to in normal web operations. Like there’s https, there is also a protocol called LDAPS, which is ldap + tls for the same reason you’ve got https. LDAP is (almost) always running in the same private network as the service, so many organizations don’t require using LDAPS, but others do require it. Running the ldapadmin docker run -p 6443:443 \\ --name ldap-admin \\ --env PHPLDAPADMIN_LDAP_HOSTS=ldap-service \\ --network ldap-net \\ --detach osixia/phpldapadmin dn for admin cn=admin,dc=example,dc=org pw: admin https://localhost:6443 # Replace with valid license export RSC_LICENSE=XXXX-XXXX-XXXX-XXXX-XXXX-XXXX-XXXX # Run without persistent data and using default configuration docker run -it --privileged \\ --name rsc \\ --volume $PWD/rstudio-connect.gcfg:/etc/rstudio-connect/rstudio-connect.gcfg \\ -p 3939:3939 \\ -e RSC_LICENSE=$RSC_LICENSE \\ --network ldap-net \\ rstudio/rstudio-connect:latest 7.3.3.4 Single vs Double Bind There are two different ways to establish a connection between your server and the LDAP server. The first method is called Single Bind. In a single bind authentication, the user credentials are used both to authenticate to the LDAP server, and to query the server. In a Double Bind configuration, there is a separate administrative service account, used to authenticate to the LDAP server. Once authentication is complete, then the user is queried in the system. Single bind configurations are often more limited than double bind ones. For example, in most cases you’ll only be able to see the single user as well as the groups they’re a part of. This can limit application functionality in some cases. On the other hand, there need be no master key maintained on your server, and some admins may prefer it for security reasons. We can see this really concretely. In the example above, you used a double bind by supplying admin credentials to LDAP. Let’s say instead, you just provide a single user’s credentials. In that case, I don’t get anything back if I just do a general search. ldapsearch -x -H ldap://localhost:6389 -b dc=example,dc=org -D &quot;cn=joe,dc=engineering,dc=example,dc=org&quot; -w joe # extended LDIF # # LDAPv3 # base &lt;dc=example,dc=org&gt; with scope subtree # filter: (objectclass=*) # requesting: ALL # # search result search: 2 result: 32 No such object # numResponses: 1 But just searching for information about Joe does return his own information. ldapsearch -x -H ldap://localhost:6389 -b cn=joe,dc=engineering,dc=example,dc=org -D &quot;cn=joe,dc=engineering,dc=example,dc=org&quot; -w joe  32 ✘ # extended LDIF # # LDAPv3 # base &lt;cn=joe,dc=engineering,dc=example,dc=org&gt; with scope subtree # filter: (objectclass=*) # requesting: ALL # # joe, engineering.example.org dn: cn=joe,dc=engineering,dc=example,dc=org cn: joe gidNumber: 500 givenName: Joe homeDirectory: /home/joe loginShell: /bin/sh mail: joe@example.org objectClass: inetOrgPerson objectClass: posixAccount objectClass: top sn: Golly uid: test\\joe uidNumber: 1000 userPassword:: e01ENX1qL01raWZrdk0wRm1sTDZQM0MxTUlnPT0= # search result search: 2 result: 0 Success # numResponses: 2 # numEntries: 1 7.3.4 Kerberos Tickets Kerberos is a relatively old ticket-based auth technology. In Kerberos, encrypted tickets are passed around between servers. Because these tickets live entirely on servers under the control of the organization, they are generally quite secure. Though Kerberos is freely available, it was widely adopted along with Active Directory, and it’s used almost exclusively in places that are running a lot of Microsoft products. A frequent use of Kerberos tickets is to establish database connections. Because the tickets are passed around from server to server, Kerberos can be used to create a true SSO experience for users. 7.3.4.1 How Kerberos Works All of Kerberos works by sending information to and from the central Kerberos Domain Controller (KDC). In Kerberos, authentication and authorization are handled independently. When a Kerberos session is initialized, the service sends the users credentials off to the KDC and requests something called the Ticket Granting Ticket (TGT) from the KDC. TGTs have a set expiration period. When they expire, the client has to request an updated TGT. This is one reason why Kerberos is considered quite secure - even if someone managed to steal a TGT, they’d only be able to use it for a little while before it went stale and could be revoked. When the user wants to actually do something, they send the TGT back to the KDC again and get a session key (sometimes referred to as a service ticket) that allows access to the service, usually with a specified expiration period. 7.3.4.2 Try out Kerberos #TODO 7.3.5 SAML These days Security Assertion Markup Language (SAML) is probably the most common system that provides true SSO – including single login and centrally-managed permissions. SAML does this by passing around XML tokens.7 The way this generally works is that a user attempts to login to a Service Provider (SP). The SP redirects the user to an Identity Provider (IdP), which checks either for a preexisting token in the users browser, or verifies the users credentials. The IdP checks for the user’s authorization to access the SP in question, and sends an authorization token back to the SP. Relative to LDAP/AD, which is from the early 1990s, SAML is a new kid on the block. SAML 1.0 was introduced in 2002, and SAML 2.0, which is the current standard, came out in 2005. Many large enterprises are switching their systems over to use SAML or have already done so. One superpower of SAML IdPs is that many of them can federate identity management to other systems. So, it’s pretty common for large enterprises to maintain their user base in one or more LDAP/AD system, but actually use a SAML IdP to do authentication and authorization. In fact, this is what Azure Active Directory (AAD), which is Microsoft Azure’s hosted authentication offering does. It is possible to use LDAP/AD with AAD, but most organizations use it with SAML. One of the nice things about SAML is that credentials are never shared directly with the SP. This is one of the ways in which SAML is fundamentally more secure than LDAP/AD – the users credentials are only ever shared with the IdP. There are two different ways logins can occur – starting from the SP, or starting from the IdP. In SAML, the XML tokens that are passed back and forth are called assertions. 7.3.5.1 Try SAML We’re going to use a simple SAML IdP to try out SAML a bit. This container only supports a single SP. Any IdP that might be used in an enterprise environment is going to support many SPs simultaneously. Let’s go through the environment variables we’re providing to this docker run command. We’re providing three different arguments: The SP_ENTITY_ID is the URL of the SP_ASSERTION_CONSUMER_SERVICE is the URL of the SP that is prepared to receive the authorized responses coming back from the SAML IdP. SP_SINGLE_LOGOUT_SERVICE is the URL where the SP will receive a logout command once someone has been logged out at the IdP level. Many SPs do not implement single logout. docker run --name=saml_idp \\ -p 8080:8080 \\ -p 8443:8443 \\ -e SIMPLESAMLPHP_SP_ENTITY_ID=http://app.example.com \\ -e SIMPLESAMLPHP_SP_ASSERTION_CONSUMER_SERVICE=http://localhost/simplesaml/module.php/saml/sp/saml2-acs.php/test-sp \\ -e SIMPLESAMLPHP_SP_SINGLE_LOGOUT_SERVICE=http://localhost/simplesaml/module.php/saml/sp/saml2-logout.php/test-sp \\ -d alexkgold/auth http://localhost:8080/simplesaml admin/secret 7.3.6 OIDC/OAuth2.0 OIDC/OAuth is slightly newer than SAML, created in 2007 by engineers at Google and Twitter. OAuth 2.0 – the current standard was released in 2012. If you’re being pedantic, OAuth is a authorization protocol, and OpenID Connect (OIDC) is an authorization protocol that uses OAuth. In most cases, people will just call it OAuth. #TODO: this picture is bad In an enterprise context, OAuth/OIDC is conceptually very similar to SAML – but instead of passing around XML tokens, it’s based on JSON Web Tokens (JWT, usually pronounced “jot”). The fact that JSON is much more human-readable than XML is one of the big advantages of OIDC/OAuth compared to SAML. #TODO: try it out 7.3.6.1 OAuth/OIDC vs SAML From a practical perspective, the biggest difference between OAuth/OIDC and SAML is that SAML is quite strict about what SPs are allowed. Each SP needs to be registered at a specific web address that the IdP knows it’s allowed to receive requests from. In contrast, OAuth/OIDC was designed to be used to delegate authentication and authorization to different kinds of services that might be widely available on the internet. If you’ve ever allowed a website to Login with Apple/Google/Facebook/Github, that has been an application of OAuth/OIDC. Because the set of allowable SPs is fixed under SAML, it’s more common in enterprise settings. Some admins consider SAML more secure for that reason as well. In some situations, SAML is used for authentication and OAuth is used for access to other services. Most commonly in the data science world, this can come up when a user logs into a service like RStudio Server and is then authorized to a database using an OAuth JWT. Resources: https://www.okta.com/identity-101/saml-vs-oauth/ https://www.okta.com/identity-101/whats-the-difference-between-oauth-openid-connect-and-saml/ https://phoenixnap.com/blog/kerberos-authentication https://www.dnsstuff.com/rbac-vs-abac-access-control Depending on the system, this token or ticket can live in various places.↩︎ As we’ll see in a second, permissions can actually belong to entities other than people…but that’s a little complicated for the moment.↩︎ If you think about this for a minute, it may occur to you “isn’t RBAC basically the same as doing ACLs at the group level?” Yes, yes it is, but it can be helpful to have groups and roles be separated, even if there’s a 1-1 mapping.↩︎ XML is a markup language, much like HTML. The good thing about XML is that it’s very flexible. The bad thing is that it’s relatively hard for a human to easily read.↩︎ "],["choosing-and-using-a-data-architecture.html", "Chapter 8 Choosing and Using A Data Architecture 8.1 Options for Data Storage 8.2 How to pick 8.3 Working with File Systems 8.4 Working with Databases 8.5 Exercises", " Chapter 8 Choosing and Using A Data Architecture Learning Objectives Understand options for data architectures Understand interactions between data and auth DataOps is a huge field – a lot of it touches data science Some of the biggest consideration when you’re thinking about data science 8.1 Options for Data Storage 3 basic types of data stores: flat file (csv, pickle/rds), flat file w/ interactive read (arrow), database Implications how to configure access Database is standalone option, usually requires configuring separate server Other two can be configured two main ways: File system Bucket Storage Data Lake – usually for raw data, basically bucket storage writ large, sometimes include interacticve read capabilities Data Warehouse – usually a database 8.2 How to pick Flat files are simplest Flat files w/ interactive read are great for med-large data Databases often already exist If not, a few tips: SQL-based databases are the backbone of analytics work Database as a service is a basic cloud provider There are SO many options TODO: How much more in depth am I going here? 8.3 Working with File Systems Working on your laptop file system isn’t great Working in a cloud provider is different than on your laptop Volumes are separate from compute Often have automated snapshots/backups Volumes can also be shared across multiple servers Process of putting a volume on a server is called mounting 8.4 Working with Databases Two main ways to work with databases from R/Python - direct connection or w/ connector/driver (JDBC/ODBC) https://docs.google.com/presentation/d/1wJ3YnB4ob3AkNRDLpvYy3B-JMUaVsPjl3AA5QkN7fII/edit 8.4.1 Security Row-level security Kerberos JWT IAM roles 8.5 Exercises Connect to and use an S3 bucket from the command line and from R/Python. Stand up a container with Postgres in it, connect using isql, then from R/Python using ODBC/JDBC. Stand up an EC2 instance w/ EBS volume, change EBS to different instances. Stand up an NFS instance and mount onto a server. "],["environments-and-reproducibility.html", "Chapter 9 Environments and Reproducibility", " Chapter 9 Environments and Reproducibility Learning Objectives: Understand the layers that go into reproducing an output Understand which part is “the environment” Understand some general strategies/tooling for reproducing Managing environments is hard. Necessary in data science primarily for repro. At least 7 levels: Hardware Operating System System Libraries Programming Language Packages App Code App Data -&gt; Difference between environment and data is sketchy All of it contributes to “state” [graphic: Image of concentric environments] Different requirements depending on if you’re talking about development (IDE) environment or production environment. Repository vs library mgmt Tools to Reproduce Parts of Stack: IaC tooling Provisioning Configuration mgmt AMIs virtualenv/conda/renv Docker/Containers "],["it-and-data-science-workflows.html", "Chapter 10 IT and Data Science Workflows 10.1 Dev/Test/Prod 10.2 Cattle, Not Pets 10.3 How Does Stuff Actually Get Deployed 10.4 Exercises", " Chapter 10 IT and Data Science Workflows Learning Objectives: Understand what dev/test/prod means Understand how dev/test/prod might apply to data science Well developed patterns for putting things into prod in IT Data science starting to adopt, but not exactly the same 10.1 Dev/Test/Prod Classic workflow - three precise mirrors Do dev work in dev env Test for functionality and with users (User Acceptance Testing - UAT) Deploy to production environment Doesn’t quite work for data science Often, dev is a distinct (RStudio IDE/Jupyter) Because more exploratory than pure software development Test/Prod can be on one server or multiple, depending on org If you’re managing your own Data Science server, you’ll want an additional staging environment – unrelated to app changes, purely for IT changes [Graphic: deploying app changes and server changes] 10.2 Cattle, Not Pets Manual promotion from place to place = bad State maintenance Drift Cattle not Pets Infrastructure as code tooling Terraform AWS CFN, Azure Resource Manager, Google Cloud Deployment Manager Chef Puppet Saltstack Ansible Docker Vagrant Helm/Helmfile Important to ensure idempotence 10.3 How Does Stuff Actually Get Deployed [Graphic: CI/CD deployment into test/prod] CI/CD - Jenkins, Travis, GHA Powered by Git, mostly Intro to using GHA 10.4 Exercises Create something and add GHA integration [TODO: What to use as example?] Stand up some virtual environments using ___ [TODO: Which ones to try?] "],["scaling.html", "Chapter 11 Scaling 11.1 Types of Scaling 11.2 Load-Balancing configurations 11.3 Adding servers in real time", " Chapter 11 Scaling 11.1 Types of Scaling Vertical Scaling – just make the server bigger Horizontal – add more parallel servers Sometimes called load-balancing Reliability - or high-availability (HA) Make sure the service doesn’t go down (or less often) Eliminate single-points-of-failure Really hard to do all the way – spectrum of completeness + difficulty Health Checks/Heartbeats – periodic checkins to ensure server/service healthy 11.2 Load-Balancing configurations [Graphic: network diagram of lb-config] Active/Active - all servers accept traffic Single vs multiple masters Active/Passive (fallover/failover) - secondary server, remains inert until 2nd one fails Disaster Recovery - backup data to another disk – somewhat slower resumption of service 11.3 Adding servers in real time If you find yourself under heavy load, you’ll want to add more servers If you’re using infrastructure as code tooling, this is easy It’s also easy if using some sort of docker orchestration, as the underlying hosts are all the same from the perspective of docker/K8S "],["docker.html", "Chapter 12 Docker 12.1 Container Orchestration 12.2 Exercises", " Chapter 12 Docker Docker is so hype right now. Get ready for layers of abstractions till your head spins. Docker containers are a way to isolate an app and its dependencies from the underlying host [Graphic: Docker on underlying host] A docker container is easily portable from one place to another for that reason The “secret sauce” of docker is that containers launch REALLY quickly. Lifecycle: Dockerfile -&gt; Image -&gt; Container [Graphic: Docker lifecycle] 12.1 Container Orchestration Kubernetes (K8S) – software for deploying and managing containers Helm is the standard tool for defining kubernetes deployments. Helmfile is a templating system for helm. There are other competitors, most notably docker swarm, but K8S is by far the biggest The line is fuzzy though – there are container orchestration services that aren’t K8S or even abstract a level up from K8S. [TODO: Graphic of K8S/Docker] 12.2 Exercises Put a shiny app in a container, run it on your desktop. Put that container into a container registry. Use a local K8S distribution to run several instances of that container. "],["offline.html", "Chapter 13 Offline", " Chapter 13 Offline Some organizations require that servers not be connected to the internet (airgapped/offline) This makes things hard. Understanding whether it’s inbound connections that are disallowed or both outbound + inbound is important. Difficulties that tend to arise: Downloading software updates Getting R/Python packages Software licensing (often reach to license servers) "],["things-to-add-additional-topics.html", "Chapter 14 Things To Add (Additional Topics)", " Chapter 14 Things To Add (Additional Topics) Security groups "],["determining-what-you-need.html", "Chapter 15 Determining What You Need", " Chapter 15 Determining What You Need Determining Requirements for your platform Platform requirements combination of Work being done on the platform Data Science needs (i.e. horsepower) Existing state of org’s data Type of work data scientists might do Batch Jobs ETL Reports Data Generation Real-Time Interactive Models APIs Apps Data Science + IT Needs Languages (R, Python, SQL) Sharing – internal, external Online or offline? Data Security – row based access? Centralized workbench vs individual workstations Existing Assets BI/Analytics Platforms Data Connections Checklist of above "],["getting-started-with-terminal.html", "Chapter 16 Getting Started with Terminal", " Chapter 16 Getting Started with Terminal Terminal in Mac Iterm2 bash, zsh, fish windows WSL Powershell PuTTY – no longer needed, but still an option Do need to enable SSH client tmux vim/nano "],["useful-shell-commands.html", "Chapter 17 Useful Shell Commands 17.1 Miscellaneous Symbols 17.2 Moving yourself and your files 17.3 Checking out Files 17.4 Checking out Server Activity 17.5 Checking out Networking 17.6 User Management", " Chapter 17 Useful Shell Commands Unlike on your Windows or Mac desktop, most work on a server is done via the shell – an interactive text-based prompt. If you master the set of commands below, you’ll always feel like a real hacker. In most cases, you’ll be using the bash shell. If you’re using Linux or Mac, that’ll be the default. Below, I’m intentionally mixing up bash commands and Linux system commands because they’re useful. If you know the difference and are pedantic enough to care, this list isn’t for you anyway. 17.1 Miscellaneous Symbols Symbol What it is Helpful options Example / system root ~ your home directory echo ~ / home/alex.gold . current working directory man manual | the pipe echo $ sudo su 17.2 Moving yourself and your files C ommand What it does Helpful options Example pwd print working directory $ pwd /U sers/alex.gold/ cd change directory $ cd ~/Documents ls list -l - format as list -a - all include hidden files $ ls . $ ls -la rm remove delete permanently! -r - recursively a directory and included files -f - force - don’t ask for each file $ rm old_doc r m -rf old_docs/ BE VERY CAREFUL WITH -rf cp copy mv move chmod 17.3 Checking out Files Often useful in server contexts for reading log files. C ommand What it does Helpful options Example cat less tail -f grep tar 17.4 Checking out Server Activity C ommand What it does Helpful options Example df -h top ps lsof 17.5 Checking out Networking C ommand What it does Helpful options Example ping ne tstat curl 17.6 User Management C ommand What it does Helpful options Example w hoami p asswd us eradd "],["more-to-say.html", "A More to Say A.1 An aside on open source", " A More to Say Yeah! I have finished my book, but I have more to say about some topics. Let me explain them in this appendix. To know more about bookdown, see https://bookdown.org. A.1 An aside on open source I personally find open source software fascinating. As a data scientist working in R or Python, you might care about the water you’re swimming in. If you don’t, that’s fine too and you don’t need to worry about this. Feel free to skip ahead. Unlike Mac OS or Windows, Linux is open source. You’ve probably already heard of open source licensing, as the R and Python languages themselves, as well as the packages and libraries you use with them, are open source licensed. What it means to be open sourced is that if you want to read the code for Windows or Mac OS, you have to go get a job at Microsoft or Apple and join a team working on those code bases. In contrast, a quick google search will allow you to read the source code for Linux, R, Python, pandas, or the tidyverse. Reading source code is one thing, but the fascinating (and controversial) part of open source licensing is when it comes to reuse. There is a menagerie of different types of open source licenses. The differences between open source licenses mainly concern how permissible they are in terms of reuse of the software. In contrast to the permissibility of standard copyright, which is about whether and how you’re allowed to create derivative works, the variations in open source licenses are about how you’re allowed to license permissive works. The most permissive open source licenses basically amount to “do whatever you want with my software”, while more restrictive ones, often called copyleft, require that any derivative works also be open source. There have even been discussions recently of creating open source licenses that restrict the usage of the software along certain ethical dimensions. In a “traditional” software company, software is written by the company, and the software is then sold to make money. Increasingly, companies are writing open source software and monetizing this business in a number of different ways. Many versions of Linux are free, while others have an associated cost. Even more confusingly, there are several distributions that share a common codebase, where some are free and some paid. For example, Red Hat, the company behind the paid RHEL are also the primary maintainers of CentOS, and CentOS is the basis for Amazon Linux, which you can get on an Amazon Web Services Server (more on that in a moment). "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
